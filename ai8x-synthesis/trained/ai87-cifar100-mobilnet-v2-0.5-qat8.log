2022-01-29 10:14:51,801 - Log file for this run: /home/ermanokman/repos/github/ai8x-training/logs/2022.01.29-101451/2022.01.29-101451.log
2022-01-29 10:14:51,801 - Number of CPUs: 24
2022-01-29 10:14:51,826 - Number of GPUs: 1
2022-01-29 10:14:51,826 - CUDA version: 11.1
2022-01-29 10:14:51,826 - CUDNN version: 8005
2022-01-29 10:14:51,826 - Kernel: 5.4.0-90-generic
2022-01-29 10:14:51,826 - Python: 3.8.11 (default, Aug  3 2021, 15:09:35) 
[GCC 7.5.0]
2022-01-29 10:14:51,826 - pip freeze: {'absl-py': '0.13.0', 'aiohttp': '3.7.4', 'appdirs': '1.4.4', 'argon2-cffi': '20.1.0', 'astroid': '2.5', 'async-generator': '1.10', 'async-timeout': '3.0.1', 'atomicwrites': '1.4.0', 'attrs': '21.2.0', 'audioread': '2.1.9', 'backcall': '0.2.0', 'bleach': '3.3.0', 'blinker': '1.4', 'bottleneck': '1.3.2', 'bqplot': '0.11.5', 'brotlipy': '0.7.0', 'cachetools': '4.2.2', 'certifi': '2021.5.30', 'cffi': '1.14.6', 'chardet': '4.0.0', 'click': '8.0.1', 'cloudpickle': '1.6.0', 'colorama': '0.4.4', 'coverage': '5.5', 'cryptography': '3.4.7', 'cycler': '0.10.0', 'cython': '0.29.24', 'cytoolz': '0.11.0', 'dask': '2021.7.2', 'decorator': '5.0.9', 'defusedxml': '0.7.1', 'deprecated': '1.2.12', 'distiller': '0.4.0rc0', 'entrypoints': '0.3', 'fsspec': '2021.7.0', 'gitdb': '4.0.7', 'gitpython': '3.1.0', 'google-auth': '1.33.0', 'google-auth-oauthlib': '0.4.4', 'graphviz': '0.10.1', 'grpcio': '1.36.1', 'gym': '0.12.5', 'idna': '2.10', 'imageio': '2.9.0', 'importlib-metadata': '3.10.0', 'ipykernel': '5.5.3', 'ipython': '7.22.0', 'ipython-genutils': '0.2.0', 'ipywidgets': '7.4.2', 'isort': '5.9.2', 'jedi': '0.17.0', 'jinja2': '2.11.3', 'joblib': '1.0.1', 'jsonpatch': '1.32', 'jsonpointer': '2.1', 'jsonschema': '3.2.0', 'jupyter': '1.0.0', 'jupyter-client': '6.1.12', 'jupyter-console': '6.4.0', 'jupyter-core': '4.7.1', 'jupyterlab-pygments': '0.1.2', 'kiwisolver': '1.3.1', 'lazy-object-proxy': '1.6.0', 'librosa': '0.8.1', 'llvmlite': '0.36.0', 'locket': '0.2.1', 'markdown': '3.3.4', 'markupsafe': '1.1.1', 'matplotlib': '3.3.4', 'mccabe': '0.6.1', 'mistune': '0.8.4', 'mkl-fft': '1.3.0', 'mkl-random': '1.1.1', 'mkl-service': '2.3.0', 'more-itertools': '8.7.0', 'multidict': '5.1.0', 'munch': '2.5.0', 'nbclient': '0.5.3', 'nbconvert': '6.0.7', 'nbformat': '5.1.3', 'nest-asyncio': '1.5.1', 'networkx': '2.6.2', 'notebook': '6.3.0', 'numba': '0.53.1', 'numexpr': '2.7.3', 'numpy': '1.20.2', 'oauthlib': '3.1.1', 'olefile': '0.46', 'onnx': '1.10.1', 'packaging': '21.0', 'pandas': '1.3.1', 'pandocfilters': '1.4.3', 'parso': '0.8.2', 'partd': '1.2.0', 'pathspec': '0.7.0', 'pexpect': '4.8.0', 'pickleshare': '0.7.5', 'pillow': '8.3.1', 'pip': '21.2.2', 'pluggy': '0.13.1', 'pooch': '1.4.0', 'pretrainedmodels': '0.7.4', 'prometheus-client': '0.10.0', 'prompt-toolkit': '3.0.17', 'protobuf': '3.16.0', 'ptyprocess': '0.7.0', 'py': '1.10.0', 'pyasn1': '0.4.8', 'pyasn1-modules': '0.2.8', 'pycparser': '2.20', 'pydot': '1.4.1', 'pygithub': '1.55', 'pyglet': '1.5.15', 'pygments': '2.9.0', 'pyjwt': '2.1.0', 'pylint': '2.6.0', 'pynacl': '1.4.0', 'pyopenssl': '20.0.1', 'pyparsing': '2.4.7', 'pyqt5': '5.12.3', 'pyqt5-sip': '4.19.18', 'pyqtchart': '5.12', 'pyqtwebengine': '5.12.1', 'pyrsistent': '0.17.3', 'pysocks': '1.7.1', 'pytest': '4.6.11', 'python-dateutil': '2.8.2', 'pytsmod': '0.3.3', 'pytz': '2021.1', 'pywavelets': '1.1.1', 'pyyaml': '5.4.1', 'pyzmq': '22.0.3', 'qgrid': '1.1.1', 'qtconsole': '5.0.3', 'qtpy': '1.9.0', 'requests': '2.25.1', 'requests-oauthlib': '1.3.0', 'resampy': '0.2.2', 'rsa': '4.7.2', 'scikit-image': '0.18.1', 'scikit-learn': '0.23.2', 'scipy': '1.6.2', 'send2trash': '1.5.0', 'setuptools': '52.0.0.post20210125', 'shap': '0.37.0', 'six': '1.16.0', 'slicer': '0.0.7', 'smmap': '4.0.0', 'soundfile': '0.10.3.post1', 'tabulate': '0.8.3', 'tensorboard': '2.4.0', 'tensorboard-plugin-wit': '1.6.0', 'terminado': '0.9.4', 'testpath': '0.4.4', 'threadpoolctl': '2.2.0', 'tifffile': '2020.10.1', 'toml': '0.10.2', 'toolz': '0.11.1', 'torch': '1.8.1+cu111', 'torchaudio': '0.8.0a0+e4e171a', 'torchfile': '0.1.0', 'torchnet': '0.0.4', 'torchvision': '0.9.1+cu111', 'tornado': '6.1', 'tqdm': '4.33.0', 'traitlets': '5.0.5', 'traittypes': '0.2.1', 'typing-extensions': '3.10.0.0', 'urllib3': '1.26.6', 'visdom': '0.1.8.9', 'wcwidth': '0.2.5', 'webencodings': '0.5.1', 'websocket-client': '0.58.0', 'werkzeug': '1.0.1', 'wheel': '0.36.2', 'widgetsnbextension': '3.4.2', 'wrapt': '1.12.1', 'xlsxwriter': '1.3.8', 'yamllint': '1.26.1', 'yarl': '1.6.3', 'zipp': '3.5.0'}
2022-01-29 10:14:51,826 - Command line: train.py --deterministic --epochs 300 --optimizer SGD --lr 0.1 --compress schedule-cifar100-mobilenetv2.yaml --model ai87netmobilenetv2cifar100_m0_5 --dataset CIFAR100 --device MAX78002 --batch-size 128 --print-freq 100 --validation-split 0 --use-bias --qat-policy qat_policy_cifar100_mobilenetv2.yaml
2022-01-29 10:14:51,827 - Distiller: 0.4.0rc0
2022-01-29 10:14:51,827 - set_deterministic was invoked
2022-01-29 10:14:53,630 - Optimizer Type: <class 'torch.optim.sgd.SGD'>
2022-01-29 10:14:53,630 - Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False}
2022-01-29 10:14:54,678 - set_deterministic was invoked
2022-01-29 10:14:54,681 - Dataset sizes:
	training=50000
	validation=10000
	test=10000
2022-01-29 10:14:54,681 - Reading compression schedule from: schedule-cifar100-mobilenetv2.yaml
2022-01-29 10:14:54,683 - Schedule contents:
{
  "lr_schedulers": {
    "training_lr": {
      "class": "MultiStepLR",
      "milestones": [
        100,
        150,
        175,
        250
      ],
      "gamma": 0.235
    }
  },
  "policies": [
    {
      "lr_scheduler": {
        "instance_name": "training_lr"
      },
      "starting_epoch": 0,
      "ending_epoch": 250,
      "frequency": 1
    }
  ]
}
2022-01-29 10:14:54,688 - 

2022-01-29 10:14:54,688 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:15:00,306 - Epoch: [0][  100/  391]    Overall Loss 4.571751    Objective Loss 4.571751                                        LR 0.100000    Time 0.056159    
2022-01-29 10:15:05,486 - Epoch: [0][  200/  391]    Overall Loss 4.451554    Objective Loss 4.451554                                        LR 0.100000    Time 0.053974    
2022-01-29 10:15:10,665 - Epoch: [0][  300/  391]    Overall Loss 4.332030    Objective Loss 4.332030                                        LR 0.100000    Time 0.053243    
2022-01-29 10:15:15,373 - Epoch: [0][  391/  391]    Overall Loss 4.255226    Objective Loss 4.255226    Top1 8.173077    Top5 26.442308    LR 0.100000    Time 0.052892    
2022-01-29 10:15:15,430 - --- validate (epoch=0)-----------
2022-01-29 10:15:15,431 - 10000 samples (128 per mini-batch)
2022-01-29 10:15:17,112 - Epoch: [0][   79/   79]    Loss 4.008284    Top1 6.170000    Top5 24.630000    
2022-01-29 10:15:17,165 - ==> Top1: 6.170    Top5: 24.630    Loss: 4.008

2022-01-29 10:15:17,171 - ==> Best [Top1: 6.170   Top5: 24.630   Sparsity:0.00   Params: 627712 on epoch: 0]
2022-01-29 10:15:17,171 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:15:17,209 - 

2022-01-29 10:15:17,209 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:15:22,636 - Epoch: [1][  100/  391]    Overall Loss 3.914118    Objective Loss 3.914118                                        LR 0.100000    Time 0.054237    
2022-01-29 10:15:27,846 - Epoch: [1][  200/  391]    Overall Loss 3.884658    Objective Loss 3.884658                                        LR 0.100000    Time 0.053164    
2022-01-29 10:15:33,042 - Epoch: [1][  300/  391]    Overall Loss 3.844007    Objective Loss 3.844007                                        LR 0.100000    Time 0.052760    
2022-01-29 10:15:37,766 - Epoch: [1][  391/  391]    Overall Loss 3.818391    Objective Loss 3.818391    Top1 11.057692    Top5 30.769231    LR 0.100000    Time 0.052562    
2022-01-29 10:15:37,824 - --- validate (epoch=1)-----------
2022-01-29 10:15:37,824 - 10000 samples (128 per mini-batch)
2022-01-29 10:15:39,500 - Epoch: [1][   79/   79]    Loss 3.679988    Top1 10.750000    Top5 35.750000    
2022-01-29 10:15:39,553 - ==> Top1: 10.750    Top5: 35.750    Loss: 3.680

2022-01-29 10:15:39,558 - ==> Best [Top1: 10.750   Top5: 35.750   Sparsity:0.00   Params: 627712 on epoch: 1]
2022-01-29 10:15:39,558 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:15:39,607 - 

2022-01-29 10:15:39,607 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:15:45,087 - Epoch: [2][  100/  391]    Overall Loss 3.636802    Objective Loss 3.636802                                        LR 0.100000    Time 0.054770    
2022-01-29 10:15:50,273 - Epoch: [2][  200/  391]    Overall Loss 3.608686    Objective Loss 3.608686                                        LR 0.100000    Time 0.053309    
2022-01-29 10:15:55,462 - Epoch: [2][  300/  391]    Overall Loss 3.593660    Objective Loss 3.593660                                        LR 0.100000    Time 0.052835    
2022-01-29 10:16:00,178 - Epoch: [2][  391/  391]    Overall Loss 3.571370    Objective Loss 3.571370    Top1 12.980769    Top5 41.826923    LR 0.100000    Time 0.052598    
2022-01-29 10:16:00,238 - --- validate (epoch=2)-----------
2022-01-29 10:16:00,238 - 10000 samples (128 per mini-batch)
2022-01-29 10:16:01,971 - Epoch: [2][   79/   79]    Loss 3.705363    Top1 11.210000    Top5 36.610000    
2022-01-29 10:16:02,027 - ==> Top1: 11.210    Top5: 36.610    Loss: 3.705

2022-01-29 10:16:02,032 - ==> Best [Top1: 11.210   Top5: 36.610   Sparsity:0.00   Params: 627712 on epoch: 2]
2022-01-29 10:16:02,032 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:16:02,076 - 

2022-01-29 10:16:02,076 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:16:07,608 - Epoch: [3][  100/  391]    Overall Loss 3.424059    Objective Loss 3.424059                                        LR 0.100000    Time 0.055294    
2022-01-29 10:16:12,845 - Epoch: [3][  200/  391]    Overall Loss 3.401189    Objective Loss 3.401189                                        LR 0.100000    Time 0.053828    
2022-01-29 10:16:18,098 - Epoch: [3][  300/  391]    Overall Loss 3.387947    Objective Loss 3.387947                                        LR 0.100000    Time 0.053390    
2022-01-29 10:16:22,889 - Epoch: [3][  391/  391]    Overall Loss 3.365549    Objective Loss 3.365549    Top1 16.346154    Top5 47.596154    LR 0.100000    Time 0.053217    
2022-01-29 10:16:22,952 - --- validate (epoch=3)-----------
2022-01-29 10:16:22,952 - 10000 samples (128 per mini-batch)
2022-01-29 10:16:24,634 - Epoch: [3][   79/   79]    Loss 3.306038    Top1 17.530000    Top5 46.620000    
2022-01-29 10:16:24,693 - ==> Top1: 17.530    Top5: 46.620    Loss: 3.306

2022-01-29 10:16:24,698 - ==> Best [Top1: 17.530   Top5: 46.620   Sparsity:0.00   Params: 627712 on epoch: 3]
2022-01-29 10:16:24,698 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:16:24,747 - 

2022-01-29 10:16:24,747 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:16:30,188 - Epoch: [4][  100/  391]    Overall Loss 3.265655    Objective Loss 3.265655                                        LR 0.100000    Time 0.054377    
2022-01-29 10:16:35,373 - Epoch: [4][  200/  391]    Overall Loss 3.244855    Objective Loss 3.244855                                        LR 0.100000    Time 0.053113    
2022-01-29 10:16:40,544 - Epoch: [4][  300/  391]    Overall Loss 3.229055    Objective Loss 3.229055                                        LR 0.100000    Time 0.052640    
2022-01-29 10:16:45,257 - Epoch: [4][  391/  391]    Overall Loss 3.203967    Objective Loss 3.203967    Top1 17.307692    Top5 55.288462    LR 0.100000    Time 0.052442    
2022-01-29 10:16:45,316 - --- validate (epoch=4)-----------
2022-01-29 10:16:45,316 - 10000 samples (128 per mini-batch)
2022-01-29 10:16:46,962 - Epoch: [4][   79/   79]    Loss 3.249346    Top1 18.630000    Top5 48.640000    
2022-01-29 10:16:47,025 - ==> Top1: 18.630    Top5: 48.640    Loss: 3.249

2022-01-29 10:16:47,031 - ==> Best [Top1: 18.630   Top5: 48.640   Sparsity:0.00   Params: 627712 on epoch: 4]
2022-01-29 10:16:47,031 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:16:47,074 - 

2022-01-29 10:16:47,074 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:16:52,574 - Epoch: [5][  100/  391]    Overall Loss 3.089587    Objective Loss 3.089587                                        LR 0.100000    Time 0.054973    
2022-01-29 10:16:57,800 - Epoch: [5][  200/  391]    Overall Loss 3.098415    Objective Loss 3.098415                                        LR 0.100000    Time 0.053613    
2022-01-29 10:17:03,037 - Epoch: [5][  300/  391]    Overall Loss 3.087024    Objective Loss 3.087024                                        LR 0.100000    Time 0.053195    
2022-01-29 10:17:07,803 - Epoch: [5][  391/  391]    Overall Loss 3.076564    Objective Loss 3.076564    Top1 20.673077    Top5 56.250000    LR 0.100000    Time 0.053003    
2022-01-29 10:17:07,864 - --- validate (epoch=5)-----------
2022-01-29 10:17:07,864 - 10000 samples (128 per mini-batch)
2022-01-29 10:17:09,525 - Epoch: [5][   79/   79]    Loss 3.027013    Top1 22.240000    Top5 53.740000    
2022-01-29 10:17:09,578 - ==> Top1: 22.240    Top5: 53.740    Loss: 3.027

2022-01-29 10:17:09,583 - ==> Best [Top1: 22.240   Top5: 53.740   Sparsity:0.00   Params: 627712 on epoch: 5]
2022-01-29 10:17:09,583 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:17:09,633 - 

2022-01-29 10:17:09,633 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:17:15,166 - Epoch: [6][  100/  391]    Overall Loss 2.977693    Objective Loss 2.977693                                        LR 0.100000    Time 0.055303    
2022-01-29 10:17:20,290 - Epoch: [6][  200/  391]    Overall Loss 2.963414    Objective Loss 2.963414                                        LR 0.100000    Time 0.053269    
2022-01-29 10:17:25,325 - Epoch: [6][  300/  391]    Overall Loss 2.947221    Objective Loss 2.947221                                        LR 0.100000    Time 0.052291    
2022-01-29 10:17:29,899 - Epoch: [6][  391/  391]    Overall Loss 2.939029    Objective Loss 2.939029    Top1 20.192308    Top5 56.250000    LR 0.100000    Time 0.051816    
2022-01-29 10:17:29,963 - --- validate (epoch=6)-----------
2022-01-29 10:17:29,963 - 10000 samples (128 per mini-batch)
2022-01-29 10:17:31,630 - Epoch: [6][   79/   79]    Loss 2.948091    Top1 23.780000    Top5 56.180000    
2022-01-29 10:17:31,689 - ==> Top1: 23.780    Top5: 56.180    Loss: 2.948

2022-01-29 10:17:31,694 - ==> Best [Top1: 23.780   Top5: 56.180   Sparsity:0.00   Params: 627712 on epoch: 6]
2022-01-29 10:17:31,694 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:17:31,742 - 

2022-01-29 10:17:31,742 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:17:37,053 - Epoch: [7][  100/  391]    Overall Loss 2.850008    Objective Loss 2.850008                                        LR 0.100000    Time 0.053077    
2022-01-29 10:17:42,142 - Epoch: [7][  200/  391]    Overall Loss 2.847650    Objective Loss 2.847650                                        LR 0.100000    Time 0.051978    
2022-01-29 10:17:47,283 - Epoch: [7][  300/  391]    Overall Loss 2.837973    Objective Loss 2.837973                                        LR 0.100000    Time 0.051787    
2022-01-29 10:17:51,996 - Epoch: [7][  391/  391]    Overall Loss 2.822959    Objective Loss 2.822959    Top1 20.673077    Top5 62.019231    LR 0.100000    Time 0.051786    
2022-01-29 10:17:52,053 - --- validate (epoch=7)-----------
2022-01-29 10:17:52,054 - 10000 samples (128 per mini-batch)
2022-01-29 10:17:53,795 - Epoch: [7][   79/   79]    Loss 2.868701    Top1 24.760000    Top5 58.670000    
2022-01-29 10:17:53,856 - ==> Top1: 24.760    Top5: 58.670    Loss: 2.869

2022-01-29 10:17:53,861 - ==> Best [Top1: 24.760   Top5: 58.670   Sparsity:0.00   Params: 627712 on epoch: 7]
2022-01-29 10:17:53,861 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:17:53,911 - 

2022-01-29 10:17:53,911 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:17:59,361 - Epoch: [8][  100/  391]    Overall Loss 2.744813    Objective Loss 2.744813                                        LR 0.100000    Time 0.054475    
2022-01-29 10:18:04,592 - Epoch: [8][  200/  391]    Overall Loss 2.732077    Objective Loss 2.732077                                        LR 0.100000    Time 0.053390    
2022-01-29 10:18:09,817 - Epoch: [8][  300/  391]    Overall Loss 2.734193    Objective Loss 2.734193                                        LR 0.100000    Time 0.053006    
2022-01-29 10:18:14,551 - Epoch: [8][  391/  391]    Overall Loss 2.723776    Objective Loss 2.723776    Top1 28.365385    Top5 61.057692    LR 0.100000    Time 0.052774    
2022-01-29 10:18:14,602 - --- validate (epoch=8)-----------
2022-01-29 10:18:14,602 - 10000 samples (128 per mini-batch)
2022-01-29 10:18:16,295 - Epoch: [8][   79/   79]    Loss 2.943922    Top1 25.500000    Top5 57.200000    
2022-01-29 10:18:16,353 - ==> Top1: 25.500    Top5: 57.200    Loss: 2.944

2022-01-29 10:18:16,358 - ==> Best [Top1: 25.500   Top5: 57.200   Sparsity:0.00   Params: 627712 on epoch: 8]
2022-01-29 10:18:16,359 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:18:16,401 - 

2022-01-29 10:18:16,401 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:18:21,900 - Epoch: [9][  100/  391]    Overall Loss 2.646291    Objective Loss 2.646291                                        LR 0.100000    Time 0.054955    
2022-01-29 10:18:27,119 - Epoch: [9][  200/  391]    Overall Loss 2.650404    Objective Loss 2.650404                                        LR 0.100000    Time 0.053570    
2022-01-29 10:18:32,350 - Epoch: [9][  300/  391]    Overall Loss 2.646785    Objective Loss 2.646785                                        LR 0.100000    Time 0.053148    
2022-01-29 10:18:37,083 - Epoch: [9][  391/  391]    Overall Loss 2.641573    Objective Loss 2.641573    Top1 30.288462    Top5 59.134615    LR 0.100000    Time 0.052881    
2022-01-29 10:18:37,140 - --- validate (epoch=9)-----------
2022-01-29 10:18:37,140 - 10000 samples (128 per mini-batch)
2022-01-29 10:18:38,837 - Epoch: [9][   79/   79]    Loss 2.763304    Top1 29.130000    Top5 61.240000    
2022-01-29 10:18:38,900 - ==> Top1: 29.130    Top5: 61.240    Loss: 2.763

2022-01-29 10:18:38,906 - ==> Best [Top1: 29.130   Top5: 61.240   Sparsity:0.00   Params: 627712 on epoch: 9]
2022-01-29 10:18:38,906 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:18:38,949 - 

2022-01-29 10:18:38,949 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:18:44,434 - Epoch: [10][  100/  391]    Overall Loss 2.596624    Objective Loss 2.596624                                        LR 0.100000    Time 0.054818    
2022-01-29 10:18:49,670 - Epoch: [10][  200/  391]    Overall Loss 2.570120    Objective Loss 2.570120                                        LR 0.100000    Time 0.053584    
2022-01-29 10:18:54,890 - Epoch: [10][  300/  391]    Overall Loss 2.565254    Objective Loss 2.565254                                        LR 0.100000    Time 0.053122    
2022-01-29 10:18:59,635 - Epoch: [10][  391/  391]    Overall Loss 2.560359    Objective Loss 2.560359    Top1 31.730769    Top5 64.423077    LR 0.100000    Time 0.052892    
2022-01-29 10:18:59,695 - --- validate (epoch=10)-----------
2022-01-29 10:18:59,695 - 10000 samples (128 per mini-batch)
2022-01-29 10:19:01,381 - Epoch: [10][   79/   79]    Loss 2.917214    Top1 25.930000    Top5 57.970000    
2022-01-29 10:19:01,436 - ==> Top1: 25.930    Top5: 57.970    Loss: 2.917

2022-01-29 10:19:01,441 - ==> Best [Top1: 29.130   Top5: 61.240   Sparsity:0.00   Params: 627712 on epoch: 9]
2022-01-29 10:19:01,441 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:19:01,486 - 

2022-01-29 10:19:01,486 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:19:06,906 - Epoch: [11][  100/  391]    Overall Loss 2.506429    Objective Loss 2.506429                                        LR 0.100000    Time 0.054166    
2022-01-29 10:19:12,115 - Epoch: [11][  200/  391]    Overall Loss 2.495799    Objective Loss 2.495799                                        LR 0.100000    Time 0.053127    
2022-01-29 10:19:17,300 - Epoch: [11][  300/  391]    Overall Loss 2.487199    Objective Loss 2.487199                                        LR 0.100000    Time 0.052698    
2022-01-29 10:19:22,042 - Epoch: [11][  391/  391]    Overall Loss 2.487295    Objective Loss 2.487295    Top1 37.019231    Top5 68.750000    LR 0.100000    Time 0.052558    
2022-01-29 10:19:22,100 - --- validate (epoch=11)-----------
2022-01-29 10:19:22,100 - 10000 samples (128 per mini-batch)
2022-01-29 10:19:23,762 - Epoch: [11][   79/   79]    Loss 2.523471    Top1 33.370000    Top5 66.790000    
2022-01-29 10:19:23,823 - ==> Top1: 33.370    Top5: 66.790    Loss: 2.523

2022-01-29 10:19:23,828 - ==> Best [Top1: 33.370   Top5: 66.790   Sparsity:0.00   Params: 627712 on epoch: 11]
2022-01-29 10:19:23,828 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:19:23,876 - 

2022-01-29 10:19:23,876 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:19:29,249 - Epoch: [12][  100/  391]    Overall Loss 2.438369    Objective Loss 2.438369                                        LR 0.100000    Time 0.053703    
2022-01-29 10:19:34,507 - Epoch: [12][  200/  391]    Overall Loss 2.423155    Objective Loss 2.423155                                        LR 0.100000    Time 0.053135    
2022-01-29 10:19:39,814 - Epoch: [12][  300/  391]    Overall Loss 2.424530    Objective Loss 2.424530                                        LR 0.100000    Time 0.053109    
2022-01-29 10:19:44,602 - Epoch: [12][  391/  391]    Overall Loss 2.420100    Objective Loss 2.420100    Top1 33.653846    Top5 68.269231    LR 0.100000    Time 0.052995    
2022-01-29 10:19:44,659 - --- validate (epoch=12)-----------
2022-01-29 10:19:44,659 - 10000 samples (128 per mini-batch)
2022-01-29 10:19:46,381 - Epoch: [12][   79/   79]    Loss 2.537284    Top1 33.300000    Top5 65.850000    
2022-01-29 10:19:46,439 - ==> Top1: 33.300    Top5: 65.850    Loss: 2.537

2022-01-29 10:19:46,445 - ==> Best [Top1: 33.370   Top5: 66.790   Sparsity:0.00   Params: 627712 on epoch: 11]
2022-01-29 10:19:46,445 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:19:46,489 - 

2022-01-29 10:19:46,489 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:19:51,994 - Epoch: [13][  100/  391]    Overall Loss 2.345885    Objective Loss 2.345885                                        LR 0.100000    Time 0.055022    
2022-01-29 10:19:57,209 - Epoch: [13][  200/  391]    Overall Loss 2.351730    Objective Loss 2.351730                                        LR 0.100000    Time 0.053584    
2022-01-29 10:20:02,437 - Epoch: [13][  300/  391]    Overall Loss 2.352617    Objective Loss 2.352617                                        LR 0.100000    Time 0.053146    
2022-01-29 10:20:07,181 - Epoch: [13][  391/  391]    Overall Loss 2.352295    Objective Loss 2.352295    Top1 36.057692    Top5 70.192308    LR 0.100000    Time 0.052908    
2022-01-29 10:20:07,241 - --- validate (epoch=13)-----------
2022-01-29 10:20:07,241 - 10000 samples (128 per mini-batch)
2022-01-29 10:20:09,103 - Epoch: [13][   79/   79]    Loss 2.372984    Top1 35.690000    Top5 69.750000    
2022-01-29 10:20:09,155 - ==> Top1: 35.690    Top5: 69.750    Loss: 2.373

2022-01-29 10:20:09,161 - ==> Best [Top1: 35.690   Top5: 69.750   Sparsity:0.00   Params: 627712 on epoch: 13]
2022-01-29 10:20:09,161 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:20:09,206 - 

2022-01-29 10:20:09,206 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:20:14,657 - Epoch: [14][  100/  391]    Overall Loss 2.277311    Objective Loss 2.277311                                        LR 0.100000    Time 0.054485    
2022-01-29 10:20:19,937 - Epoch: [14][  200/  391]    Overall Loss 2.301145    Objective Loss 2.301145                                        LR 0.100000    Time 0.053636    
2022-01-29 10:20:25,152 - Epoch: [14][  300/  391]    Overall Loss 2.303067    Objective Loss 2.303067                                        LR 0.100000    Time 0.053139    
2022-01-29 10:20:29,828 - Epoch: [14][  391/  391]    Overall Loss 2.300007    Objective Loss 2.300007    Top1 36.057692    Top5 73.076923    LR 0.100000    Time 0.052728    
2022-01-29 10:20:29,886 - --- validate (epoch=14)-----------
2022-01-29 10:20:29,886 - 10000 samples (128 per mini-batch)
2022-01-29 10:20:31,620 - Epoch: [14][   79/   79]    Loss 2.519233    Top1 34.010000    Top5 66.720000    
2022-01-29 10:20:31,672 - ==> Top1: 34.010    Top5: 66.720    Loss: 2.519

2022-01-29 10:20:31,677 - ==> Best [Top1: 35.690   Top5: 69.750   Sparsity:0.00   Params: 627712 on epoch: 13]
2022-01-29 10:20:31,677 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:20:31,721 - 

2022-01-29 10:20:31,721 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:20:37,088 - Epoch: [15][  100/  391]    Overall Loss 2.260930    Objective Loss 2.260930                                        LR 0.100000    Time 0.053639    
2022-01-29 10:20:42,233 - Epoch: [15][  200/  391]    Overall Loss 2.263162    Objective Loss 2.263162                                        LR 0.100000    Time 0.052543    
2022-01-29 10:20:47,386 - Epoch: [15][  300/  391]    Overall Loss 2.257109    Objective Loss 2.257109                                        LR 0.100000    Time 0.052202    
2022-01-29 10:20:52,131 - Epoch: [15][  391/  391]    Overall Loss 2.257285    Objective Loss 2.257285    Top1 40.384615    Top5 74.519231    LR 0.100000    Time 0.052186    
2022-01-29 10:20:52,190 - --- validate (epoch=15)-----------
2022-01-29 10:20:52,191 - 10000 samples (128 per mini-batch)
2022-01-29 10:20:53,861 - Epoch: [15][   79/   79]    Loss 2.390403    Top1 36.920000    Top5 69.700000    
2022-01-29 10:20:53,921 - ==> Top1: 36.920    Top5: 69.700    Loss: 2.390

2022-01-29 10:20:53,926 - ==> Best [Top1: 36.920   Top5: 69.700   Sparsity:0.00   Params: 627712 on epoch: 15]
2022-01-29 10:20:53,926 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:20:53,976 - 

2022-01-29 10:20:53,976 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:20:59,417 - Epoch: [16][  100/  391]    Overall Loss 2.190665    Objective Loss 2.190665                                        LR 0.100000    Time 0.054383    
2022-01-29 10:21:04,501 - Epoch: [16][  200/  391]    Overall Loss 2.220205    Objective Loss 2.220205                                        LR 0.100000    Time 0.052607    
2022-01-29 10:21:09,587 - Epoch: [16][  300/  391]    Overall Loss 2.214940    Objective Loss 2.214940                                        LR 0.100000    Time 0.052021    
2022-01-29 10:21:14,211 - Epoch: [16][  391/  391]    Overall Loss 2.211407    Objective Loss 2.211407    Top1 42.788462    Top5 73.557692    LR 0.100000    Time 0.051738    
2022-01-29 10:21:14,271 - --- validate (epoch=16)-----------
2022-01-29 10:21:14,271 - 10000 samples (128 per mini-batch)
2022-01-29 10:21:16,094 - Epoch: [16][   79/   79]    Loss 2.461370    Top1 35.270000    Top5 68.320000    
2022-01-29 10:21:16,146 - ==> Top1: 35.270    Top5: 68.320    Loss: 2.461

2022-01-29 10:21:16,151 - ==> Best [Top1: 36.920   Top5: 69.700   Sparsity:0.00   Params: 627712 on epoch: 15]
2022-01-29 10:21:16,151 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:21:16,188 - 

2022-01-29 10:21:16,188 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:21:21,605 - Epoch: [17][  100/  391]    Overall Loss 2.162513    Objective Loss 2.162513                                        LR 0.100000    Time 0.054138    
2022-01-29 10:21:26,807 - Epoch: [17][  200/  391]    Overall Loss 2.156837    Objective Loss 2.156837                                        LR 0.100000    Time 0.053074    
2022-01-29 10:21:32,007 - Epoch: [17][  300/  391]    Overall Loss 2.162242    Objective Loss 2.162242                                        LR 0.100000    Time 0.052713    
2022-01-29 10:21:36,740 - Epoch: [17][  391/  391]    Overall Loss 2.164468    Objective Loss 2.164468    Top1 45.192308    Top5 74.038462    LR 0.100000    Time 0.052549    
2022-01-29 10:21:36,800 - --- validate (epoch=17)-----------
2022-01-29 10:21:36,800 - 10000 samples (128 per mini-batch)
2022-01-29 10:21:38,481 - Epoch: [17][   79/   79]    Loss 2.849899    Top1 30.250000    Top5 61.670000    
2022-01-29 10:21:38,537 - ==> Top1: 30.250    Top5: 61.670    Loss: 2.850

2022-01-29 10:21:38,543 - ==> Best [Top1: 36.920   Top5: 69.700   Sparsity:0.00   Params: 627712 on epoch: 15]
2022-01-29 10:21:38,543 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:21:38,586 - 

2022-01-29 10:21:38,587 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:21:44,009 - Epoch: [18][  100/  391]    Overall Loss 2.109573    Objective Loss 2.109573                                        LR 0.100000    Time 0.054196    
2022-01-29 10:21:49,162 - Epoch: [18][  200/  391]    Overall Loss 2.129003    Objective Loss 2.129003                                        LR 0.100000    Time 0.052861    
2022-01-29 10:21:54,306 - Epoch: [18][  300/  391]    Overall Loss 2.139710    Objective Loss 2.139710                                        LR 0.100000    Time 0.052383    
2022-01-29 10:21:59,026 - Epoch: [18][  391/  391]    Overall Loss 2.137128    Objective Loss 2.137128    Top1 36.057692    Top5 73.557692    LR 0.100000    Time 0.052262    
2022-01-29 10:21:59,085 - --- validate (epoch=18)-----------
2022-01-29 10:21:59,085 - 10000 samples (128 per mini-batch)
2022-01-29 10:22:00,773 - Epoch: [18][   79/   79]    Loss 2.265792    Top1 38.790000    Top5 70.970000    
2022-01-29 10:22:00,833 - ==> Top1: 38.790    Top5: 70.970    Loss: 2.266

2022-01-29 10:22:00,838 - ==> Best [Top1: 38.790   Top5: 70.970   Sparsity:0.00   Params: 627712 on epoch: 18]
2022-01-29 10:22:00,838 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:22:00,887 - 

2022-01-29 10:22:00,887 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:22:06,375 - Epoch: [19][  100/  391]    Overall Loss 2.108033    Objective Loss 2.108033                                        LR 0.100000    Time 0.054855    
2022-01-29 10:22:11,563 - Epoch: [19][  200/  391]    Overall Loss 2.099013    Objective Loss 2.099013                                        LR 0.100000    Time 0.053361    
2022-01-29 10:22:16,710 - Epoch: [19][  300/  391]    Overall Loss 2.102728    Objective Loss 2.102728                                        LR 0.100000    Time 0.052729    
2022-01-29 10:22:21,363 - Epoch: [19][  391/  391]    Overall Loss 2.104159    Objective Loss 2.104159    Top1 44.230769    Top5 75.961538    LR 0.100000    Time 0.052356    
2022-01-29 10:22:21,421 - --- validate (epoch=19)-----------
2022-01-29 10:22:21,422 - 10000 samples (128 per mini-batch)
2022-01-29 10:22:23,193 - Epoch: [19][   79/   79]    Loss 2.292224    Top1 38.470000    Top5 72.110000    
2022-01-29 10:22:23,247 - ==> Top1: 38.470    Top5: 72.110    Loss: 2.292

2022-01-29 10:22:23,252 - ==> Best [Top1: 38.790   Top5: 70.970   Sparsity:0.00   Params: 627712 on epoch: 18]
2022-01-29 10:22:23,252 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:22:23,296 - 

2022-01-29 10:22:23,296 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:22:28,800 - Epoch: [20][  100/  391]    Overall Loss 2.046616    Objective Loss 2.046616                                        LR 0.100000    Time 0.055009    
2022-01-29 10:22:34,002 - Epoch: [20][  200/  391]    Overall Loss 2.053841    Objective Loss 2.053841                                        LR 0.100000    Time 0.053512    
2022-01-29 10:22:39,207 - Epoch: [20][  300/  391]    Overall Loss 2.065639    Objective Loss 2.065639                                        LR 0.100000    Time 0.053023    
2022-01-29 10:22:43,898 - Epoch: [20][  391/  391]    Overall Loss 2.066505    Objective Loss 2.066505    Top1 47.115385    Top5 79.807692    LR 0.100000    Time 0.052677    
2022-01-29 10:22:43,958 - --- validate (epoch=20)-----------
2022-01-29 10:22:43,959 - 10000 samples (128 per mini-batch)
2022-01-29 10:22:45,631 - Epoch: [20][   79/   79]    Loss 2.206770    Top1 40.490000    Top5 73.720000    
2022-01-29 10:22:45,691 - ==> Top1: 40.490    Top5: 73.720    Loss: 2.207

2022-01-29 10:22:45,696 - ==> Best [Top1: 40.490   Top5: 73.720   Sparsity:0.00   Params: 627712 on epoch: 20]
2022-01-29 10:22:45,696 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:22:45,744 - 

2022-01-29 10:22:45,745 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:22:51,178 - Epoch: [21][  100/  391]    Overall Loss 2.058194    Objective Loss 2.058194                                        LR 0.100000    Time 0.054304    
2022-01-29 10:22:56,388 - Epoch: [21][  200/  391]    Overall Loss 2.041638    Objective Loss 2.041638                                        LR 0.100000    Time 0.053199    
2022-01-29 10:23:01,600 - Epoch: [21][  300/  391]    Overall Loss 2.033121    Objective Loss 2.033121                                        LR 0.100000    Time 0.052837    
2022-01-29 10:23:06,341 - Epoch: [21][  391/  391]    Overall Loss 2.036998    Objective Loss 2.036998    Top1 42.307692    Top5 77.403846    LR 0.100000    Time 0.052665    
2022-01-29 10:23:06,398 - --- validate (epoch=21)-----------
2022-01-29 10:23:06,398 - 10000 samples (128 per mini-batch)
2022-01-29 10:23:08,174 - Epoch: [21][   79/   79]    Loss 2.251535    Top1 39.740000    Top5 71.970000    
2022-01-29 10:23:08,234 - ==> Top1: 39.740    Top5: 71.970    Loss: 2.252

2022-01-29 10:23:08,239 - ==> Best [Top1: 40.490   Top5: 73.720   Sparsity:0.00   Params: 627712 on epoch: 20]
2022-01-29 10:23:08,239 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:23:08,278 - 

2022-01-29 10:23:08,278 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:23:13,667 - Epoch: [22][  100/  391]    Overall Loss 2.005558    Objective Loss 2.005558                                        LR 0.100000    Time 0.053865    
2022-01-29 10:23:18,839 - Epoch: [22][  200/  391]    Overall Loss 1.997100    Objective Loss 1.997100                                        LR 0.100000    Time 0.052787    
2022-01-29 10:23:24,012 - Epoch: [22][  300/  391]    Overall Loss 2.015425    Objective Loss 2.015425                                        LR 0.100000    Time 0.052432    
2022-01-29 10:23:28,684 - Epoch: [22][  391/  391]    Overall Loss 2.010697    Objective Loss 2.010697    Top1 49.038462    Top5 75.000000    LR 0.100000    Time 0.052176    
2022-01-29 10:23:28,743 - --- validate (epoch=22)-----------
2022-01-29 10:23:28,743 - 10000 samples (128 per mini-batch)
2022-01-29 10:23:30,367 - Epoch: [22][   79/   79]    Loss 2.207009    Top1 40.620000    Top5 73.500000    
2022-01-29 10:23:30,422 - ==> Top1: 40.620    Top5: 73.500    Loss: 2.207

2022-01-29 10:23:30,427 - ==> Best [Top1: 40.620   Top5: 73.500   Sparsity:0.00   Params: 627712 on epoch: 22]
2022-01-29 10:23:30,427 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:23:30,475 - 

2022-01-29 10:23:30,476 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:23:35,911 - Epoch: [23][  100/  391]    Overall Loss 1.984060    Objective Loss 1.984060                                        LR 0.100000    Time 0.054329    
2022-01-29 10:23:41,111 - Epoch: [23][  200/  391]    Overall Loss 1.982866    Objective Loss 1.982866                                        LR 0.100000    Time 0.053162    
2022-01-29 10:23:46,301 - Epoch: [23][  300/  391]    Overall Loss 1.992053    Objective Loss 1.992053                                        LR 0.100000    Time 0.052736    
2022-01-29 10:23:51,019 - Epoch: [23][  391/  391]    Overall Loss 1.991746    Objective Loss 1.991746    Top1 41.346154    Top5 72.115385    LR 0.100000    Time 0.052529    
2022-01-29 10:23:51,072 - --- validate (epoch=23)-----------
2022-01-29 10:23:51,072 - 10000 samples (128 per mini-batch)
2022-01-29 10:23:52,827 - Epoch: [23][   79/   79]    Loss 2.161251    Top1 41.520000    Top5 74.370000    
2022-01-29 10:23:52,887 - ==> Top1: 41.520    Top5: 74.370    Loss: 2.161

2022-01-29 10:23:52,893 - ==> Best [Top1: 41.520   Top5: 74.370   Sparsity:0.00   Params: 627712 on epoch: 23]
2022-01-29 10:23:52,893 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:23:52,941 - 

2022-01-29 10:23:52,941 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:23:58,358 - Epoch: [24][  100/  391]    Overall Loss 1.948951    Objective Loss 1.948951                                        LR 0.100000    Time 0.054145    
2022-01-29 10:24:03,482 - Epoch: [24][  200/  391]    Overall Loss 1.944103    Objective Loss 1.944103                                        LR 0.100000    Time 0.052692    
2022-01-29 10:24:08,615 - Epoch: [24][  300/  391]    Overall Loss 1.959394    Objective Loss 1.959394                                        LR 0.100000    Time 0.052232    
2022-01-29 10:24:13,273 - Epoch: [24][  391/  391]    Overall Loss 1.964464    Objective Loss 1.964464    Top1 42.307692    Top5 80.288462    LR 0.100000    Time 0.051987    
2022-01-29 10:24:13,331 - --- validate (epoch=24)-----------
2022-01-29 10:24:13,331 - 10000 samples (128 per mini-batch)
2022-01-29 10:24:14,985 - Epoch: [24][   79/   79]    Loss 2.184014    Top1 41.410000    Top5 73.410000    
2022-01-29 10:24:15,051 - ==> Top1: 41.410    Top5: 73.410    Loss: 2.184

2022-01-29 10:24:15,057 - ==> Best [Top1: 41.520   Top5: 74.370   Sparsity:0.00   Params: 627712 on epoch: 23]
2022-01-29 10:24:15,057 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:24:15,096 - 

2022-01-29 10:24:15,096 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:24:20,569 - Epoch: [25][  100/  391]    Overall Loss 1.905102    Objective Loss 1.905102                                        LR 0.100000    Time 0.054699    
2022-01-29 10:24:25,800 - Epoch: [25][  200/  391]    Overall Loss 1.920577    Objective Loss 1.920577                                        LR 0.100000    Time 0.053501    
2022-01-29 10:24:31,028 - Epoch: [25][  300/  391]    Overall Loss 1.925414    Objective Loss 1.925414                                        LR 0.100000    Time 0.053093    
2022-01-29 10:24:35,766 - Epoch: [25][  391/  391]    Overall Loss 1.930350    Objective Loss 1.930350    Top1 46.153846    Top5 75.961538    LR 0.100000    Time 0.052850    
2022-01-29 10:24:35,824 - --- validate (epoch=25)-----------
2022-01-29 10:24:35,824 - 10000 samples (128 per mini-batch)
2022-01-29 10:24:37,509 - Epoch: [25][   79/   79]    Loss 2.140262    Top1 42.720000    Top5 74.740000    
2022-01-29 10:24:37,566 - ==> Top1: 42.720    Top5: 74.740    Loss: 2.140

2022-01-29 10:24:37,571 - ==> Best [Top1: 42.720   Top5: 74.740   Sparsity:0.00   Params: 627712 on epoch: 25]
2022-01-29 10:24:37,572 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:24:37,621 - 

2022-01-29 10:24:37,621 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:24:43,119 - Epoch: [26][  100/  391]    Overall Loss 1.877313    Objective Loss 1.877313                                        LR 0.100000    Time 0.054953    
2022-01-29 10:24:48,333 - Epoch: [26][  200/  391]    Overall Loss 1.892188    Objective Loss 1.892188                                        LR 0.100000    Time 0.053543    
2022-01-29 10:24:53,556 - Epoch: [26][  300/  391]    Overall Loss 1.902221    Objective Loss 1.902221                                        LR 0.100000    Time 0.053104    
2022-01-29 10:24:58,334 - Epoch: [26][  391/  391]    Overall Loss 1.903206    Objective Loss 1.903206    Top1 49.038462    Top5 83.173077    LR 0.100000    Time 0.052962    
2022-01-29 10:24:58,394 - --- validate (epoch=26)-----------
2022-01-29 10:24:58,394 - 10000 samples (128 per mini-batch)
2022-01-29 10:25:00,065 - Epoch: [26][   79/   79]    Loss 2.192407    Top1 41.350000    Top5 73.370000    
2022-01-29 10:25:00,125 - ==> Top1: 41.350    Top5: 73.370    Loss: 2.192

2022-01-29 10:25:00,130 - ==> Best [Top1: 42.720   Top5: 74.740   Sparsity:0.00   Params: 627712 on epoch: 25]
2022-01-29 10:25:00,130 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:25:00,175 - 

2022-01-29 10:25:00,176 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:25:05,716 - Epoch: [27][  100/  391]    Overall Loss 1.852785    Objective Loss 1.852785                                        LR 0.100000    Time 0.055381    
2022-01-29 10:25:10,967 - Epoch: [27][  200/  391]    Overall Loss 1.864038    Objective Loss 1.864038                                        LR 0.100000    Time 0.053939    
2022-01-29 10:25:16,175 - Epoch: [27][  300/  391]    Overall Loss 1.878539    Objective Loss 1.878539                                        LR 0.100000    Time 0.053317    
2022-01-29 10:25:20,890 - Epoch: [27][  391/  391]    Overall Loss 1.882744    Objective Loss 1.882744    Top1 49.038462    Top5 82.211538    LR 0.100000    Time 0.052966    
2022-01-29 10:25:20,956 - --- validate (epoch=27)-----------
2022-01-29 10:25:20,956 - 10000 samples (128 per mini-batch)
2022-01-29 10:25:22,589 - Epoch: [27][   79/   79]    Loss 2.116687    Top1 42.860000    Top5 75.850000    
2022-01-29 10:25:22,643 - ==> Top1: 42.860    Top5: 75.850    Loss: 2.117

2022-01-29 10:25:22,648 - ==> Best [Top1: 42.860   Top5: 75.850   Sparsity:0.00   Params: 627712 on epoch: 27]
2022-01-29 10:25:22,648 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:25:22,696 - 

2022-01-29 10:25:22,696 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:25:28,048 - Epoch: [28][  100/  391]    Overall Loss 1.862489    Objective Loss 1.862489                                        LR 0.100000    Time 0.053491    
2022-01-29 10:25:33,191 - Epoch: [28][  200/  391]    Overall Loss 1.857189    Objective Loss 1.857189                                        LR 0.100000    Time 0.052456    
2022-01-29 10:25:38,389 - Epoch: [28][  300/  391]    Overall Loss 1.865313    Objective Loss 1.865313                                        LR 0.100000    Time 0.052293    
2022-01-29 10:25:43,119 - Epoch: [28][  391/  391]    Overall Loss 1.866503    Objective Loss 1.866503    Top1 46.153846    Top5 77.884615    LR 0.100000    Time 0.052218    
2022-01-29 10:25:43,178 - --- validate (epoch=28)-----------
2022-01-29 10:25:43,178 - 10000 samples (128 per mini-batch)
2022-01-29 10:25:45,012 - Epoch: [28][   79/   79]    Loss 2.015247    Top1 45.020000    Top5 77.560000    
2022-01-29 10:25:45,068 - ==> Top1: 45.020    Top5: 77.560    Loss: 2.015

2022-01-29 10:25:45,073 - ==> Best [Top1: 45.020   Top5: 77.560   Sparsity:0.00   Params: 627712 on epoch: 28]
2022-01-29 10:25:45,073 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:25:45,117 - 

2022-01-29 10:25:45,117 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:25:50,591 - Epoch: [29][  100/  391]    Overall Loss 1.835250    Objective Loss 1.835250                                        LR 0.100000    Time 0.054707    
2022-01-29 10:25:55,807 - Epoch: [29][  200/  391]    Overall Loss 1.840920    Objective Loss 1.840920                                        LR 0.100000    Time 0.053433    
2022-01-29 10:26:01,030 - Epoch: [29][  300/  391]    Overall Loss 1.848263    Objective Loss 1.848263                                        LR 0.100000    Time 0.053029    
2022-01-29 10:26:05,731 - Epoch: [29][  391/  391]    Overall Loss 1.851816    Objective Loss 1.851816    Top1 48.557692    Top5 82.211538    LR 0.100000    Time 0.052709    
2022-01-29 10:26:05,791 - --- validate (epoch=29)-----------
2022-01-29 10:26:05,791 - 10000 samples (128 per mini-batch)
2022-01-29 10:26:07,586 - Epoch: [29][   79/   79]    Loss 2.212116    Top1 41.130000    Top5 73.650000    
2022-01-29 10:26:07,639 - ==> Top1: 41.130    Top5: 73.650    Loss: 2.212

2022-01-29 10:26:07,644 - ==> Best [Top1: 45.020   Top5: 77.560   Sparsity:0.00   Params: 627712 on epoch: 28]
2022-01-29 10:26:07,644 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:26:07,690 - 

2022-01-29 10:26:07,690 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:26:13,141 - Epoch: [30][  100/  391]    Overall Loss 1.792201    Objective Loss 1.792201                                        LR 0.100000    Time 0.054482    
2022-01-29 10:26:18,277 - Epoch: [30][  200/  391]    Overall Loss 1.802790    Objective Loss 1.802790                                        LR 0.100000    Time 0.052917    
2022-01-29 10:26:23,409 - Epoch: [30][  300/  391]    Overall Loss 1.816116    Objective Loss 1.816116                                        LR 0.100000    Time 0.052383    
2022-01-29 10:26:28,081 - Epoch: [30][  391/  391]    Overall Loss 1.817999    Objective Loss 1.817999    Top1 50.961538    Top5 83.173077    LR 0.100000    Time 0.052139    
2022-01-29 10:26:28,140 - --- validate (epoch=30)-----------
2022-01-29 10:26:28,141 - 10000 samples (128 per mini-batch)
2022-01-29 10:26:29,797 - Epoch: [30][   79/   79]    Loss 2.087355    Top1 44.480000    Top5 76.160000    
2022-01-29 10:26:29,855 - ==> Top1: 44.480    Top5: 76.160    Loss: 2.087

2022-01-29 10:26:29,860 - ==> Best [Top1: 45.020   Top5: 77.560   Sparsity:0.00   Params: 627712 on epoch: 28]
2022-01-29 10:26:29,860 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:26:29,904 - 

2022-01-29 10:26:29,904 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:26:35,342 - Epoch: [31][  100/  391]    Overall Loss 1.784962    Objective Loss 1.784962                                        LR 0.100000    Time 0.054353    
2022-01-29 10:26:40,485 - Epoch: [31][  200/  391]    Overall Loss 1.786918    Objective Loss 1.786918                                        LR 0.100000    Time 0.052888    
2022-01-29 10:26:45,633 - Epoch: [31][  300/  391]    Overall Loss 1.799875    Objective Loss 1.799875                                        LR 0.100000    Time 0.052416    
2022-01-29 10:26:50,293 - Epoch: [31][  391/  391]    Overall Loss 1.805838    Objective Loss 1.805838    Top1 52.403846    Top5 77.884615    LR 0.100000    Time 0.052133    
2022-01-29 10:26:50,351 - --- validate (epoch=31)-----------
2022-01-29 10:26:50,351 - 10000 samples (128 per mini-batch)
2022-01-29 10:26:52,092 - Epoch: [31][   79/   79]    Loss 2.044945    Top1 44.860000    Top5 76.640000    
2022-01-29 10:26:52,146 - ==> Top1: 44.860    Top5: 76.640    Loss: 2.045

2022-01-29 10:26:52,152 - ==> Best [Top1: 45.020   Top5: 77.560   Sparsity:0.00   Params: 627712 on epoch: 28]
2022-01-29 10:26:52,152 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:26:52,196 - 

2022-01-29 10:26:52,196 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:26:57,504 - Epoch: [32][  100/  391]    Overall Loss 1.768382    Objective Loss 1.768382                                        LR 0.100000    Time 0.053054    
2022-01-29 10:27:02,641 - Epoch: [32][  200/  391]    Overall Loss 1.787530    Objective Loss 1.787530                                        LR 0.100000    Time 0.052209    
2022-01-29 10:27:07,863 - Epoch: [32][  300/  391]    Overall Loss 1.786341    Objective Loss 1.786341                                        LR 0.100000    Time 0.052207    
2022-01-29 10:27:12,616 - Epoch: [32][  391/  391]    Overall Loss 1.788472    Objective Loss 1.788472    Top1 52.884615    Top5 84.615385    LR 0.100000    Time 0.052209    
2022-01-29 10:27:12,681 - --- validate (epoch=32)-----------
2022-01-29 10:27:12,681 - 10000 samples (128 per mini-batch)
2022-01-29 10:27:14,488 - Epoch: [32][   79/   79]    Loss 2.173487    Top1 41.530000    Top5 74.410000    
2022-01-29 10:27:14,547 - ==> Top1: 41.530    Top5: 74.410    Loss: 2.173

2022-01-29 10:27:14,552 - ==> Best [Top1: 45.020   Top5: 77.560   Sparsity:0.00   Params: 627712 on epoch: 28]
2022-01-29 10:27:14,552 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:27:14,597 - 

2022-01-29 10:27:14,597 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:27:20,139 - Epoch: [33][  100/  391]    Overall Loss 1.752236    Objective Loss 1.752236                                        LR 0.100000    Time 0.055387    
2022-01-29 10:27:25,440 - Epoch: [33][  200/  391]    Overall Loss 1.756371    Objective Loss 1.756371                                        LR 0.100000    Time 0.054194    
2022-01-29 10:27:30,743 - Epoch: [33][  300/  391]    Overall Loss 1.767937    Objective Loss 1.767937                                        LR 0.100000    Time 0.053803    
2022-01-29 10:27:35,526 - Epoch: [33][  391/  391]    Overall Loss 1.774112    Objective Loss 1.774112    Top1 47.596154    Top5 74.519231    LR 0.100000    Time 0.053513    
2022-01-29 10:27:35,583 - --- validate (epoch=33)-----------
2022-01-29 10:27:35,583 - 10000 samples (128 per mini-batch)
2022-01-29 10:27:37,251 - Epoch: [33][   79/   79]    Loss 1.914087    Top1 47.520000    Top5 79.230000    
2022-01-29 10:27:37,310 - ==> Top1: 47.520    Top5: 79.230    Loss: 1.914

2022-01-29 10:27:37,315 - ==> Best [Top1: 47.520   Top5: 79.230   Sparsity:0.00   Params: 627712 on epoch: 33]
2022-01-29 10:27:37,315 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:27:37,363 - 

2022-01-29 10:27:37,363 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:27:42,871 - Epoch: [34][  100/  391]    Overall Loss 1.718574    Objective Loss 1.718574                                        LR 0.100000    Time 0.055046    
2022-01-29 10:27:48,164 - Epoch: [34][  200/  391]    Overall Loss 1.733321    Objective Loss 1.733321                                        LR 0.100000    Time 0.053985    
2022-01-29 10:27:53,383 - Epoch: [34][  300/  391]    Overall Loss 1.734028    Objective Loss 1.734028                                        LR 0.100000    Time 0.053384    
2022-01-29 10:27:58,134 - Epoch: [34][  391/  391]    Overall Loss 1.744134    Objective Loss 1.744134    Top1 46.153846    Top5 84.134615    LR 0.100000    Time 0.053109    
2022-01-29 10:27:58,194 - --- validate (epoch=34)-----------
2022-01-29 10:27:58,194 - 10000 samples (128 per mini-batch)
2022-01-29 10:27:59,841 - Epoch: [34][   79/   79]    Loss 2.109748    Top1 43.490000    Top5 75.040000    
2022-01-29 10:27:59,894 - ==> Top1: 43.490    Top5: 75.040    Loss: 2.110

2022-01-29 10:27:59,899 - ==> Best [Top1: 47.520   Top5: 79.230   Sparsity:0.00   Params: 627712 on epoch: 33]
2022-01-29 10:27:59,899 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:27:59,944 - 

2022-01-29 10:27:59,944 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:28:05,402 - Epoch: [35][  100/  391]    Overall Loss 1.698578    Objective Loss 1.698578                                        LR 0.100000    Time 0.054548    
2022-01-29 10:28:10,619 - Epoch: [35][  200/  391]    Overall Loss 1.711169    Objective Loss 1.711169                                        LR 0.100000    Time 0.053356    
2022-01-29 10:28:15,840 - Epoch: [35][  300/  391]    Overall Loss 1.728056    Objective Loss 1.728056                                        LR 0.100000    Time 0.052972    
2022-01-29 10:28:20,582 - Epoch: [35][  391/  391]    Overall Loss 1.733552    Objective Loss 1.733552    Top1 50.000000    Top5 80.769231    LR 0.100000    Time 0.052770    
2022-01-29 10:28:20,641 - --- validate (epoch=35)-----------
2022-01-29 10:28:20,642 - 10000 samples (128 per mini-batch)
2022-01-29 10:28:22,376 - Epoch: [35][   79/   79]    Loss 2.078176    Top1 45.380000    Top5 75.990000    
2022-01-29 10:28:22,431 - ==> Top1: 45.380    Top5: 75.990    Loss: 2.078

2022-01-29 10:28:22,437 - ==> Best [Top1: 47.520   Top5: 79.230   Sparsity:0.00   Params: 627712 on epoch: 33]
2022-01-29 10:28:22,437 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:28:22,481 - 

2022-01-29 10:28:22,481 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:28:27,986 - Epoch: [36][  100/  391]    Overall Loss 1.690102    Objective Loss 1.690102                                        LR 0.100000    Time 0.055023    
2022-01-29 10:28:33,279 - Epoch: [36][  200/  391]    Overall Loss 1.704424    Objective Loss 1.704424                                        LR 0.100000    Time 0.053975    
2022-01-29 10:28:38,593 - Epoch: [36][  300/  391]    Overall Loss 1.715933    Objective Loss 1.715933                                        LR 0.100000    Time 0.053694    
2022-01-29 10:28:43,334 - Epoch: [36][  391/  391]    Overall Loss 1.721732    Objective Loss 1.721732    Top1 56.730769    Top5 82.692308    LR 0.100000    Time 0.053320    
2022-01-29 10:28:43,390 - --- validate (epoch=36)-----------
2022-01-29 10:28:43,390 - 10000 samples (128 per mini-batch)
2022-01-29 10:28:45,062 - Epoch: [36][   79/   79]    Loss 2.125582    Top1 43.240000    Top5 75.810000    
2022-01-29 10:28:45,114 - ==> Top1: 43.240    Top5: 75.810    Loss: 2.126

2022-01-29 10:28:45,120 - ==> Best [Top1: 47.520   Top5: 79.230   Sparsity:0.00   Params: 627712 on epoch: 33]
2022-01-29 10:28:45,120 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:28:45,165 - 

2022-01-29 10:28:45,165 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:28:50,724 - Epoch: [37][  100/  391]    Overall Loss 1.692577    Objective Loss 1.692577                                        LR 0.100000    Time 0.055563    
2022-01-29 10:28:55,951 - Epoch: [37][  200/  391]    Overall Loss 1.695459    Objective Loss 1.695459                                        LR 0.100000    Time 0.053911    
2022-01-29 10:29:01,243 - Epoch: [37][  300/  391]    Overall Loss 1.693880    Objective Loss 1.693880                                        LR 0.100000    Time 0.053578    
2022-01-29 10:29:06,024 - Epoch: [37][  391/  391]    Overall Loss 1.698625    Objective Loss 1.698625    Top1 47.115385    Top5 80.769231    LR 0.100000    Time 0.053333    
2022-01-29 10:29:06,084 - --- validate (epoch=37)-----------
2022-01-29 10:29:06,085 - 10000 samples (128 per mini-batch)
2022-01-29 10:29:07,733 - Epoch: [37][   79/   79]    Loss 2.058962    Top1 44.760000    Top5 76.820000    
2022-01-29 10:29:07,797 - ==> Top1: 44.760    Top5: 76.820    Loss: 2.059

2022-01-29 10:29:07,802 - ==> Best [Top1: 47.520   Top5: 79.230   Sparsity:0.00   Params: 627712 on epoch: 33]
2022-01-29 10:29:07,802 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:29:07,839 - 

2022-01-29 10:29:07,839 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:29:13,413 - Epoch: [38][  100/  391]    Overall Loss 1.656198    Objective Loss 1.656198                                        LR 0.100000    Time 0.055719    
2022-01-29 10:29:18,697 - Epoch: [38][  200/  391]    Overall Loss 1.669563    Objective Loss 1.669563                                        LR 0.100000    Time 0.054275    
2022-01-29 10:29:23,922 - Epoch: [38][  300/  391]    Overall Loss 1.676028    Objective Loss 1.676028                                        LR 0.100000    Time 0.053596    
2022-01-29 10:29:28,653 - Epoch: [38][  391/  391]    Overall Loss 1.679102    Objective Loss 1.679102    Top1 57.211538    Top5 83.173077    LR 0.100000    Time 0.053221    
2022-01-29 10:29:28,713 - --- validate (epoch=38)-----------
2022-01-29 10:29:28,714 - 10000 samples (128 per mini-batch)
2022-01-29 10:29:30,486 - Epoch: [38][   79/   79]    Loss 2.140104    Top1 43.360000    Top5 75.350000    
2022-01-29 10:29:30,541 - ==> Top1: 43.360    Top5: 75.350    Loss: 2.140

2022-01-29 10:29:30,547 - ==> Best [Top1: 47.520   Top5: 79.230   Sparsity:0.00   Params: 627712 on epoch: 33]
2022-01-29 10:29:30,547 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:29:30,592 - 

2022-01-29 10:29:30,592 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:29:36,014 - Epoch: [39][  100/  391]    Overall Loss 1.638458    Objective Loss 1.638458                                        LR 0.100000    Time 0.054191    
2022-01-29 10:29:41,221 - Epoch: [39][  200/  391]    Overall Loss 1.663732    Objective Loss 1.663732                                        LR 0.100000    Time 0.053125    
2022-01-29 10:29:46,429 - Epoch: [39][  300/  391]    Overall Loss 1.671865    Objective Loss 1.671865                                        LR 0.100000    Time 0.052775    
2022-01-29 10:29:51,257 - Epoch: [39][  391/  391]    Overall Loss 1.674941    Objective Loss 1.674941    Top1 58.653846    Top5 85.096154    LR 0.100000    Time 0.052839    
2022-01-29 10:29:51,316 - --- validate (epoch=39)-----------
2022-01-29 10:29:51,317 - 10000 samples (128 per mini-batch)
2022-01-29 10:29:53,056 - Epoch: [39][   79/   79]    Loss 1.911914    Top1 47.810000    Top5 78.550000    
2022-01-29 10:29:53,115 - ==> Top1: 47.810    Top5: 78.550    Loss: 1.912

2022-01-29 10:29:53,120 - ==> Best [Top1: 47.810   Top5: 78.550   Sparsity:0.00   Params: 627712 on epoch: 39]
2022-01-29 10:29:53,121 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:29:53,170 - 

2022-01-29 10:29:53,170 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:29:58,726 - Epoch: [40][  100/  391]    Overall Loss 1.618313    Objective Loss 1.618313                                        LR 0.100000    Time 0.055531    
2022-01-29 10:30:03,982 - Epoch: [40][  200/  391]    Overall Loss 1.640449    Objective Loss 1.640449                                        LR 0.100000    Time 0.054044    
2022-01-29 10:30:09,267 - Epoch: [40][  300/  391]    Overall Loss 1.652562    Objective Loss 1.652562                                        LR 0.100000    Time 0.053640    
2022-01-29 10:30:14,075 - Epoch: [40][  391/  391]    Overall Loss 1.647145    Objective Loss 1.647145    Top1 58.173077    Top5 88.942308    LR 0.100000    Time 0.053451    
2022-01-29 10:30:14,135 - --- validate (epoch=40)-----------
2022-01-29 10:30:14,135 - 10000 samples (128 per mini-batch)
2022-01-29 10:30:15,822 - Epoch: [40][   79/   79]    Loss 1.901560    Top1 48.380000    Top5 79.160000    
2022-01-29 10:30:15,878 - ==> Top1: 48.380    Top5: 79.160    Loss: 1.902

2022-01-29 10:30:15,884 - ==> Best [Top1: 48.380   Top5: 79.160   Sparsity:0.00   Params: 627712 on epoch: 40]
2022-01-29 10:30:15,884 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:30:15,933 - 

2022-01-29 10:30:15,933 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:30:21,493 - Epoch: [41][  100/  391]    Overall Loss 1.607876    Objective Loss 1.607876                                        LR 0.100000    Time 0.055567    
2022-01-29 10:30:26,813 - Epoch: [41][  200/  391]    Overall Loss 1.616678    Objective Loss 1.616678                                        LR 0.100000    Time 0.054383    
2022-01-29 10:30:32,100 - Epoch: [41][  300/  391]    Overall Loss 1.630780    Objective Loss 1.630780                                        LR 0.100000    Time 0.053874    
2022-01-29 10:30:36,869 - Epoch: [41][  391/  391]    Overall Loss 1.639289    Objective Loss 1.639289    Top1 60.096154    Top5 87.019231    LR 0.100000    Time 0.053532    
2022-01-29 10:30:36,923 - --- validate (epoch=41)-----------
2022-01-29 10:30:36,923 - 10000 samples (128 per mini-batch)
2022-01-29 10:30:38,578 - Epoch: [41][   79/   79]    Loss 2.005244    Top1 46.280000    Top5 77.760000    
2022-01-29 10:30:38,638 - ==> Top1: 46.280    Top5: 77.760    Loss: 2.005

2022-01-29 10:30:38,643 - ==> Best [Top1: 48.380   Top5: 79.160   Sparsity:0.00   Params: 627712 on epoch: 40]
2022-01-29 10:30:38,643 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:30:38,689 - 

2022-01-29 10:30:38,689 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:30:44,129 - Epoch: [42][  100/  391]    Overall Loss 1.607385    Objective Loss 1.607385                                        LR 0.100000    Time 0.054376    
2022-01-29 10:30:49,387 - Epoch: [42][  200/  391]    Overall Loss 1.612522    Objective Loss 1.612522                                        LR 0.100000    Time 0.053471    
2022-01-29 10:30:54,642 - Epoch: [42][  300/  391]    Overall Loss 1.622958    Objective Loss 1.622958                                        LR 0.100000    Time 0.053161    
2022-01-29 10:30:59,436 - Epoch: [42][  391/  391]    Overall Loss 1.633201    Objective Loss 1.633201    Top1 53.365385    Top5 86.057692    LR 0.100000    Time 0.053048    
2022-01-29 10:30:59,493 - --- validate (epoch=42)-----------
2022-01-29 10:30:59,493 - 10000 samples (128 per mini-batch)
2022-01-29 10:31:01,314 - Epoch: [42][   79/   79]    Loss 2.088392    Top1 43.800000    Top5 75.770000    
2022-01-29 10:31:01,367 - ==> Top1: 43.800    Top5: 75.770    Loss: 2.088

2022-01-29 10:31:01,373 - ==> Best [Top1: 48.380   Top5: 79.160   Sparsity:0.00   Params: 627712 on epoch: 40]
2022-01-29 10:31:01,373 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:31:01,409 - 

2022-01-29 10:31:01,409 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:31:06,880 - Epoch: [43][  100/  391]    Overall Loss 1.570824    Objective Loss 1.570824                                        LR 0.100000    Time 0.054683    
2022-01-29 10:31:12,162 - Epoch: [43][  200/  391]    Overall Loss 1.587884    Objective Loss 1.587884                                        LR 0.100000    Time 0.053749    
2022-01-29 10:31:17,435 - Epoch: [43][  300/  391]    Overall Loss 1.602116    Objective Loss 1.602116                                        LR 0.100000    Time 0.053404    
2022-01-29 10:31:22,222 - Epoch: [43][  391/  391]    Overall Loss 1.613134    Objective Loss 1.613134    Top1 54.326923    Top5 82.692308    LR 0.100000    Time 0.053217    
2022-01-29 10:31:22,280 - --- validate (epoch=43)-----------
2022-01-29 10:31:22,280 - 10000 samples (128 per mini-batch)
2022-01-29 10:31:23,980 - Epoch: [43][   79/   79]    Loss 1.941212    Top1 46.780000    Top5 78.840000    
2022-01-29 10:31:24,032 - ==> Top1: 46.780    Top5: 78.840    Loss: 1.941

2022-01-29 10:31:24,037 - ==> Best [Top1: 48.380   Top5: 79.160   Sparsity:0.00   Params: 627712 on epoch: 40]
2022-01-29 10:31:24,037 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:31:24,075 - 

2022-01-29 10:31:24,075 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:31:29,544 - Epoch: [44][  100/  391]    Overall Loss 1.576961    Objective Loss 1.576961                                        LR 0.100000    Time 0.054668    
2022-01-29 10:31:34,821 - Epoch: [44][  200/  391]    Overall Loss 1.588457    Objective Loss 1.588457                                        LR 0.100000    Time 0.053713    
2022-01-29 10:31:40,100 - Epoch: [44][  300/  391]    Overall Loss 1.603174    Objective Loss 1.603174                                        LR 0.100000    Time 0.053404    
2022-01-29 10:31:44,807 - Epoch: [44][  391/  391]    Overall Loss 1.605295    Objective Loss 1.605295    Top1 50.961538    Top5 81.730769    LR 0.100000    Time 0.053009    
2022-01-29 10:31:44,864 - --- validate (epoch=44)-----------
2022-01-29 10:31:44,865 - 10000 samples (128 per mini-batch)
2022-01-29 10:31:46,546 - Epoch: [44][   79/   79]    Loss 1.948510    Top1 46.960000    Top5 79.280000    
2022-01-29 10:31:46,601 - ==> Top1: 46.960    Top5: 79.280    Loss: 1.949

2022-01-29 10:31:46,606 - ==> Best [Top1: 48.380   Top5: 79.160   Sparsity:0.00   Params: 627712 on epoch: 40]
2022-01-29 10:31:46,606 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:31:46,651 - 

2022-01-29 10:31:46,651 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:31:52,194 - Epoch: [45][  100/  391]    Overall Loss 1.530535    Objective Loss 1.530535                                        LR 0.100000    Time 0.055404    
2022-01-29 10:31:57,466 - Epoch: [45][  200/  391]    Overall Loss 1.555748    Objective Loss 1.555748                                        LR 0.100000    Time 0.054058    
2022-01-29 10:32:02,703 - Epoch: [45][  300/  391]    Overall Loss 1.577246    Objective Loss 1.577246                                        LR 0.100000    Time 0.053491    
2022-01-29 10:32:07,432 - Epoch: [45][  391/  391]    Overall Loss 1.595832    Objective Loss 1.595832    Top1 55.769231    Top5 80.769231    LR 0.100000    Time 0.053135    
2022-01-29 10:32:07,499 - --- validate (epoch=45)-----------
2022-01-29 10:32:07,499 - 10000 samples (128 per mini-batch)
2022-01-29 10:32:09,187 - Epoch: [45][   79/   79]    Loss 1.884316    Top1 48.360000    Top5 79.460000    
2022-01-29 10:32:09,245 - ==> Top1: 48.360    Top5: 79.460    Loss: 1.884

2022-01-29 10:32:09,250 - ==> Best [Top1: 48.380   Top5: 79.160   Sparsity:0.00   Params: 627712 on epoch: 40]
2022-01-29 10:32:09,250 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:32:09,295 - 

2022-01-29 10:32:09,295 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:32:14,659 - Epoch: [46][  100/  391]    Overall Loss 1.537065    Objective Loss 1.537065                                        LR 0.100000    Time 0.053609    
2022-01-29 10:32:19,867 - Epoch: [46][  200/  391]    Overall Loss 1.557389    Objective Loss 1.557389                                        LR 0.100000    Time 0.052841    
2022-01-29 10:32:25,074 - Epoch: [46][  300/  391]    Overall Loss 1.567427    Objective Loss 1.567427                                        LR 0.100000    Time 0.052581    
2022-01-29 10:32:29,824 - Epoch: [46][  391/  391]    Overall Loss 1.577223    Objective Loss 1.577223    Top1 57.692308    Top5 89.903846    LR 0.100000    Time 0.052489    
2022-01-29 10:32:29,882 - --- validate (epoch=46)-----------
2022-01-29 10:32:29,882 - 10000 samples (128 per mini-batch)
2022-01-29 10:32:31,560 - Epoch: [46][   79/   79]    Loss 1.881084    Top1 48.770000    Top5 79.920000    
2022-01-29 10:32:31,619 - ==> Top1: 48.770    Top5: 79.920    Loss: 1.881

2022-01-29 10:32:31,624 - ==> Best [Top1: 48.770   Top5: 79.920   Sparsity:0.00   Params: 627712 on epoch: 46]
2022-01-29 10:32:31,625 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:32:31,674 - 

2022-01-29 10:32:31,674 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:32:37,205 - Epoch: [47][  100/  391]    Overall Loss 1.532909    Objective Loss 1.532909                                        LR 0.100000    Time 0.055286    
2022-01-29 10:32:42,461 - Epoch: [47][  200/  391]    Overall Loss 1.545441    Objective Loss 1.545441                                        LR 0.100000    Time 0.053920    
2022-01-29 10:32:47,701 - Epoch: [47][  300/  391]    Overall Loss 1.564423    Objective Loss 1.564423                                        LR 0.100000    Time 0.053411    
2022-01-29 10:32:52,457 - Epoch: [47][  391/  391]    Overall Loss 1.568552    Objective Loss 1.568552    Top1 53.846154    Top5 86.538462    LR 0.100000    Time 0.053141    
2022-01-29 10:32:52,516 - --- validate (epoch=47)-----------
2022-01-29 10:32:52,516 - 10000 samples (128 per mini-batch)
2022-01-29 10:32:54,212 - Epoch: [47][   79/   79]    Loss 2.074577    Top1 45.950000    Top5 75.810000    
2022-01-29 10:32:54,265 - ==> Top1: 45.950    Top5: 75.810    Loss: 2.075

2022-01-29 10:32:54,271 - ==> Best [Top1: 48.770   Top5: 79.920   Sparsity:0.00   Params: 627712 on epoch: 46]
2022-01-29 10:32:54,271 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:32:54,316 - 

2022-01-29 10:32:54,316 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:32:59,814 - Epoch: [48][  100/  391]    Overall Loss 1.540757    Objective Loss 1.540757                                        LR 0.100000    Time 0.054957    
2022-01-29 10:33:05,098 - Epoch: [48][  200/  391]    Overall Loss 1.556765    Objective Loss 1.556765                                        LR 0.100000    Time 0.053891    
2022-01-29 10:33:10,322 - Epoch: [48][  300/  391]    Overall Loss 1.553165    Objective Loss 1.553165                                        LR 0.100000    Time 0.053339    
2022-01-29 10:33:15,097 - Epoch: [48][  391/  391]    Overall Loss 1.560190    Objective Loss 1.560190    Top1 51.442308    Top5 80.769231    LR 0.100000    Time 0.053135    
2022-01-29 10:33:15,158 - --- validate (epoch=48)-----------
2022-01-29 10:33:15,158 - 10000 samples (128 per mini-batch)
2022-01-29 10:33:16,842 - Epoch: [48][   79/   79]    Loss 1.968623    Top1 47.160000    Top5 78.680000    
2022-01-29 10:33:16,893 - ==> Top1: 47.160    Top5: 78.680    Loss: 1.969

2022-01-29 10:33:16,898 - ==> Best [Top1: 48.770   Top5: 79.920   Sparsity:0.00   Params: 627712 on epoch: 46]
2022-01-29 10:33:16,898 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:33:16,944 - 

2022-01-29 10:33:16,944 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:33:22,409 - Epoch: [49][  100/  391]    Overall Loss 1.528128    Objective Loss 1.528128                                        LR 0.100000    Time 0.054619    
2022-01-29 10:33:27,639 - Epoch: [49][  200/  391]    Overall Loss 1.524756    Objective Loss 1.524756                                        LR 0.100000    Time 0.053457    
2022-01-29 10:33:32,877 - Epoch: [49][  300/  391]    Overall Loss 1.540908    Objective Loss 1.540908                                        LR 0.100000    Time 0.053092    
2022-01-29 10:33:37,633 - Epoch: [49][  391/  391]    Overall Loss 1.545686    Objective Loss 1.545686    Top1 54.326923    Top5 80.769231    LR 0.100000    Time 0.052898    
2022-01-29 10:33:37,699 - --- validate (epoch=49)-----------
2022-01-29 10:33:37,699 - 10000 samples (128 per mini-batch)
2022-01-29 10:33:39,426 - Epoch: [49][   79/   79]    Loss 1.804747    Top1 50.910000    Top5 81.480000    
2022-01-29 10:33:39,480 - ==> Top1: 50.910    Top5: 81.480    Loss: 1.805

2022-01-29 10:33:39,486 - ==> Best [Top1: 50.910   Top5: 81.480   Sparsity:0.00   Params: 627712 on epoch: 49]
2022-01-29 10:33:39,486 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:33:39,535 - 

2022-01-29 10:33:39,535 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:33:44,947 - Epoch: [50][  100/  391]    Overall Loss 1.515918    Objective Loss 1.515918                                        LR 0.100000    Time 0.054089    
2022-01-29 10:33:50,157 - Epoch: [50][  200/  391]    Overall Loss 1.531387    Objective Loss 1.531387                                        LR 0.100000    Time 0.053091    
2022-01-29 10:33:55,373 - Epoch: [50][  300/  391]    Overall Loss 1.532109    Objective Loss 1.532109                                        LR 0.100000    Time 0.052780    
2022-01-29 10:34:00,102 - Epoch: [50][  391/  391]    Overall Loss 1.542184    Objective Loss 1.542184    Top1 51.442308    Top5 82.692308    LR 0.100000    Time 0.052588    
2022-01-29 10:34:00,161 - --- validate (epoch=50)-----------
2022-01-29 10:34:00,161 - 10000 samples (128 per mini-batch)
2022-01-29 10:34:01,886 - Epoch: [50][   79/   79]    Loss 1.913774    Top1 48.650000    Top5 79.250000    
2022-01-29 10:34:01,939 - ==> Top1: 48.650    Top5: 79.250    Loss: 1.914

2022-01-29 10:34:01,944 - ==> Best [Top1: 50.910   Top5: 81.480   Sparsity:0.00   Params: 627712 on epoch: 49]
2022-01-29 10:34:01,944 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:34:01,989 - 

2022-01-29 10:34:01,990 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:34:07,575 - Epoch: [51][  100/  391]    Overall Loss 1.478959    Objective Loss 1.478959                                        LR 0.100000    Time 0.055826    
2022-01-29 10:34:12,838 - Epoch: [51][  200/  391]    Overall Loss 1.512249    Objective Loss 1.512249                                        LR 0.100000    Time 0.054222    
2022-01-29 10:34:18,075 - Epoch: [51][  300/  391]    Overall Loss 1.518952    Objective Loss 1.518952                                        LR 0.100000    Time 0.053601    
2022-01-29 10:34:22,856 - Epoch: [51][  391/  391]    Overall Loss 1.534972    Objective Loss 1.534972    Top1 50.480769    Top5 85.096154    LR 0.100000    Time 0.053352    
2022-01-29 10:34:22,916 - --- validate (epoch=51)-----------
2022-01-29 10:34:22,916 - 10000 samples (128 per mini-batch)
2022-01-29 10:34:24,613 - Epoch: [51][   79/   79]    Loss 1.951245    Top1 47.170000    Top5 78.110000    
2022-01-29 10:34:24,667 - ==> Top1: 47.170    Top5: 78.110    Loss: 1.951

2022-01-29 10:34:24,673 - ==> Best [Top1: 50.910   Top5: 81.480   Sparsity:0.00   Params: 627712 on epoch: 49]
2022-01-29 10:34:24,673 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:34:24,718 - 

2022-01-29 10:34:24,718 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:34:30,306 - Epoch: [52][  100/  391]    Overall Loss 1.464391    Objective Loss 1.464391                                        LR 0.100000    Time 0.055853    
2022-01-29 10:34:35,604 - Epoch: [52][  200/  391]    Overall Loss 1.473113    Objective Loss 1.473113                                        LR 0.100000    Time 0.054412    
2022-01-29 10:34:40,834 - Epoch: [52][  300/  391]    Overall Loss 1.497414    Objective Loss 1.497414                                        LR 0.100000    Time 0.053704    
2022-01-29 10:34:45,579 - Epoch: [52][  391/  391]    Overall Loss 1.514072    Objective Loss 1.514072    Top1 54.326923    Top5 87.019231    LR 0.100000    Time 0.053340    
2022-01-29 10:34:45,640 - --- validate (epoch=52)-----------
2022-01-29 10:34:45,640 - 10000 samples (128 per mini-batch)
2022-01-29 10:34:47,337 - Epoch: [52][   79/   79]    Loss 1.749429    Top1 51.400000    Top5 81.530000    
2022-01-29 10:34:47,395 - ==> Top1: 51.400    Top5: 81.530    Loss: 1.749

2022-01-29 10:34:47,401 - ==> Best [Top1: 51.400   Top5: 81.530   Sparsity:0.00   Params: 627712 on epoch: 52]
2022-01-29 10:34:47,401 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:34:47,449 - 

2022-01-29 10:34:47,449 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:34:52,782 - Epoch: [53][  100/  391]    Overall Loss 1.462382    Objective Loss 1.462382                                        LR 0.100000    Time 0.053300    
2022-01-29 10:34:58,006 - Epoch: [53][  200/  391]    Overall Loss 1.493999    Objective Loss 1.493999                                        LR 0.100000    Time 0.052766    
2022-01-29 10:35:03,201 - Epoch: [53][  300/  391]    Overall Loss 1.503770    Objective Loss 1.503770                                        LR 0.100000    Time 0.052494    
2022-01-29 10:35:07,925 - Epoch: [53][  391/  391]    Overall Loss 1.512376    Objective Loss 1.512376    Top1 60.096154    Top5 84.615385    LR 0.100000    Time 0.052355    
2022-01-29 10:35:07,977 - --- validate (epoch=53)-----------
2022-01-29 10:35:07,978 - 10000 samples (128 per mini-batch)
2022-01-29 10:35:09,652 - Epoch: [53][   79/   79]    Loss 1.897041    Top1 48.750000    Top5 79.600000    
2022-01-29 10:35:09,711 - ==> Top1: 48.750    Top5: 79.600    Loss: 1.897

2022-01-29 10:35:09,717 - ==> Best [Top1: 51.400   Top5: 81.530   Sparsity:0.00   Params: 627712 on epoch: 52]
2022-01-29 10:35:09,717 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:35:09,762 - 

2022-01-29 10:35:09,762 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:35:15,269 - Epoch: [54][  100/  391]    Overall Loss 1.479535    Objective Loss 1.479535                                        LR 0.100000    Time 0.055043    
2022-01-29 10:35:20,493 - Epoch: [54][  200/  391]    Overall Loss 1.487331    Objective Loss 1.487331                                        LR 0.100000    Time 0.053635    
2022-01-29 10:35:25,648 - Epoch: [54][  300/  391]    Overall Loss 1.501824    Objective Loss 1.501824                                        LR 0.100000    Time 0.052939    
2022-01-29 10:35:30,298 - Epoch: [54][  391/  391]    Overall Loss 1.505712    Objective Loss 1.505712    Top1 54.807692    Top5 84.134615    LR 0.100000    Time 0.052507    
2022-01-29 10:35:30,356 - --- validate (epoch=54)-----------
2022-01-29 10:35:30,356 - 10000 samples (128 per mini-batch)
2022-01-29 10:35:31,995 - Epoch: [54][   79/   79]    Loss 1.780918    Top1 50.630000    Top5 81.140000    
2022-01-29 10:35:32,056 - ==> Top1: 50.630    Top5: 81.140    Loss: 1.781

2022-01-29 10:35:32,061 - ==> Best [Top1: 51.400   Top5: 81.530   Sparsity:0.00   Params: 627712 on epoch: 52]
2022-01-29 10:35:32,061 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:35:32,098 - 

2022-01-29 10:35:32,098 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:35:37,544 - Epoch: [55][  100/  391]    Overall Loss 1.465790    Objective Loss 1.465790                                        LR 0.100000    Time 0.054425    
2022-01-29 10:35:42,757 - Epoch: [55][  200/  391]    Overall Loss 1.480006    Objective Loss 1.480006                                        LR 0.100000    Time 0.053274    
2022-01-29 10:35:47,976 - Epoch: [55][  300/  391]    Overall Loss 1.495063    Objective Loss 1.495063                                        LR 0.100000    Time 0.052909    
2022-01-29 10:35:52,715 - Epoch: [55][  391/  391]    Overall Loss 1.500350    Objective Loss 1.500350    Top1 58.653846    Top5 84.615385    LR 0.100000    Time 0.052714    
2022-01-29 10:35:52,774 - --- validate (epoch=55)-----------
2022-01-29 10:35:52,774 - 10000 samples (128 per mini-batch)
2022-01-29 10:35:54,494 - Epoch: [55][   79/   79]    Loss 1.767244    Top1 51.680000    Top5 81.720000    
2022-01-29 10:35:54,548 - ==> Top1: 51.680    Top5: 81.720    Loss: 1.767

2022-01-29 10:35:54,554 - ==> Best [Top1: 51.680   Top5: 81.720   Sparsity:0.00   Params: 627712 on epoch: 55]
2022-01-29 10:35:54,554 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:35:54,603 - 

2022-01-29 10:35:54,604 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:36:00,018 - Epoch: [56][  100/  391]    Overall Loss 1.455769    Objective Loss 1.455769                                        LR 0.100000    Time 0.054120    
2022-01-29 10:36:05,219 - Epoch: [56][  200/  391]    Overall Loss 1.461842    Objective Loss 1.461842                                        LR 0.100000    Time 0.053058    
2022-01-29 10:36:10,477 - Epoch: [56][  300/  391]    Overall Loss 1.475997    Objective Loss 1.475997                                        LR 0.100000    Time 0.052897    
2022-01-29 10:36:15,263 - Epoch: [56][  391/  391]    Overall Loss 1.486155    Objective Loss 1.486155    Top1 54.807692    Top5 87.980769    LR 0.100000    Time 0.052824    
2022-01-29 10:36:15,320 - --- validate (epoch=56)-----------
2022-01-29 10:36:15,320 - 10000 samples (128 per mini-batch)
2022-01-29 10:36:17,068 - Epoch: [56][   79/   79]    Loss 2.021573    Top1 46.490000    Top5 77.660000    
2022-01-29 10:36:17,120 - ==> Top1: 46.490    Top5: 77.660    Loss: 2.022

2022-01-29 10:36:17,125 - ==> Best [Top1: 51.680   Top5: 81.720   Sparsity:0.00   Params: 627712 on epoch: 55]
2022-01-29 10:36:17,125 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:36:17,170 - 

2022-01-29 10:36:17,170 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:36:22,578 - Epoch: [57][  100/  391]    Overall Loss 1.431185    Objective Loss 1.431185                                        LR 0.100000    Time 0.054045    
2022-01-29 10:36:27,755 - Epoch: [57][  200/  391]    Overall Loss 1.462935    Objective Loss 1.462935                                        LR 0.100000    Time 0.052902    
2022-01-29 10:36:32,939 - Epoch: [57][  300/  391]    Overall Loss 1.471724    Objective Loss 1.471724                                        LR 0.100000    Time 0.052544    
2022-01-29 10:36:37,664 - Epoch: [57][  391/  391]    Overall Loss 1.470110    Objective Loss 1.470110    Top1 56.250000    Top5 85.096154    LR 0.100000    Time 0.052398    
2022-01-29 10:36:37,723 - --- validate (epoch=57)-----------
2022-01-29 10:36:37,723 - 10000 samples (128 per mini-batch)
2022-01-29 10:36:39,463 - Epoch: [57][   79/   79]    Loss 1.804883    Top1 50.380000    Top5 81.030000    
2022-01-29 10:36:39,521 - ==> Top1: 50.380    Top5: 81.030    Loss: 1.805

2022-01-29 10:36:39,527 - ==> Best [Top1: 51.680   Top5: 81.720   Sparsity:0.00   Params: 627712 on epoch: 55]
2022-01-29 10:36:39,527 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:36:39,566 - 

2022-01-29 10:36:39,566 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:36:45,083 - Epoch: [58][  100/  391]    Overall Loss 1.427977    Objective Loss 1.427977                                        LR 0.100000    Time 0.055142    
2022-01-29 10:36:50,323 - Epoch: [58][  200/  391]    Overall Loss 1.452715    Objective Loss 1.452715                                        LR 0.100000    Time 0.053764    
2022-01-29 10:36:55,553 - Epoch: [58][  300/  391]    Overall Loss 1.454681    Objective Loss 1.454681                                        LR 0.100000    Time 0.053274    
2022-01-29 10:37:00,308 - Epoch: [58][  391/  391]    Overall Loss 1.465877    Objective Loss 1.465877    Top1 56.730769    Top5 85.576923    LR 0.100000    Time 0.053033    
2022-01-29 10:37:00,368 - --- validate (epoch=58)-----------
2022-01-29 10:37:00,369 - 10000 samples (128 per mini-batch)
2022-01-29 10:37:02,035 - Epoch: [58][   79/   79]    Loss 1.915356    Top1 48.040000    Top5 79.290000    
2022-01-29 10:37:02,097 - ==> Top1: 48.040    Top5: 79.290    Loss: 1.915

2022-01-29 10:37:02,103 - ==> Best [Top1: 51.680   Top5: 81.720   Sparsity:0.00   Params: 627712 on epoch: 55]
2022-01-29 10:37:02,103 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:37:02,140 - 

2022-01-29 10:37:02,140 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:37:07,656 - Epoch: [59][  100/  391]    Overall Loss 1.429259    Objective Loss 1.429259                                        LR 0.100000    Time 0.055128    
2022-01-29 10:37:12,888 - Epoch: [59][  200/  391]    Overall Loss 1.442734    Objective Loss 1.442734                                        LR 0.100000    Time 0.053720    
2022-01-29 10:37:18,119 - Epoch: [59][  300/  391]    Overall Loss 1.459148    Objective Loss 1.459148                                        LR 0.100000    Time 0.053245    
2022-01-29 10:37:22,869 - Epoch: [59][  391/  391]    Overall Loss 1.463459    Objective Loss 1.463459    Top1 62.500000    Top5 87.019231    LR 0.100000    Time 0.053000    
2022-01-29 10:37:22,929 - --- validate (epoch=59)-----------
2022-01-29 10:37:22,929 - 10000 samples (128 per mini-batch)
2022-01-29 10:37:24,626 - Epoch: [59][   79/   79]    Loss 1.875027    Top1 49.150000    Top5 79.450000    
2022-01-29 10:37:24,685 - ==> Top1: 49.150    Top5: 79.450    Loss: 1.875

2022-01-29 10:37:24,691 - ==> Best [Top1: 51.680   Top5: 81.720   Sparsity:0.00   Params: 627712 on epoch: 55]
2022-01-29 10:37:24,691 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:37:24,736 - 

2022-01-29 10:37:24,736 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:37:30,189 - Epoch: [60][  100/  391]    Overall Loss 1.426132    Objective Loss 1.426132                                        LR 0.100000    Time 0.054503    
2022-01-29 10:37:35,475 - Epoch: [60][  200/  391]    Overall Loss 1.421892    Objective Loss 1.421892                                        LR 0.100000    Time 0.053677    
2022-01-29 10:37:40,766 - Epoch: [60][  300/  391]    Overall Loss 1.440806    Objective Loss 1.440806                                        LR 0.100000    Time 0.053419    
2022-01-29 10:37:45,566 - Epoch: [60][  391/  391]    Overall Loss 1.450611    Objective Loss 1.450611    Top1 57.692308    Top5 86.538462    LR 0.100000    Time 0.053261    
2022-01-29 10:37:45,625 - --- validate (epoch=60)-----------
2022-01-29 10:37:45,625 - 10000 samples (128 per mini-batch)
2022-01-29 10:37:47,319 - Epoch: [60][   79/   79]    Loss 2.119899    Top1 44.650000    Top5 76.390000    
2022-01-29 10:37:47,378 - ==> Top1: 44.650    Top5: 76.390    Loss: 2.120

2022-01-29 10:37:47,384 - ==> Best [Top1: 51.680   Top5: 81.720   Sparsity:0.00   Params: 627712 on epoch: 55]
2022-01-29 10:37:47,384 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:37:47,429 - 

2022-01-29 10:37:47,429 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:37:52,987 - Epoch: [61][  100/  391]    Overall Loss 1.412717    Objective Loss 1.412717                                        LR 0.100000    Time 0.055558    
2022-01-29 10:37:58,194 - Epoch: [61][  200/  391]    Overall Loss 1.430373    Objective Loss 1.430373                                        LR 0.100000    Time 0.053808    
2022-01-29 10:38:03,402 - Epoch: [61][  300/  391]    Overall Loss 1.441615    Objective Loss 1.441615                                        LR 0.100000    Time 0.053227    
2022-01-29 10:38:08,139 - Epoch: [61][  391/  391]    Overall Loss 1.446457    Objective Loss 1.446457    Top1 59.615385    Top5 87.019231    LR 0.100000    Time 0.052952    
2022-01-29 10:38:08,196 - --- validate (epoch=61)-----------
2022-01-29 10:38:08,196 - 10000 samples (128 per mini-batch)
2022-01-29 10:38:09,974 - Epoch: [61][   79/   79]    Loss 1.827099    Top1 49.930000    Top5 81.120000    
2022-01-29 10:38:10,032 - ==> Top1: 49.930    Top5: 81.120    Loss: 1.827

2022-01-29 10:38:10,038 - ==> Best [Top1: 51.680   Top5: 81.720   Sparsity:0.00   Params: 627712 on epoch: 55]
2022-01-29 10:38:10,038 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:38:10,083 - 

2022-01-29 10:38:10,083 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:38:15,560 - Epoch: [62][  100/  391]    Overall Loss 1.394851    Objective Loss 1.394851                                        LR 0.100000    Time 0.054743    
2022-01-29 10:38:20,747 - Epoch: [62][  200/  391]    Overall Loss 1.405618    Objective Loss 1.405618                                        LR 0.100000    Time 0.053297    
2022-01-29 10:38:25,951 - Epoch: [62][  300/  391]    Overall Loss 1.426008    Objective Loss 1.426008                                        LR 0.100000    Time 0.052874    
2022-01-29 10:38:30,678 - Epoch: [62][  391/  391]    Overall Loss 1.435311    Objective Loss 1.435311    Top1 56.250000    Top5 84.615385    LR 0.100000    Time 0.052657    
2022-01-29 10:38:30,735 - --- validate (epoch=62)-----------
2022-01-29 10:38:30,735 - 10000 samples (128 per mini-batch)
2022-01-29 10:38:32,427 - Epoch: [62][   79/   79]    Loss 1.754260    Top1 51.160000    Top5 82.000000    
2022-01-29 10:38:32,486 - ==> Top1: 51.160    Top5: 82.000    Loss: 1.754

2022-01-29 10:38:32,491 - ==> Best [Top1: 51.680   Top5: 81.720   Sparsity:0.00   Params: 627712 on epoch: 55]
2022-01-29 10:38:32,491 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:38:32,536 - 

2022-01-29 10:38:32,537 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:38:38,049 - Epoch: [63][  100/  391]    Overall Loss 1.394198    Objective Loss 1.394198                                        LR 0.100000    Time 0.055093    
2022-01-29 10:38:43,351 - Epoch: [63][  200/  391]    Overall Loss 1.397059    Objective Loss 1.397059                                        LR 0.100000    Time 0.054056    
2022-01-29 10:38:48,658 - Epoch: [63][  300/  391]    Overall Loss 1.417976    Objective Loss 1.417976                                        LR 0.100000    Time 0.053724    
2022-01-29 10:38:53,483 - Epoch: [63][  391/  391]    Overall Loss 1.428847    Objective Loss 1.428847    Top1 58.653846    Top5 83.173077    LR 0.100000    Time 0.053558    
2022-01-29 10:38:53,540 - --- validate (epoch=63)-----------
2022-01-29 10:38:53,540 - 10000 samples (128 per mini-batch)
2022-01-29 10:38:55,289 - Epoch: [63][   79/   79]    Loss 1.944729    Top1 48.070000    Top5 79.320000    
2022-01-29 10:38:55,347 - ==> Top1: 48.070    Top5: 79.320    Loss: 1.945

2022-01-29 10:38:55,353 - ==> Best [Top1: 51.680   Top5: 81.720   Sparsity:0.00   Params: 627712 on epoch: 55]
2022-01-29 10:38:55,353 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:38:55,391 - 

2022-01-29 10:38:55,391 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:39:00,934 - Epoch: [64][  100/  391]    Overall Loss 1.385273    Objective Loss 1.385273                                        LR 0.100000    Time 0.055409    
2022-01-29 10:39:06,199 - Epoch: [64][  200/  391]    Overall Loss 1.412788    Objective Loss 1.412788                                        LR 0.100000    Time 0.054023    
2022-01-29 10:39:11,453 - Epoch: [64][  300/  391]    Overall Loss 1.416603    Objective Loss 1.416603                                        LR 0.100000    Time 0.053525    
2022-01-29 10:39:16,226 - Epoch: [64][  391/  391]    Overall Loss 1.424146    Objective Loss 1.424146    Top1 63.942308    Top5 92.788462    LR 0.100000    Time 0.053273    
2022-01-29 10:39:16,285 - --- validate (epoch=64)-----------
2022-01-29 10:39:16,285 - 10000 samples (128 per mini-batch)
2022-01-29 10:39:17,950 - Epoch: [64][   79/   79]    Loss 1.897615    Top1 48.910000    Top5 79.480000    
2022-01-29 10:39:18,007 - ==> Top1: 48.910    Top5: 79.480    Loss: 1.898

2022-01-29 10:39:18,012 - ==> Best [Top1: 51.680   Top5: 81.720   Sparsity:0.00   Params: 627712 on epoch: 55]
2022-01-29 10:39:18,012 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:39:18,052 - 

2022-01-29 10:39:18,052 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:39:23,601 - Epoch: [65][  100/  391]    Overall Loss 1.370410    Objective Loss 1.370410                                        LR 0.100000    Time 0.055459    
2022-01-29 10:39:28,903 - Epoch: [65][  200/  391]    Overall Loss 1.389671    Objective Loss 1.389671                                        LR 0.100000    Time 0.054235    
2022-01-29 10:39:34,143 - Epoch: [65][  300/  391]    Overall Loss 1.409024    Objective Loss 1.409024                                        LR 0.100000    Time 0.053622    
2022-01-29 10:39:38,903 - Epoch: [65][  391/  391]    Overall Loss 1.419689    Objective Loss 1.419689    Top1 56.250000    Top5 83.653846    LR 0.100000    Time 0.053313    
2022-01-29 10:39:38,961 - --- validate (epoch=65)-----------
2022-01-29 10:39:38,961 - 10000 samples (128 per mini-batch)
2022-01-29 10:39:40,624 - Epoch: [65][   79/   79]    Loss 1.853676    Top1 50.090000    Top5 80.920000    
2022-01-29 10:39:40,680 - ==> Top1: 50.090    Top5: 80.920    Loss: 1.854

2022-01-29 10:39:40,686 - ==> Best [Top1: 51.680   Top5: 81.720   Sparsity:0.00   Params: 627712 on epoch: 55]
2022-01-29 10:39:40,686 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:39:40,732 - 

2022-01-29 10:39:40,732 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:39:46,233 - Epoch: [66][  100/  391]    Overall Loss 1.378727    Objective Loss 1.378727                                        LR 0.100000    Time 0.054982    
2022-01-29 10:39:51,477 - Epoch: [66][  200/  391]    Overall Loss 1.392281    Objective Loss 1.392281                                        LR 0.100000    Time 0.053707    
2022-01-29 10:39:56,721 - Epoch: [66][  300/  391]    Overall Loss 1.403429    Objective Loss 1.403429                                        LR 0.100000    Time 0.053281    
2022-01-29 10:40:01,494 - Epoch: [66][  391/  391]    Overall Loss 1.406361    Objective Loss 1.406361    Top1 51.442308    Top5 85.096154    LR 0.100000    Time 0.053086    
2022-01-29 10:40:01,554 - --- validate (epoch=66)-----------
2022-01-29 10:40:01,554 - 10000 samples (128 per mini-batch)
2022-01-29 10:40:03,212 - Epoch: [66][   79/   79]    Loss 1.876429    Top1 49.160000    Top5 80.180000    
2022-01-29 10:40:03,271 - ==> Top1: 49.160    Top5: 80.180    Loss: 1.876

2022-01-29 10:40:03,276 - ==> Best [Top1: 51.680   Top5: 81.720   Sparsity:0.00   Params: 627712 on epoch: 55]
2022-01-29 10:40:03,277 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:40:03,320 - 

2022-01-29 10:40:03,321 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:40:08,774 - Epoch: [67][  100/  391]    Overall Loss 1.370210    Objective Loss 1.370210                                        LR 0.100000    Time 0.054505    
2022-01-29 10:40:14,004 - Epoch: [67][  200/  391]    Overall Loss 1.372750    Objective Loss 1.372750                                        LR 0.100000    Time 0.053399    
2022-01-29 10:40:19,229 - Epoch: [67][  300/  391]    Overall Loss 1.389763    Objective Loss 1.389763                                        LR 0.100000    Time 0.053015    
2022-01-29 10:40:23,985 - Epoch: [67][  391/  391]    Overall Loss 1.400467    Objective Loss 1.400467    Top1 67.307692    Top5 87.980769    LR 0.100000    Time 0.052839    
2022-01-29 10:40:24,039 - --- validate (epoch=67)-----------
2022-01-29 10:40:24,039 - 10000 samples (128 per mini-batch)
2022-01-29 10:40:25,753 - Epoch: [67][   79/   79]    Loss 1.746974    Top1 52.130000    Top5 82.080000    
2022-01-29 10:40:25,807 - ==> Top1: 52.130    Top5: 82.080    Loss: 1.747

2022-01-29 10:40:25,812 - ==> Best [Top1: 52.130   Top5: 82.080   Sparsity:0.00   Params: 627712 on epoch: 67]
2022-01-29 10:40:25,812 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:40:25,861 - 

2022-01-29 10:40:25,861 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:40:31,347 - Epoch: [68][  100/  391]    Overall Loss 1.369863    Objective Loss 1.369863                                        LR 0.100000    Time 0.054830    
2022-01-29 10:40:36,566 - Epoch: [68][  200/  391]    Overall Loss 1.383261    Objective Loss 1.383261                                        LR 0.100000    Time 0.053510    
2022-01-29 10:40:41,801 - Epoch: [68][  300/  391]    Overall Loss 1.386348    Objective Loss 1.386348                                        LR 0.100000    Time 0.053119    
2022-01-29 10:40:46,546 - Epoch: [68][  391/  391]    Overall Loss 1.390423    Objective Loss 1.390423    Top1 63.942308    Top5 90.865385    LR 0.100000    Time 0.052890    
2022-01-29 10:40:46,606 - --- validate (epoch=68)-----------
2022-01-29 10:40:46,606 - 10000 samples (128 per mini-batch)
2022-01-29 10:40:48,274 - Epoch: [68][   79/   79]    Loss 1.887251    Top1 49.050000    Top5 80.230000    
2022-01-29 10:40:48,333 - ==> Top1: 49.050    Top5: 80.230    Loss: 1.887

2022-01-29 10:40:48,338 - ==> Best [Top1: 52.130   Top5: 82.080   Sparsity:0.00   Params: 627712 on epoch: 67]
2022-01-29 10:40:48,338 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:40:48,382 - 

2022-01-29 10:40:48,382 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:40:53,910 - Epoch: [69][  100/  391]    Overall Loss 1.342888    Objective Loss 1.342888                                        LR 0.100000    Time 0.055252    
2022-01-29 10:40:59,201 - Epoch: [69][  200/  391]    Overall Loss 1.355793    Objective Loss 1.355793                                        LR 0.100000    Time 0.054077    
2022-01-29 10:41:04,476 - Epoch: [69][  300/  391]    Overall Loss 1.378960    Objective Loss 1.378960                                        LR 0.100000    Time 0.053632    
2022-01-29 10:41:09,237 - Epoch: [69][  391/  391]    Overall Loss 1.390814    Objective Loss 1.390814    Top1 57.211538    Top5 87.980769    LR 0.100000    Time 0.053326    
2022-01-29 10:41:09,298 - --- validate (epoch=69)-----------
2022-01-29 10:41:09,298 - 10000 samples (128 per mini-batch)
2022-01-29 10:41:11,004 - Epoch: [69][   79/   79]    Loss 1.701273    Top1 53.030000    Top5 83.160000    
2022-01-29 10:41:11,060 - ==> Top1: 53.030    Top5: 83.160    Loss: 1.701

2022-01-29 10:41:11,065 - ==> Best [Top1: 53.030   Top5: 83.160   Sparsity:0.00   Params: 627712 on epoch: 69]
2022-01-29 10:41:11,065 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:41:11,109 - 

2022-01-29 10:41:11,109 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:41:16,592 - Epoch: [70][  100/  391]    Overall Loss 1.357155    Objective Loss 1.357155                                        LR 0.100000    Time 0.054804    
2022-01-29 10:41:21,855 - Epoch: [70][  200/  391]    Overall Loss 1.362114    Objective Loss 1.362114                                        LR 0.100000    Time 0.053708    
2022-01-29 10:41:27,102 - Epoch: [70][  300/  391]    Overall Loss 1.385364    Objective Loss 1.385364                                        LR 0.100000    Time 0.053292    
2022-01-29 10:41:31,900 - Epoch: [70][  391/  391]    Overall Loss 1.386813    Objective Loss 1.386813    Top1 62.500000    Top5 91.346154    LR 0.100000    Time 0.053160    
2022-01-29 10:41:31,956 - --- validate (epoch=70)-----------
2022-01-29 10:41:31,956 - 10000 samples (128 per mini-batch)
2022-01-29 10:41:33,689 - Epoch: [70][   79/   79]    Loss 1.866671    Top1 49.310000    Top5 80.020000    
2022-01-29 10:41:33,748 - ==> Top1: 49.310    Top5: 80.020    Loss: 1.867

2022-01-29 10:41:33,753 - ==> Best [Top1: 53.030   Top5: 83.160   Sparsity:0.00   Params: 627712 on epoch: 69]
2022-01-29 10:41:33,753 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:41:33,791 - 

2022-01-29 10:41:33,791 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:41:39,286 - Epoch: [71][  100/  391]    Overall Loss 1.313213    Objective Loss 1.313213                                        LR 0.100000    Time 0.054919    
2022-01-29 10:41:44,552 - Epoch: [71][  200/  391]    Overall Loss 1.343344    Objective Loss 1.343344                                        LR 0.100000    Time 0.053787    
2022-01-29 10:41:49,831 - Epoch: [71][  300/  391]    Overall Loss 1.356156    Objective Loss 1.356156                                        LR 0.100000    Time 0.053453    
2022-01-29 10:41:54,568 - Epoch: [71][  391/  391]    Overall Loss 1.374279    Objective Loss 1.374279    Top1 62.500000    Top5 91.826923    LR 0.100000    Time 0.053126    
2022-01-29 10:41:54,624 - --- validate (epoch=71)-----------
2022-01-29 10:41:54,625 - 10000 samples (128 per mini-batch)
2022-01-29 10:41:56,294 - Epoch: [71][   79/   79]    Loss 1.800579    Top1 51.130000    Top5 81.460000    
2022-01-29 10:41:56,346 - ==> Top1: 51.130    Top5: 81.460    Loss: 1.801

2022-01-29 10:41:56,351 - ==> Best [Top1: 53.030   Top5: 83.160   Sparsity:0.00   Params: 627712 on epoch: 69]
2022-01-29 10:41:56,351 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:41:56,396 - 

2022-01-29 10:41:56,396 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:42:01,884 - Epoch: [72][  100/  391]    Overall Loss 1.334514    Objective Loss 1.334514                                        LR 0.100000    Time 0.054855    
2022-01-29 10:42:07,060 - Epoch: [72][  200/  391]    Overall Loss 1.333335    Objective Loss 1.333335                                        LR 0.100000    Time 0.053301    
2022-01-29 10:42:12,265 - Epoch: [72][  300/  391]    Overall Loss 1.360502    Objective Loss 1.360502                                        LR 0.100000    Time 0.052882    
2022-01-29 10:42:16,958 - Epoch: [72][  391/  391]    Overall Loss 1.369622    Objective Loss 1.369622    Top1 52.403846    Top5 86.538462    LR 0.100000    Time 0.052575    
2022-01-29 10:42:17,016 - --- validate (epoch=72)-----------
2022-01-29 10:42:17,017 - 10000 samples (128 per mini-batch)
2022-01-29 10:42:18,638 - Epoch: [72][   79/   79]    Loss 1.789140    Top1 51.700000    Top5 81.220000    
2022-01-29 10:42:18,697 - ==> Top1: 51.700    Top5: 81.220    Loss: 1.789

2022-01-29 10:42:18,702 - ==> Best [Top1: 53.030   Top5: 83.160   Sparsity:0.00   Params: 627712 on epoch: 69]
2022-01-29 10:42:18,703 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:42:18,747 - 

2022-01-29 10:42:18,747 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:42:24,190 - Epoch: [73][  100/  391]    Overall Loss 1.333006    Objective Loss 1.333006                                        LR 0.100000    Time 0.054396    
2022-01-29 10:42:29,284 - Epoch: [73][  200/  391]    Overall Loss 1.335135    Objective Loss 1.335135                                        LR 0.100000    Time 0.052664    
2022-01-29 10:42:34,454 - Epoch: [73][  300/  391]    Overall Loss 1.353802    Objective Loss 1.353802                                        LR 0.100000    Time 0.052339    
2022-01-29 10:42:39,212 - Epoch: [73][  391/  391]    Overall Loss 1.365022    Objective Loss 1.365022    Top1 62.019231    Top5 89.903846    LR 0.100000    Time 0.052325    
2022-01-29 10:42:39,271 - --- validate (epoch=73)-----------
2022-01-29 10:42:39,271 - 10000 samples (128 per mini-batch)
2022-01-29 10:42:40,984 - Epoch: [73][   79/   79]    Loss 1.736698    Top1 52.720000    Top5 82.340000    
2022-01-29 10:42:41,038 - ==> Top1: 52.720    Top5: 82.340    Loss: 1.737

2022-01-29 10:42:41,043 - ==> Best [Top1: 53.030   Top5: 83.160   Sparsity:0.00   Params: 627712 on epoch: 69]
2022-01-29 10:42:41,043 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:42:41,088 - 

2022-01-29 10:42:41,088 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:42:46,525 - Epoch: [74][  100/  391]    Overall Loss 1.314935    Objective Loss 1.314935                                        LR 0.100000    Time 0.054344    
2022-01-29 10:42:51,727 - Epoch: [74][  200/  391]    Overall Loss 1.339777    Objective Loss 1.339777                                        LR 0.100000    Time 0.053177    
2022-01-29 10:42:56,943 - Epoch: [74][  300/  391]    Overall Loss 1.351673    Objective Loss 1.351673                                        LR 0.100000    Time 0.052836    
2022-01-29 10:43:01,637 - Epoch: [74][  391/  391]    Overall Loss 1.357422    Objective Loss 1.357422    Top1 62.019231    Top5 90.384615    LR 0.100000    Time 0.052541    
2022-01-29 10:43:01,702 - --- validate (epoch=74)-----------
2022-01-29 10:43:01,703 - 10000 samples (128 per mini-batch)
2022-01-29 10:43:03,348 - Epoch: [74][   79/   79]    Loss 1.901880    Top1 49.210000    Top5 80.350000    
2022-01-29 10:43:03,399 - ==> Top1: 49.210    Top5: 80.350    Loss: 1.902

2022-01-29 10:43:03,404 - ==> Best [Top1: 53.030   Top5: 83.160   Sparsity:0.00   Params: 627712 on epoch: 69]
2022-01-29 10:43:03,405 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:43:03,448 - 

2022-01-29 10:43:03,449 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:43:08,875 - Epoch: [75][  100/  391]    Overall Loss 1.332027    Objective Loss 1.332027                                        LR 0.100000    Time 0.054239    
2022-01-29 10:43:14,044 - Epoch: [75][  200/  391]    Overall Loss 1.348071    Objective Loss 1.348071                                        LR 0.100000    Time 0.052963    
2022-01-29 10:43:19,283 - Epoch: [75][  300/  391]    Overall Loss 1.353630    Objective Loss 1.353630                                        LR 0.100000    Time 0.052767    
2022-01-29 10:43:23,980 - Epoch: [75][  391/  391]    Overall Loss 1.356393    Objective Loss 1.356393    Top1 61.538462    Top5 88.461538    LR 0.100000    Time 0.052497    
2022-01-29 10:43:24,039 - --- validate (epoch=75)-----------
2022-01-29 10:43:24,039 - 10000 samples (128 per mini-batch)
2022-01-29 10:43:25,698 - Epoch: [75][   79/   79]    Loss 1.692827    Top1 53.380000    Top5 83.150000    
2022-01-29 10:43:25,755 - ==> Top1: 53.380    Top5: 83.150    Loss: 1.693

2022-01-29 10:43:25,760 - ==> Best [Top1: 53.380   Top5: 83.150   Sparsity:0.00   Params: 627712 on epoch: 75]
2022-01-29 10:43:25,760 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:43:25,809 - 

2022-01-29 10:43:25,809 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:43:31,225 - Epoch: [76][  100/  391]    Overall Loss 1.300823    Objective Loss 1.300823                                        LR 0.100000    Time 0.054130    
2022-01-29 10:43:36,355 - Epoch: [76][  200/  391]    Overall Loss 1.332834    Objective Loss 1.332834                                        LR 0.100000    Time 0.052711    
2022-01-29 10:43:41,486 - Epoch: [76][  300/  391]    Overall Loss 1.345544    Objective Loss 1.345544                                        LR 0.100000    Time 0.052240    
2022-01-29 10:43:46,178 - Epoch: [76][  391/  391]    Overall Loss 1.358003    Objective Loss 1.358003    Top1 61.538462    Top5 88.461538    LR 0.100000    Time 0.052082    
2022-01-29 10:43:46,238 - --- validate (epoch=76)-----------
2022-01-29 10:43:46,238 - 10000 samples (128 per mini-batch)
2022-01-29 10:43:47,920 - Epoch: [76][   79/   79]    Loss 2.023743    Top1 46.360000    Top5 77.650000    
2022-01-29 10:43:47,973 - ==> Top1: 46.360    Top5: 77.650    Loss: 2.024

2022-01-29 10:43:47,979 - ==> Best [Top1: 53.380   Top5: 83.150   Sparsity:0.00   Params: 627712 on epoch: 75]
2022-01-29 10:43:47,979 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:43:48,024 - 

2022-01-29 10:43:48,024 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:43:53,468 - Epoch: [77][  100/  391]    Overall Loss 1.278687    Objective Loss 1.278687                                        LR 0.100000    Time 0.054412    
2022-01-29 10:43:58,724 - Epoch: [77][  200/  391]    Overall Loss 1.321069    Objective Loss 1.321069                                        LR 0.100000    Time 0.053476    
2022-01-29 10:44:04,036 - Epoch: [77][  300/  391]    Overall Loss 1.330803    Objective Loss 1.330803                                        LR 0.100000    Time 0.053356    
2022-01-29 10:44:08,862 - Epoch: [77][  391/  391]    Overall Loss 1.343158    Objective Loss 1.343158    Top1 61.057692    Top5 93.269231    LR 0.100000    Time 0.053280    
2022-01-29 10:44:08,922 - --- validate (epoch=77)-----------
2022-01-29 10:44:08,922 - 10000 samples (128 per mini-batch)
2022-01-29 10:44:10,670 - Epoch: [77][   79/   79]    Loss 1.742445    Top1 52.290000    Top5 82.010000    
2022-01-29 10:44:10,725 - ==> Top1: 52.290    Top5: 82.010    Loss: 1.742

2022-01-29 10:44:10,731 - ==> Best [Top1: 53.380   Top5: 83.150   Sparsity:0.00   Params: 627712 on epoch: 75]
2022-01-29 10:44:10,731 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:44:10,774 - 

2022-01-29 10:44:10,774 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:44:16,127 - Epoch: [78][  100/  391]    Overall Loss 1.291758    Objective Loss 1.291758                                        LR 0.100000    Time 0.053494    
2022-01-29 10:44:21,321 - Epoch: [78][  200/  391]    Overall Loss 1.311134    Objective Loss 1.311134                                        LR 0.100000    Time 0.052712    
2022-01-29 10:44:26,523 - Epoch: [78][  300/  391]    Overall Loss 1.327046    Objective Loss 1.327046                                        LR 0.100000    Time 0.052480    
2022-01-29 10:44:31,169 - Epoch: [78][  391/  391]    Overall Loss 1.330298    Objective Loss 1.330298    Top1 62.980769    Top5 92.307692    LR 0.100000    Time 0.052146    
2022-01-29 10:44:31,228 - --- validate (epoch=78)-----------
2022-01-29 10:44:31,228 - 10000 samples (128 per mini-batch)
2022-01-29 10:44:32,883 - Epoch: [78][   79/   79]    Loss 1.737442    Top1 51.750000    Top5 82.370000    
2022-01-29 10:44:32,935 - ==> Top1: 51.750    Top5: 82.370    Loss: 1.737

2022-01-29 10:44:32,940 - ==> Best [Top1: 53.380   Top5: 83.150   Sparsity:0.00   Params: 627712 on epoch: 75]
2022-01-29 10:44:32,940 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:44:32,984 - 

2022-01-29 10:44:32,984 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:44:38,365 - Epoch: [79][  100/  391]    Overall Loss 1.295993    Objective Loss 1.295993                                        LR 0.100000    Time 0.053777    
2022-01-29 10:44:43,470 - Epoch: [79][  200/  391]    Overall Loss 1.312576    Objective Loss 1.312576                                        LR 0.100000    Time 0.052409    
2022-01-29 10:44:48,571 - Epoch: [79][  300/  391]    Overall Loss 1.326115    Objective Loss 1.326115                                        LR 0.100000    Time 0.051940    
2022-01-29 10:44:53,236 - Epoch: [79][  391/  391]    Overall Loss 1.335443    Objective Loss 1.335443    Top1 59.615385    Top5 88.942308    LR 0.100000    Time 0.051779    
2022-01-29 10:44:53,292 - --- validate (epoch=79)-----------
2022-01-29 10:44:53,293 - 10000 samples (128 per mini-batch)
2022-01-29 10:44:54,947 - Epoch: [79][   79/   79]    Loss 1.681212    Top1 53.560000    Top5 83.530000    
2022-01-29 10:44:55,005 - ==> Top1: 53.560    Top5: 83.530    Loss: 1.681

2022-01-29 10:44:55,010 - ==> Best [Top1: 53.560   Top5: 83.530   Sparsity:0.00   Params: 627712 on epoch: 79]
2022-01-29 10:44:55,010 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:44:55,058 - 

2022-01-29 10:44:55,058 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:45:00,514 - Epoch: [80][  100/  391]    Overall Loss 1.299686    Objective Loss 1.299686                                        LR 0.100000    Time 0.054534    
2022-01-29 10:45:05,707 - Epoch: [80][  200/  391]    Overall Loss 1.304269    Objective Loss 1.304269                                        LR 0.100000    Time 0.053227    
2022-01-29 10:45:10,904 - Epoch: [80][  300/  391]    Overall Loss 1.327817    Objective Loss 1.327817                                        LR 0.100000    Time 0.052803    
2022-01-29 10:45:15,628 - Epoch: [80][  391/  391]    Overall Loss 1.337555    Objective Loss 1.337555    Top1 57.211538    Top5 86.057692    LR 0.100000    Time 0.052596    
2022-01-29 10:45:15,686 - --- validate (epoch=80)-----------
2022-01-29 10:45:15,686 - 10000 samples (128 per mini-batch)
2022-01-29 10:45:17,395 - Epoch: [80][   79/   79]    Loss 1.743675    Top1 52.180000    Top5 82.200000    
2022-01-29 10:45:17,448 - ==> Top1: 52.180    Top5: 82.200    Loss: 1.744

2022-01-29 10:45:17,454 - ==> Best [Top1: 53.560   Top5: 83.530   Sparsity:0.00   Params: 627712 on epoch: 79]
2022-01-29 10:45:17,454 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:45:17,491 - 

2022-01-29 10:45:17,491 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:45:22,935 - Epoch: [81][  100/  391]    Overall Loss 1.258777    Objective Loss 1.258777                                        LR 0.100000    Time 0.054407    
2022-01-29 10:45:28,139 - Epoch: [81][  200/  391]    Overall Loss 1.292829    Objective Loss 1.292829                                        LR 0.100000    Time 0.053223    
2022-01-29 10:45:33,404 - Epoch: [81][  300/  391]    Overall Loss 1.312830    Objective Loss 1.312830                                        LR 0.100000    Time 0.053026    
2022-01-29 10:45:38,156 - Epoch: [81][  391/  391]    Overall Loss 1.319573    Objective Loss 1.319573    Top1 60.576923    Top5 85.576923    LR 0.100000    Time 0.052836    
2022-01-29 10:45:38,215 - --- validate (epoch=81)-----------
2022-01-29 10:45:38,216 - 10000 samples (128 per mini-batch)
2022-01-29 10:45:39,868 - Epoch: [81][   79/   79]    Loss 1.765239    Top1 51.650000    Top5 82.530000    
2022-01-29 10:45:39,927 - ==> Top1: 51.650    Top5: 82.530    Loss: 1.765

2022-01-29 10:45:39,933 - ==> Best [Top1: 53.560   Top5: 83.530   Sparsity:0.00   Params: 627712 on epoch: 79]
2022-01-29 10:45:39,933 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:45:39,978 - 

2022-01-29 10:45:39,978 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:45:45,566 - Epoch: [82][  100/  391]    Overall Loss 1.292610    Objective Loss 1.292610                                        LR 0.100000    Time 0.055850    
2022-01-29 10:45:50,830 - Epoch: [82][  200/  391]    Overall Loss 1.308805    Objective Loss 1.308805                                        LR 0.100000    Time 0.054240    
2022-01-29 10:45:56,094 - Epoch: [82][  300/  391]    Overall Loss 1.313823    Objective Loss 1.313823                                        LR 0.100000    Time 0.053703    
2022-01-29 10:46:00,853 - Epoch: [82][  391/  391]    Overall Loss 1.319743    Objective Loss 1.319743    Top1 67.788462    Top5 91.346154    LR 0.100000    Time 0.053373    
2022-01-29 10:46:00,910 - --- validate (epoch=82)-----------
2022-01-29 10:46:00,910 - 10000 samples (128 per mini-batch)
2022-01-29 10:46:02,776 - Epoch: [82][   79/   79]    Loss 1.789029    Top1 51.870000    Top5 82.420000    
2022-01-29 10:46:02,830 - ==> Top1: 51.870    Top5: 82.420    Loss: 1.789

2022-01-29 10:46:02,835 - ==> Best [Top1: 53.560   Top5: 83.530   Sparsity:0.00   Params: 627712 on epoch: 79]
2022-01-29 10:46:02,835 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:46:02,880 - 

2022-01-29 10:46:02,881 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:46:08,432 - Epoch: [83][  100/  391]    Overall Loss 1.280288    Objective Loss 1.280288                                        LR 0.100000    Time 0.055484    
2022-01-29 10:46:13,710 - Epoch: [83][  200/  391]    Overall Loss 1.294669    Objective Loss 1.294669                                        LR 0.100000    Time 0.054127    
2022-01-29 10:46:18,945 - Epoch: [83][  300/  391]    Overall Loss 1.313992    Objective Loss 1.313992                                        LR 0.100000    Time 0.053532    
2022-01-29 10:46:23,744 - Epoch: [83][  391/  391]    Overall Loss 1.318801    Objective Loss 1.318801    Top1 64.423077    Top5 90.865385    LR 0.100000    Time 0.053347    
2022-01-29 10:46:23,804 - --- validate (epoch=83)-----------
2022-01-29 10:46:23,804 - 10000 samples (128 per mini-batch)
2022-01-29 10:46:25,509 - Epoch: [83][   79/   79]    Loss 1.730728    Top1 52.690000    Top5 83.270000    
2022-01-29 10:46:25,563 - ==> Top1: 52.690    Top5: 83.270    Loss: 1.731

2022-01-29 10:46:25,569 - ==> Best [Top1: 53.560   Top5: 83.530   Sparsity:0.00   Params: 627712 on epoch: 79]
2022-01-29 10:46:25,569 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:46:25,614 - 

2022-01-29 10:46:25,614 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:46:31,124 - Epoch: [84][  100/  391]    Overall Loss 1.243247    Objective Loss 1.243247                                        LR 0.100000    Time 0.055069    
2022-01-29 10:46:36,356 - Epoch: [84][  200/  391]    Overall Loss 1.257695    Objective Loss 1.257695                                        LR 0.100000    Time 0.053691    
2022-01-29 10:46:41,612 - Epoch: [84][  300/  391]    Overall Loss 1.285146    Objective Loss 1.285146                                        LR 0.100000    Time 0.053311    
2022-01-29 10:46:46,367 - Epoch: [84][  391/  391]    Overall Loss 1.303331    Objective Loss 1.303331    Top1 59.134615    Top5 86.057692    LR 0.100000    Time 0.053063    
2022-01-29 10:46:46,425 - --- validate (epoch=84)-----------
2022-01-29 10:46:46,425 - 10000 samples (128 per mini-batch)
2022-01-29 10:46:48,152 - Epoch: [84][   79/   79]    Loss 1.683473    Top1 53.790000    Top5 83.380000    
2022-01-29 10:46:48,211 - ==> Top1: 53.790    Top5: 83.380    Loss: 1.683

2022-01-29 10:46:48,217 - ==> Best [Top1: 53.790   Top5: 83.380   Sparsity:0.00   Params: 627712 on epoch: 84]
2022-01-29 10:46:48,217 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:46:48,259 - 

2022-01-29 10:46:48,260 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:46:53,712 - Epoch: [85][  100/  391]    Overall Loss 1.255372    Objective Loss 1.255372                                        LR 0.100000    Time 0.054494    
2022-01-29 10:46:58,932 - Epoch: [85][  200/  391]    Overall Loss 1.288792    Objective Loss 1.288792                                        LR 0.100000    Time 0.053345    
2022-01-29 10:47:04,091 - Epoch: [85][  300/  391]    Overall Loss 1.300403    Objective Loss 1.300403                                        LR 0.100000    Time 0.052757    
2022-01-29 10:47:08,751 - Epoch: [85][  391/  391]    Overall Loss 1.303233    Objective Loss 1.303233    Top1 66.826923    Top5 91.346154    LR 0.100000    Time 0.052395    
2022-01-29 10:47:08,809 - --- validate (epoch=85)-----------
2022-01-29 10:47:08,809 - 10000 samples (128 per mini-batch)
2022-01-29 10:47:10,456 - Epoch: [85][   79/   79]    Loss 1.765341    Top1 51.500000    Top5 81.890000    
2022-01-29 10:47:10,517 - ==> Top1: 51.500    Top5: 81.890    Loss: 1.765

2022-01-29 10:47:10,522 - ==> Best [Top1: 53.790   Top5: 83.380   Sparsity:0.00   Params: 627712 on epoch: 84]
2022-01-29 10:47:10,522 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:47:10,566 - 

2022-01-29 10:47:10,566 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:47:15,997 - Epoch: [86][  100/  391]    Overall Loss 1.240888    Objective Loss 1.240888                                        LR 0.100000    Time 0.054284    
2022-01-29 10:47:21,118 - Epoch: [86][  200/  391]    Overall Loss 1.272878    Objective Loss 1.272878                                        LR 0.100000    Time 0.052743    
2022-01-29 10:47:26,227 - Epoch: [86][  300/  391]    Overall Loss 1.288780    Objective Loss 1.288780                                        LR 0.100000    Time 0.052186    
2022-01-29 10:47:30,864 - Epoch: [86][  391/  391]    Overall Loss 1.295597    Objective Loss 1.295597    Top1 57.211538    Top5 87.500000    LR 0.100000    Time 0.051898    
2022-01-29 10:47:30,923 - --- validate (epoch=86)-----------
2022-01-29 10:47:30,923 - 10000 samples (128 per mini-batch)
2022-01-29 10:47:32,578 - Epoch: [86][   79/   79]    Loss 1.906629    Top1 49.170000    Top5 80.110000    
2022-01-29 10:47:32,634 - ==> Top1: 49.170    Top5: 80.110    Loss: 1.907

2022-01-29 10:47:32,639 - ==> Best [Top1: 53.790   Top5: 83.380   Sparsity:0.00   Params: 627712 on epoch: 84]
2022-01-29 10:47:32,639 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:47:32,683 - 

2022-01-29 10:47:32,683 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:47:38,059 - Epoch: [87][  100/  391]    Overall Loss 1.245836    Objective Loss 1.245836                                        LR 0.100000    Time 0.053731    
2022-01-29 10:47:43,146 - Epoch: [87][  200/  391]    Overall Loss 1.271177    Objective Loss 1.271177                                        LR 0.100000    Time 0.052297    
2022-01-29 10:47:48,233 - Epoch: [87][  300/  391]    Overall Loss 1.289484    Objective Loss 1.289484                                        LR 0.100000    Time 0.051818    
2022-01-29 10:47:52,859 - Epoch: [87][  391/  391]    Overall Loss 1.296531    Objective Loss 1.296531    Top1 57.211538    Top5 89.903846    LR 0.100000    Time 0.051585    
2022-01-29 10:47:52,917 - --- validate (epoch=87)-----------
2022-01-29 10:47:52,918 - 10000 samples (128 per mini-batch)
2022-01-29 10:47:54,572 - Epoch: [87][   79/   79]    Loss 1.769005    Top1 51.430000    Top5 81.770000    
2022-01-29 10:47:54,627 - ==> Top1: 51.430    Top5: 81.770    Loss: 1.769

2022-01-29 10:47:54,633 - ==> Best [Top1: 53.790   Top5: 83.380   Sparsity:0.00   Params: 627712 on epoch: 84]
2022-01-29 10:47:54,633 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:47:54,677 - 

2022-01-29 10:47:54,677 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:48:00,058 - Epoch: [88][  100/  391]    Overall Loss 1.249871    Objective Loss 1.249871                                        LR 0.100000    Time 0.053789    
2022-01-29 10:48:05,267 - Epoch: [88][  200/  391]    Overall Loss 1.264432    Objective Loss 1.264432                                        LR 0.100000    Time 0.052935    
2022-01-29 10:48:10,490 - Epoch: [88][  300/  391]    Overall Loss 1.270671    Objective Loss 1.270671                                        LR 0.100000    Time 0.052696    
2022-01-29 10:48:15,172 - Epoch: [88][  391/  391]    Overall Loss 1.286097    Objective Loss 1.286097    Top1 63.461538    Top5 88.461538    LR 0.100000    Time 0.052403    
2022-01-29 10:48:15,223 - --- validate (epoch=88)-----------
2022-01-29 10:48:15,223 - 10000 samples (128 per mini-batch)
2022-01-29 10:48:16,857 - Epoch: [88][   79/   79]    Loss 1.982724    Top1 47.810000    Top5 79.540000    
2022-01-29 10:48:16,916 - ==> Top1: 47.810    Top5: 79.540    Loss: 1.983

2022-01-29 10:48:16,921 - ==> Best [Top1: 53.790   Top5: 83.380   Sparsity:0.00   Params: 627712 on epoch: 84]
2022-01-29 10:48:16,921 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:48:16,958 - 

2022-01-29 10:48:16,958 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:48:22,396 - Epoch: [89][  100/  391]    Overall Loss 1.249244    Objective Loss 1.249244                                        LR 0.100000    Time 0.054350    
2022-01-29 10:48:27,651 - Epoch: [89][  200/  391]    Overall Loss 1.262143    Objective Loss 1.262143                                        LR 0.100000    Time 0.053445    
2022-01-29 10:48:32,857 - Epoch: [89][  300/  391]    Overall Loss 1.281884    Objective Loss 1.281884                                        LR 0.100000    Time 0.052984    
2022-01-29 10:48:37,542 - Epoch: [89][  391/  391]    Overall Loss 1.289895    Objective Loss 1.289895    Top1 63.942308    Top5 87.980769    LR 0.100000    Time 0.052631    
2022-01-29 10:48:37,600 - --- validate (epoch=89)-----------
2022-01-29 10:48:37,600 - 10000 samples (128 per mini-batch)
2022-01-29 10:48:39,245 - Epoch: [89][   79/   79]    Loss 1.742769    Top1 52.530000    Top5 82.320000    
2022-01-29 10:48:39,298 - ==> Top1: 52.530    Top5: 82.320    Loss: 1.743

2022-01-29 10:48:39,304 - ==> Best [Top1: 53.790   Top5: 83.380   Sparsity:0.00   Params: 627712 on epoch: 84]
2022-01-29 10:48:39,304 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:48:39,349 - 

2022-01-29 10:48:39,349 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:48:44,861 - Epoch: [90][  100/  391]    Overall Loss 1.247418    Objective Loss 1.247418                                        LR 0.100000    Time 0.055091    
2022-01-29 10:48:50,069 - Epoch: [90][  200/  391]    Overall Loss 1.268603    Objective Loss 1.268603                                        LR 0.100000    Time 0.053581    
2022-01-29 10:48:55,274 - Epoch: [90][  300/  391]    Overall Loss 1.273795    Objective Loss 1.273795                                        LR 0.100000    Time 0.053071    
2022-01-29 10:49:00,017 - Epoch: [90][  391/  391]    Overall Loss 1.281940    Objective Loss 1.281940    Top1 60.576923    Top5 87.500000    LR 0.100000    Time 0.052847    
2022-01-29 10:49:00,076 - --- validate (epoch=90)-----------
2022-01-29 10:49:00,076 - 10000 samples (128 per mini-batch)
2022-01-29 10:49:01,724 - Epoch: [90][   79/   79]    Loss 1.782263    Top1 52.250000    Top5 82.740000    
2022-01-29 10:49:01,781 - ==> Top1: 52.250    Top5: 82.740    Loss: 1.782

2022-01-29 10:49:01,786 - ==> Best [Top1: 53.790   Top5: 83.380   Sparsity:0.00   Params: 627712 on epoch: 84]
2022-01-29 10:49:01,786 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:49:01,829 - 

2022-01-29 10:49:01,830 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:49:07,142 - Epoch: [91][  100/  391]    Overall Loss 1.212277    Objective Loss 1.212277                                        LR 0.100000    Time 0.053099    
2022-01-29 10:49:12,259 - Epoch: [91][  200/  391]    Overall Loss 1.238019    Objective Loss 1.238019                                        LR 0.100000    Time 0.052126    
2022-01-29 10:49:17,402 - Epoch: [91][  300/  391]    Overall Loss 1.257973    Objective Loss 1.257973                                        LR 0.100000    Time 0.051892    
2022-01-29 10:49:22,068 - Epoch: [91][  391/  391]    Overall Loss 1.268705    Objective Loss 1.268705    Top1 57.692308    Top5 86.538462    LR 0.100000    Time 0.051747    
2022-01-29 10:49:22,125 - --- validate (epoch=91)-----------
2022-01-29 10:49:22,125 - 10000 samples (128 per mini-batch)
2022-01-29 10:49:23,823 - Epoch: [91][   79/   79]    Loss 1.860368    Top1 50.110000    Top5 80.590000    
2022-01-29 10:49:23,889 - ==> Top1: 50.110    Top5: 80.590    Loss: 1.860

2022-01-29 10:49:23,894 - ==> Best [Top1: 53.790   Top5: 83.380   Sparsity:0.00   Params: 627712 on epoch: 84]
2022-01-29 10:49:23,894 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:49:23,930 - 

2022-01-29 10:49:23,930 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:49:29,305 - Epoch: [92][  100/  391]    Overall Loss 1.218927    Objective Loss 1.218927                                        LR 0.100000    Time 0.053721    
2022-01-29 10:49:34,531 - Epoch: [92][  200/  391]    Overall Loss 1.239109    Objective Loss 1.239109                                        LR 0.100000    Time 0.052985    
2022-01-29 10:49:39,828 - Epoch: [92][  300/  391]    Overall Loss 1.257858    Objective Loss 1.257858                                        LR 0.100000    Time 0.052978    
2022-01-29 10:49:44,608 - Epoch: [92][  391/  391]    Overall Loss 1.269514    Objective Loss 1.269514    Top1 56.730769    Top5 87.980769    LR 0.100000    Time 0.052870    
2022-01-29 10:49:44,666 - --- validate (epoch=92)-----------
2022-01-29 10:49:44,666 - 10000 samples (128 per mini-batch)
2022-01-29 10:49:46,331 - Epoch: [92][   79/   79]    Loss 1.745559    Top1 52.450000    Top5 82.180000    
2022-01-29 10:49:46,382 - ==> Top1: 52.450    Top5: 82.180    Loss: 1.746

2022-01-29 10:49:46,388 - ==> Best [Top1: 53.790   Top5: 83.380   Sparsity:0.00   Params: 627712 on epoch: 84]
2022-01-29 10:49:46,388 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:49:46,433 - 

2022-01-29 10:49:46,433 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:49:51,976 - Epoch: [93][  100/  391]    Overall Loss 1.237653    Objective Loss 1.237653                                        LR 0.100000    Time 0.055397    
2022-01-29 10:49:57,222 - Epoch: [93][  200/  391]    Overall Loss 1.263127    Objective Loss 1.263127                                        LR 0.100000    Time 0.053923    
2022-01-29 10:50:02,471 - Epoch: [93][  300/  391]    Overall Loss 1.267456    Objective Loss 1.267456                                        LR 0.100000    Time 0.053443    
2022-01-29 10:50:07,246 - Epoch: [93][  391/  391]    Overall Loss 1.277349    Objective Loss 1.277349    Top1 61.538462    Top5 89.903846    LR 0.100000    Time 0.053213    
2022-01-29 10:50:07,305 - --- validate (epoch=93)-----------
2022-01-29 10:50:07,306 - 10000 samples (128 per mini-batch)
2022-01-29 10:50:09,104 - Epoch: [93][   79/   79]    Loss 1.900660    Top1 50.130000    Top5 80.470000    
2022-01-29 10:50:09,164 - ==> Top1: 50.130    Top5: 80.470    Loss: 1.901

2022-01-29 10:50:09,169 - ==> Best [Top1: 53.790   Top5: 83.380   Sparsity:0.00   Params: 627712 on epoch: 84]
2022-01-29 10:50:09,169 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:50:09,207 - 

2022-01-29 10:50:09,207 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:50:14,767 - Epoch: [94][  100/  391]    Overall Loss 1.203533    Objective Loss 1.203533                                        LR 0.100000    Time 0.055573    
2022-01-29 10:50:20,084 - Epoch: [94][  200/  391]    Overall Loss 1.225906    Objective Loss 1.225906                                        LR 0.100000    Time 0.054364    
2022-01-29 10:50:25,395 - Epoch: [94][  300/  391]    Overall Loss 1.243699    Objective Loss 1.243699                                        LR 0.100000    Time 0.053945    
2022-01-29 10:50:30,226 - Epoch: [94][  391/  391]    Overall Loss 1.255194    Objective Loss 1.255194    Top1 61.057692    Top5 91.346154    LR 0.100000    Time 0.053744    
2022-01-29 10:50:30,287 - --- validate (epoch=94)-----------
2022-01-29 10:50:30,287 - 10000 samples (128 per mini-batch)
2022-01-29 10:50:31,961 - Epoch: [94][   79/   79]    Loss 1.783629    Top1 51.500000    Top5 82.280000    
2022-01-29 10:50:32,019 - ==> Top1: 51.500    Top5: 82.280    Loss: 1.784

2022-01-29 10:50:32,024 - ==> Best [Top1: 53.790   Top5: 83.380   Sparsity:0.00   Params: 627712 on epoch: 84]
2022-01-29 10:50:32,024 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:50:32,069 - 

2022-01-29 10:50:32,070 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:50:37,560 - Epoch: [95][  100/  391]    Overall Loss 1.224531    Objective Loss 1.224531                                        LR 0.100000    Time 0.054873    
2022-01-29 10:50:42,875 - Epoch: [95][  200/  391]    Overall Loss 1.234046    Objective Loss 1.234046                                        LR 0.100000    Time 0.054009    
2022-01-29 10:50:48,135 - Epoch: [95][  300/  391]    Overall Loss 1.245255    Objective Loss 1.245255                                        LR 0.100000    Time 0.053535    
2022-01-29 10:50:52,962 - Epoch: [95][  391/  391]    Overall Loss 1.252556    Objective Loss 1.252556    Top1 65.865385    Top5 90.384615    LR 0.100000    Time 0.053421    
2022-01-29 10:50:53,022 - --- validate (epoch=95)-----------
2022-01-29 10:50:53,022 - 10000 samples (128 per mini-batch)
2022-01-29 10:50:54,709 - Epoch: [95][   79/   79]    Loss 1.682569    Top1 53.660000    Top5 83.380000    
2022-01-29 10:50:54,766 - ==> Top1: 53.660    Top5: 83.380    Loss: 1.683

2022-01-29 10:50:54,772 - ==> Best [Top1: 53.790   Top5: 83.380   Sparsity:0.00   Params: 627712 on epoch: 84]
2022-01-29 10:50:54,772 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:50:54,818 - 

2022-01-29 10:50:54,818 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:51:00,397 - Epoch: [96][  100/  391]    Overall Loss 1.216235    Objective Loss 1.216235                                        LR 0.100000    Time 0.055761    
2022-01-29 10:51:05,658 - Epoch: [96][  200/  391]    Overall Loss 1.235552    Objective Loss 1.235552                                        LR 0.100000    Time 0.054185    
2022-01-29 10:51:10,932 - Epoch: [96][  300/  391]    Overall Loss 1.247028    Objective Loss 1.247028                                        LR 0.100000    Time 0.053698    
2022-01-29 10:51:15,726 - Epoch: [96][  391/  391]    Overall Loss 1.253946    Objective Loss 1.253946    Top1 56.250000    Top5 85.576923    LR 0.100000    Time 0.053459    
2022-01-29 10:51:15,786 - --- validate (epoch=96)-----------
2022-01-29 10:51:15,786 - 10000 samples (128 per mini-batch)
2022-01-29 10:51:17,493 - Epoch: [96][   79/   79]    Loss 1.725825    Top1 53.070000    Top5 82.610000    
2022-01-29 10:51:17,553 - ==> Top1: 53.070    Top5: 82.610    Loss: 1.726

2022-01-29 10:51:17,558 - ==> Best [Top1: 53.790   Top5: 83.380   Sparsity:0.00   Params: 627712 on epoch: 84]
2022-01-29 10:51:17,558 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:51:17,604 - 

2022-01-29 10:51:17,604 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:51:23,119 - Epoch: [97][  100/  391]    Overall Loss 1.215485    Objective Loss 1.215485                                        LR 0.100000    Time 0.055120    
2022-01-29 10:51:28,342 - Epoch: [97][  200/  391]    Overall Loss 1.223771    Objective Loss 1.223771                                        LR 0.100000    Time 0.053672    
2022-01-29 10:51:33,628 - Epoch: [97][  300/  391]    Overall Loss 1.236881    Objective Loss 1.236881                                        LR 0.100000    Time 0.053399    
2022-01-29 10:51:38,399 - Epoch: [97][  391/  391]    Overall Loss 1.253151    Objective Loss 1.253151    Top1 56.730769    Top5 88.461538    LR 0.100000    Time 0.053171    
2022-01-29 10:51:38,456 - --- validate (epoch=97)-----------
2022-01-29 10:51:38,456 - 10000 samples (128 per mini-batch)
2022-01-29 10:51:40,238 - Epoch: [97][   79/   79]    Loss 1.847337    Top1 50.970000    Top5 80.360000    
2022-01-29 10:51:40,292 - ==> Top1: 50.970    Top5: 80.360    Loss: 1.847

2022-01-29 10:51:40,298 - ==> Best [Top1: 53.790   Top5: 83.380   Sparsity:0.00   Params: 627712 on epoch: 84]
2022-01-29 10:51:40,298 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:51:40,344 - 

2022-01-29 10:51:40,344 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:51:45,843 - Epoch: [98][  100/  391]    Overall Loss 1.223100    Objective Loss 1.223100                                        LR 0.100000    Time 0.054960    
2022-01-29 10:51:51,100 - Epoch: [98][  200/  391]    Overall Loss 1.228529    Objective Loss 1.228529                                        LR 0.100000    Time 0.053763    
2022-01-29 10:51:56,299 - Epoch: [98][  300/  391]    Overall Loss 1.241558    Objective Loss 1.241558                                        LR 0.100000    Time 0.053169    
2022-01-29 10:52:01,019 - Epoch: [98][  391/  391]    Overall Loss 1.250915    Objective Loss 1.250915    Top1 62.019231    Top5 87.500000    LR 0.100000    Time 0.052864    
2022-01-29 10:52:01,075 - --- validate (epoch=98)-----------
2022-01-29 10:52:01,075 - 10000 samples (128 per mini-batch)
2022-01-29 10:52:02,808 - Epoch: [98][   79/   79]    Loss 1.718283    Top1 53.920000    Top5 82.890000    
2022-01-29 10:52:02,866 - ==> Top1: 53.920    Top5: 82.890    Loss: 1.718

2022-01-29 10:52:02,872 - ==> Best [Top1: 53.920   Top5: 82.890   Sparsity:0.00   Params: 627712 on epoch: 98]
2022-01-29 10:52:02,872 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:52:02,915 - 

2022-01-29 10:52:02,915 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:52:08,331 - Epoch: [99][  100/  391]    Overall Loss 1.195114    Objective Loss 1.195114                                        LR 0.100000    Time 0.054139    
2022-01-29 10:52:13,410 - Epoch: [99][  200/  391]    Overall Loss 1.239586    Objective Loss 1.239586                                        LR 0.100000    Time 0.052458    
2022-01-29 10:52:18,643 - Epoch: [99][  300/  391]    Overall Loss 1.244882    Objective Loss 1.244882                                        LR 0.100000    Time 0.052413    
2022-01-29 10:52:23,426 - Epoch: [99][  391/  391]    Overall Loss 1.252587    Objective Loss 1.252587    Top1 64.423077    Top5 90.384615    LR 0.100000    Time 0.052444    
2022-01-29 10:52:23,484 - --- validate (epoch=99)-----------
2022-01-29 10:52:23,484 - 10000 samples (128 per mini-batch)
2022-01-29 10:52:25,148 - Epoch: [99][   79/   79]    Loss 1.637317    Top1 54.300000    Top5 84.280000    
2022-01-29 10:52:25,207 - ==> Top1: 54.300    Top5: 84.280    Loss: 1.637

2022-01-29 10:52:25,212 - ==> Best [Top1: 54.300   Top5: 84.280   Sparsity:0.00   Params: 627712 on epoch: 99]
2022-01-29 10:52:25,212 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:52:25,261 - 

2022-01-29 10:52:25,262 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:52:30,797 - Epoch: [100][  100/  391]    Overall Loss 1.025602    Objective Loss 1.025602                                        LR 0.023500    Time 0.055323    
2022-01-29 10:52:36,111 - Epoch: [100][  200/  391]    Overall Loss 0.986564    Objective Loss 0.986564                                        LR 0.023500    Time 0.054226    
2022-01-29 10:52:41,419 - Epoch: [100][  300/  391]    Overall Loss 0.965674    Objective Loss 0.965674                                        LR 0.023500    Time 0.053844    
2022-01-29 10:52:46,247 - Epoch: [100][  391/  391]    Overall Loss 0.957616    Objective Loss 0.957616    Top1 66.826923    Top5 94.230769    LR 0.023500    Time 0.053657    
2022-01-29 10:52:46,308 - --- validate (epoch=100)-----------
2022-01-29 10:52:46,308 - 10000 samples (128 per mini-batch)
2022-01-29 10:52:47,998 - Epoch: [100][   79/   79]    Loss 1.319272    Top1 63.360000    Top5 88.760000    
2022-01-29 10:52:48,052 - ==> Top1: 63.360    Top5: 88.760    Loss: 1.319

2022-01-29 10:52:48,057 - ==> Best [Top1: 63.360   Top5: 88.760   Sparsity:0.00   Params: 627712 on epoch: 100]
2022-01-29 10:52:48,057 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:52:48,107 - 

2022-01-29 10:52:48,107 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:52:53,657 - Epoch: [101][  100/  391]    Overall Loss 0.873905    Objective Loss 0.873905                                        LR 0.023500    Time 0.055477    
2022-01-29 10:52:58,891 - Epoch: [101][  200/  391]    Overall Loss 0.873555    Objective Loss 0.873555                                        LR 0.023500    Time 0.053902    
2022-01-29 10:53:04,133 - Epoch: [101][  300/  391]    Overall Loss 0.874049    Objective Loss 0.874049                                        LR 0.023500    Time 0.053405    
2022-01-29 10:53:08,900 - Epoch: [101][  391/  391]    Overall Loss 0.874447    Objective Loss 0.874447    Top1 71.153846    Top5 95.673077    LR 0.023500    Time 0.053164    
2022-01-29 10:53:08,951 - --- validate (epoch=101)-----------
2022-01-29 10:53:08,951 - 10000 samples (128 per mini-batch)
2022-01-29 10:53:10,615 - Epoch: [101][   79/   79]    Loss 1.281358    Top1 63.840000    Top5 89.170000    
2022-01-29 10:53:10,669 - ==> Top1: 63.840    Top5: 89.170    Loss: 1.281

2022-01-29 10:53:10,674 - ==> Best [Top1: 63.840   Top5: 89.170   Sparsity:0.00   Params: 627712 on epoch: 101]
2022-01-29 10:53:10,674 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:53:10,722 - 

2022-01-29 10:53:10,722 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:53:16,189 - Epoch: [102][  100/  391]    Overall Loss 0.847124    Objective Loss 0.847124                                        LR 0.023500    Time 0.054635    
2022-01-29 10:53:21,421 - Epoch: [102][  200/  391]    Overall Loss 0.844994    Objective Loss 0.844994                                        LR 0.023500    Time 0.053475    
2022-01-29 10:53:26,661 - Epoch: [102][  300/  391]    Overall Loss 0.847394    Objective Loss 0.847394                                        LR 0.023500    Time 0.053113    
2022-01-29 10:53:31,430 - Epoch: [102][  391/  391]    Overall Loss 0.848215    Objective Loss 0.848215    Top1 75.480769    Top5 94.711538    LR 0.023500    Time 0.052947    
2022-01-29 10:53:31,487 - --- validate (epoch=102)-----------
2022-01-29 10:53:31,487 - 10000 samples (128 per mini-batch)
2022-01-29 10:53:33,279 - Epoch: [102][   79/   79]    Loss 1.292397    Top1 63.200000    Top5 89.010000    
2022-01-29 10:53:33,339 - ==> Top1: 63.200    Top5: 89.010    Loss: 1.292

2022-01-29 10:53:33,345 - ==> Best [Top1: 63.840   Top5: 89.170   Sparsity:0.00   Params: 627712 on epoch: 101]
2022-01-29 10:53:33,345 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:53:33,390 - 

2022-01-29 10:53:33,390 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:53:38,934 - Epoch: [103][  100/  391]    Overall Loss 0.805941    Objective Loss 0.805941                                        LR 0.023500    Time 0.055410    
2022-01-29 10:53:44,171 - Epoch: [103][  200/  391]    Overall Loss 0.813374    Objective Loss 0.813374                                        LR 0.023500    Time 0.053884    
2022-01-29 10:53:49,409 - Epoch: [103][  300/  391]    Overall Loss 0.818566    Objective Loss 0.818566                                        LR 0.023500    Time 0.053379    
2022-01-29 10:53:54,168 - Epoch: [103][  391/  391]    Overall Loss 0.825528    Objective Loss 0.825528    Top1 80.288462    Top5 96.634615    LR 0.023500    Time 0.053125    
2022-01-29 10:53:54,227 - --- validate (epoch=103)-----------
2022-01-29 10:53:54,228 - 10000 samples (128 per mini-batch)
2022-01-29 10:53:55,946 - Epoch: [103][   79/   79]    Loss 1.296886    Top1 63.890000    Top5 89.020000    
2022-01-29 10:53:55,999 - ==> Top1: 63.890    Top5: 89.020    Loss: 1.297

2022-01-29 10:53:56,005 - ==> Best [Top1: 63.890   Top5: 89.020   Sparsity:0.00   Params: 627712 on epoch: 103]
2022-01-29 10:53:56,005 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:53:56,054 - 

2022-01-29 10:53:56,054 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:54:01,593 - Epoch: [104][  100/  391]    Overall Loss 0.788490    Objective Loss 0.788490                                        LR 0.023500    Time 0.055361    
2022-01-29 10:54:06,825 - Epoch: [104][  200/  391]    Overall Loss 0.799406    Objective Loss 0.799406                                        LR 0.023500    Time 0.053836    
2022-01-29 10:54:12,055 - Epoch: [104][  300/  391]    Overall Loss 0.803564    Objective Loss 0.803564                                        LR 0.023500    Time 0.053321    
2022-01-29 10:54:16,845 - Epoch: [104][  391/  391]    Overall Loss 0.813572    Objective Loss 0.813572    Top1 73.557692    Top5 93.269231    LR 0.023500    Time 0.053162    
2022-01-29 10:54:16,905 - --- validate (epoch=104)-----------
2022-01-29 10:54:16,905 - 10000 samples (128 per mini-batch)
2022-01-29 10:54:18,682 - Epoch: [104][   79/   79]    Loss 1.320097    Top1 63.180000    Top5 88.860000    
2022-01-29 10:54:18,745 - ==> Top1: 63.180    Top5: 88.860    Loss: 1.320

2022-01-29 10:54:18,750 - ==> Best [Top1: 63.890   Top5: 89.020   Sparsity:0.00   Params: 627712 on epoch: 103]
2022-01-29 10:54:18,750 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:54:18,788 - 

2022-01-29 10:54:18,788 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:54:24,296 - Epoch: [105][  100/  391]    Overall Loss 0.770546    Objective Loss 0.770546                                        LR 0.023500    Time 0.055055    
2022-01-29 10:54:29,546 - Epoch: [105][  200/  391]    Overall Loss 0.784102    Objective Loss 0.784102                                        LR 0.023500    Time 0.053771    
2022-01-29 10:54:34,740 - Epoch: [105][  300/  391]    Overall Loss 0.795323    Objective Loss 0.795323                                        LR 0.023500    Time 0.053158    
2022-01-29 10:54:39,464 - Epoch: [105][  391/  391]    Overall Loss 0.797633    Objective Loss 0.797633    Top1 73.557692    Top5 96.153846    LR 0.023500    Time 0.052867    
2022-01-29 10:54:39,527 - --- validate (epoch=105)-----------
2022-01-29 10:54:39,527 - 10000 samples (128 per mini-batch)
2022-01-29 10:54:41,263 - Epoch: [105][   79/   79]    Loss 1.284055    Top1 63.570000    Top5 89.180000    
2022-01-29 10:54:41,323 - ==> Top1: 63.570    Top5: 89.180    Loss: 1.284

2022-01-29 10:54:41,329 - ==> Best [Top1: 63.890   Top5: 89.020   Sparsity:0.00   Params: 627712 on epoch: 103]
2022-01-29 10:54:41,329 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:54:41,374 - 

2022-01-29 10:54:41,374 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:54:46,822 - Epoch: [106][  100/  391]    Overall Loss 0.782009    Objective Loss 0.782009                                        LR 0.023500    Time 0.054451    
2022-01-29 10:54:52,050 - Epoch: [106][  200/  391]    Overall Loss 0.788761    Objective Loss 0.788761                                        LR 0.023500    Time 0.053358    
2022-01-29 10:54:57,281 - Epoch: [106][  300/  391]    Overall Loss 0.785356    Objective Loss 0.785356                                        LR 0.023500    Time 0.053006    
2022-01-29 10:55:02,036 - Epoch: [106][  391/  391]    Overall Loss 0.786109    Objective Loss 0.786109    Top1 76.923077    Top5 97.115385    LR 0.023500    Time 0.052830    
2022-01-29 10:55:02,094 - --- validate (epoch=106)-----------
2022-01-29 10:55:02,094 - 10000 samples (128 per mini-batch)
2022-01-29 10:55:03,778 - Epoch: [106][   79/   79]    Loss 1.284589    Top1 64.070000    Top5 89.240000    
2022-01-29 10:55:03,833 - ==> Top1: 64.070    Top5: 89.240    Loss: 1.285

2022-01-29 10:55:03,839 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 10:55:03,839 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:55:03,888 - 

2022-01-29 10:55:03,888 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:55:09,427 - Epoch: [107][  100/  391]    Overall Loss 0.750997    Objective Loss 0.750997                                        LR 0.023500    Time 0.055358    
2022-01-29 10:55:14,695 - Epoch: [107][  200/  391]    Overall Loss 0.764734    Objective Loss 0.764734                                        LR 0.023500    Time 0.054013    
2022-01-29 10:55:19,956 - Epoch: [107][  300/  391]    Overall Loss 0.767176    Objective Loss 0.767176                                        LR 0.023500    Time 0.053544    
2022-01-29 10:55:24,736 - Epoch: [107][  391/  391]    Overall Loss 0.776837    Objective Loss 0.776837    Top1 77.403846    Top5 95.673077    LR 0.023500    Time 0.053306    
2022-01-29 10:55:24,794 - --- validate (epoch=107)-----------
2022-01-29 10:55:24,794 - 10000 samples (128 per mini-batch)
2022-01-29 10:55:26,491 - Epoch: [107][   79/   79]    Loss 1.315897    Top1 63.350000    Top5 88.910000    
2022-01-29 10:55:26,545 - ==> Top1: 63.350    Top5: 88.910    Loss: 1.316

2022-01-29 10:55:26,550 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 10:55:26,550 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:55:26,594 - 

2022-01-29 10:55:26,594 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:55:32,122 - Epoch: [108][  100/  391]    Overall Loss 0.748972    Objective Loss 0.748972                                        LR 0.023500    Time 0.055255    
2022-01-29 10:55:37,369 - Epoch: [108][  200/  391]    Overall Loss 0.755861    Objective Loss 0.755861                                        LR 0.023500    Time 0.053857    
2022-01-29 10:55:42,677 - Epoch: [108][  300/  391]    Overall Loss 0.766486    Objective Loss 0.766486                                        LR 0.023500    Time 0.053593    
2022-01-29 10:55:47,508 - Epoch: [108][  391/  391]    Overall Loss 0.770096    Objective Loss 0.770096    Top1 74.519231    Top5 96.153846    LR 0.023500    Time 0.053474    
2022-01-29 10:55:47,568 - --- validate (epoch=108)-----------
2022-01-29 10:55:47,568 - 10000 samples (128 per mini-batch)
2022-01-29 10:55:49,288 - Epoch: [108][   79/   79]    Loss 1.316348    Top1 63.380000    Top5 88.870000    
2022-01-29 10:55:49,342 - ==> Top1: 63.380    Top5: 88.870    Loss: 1.316

2022-01-29 10:55:49,347 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 10:55:49,347 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:55:49,393 - 

2022-01-29 10:55:49,393 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:55:54,915 - Epoch: [109][  100/  391]    Overall Loss 0.742834    Objective Loss 0.742834                                        LR 0.023500    Time 0.055191    
2022-01-29 10:56:00,168 - Epoch: [109][  200/  391]    Overall Loss 0.749546    Objective Loss 0.749546                                        LR 0.023500    Time 0.053858    
2022-01-29 10:56:05,412 - Epoch: [109][  300/  391]    Overall Loss 0.752163    Objective Loss 0.752163                                        LR 0.023500    Time 0.053382    
2022-01-29 10:56:10,178 - Epoch: [109][  391/  391]    Overall Loss 0.761491    Objective Loss 0.761491    Top1 76.923077    Top5 96.153846    LR 0.023500    Time 0.053147    
2022-01-29 10:56:10,238 - --- validate (epoch=109)-----------
2022-01-29 10:56:10,238 - 10000 samples (128 per mini-batch)
2022-01-29 10:56:11,936 - Epoch: [109][   79/   79]    Loss 1.323633    Top1 63.490000    Top5 88.930000    
2022-01-29 10:56:11,987 - ==> Top1: 63.490    Top5: 88.930    Loss: 1.324

2022-01-29 10:56:11,992 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 10:56:11,992 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:56:12,030 - 

2022-01-29 10:56:12,030 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:56:17,569 - Epoch: [110][  100/  391]    Overall Loss 0.717001    Objective Loss 0.717001                                        LR 0.023500    Time 0.055366    
2022-01-29 10:56:22,799 - Epoch: [110][  200/  391]    Overall Loss 0.737100    Objective Loss 0.737100                                        LR 0.023500    Time 0.053825    
2022-01-29 10:56:28,060 - Epoch: [110][  300/  391]    Overall Loss 0.744609    Objective Loss 0.744609                                        LR 0.023500    Time 0.053418    
2022-01-29 10:56:32,771 - Epoch: [110][  391/  391]    Overall Loss 0.752390    Objective Loss 0.752390    Top1 72.596154    Top5 92.307692    LR 0.023500    Time 0.053031    
2022-01-29 10:56:32,829 - --- validate (epoch=110)-----------
2022-01-29 10:56:32,829 - 10000 samples (128 per mini-batch)
2022-01-29 10:56:34,535 - Epoch: [110][   79/   79]    Loss 1.311949    Top1 63.090000    Top5 88.910000    
2022-01-29 10:56:34,592 - ==> Top1: 63.090    Top5: 88.910    Loss: 1.312

2022-01-29 10:56:34,597 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 10:56:34,597 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:56:34,635 - 

2022-01-29 10:56:34,635 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:56:40,181 - Epoch: [111][  100/  391]    Overall Loss 0.722397    Objective Loss 0.722397                                        LR 0.023500    Time 0.055426    
2022-01-29 10:56:45,459 - Epoch: [111][  200/  391]    Overall Loss 0.726466    Objective Loss 0.726466                                        LR 0.023500    Time 0.054100    
2022-01-29 10:56:50,704 - Epoch: [111][  300/  391]    Overall Loss 0.737657    Objective Loss 0.737657                                        LR 0.023500    Time 0.053545    
2022-01-29 10:56:55,427 - Epoch: [111][  391/  391]    Overall Loss 0.747595    Objective Loss 0.747595    Top1 76.923077    Top5 96.153846    LR 0.023500    Time 0.053162    
2022-01-29 10:56:55,490 - --- validate (epoch=111)-----------
2022-01-29 10:56:55,490 - 10000 samples (128 per mini-batch)
2022-01-29 10:56:57,273 - Epoch: [111][   79/   79]    Loss 1.334349    Top1 63.020000    Top5 88.820000    
2022-01-29 10:56:57,324 - ==> Top1: 63.020    Top5: 88.820    Loss: 1.334

2022-01-29 10:56:57,329 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 10:56:57,330 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:56:57,375 - 

2022-01-29 10:56:57,375 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:57:02,795 - Epoch: [112][  100/  391]    Overall Loss 0.724930    Objective Loss 0.724930                                        LR 0.023500    Time 0.054179    
2022-01-29 10:57:07,995 - Epoch: [112][  200/  391]    Overall Loss 0.724999    Objective Loss 0.724999                                        LR 0.023500    Time 0.053082    
2022-01-29 10:57:13,157 - Epoch: [112][  300/  391]    Overall Loss 0.729123    Objective Loss 0.729123                                        LR 0.023500    Time 0.052592    
2022-01-29 10:57:17,860 - Epoch: [112][  391/  391]    Overall Loss 0.741565    Objective Loss 0.741565    Top1 72.115385    Top5 92.788462    LR 0.023500    Time 0.052379    
2022-01-29 10:57:17,920 - --- validate (epoch=112)-----------
2022-01-29 10:57:17,920 - 10000 samples (128 per mini-batch)
2022-01-29 10:57:19,671 - Epoch: [112][   79/   79]    Loss 1.333867    Top1 63.380000    Top5 89.030000    
2022-01-29 10:57:19,725 - ==> Top1: 63.380    Top5: 89.030    Loss: 1.334

2022-01-29 10:57:19,730 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 10:57:19,731 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:57:19,776 - 

2022-01-29 10:57:19,776 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:57:25,174 - Epoch: [113][  100/  391]    Overall Loss 0.709262    Objective Loss 0.709262                                        LR 0.023500    Time 0.053957    
2022-01-29 10:57:30,394 - Epoch: [113][  200/  391]    Overall Loss 0.715685    Objective Loss 0.715685                                        LR 0.023500    Time 0.053072    
2022-01-29 10:57:35,691 - Epoch: [113][  300/  391]    Overall Loss 0.728337    Objective Loss 0.728337                                        LR 0.023500    Time 0.053034    
2022-01-29 10:57:40,514 - Epoch: [113][  391/  391]    Overall Loss 0.736139    Objective Loss 0.736139    Top1 74.519231    Top5 98.076923    LR 0.023500    Time 0.053025    
2022-01-29 10:57:40,572 - --- validate (epoch=113)-----------
2022-01-29 10:57:40,572 - 10000 samples (128 per mini-batch)
2022-01-29 10:57:42,234 - Epoch: [113][   79/   79]    Loss 1.327281    Top1 62.700000    Top5 88.970000    
2022-01-29 10:57:42,291 - ==> Top1: 62.700    Top5: 88.970    Loss: 1.327

2022-01-29 10:57:42,297 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 10:57:42,297 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:57:42,342 - 

2022-01-29 10:57:42,343 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:57:47,865 - Epoch: [114][  100/  391]    Overall Loss 0.696028    Objective Loss 0.696028                                        LR 0.023500    Time 0.055193    
2022-01-29 10:57:52,964 - Epoch: [114][  200/  391]    Overall Loss 0.708907    Objective Loss 0.708907                                        LR 0.023500    Time 0.053089    
2022-01-29 10:57:58,061 - Epoch: [114][  300/  391]    Overall Loss 0.721214    Objective Loss 0.721214                                        LR 0.023500    Time 0.052377    
2022-01-29 10:58:02,757 - Epoch: [114][  391/  391]    Overall Loss 0.732352    Objective Loss 0.732352    Top1 74.519231    Top5 95.192308    LR 0.023500    Time 0.052195    
2022-01-29 10:58:02,816 - --- validate (epoch=114)-----------
2022-01-29 10:58:02,816 - 10000 samples (128 per mini-batch)
2022-01-29 10:58:04,605 - Epoch: [114][   79/   79]    Loss 1.348866    Top1 62.920000    Top5 88.690000    
2022-01-29 10:58:04,658 - ==> Top1: 62.920    Top5: 88.690    Loss: 1.349

2022-01-29 10:58:04,664 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 10:58:04,664 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:58:04,709 - 

2022-01-29 10:58:04,709 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:58:10,159 - Epoch: [115][  100/  391]    Overall Loss 0.703989    Objective Loss 0.703989                                        LR 0.023500    Time 0.054471    
2022-01-29 10:58:15,192 - Epoch: [115][  200/  391]    Overall Loss 0.709329    Objective Loss 0.709329                                        LR 0.023500    Time 0.052393    
2022-01-29 10:58:20,232 - Epoch: [115][  300/  391]    Overall Loss 0.724154    Objective Loss 0.724154                                        LR 0.023500    Time 0.051728    
2022-01-29 10:58:24,817 - Epoch: [115][  391/  391]    Overall Loss 0.727062    Objective Loss 0.727062    Top1 74.038462    Top5 95.673077    LR 0.023500    Time 0.051411    
2022-01-29 10:58:24,874 - --- validate (epoch=115)-----------
2022-01-29 10:58:24,874 - 10000 samples (128 per mini-batch)
2022-01-29 10:58:26,549 - Epoch: [115][   79/   79]    Loss 1.335004    Top1 62.890000    Top5 88.820000    
2022-01-29 10:58:26,601 - ==> Top1: 62.890    Top5: 88.820    Loss: 1.335

2022-01-29 10:58:26,606 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 10:58:26,606 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:58:26,650 - 

2022-01-29 10:58:26,650 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:58:32,075 - Epoch: [116][  100/  391]    Overall Loss 0.699411    Objective Loss 0.699411                                        LR 0.023500    Time 0.054216    
2022-01-29 10:58:37,300 - Epoch: [116][  200/  391]    Overall Loss 0.705655    Objective Loss 0.705655                                        LR 0.023500    Time 0.053230    
2022-01-29 10:58:42,530 - Epoch: [116][  300/  391]    Overall Loss 0.713892    Objective Loss 0.713892                                        LR 0.023500    Time 0.052917    
2022-01-29 10:58:47,281 - Epoch: [116][  391/  391]    Overall Loss 0.726588    Objective Loss 0.726588    Top1 71.634615    Top5 94.711538    LR 0.023500    Time 0.052751    
2022-01-29 10:58:47,341 - --- validate (epoch=116)-----------
2022-01-29 10:58:47,341 - 10000 samples (128 per mini-batch)
2022-01-29 10:58:48,996 - Epoch: [116][   79/   79]    Loss 1.401607    Top1 61.620000    Top5 87.960000    
2022-01-29 10:58:49,050 - ==> Top1: 61.620    Top5: 87.960    Loss: 1.402

2022-01-29 10:58:49,056 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 10:58:49,056 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:58:49,093 - 

2022-01-29 10:58:49,093 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:58:54,528 - Epoch: [117][  100/  391]    Overall Loss 0.695468    Objective Loss 0.695468                                        LR 0.023500    Time 0.054322    
2022-01-29 10:58:59,635 - Epoch: [117][  200/  391]    Overall Loss 0.702095    Objective Loss 0.702095                                        LR 0.023500    Time 0.052689    
2022-01-29 10:59:04,744 - Epoch: [117][  300/  391]    Overall Loss 0.716269    Objective Loss 0.716269                                        LR 0.023500    Time 0.052152    
2022-01-29 10:59:09,386 - Epoch: [117][  391/  391]    Overall Loss 0.723519    Objective Loss 0.723519    Top1 74.038462    Top5 96.153846    LR 0.023500    Time 0.051885    
2022-01-29 10:59:09,445 - --- validate (epoch=117)-----------
2022-01-29 10:59:09,446 - 10000 samples (128 per mini-batch)
2022-01-29 10:59:11,097 - Epoch: [117][   79/   79]    Loss 1.382210    Top1 62.620000    Top5 88.150000    
2022-01-29 10:59:11,156 - ==> Top1: 62.620    Top5: 88.150    Loss: 1.382

2022-01-29 10:59:11,162 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 10:59:11,162 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:59:11,206 - 

2022-01-29 10:59:11,206 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:59:16,561 - Epoch: [118][  100/  391]    Overall Loss 0.684745    Objective Loss 0.684745                                        LR 0.023500    Time 0.053522    
2022-01-29 10:59:21,628 - Epoch: [118][  200/  391]    Overall Loss 0.699097    Objective Loss 0.699097                                        LR 0.023500    Time 0.052091    
2022-01-29 10:59:26,705 - Epoch: [118][  300/  391]    Overall Loss 0.711244    Objective Loss 0.711244                                        LR 0.023500    Time 0.051647    
2022-01-29 10:59:31,320 - Epoch: [118][  391/  391]    Overall Loss 0.721929    Objective Loss 0.721929    Top1 77.884615    Top5 95.192308    LR 0.023500    Time 0.051425    
2022-01-29 10:59:31,375 - --- validate (epoch=118)-----------
2022-01-29 10:59:31,375 - 10000 samples (128 per mini-batch)
2022-01-29 10:59:33,017 - Epoch: [118][   79/   79]    Loss 1.317734    Top1 63.590000    Top5 88.770000    
2022-01-29 10:59:33,069 - ==> Top1: 63.590    Top5: 88.770    Loss: 1.318

2022-01-29 10:59:33,074 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 10:59:33,074 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:59:33,119 - 

2022-01-29 10:59:33,119 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 10:59:38,557 - Epoch: [119][  100/  391]    Overall Loss 0.682197    Objective Loss 0.682197                                        LR 0.023500    Time 0.054347    
2022-01-29 10:59:43,776 - Epoch: [119][  200/  391]    Overall Loss 0.693566    Objective Loss 0.693566                                        LR 0.023500    Time 0.053267    
2022-01-29 10:59:49,004 - Epoch: [119][  300/  391]    Overall Loss 0.704048    Objective Loss 0.704048                                        LR 0.023500    Time 0.052934    
2022-01-29 10:59:53,755 - Epoch: [119][  391/  391]    Overall Loss 0.716548    Objective Loss 0.716548    Top1 73.557692    Top5 95.673077    LR 0.023500    Time 0.052763    
2022-01-29 10:59:53,811 - --- validate (epoch=119)-----------
2022-01-29 10:59:53,811 - 10000 samples (128 per mini-batch)
2022-01-29 10:59:55,579 - Epoch: [119][   79/   79]    Loss 1.353711    Top1 63.060000    Top5 88.730000    
2022-01-29 10:59:55,635 - ==> Top1: 63.060    Top5: 88.730    Loss: 1.354

2022-01-29 10:59:55,641 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 10:59:55,641 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 10:59:55,686 - 

2022-01-29 10:59:55,686 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:00:01,153 - Epoch: [120][  100/  391]    Overall Loss 0.674209    Objective Loss 0.674209                                        LR 0.023500    Time 0.054637    
2022-01-29 11:00:06,380 - Epoch: [120][  200/  391]    Overall Loss 0.693370    Objective Loss 0.693370                                        LR 0.023500    Time 0.053449    
2022-01-29 11:00:11,610 - Epoch: [120][  300/  391]    Overall Loss 0.700532    Objective Loss 0.700532                                        LR 0.023500    Time 0.053063    
2022-01-29 11:00:16,361 - Epoch: [120][  391/  391]    Overall Loss 0.711024    Objective Loss 0.711024    Top1 78.846154    Top5 98.076923    LR 0.023500    Time 0.052863    
2022-01-29 11:00:16,419 - --- validate (epoch=120)-----------
2022-01-29 11:00:16,419 - 10000 samples (128 per mini-batch)
2022-01-29 11:00:18,095 - Epoch: [120][   79/   79]    Loss 1.400466    Top1 62.180000    Top5 87.980000    
2022-01-29 11:00:18,153 - ==> Top1: 62.180    Top5: 87.980    Loss: 1.400

2022-01-29 11:00:18,159 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:00:18,159 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:00:18,197 - 

2022-01-29 11:00:18,197 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:00:23,773 - Epoch: [121][  100/  391]    Overall Loss 0.671899    Objective Loss 0.671899                                        LR 0.023500    Time 0.055733    
2022-01-29 11:00:29,067 - Epoch: [121][  200/  391]    Overall Loss 0.693916    Objective Loss 0.693916                                        LR 0.023500    Time 0.054332    
2022-01-29 11:00:34,230 - Epoch: [121][  300/  391]    Overall Loss 0.699471    Objective Loss 0.699471                                        LR 0.023500    Time 0.053429    
2022-01-29 11:00:38,852 - Epoch: [121][  391/  391]    Overall Loss 0.710769    Objective Loss 0.710769    Top1 81.730769    Top5 98.076923    LR 0.023500    Time 0.052814    
2022-01-29 11:00:38,912 - --- validate (epoch=121)-----------
2022-01-29 11:00:38,912 - 10000 samples (128 per mini-batch)
2022-01-29 11:00:40,568 - Epoch: [121][   79/   79]    Loss 1.359528    Top1 62.860000    Top5 88.470000    
2022-01-29 11:00:40,633 - ==> Top1: 62.860    Top5: 88.470    Loss: 1.360

2022-01-29 11:00:40,639 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:00:40,639 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:00:40,676 - 

2022-01-29 11:00:40,676 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:00:46,062 - Epoch: [122][  100/  391]    Overall Loss 0.679548    Objective Loss 0.679548                                        LR 0.023500    Time 0.053824    
2022-01-29 11:00:51,137 - Epoch: [122][  200/  391]    Overall Loss 0.687849    Objective Loss 0.687849                                        LR 0.023500    Time 0.052282    
2022-01-29 11:00:56,177 - Epoch: [122][  300/  391]    Overall Loss 0.696363    Objective Loss 0.696363                                        LR 0.023500    Time 0.051652    
2022-01-29 11:01:00,763 - Epoch: [122][  391/  391]    Overall Loss 0.705091    Objective Loss 0.705091    Top1 74.519231    Top5 97.596154    LR 0.023500    Time 0.051359    
2022-01-29 11:01:00,822 - --- validate (epoch=122)-----------
2022-01-29 11:01:00,822 - 10000 samples (128 per mini-batch)
2022-01-29 11:01:02,477 - Epoch: [122][   79/   79]    Loss 1.371120    Top1 63.050000    Top5 88.440000    
2022-01-29 11:01:02,535 - ==> Top1: 63.050    Top5: 88.440    Loss: 1.371

2022-01-29 11:01:02,540 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:01:02,540 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:01:02,584 - 

2022-01-29 11:01:02,584 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:01:07,911 - Epoch: [123][  100/  391]    Overall Loss 0.670308    Objective Loss 0.670308                                        LR 0.023500    Time 0.053246    
2022-01-29 11:01:13,094 - Epoch: [123][  200/  391]    Overall Loss 0.675982    Objective Loss 0.675982                                        LR 0.023500    Time 0.052532    
2022-01-29 11:01:18,269 - Epoch: [123][  300/  391]    Overall Loss 0.688212    Objective Loss 0.688212                                        LR 0.023500    Time 0.052266    
2022-01-29 11:01:22,975 - Epoch: [123][  391/  391]    Overall Loss 0.699137    Objective Loss 0.699137    Top1 77.403846    Top5 96.153846    LR 0.023500    Time 0.052136    
2022-01-29 11:01:23,032 - --- validate (epoch=123)-----------
2022-01-29 11:01:23,033 - 10000 samples (128 per mini-batch)
2022-01-29 11:01:24,692 - Epoch: [123][   79/   79]    Loss 1.388890    Top1 62.150000    Top5 88.020000    
2022-01-29 11:01:24,751 - ==> Top1: 62.150    Top5: 88.020    Loss: 1.389

2022-01-29 11:01:24,757 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:01:24,757 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:01:24,801 - 

2022-01-29 11:01:24,802 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:01:30,273 - Epoch: [124][  100/  391]    Overall Loss 0.682830    Objective Loss 0.682830                                        LR 0.023500    Time 0.054690    
2022-01-29 11:01:35,430 - Epoch: [124][  200/  391]    Overall Loss 0.688571    Objective Loss 0.688571                                        LR 0.023500    Time 0.053124    
2022-01-29 11:01:40,519 - Epoch: [124][  300/  391]    Overall Loss 0.696263    Objective Loss 0.696263                                        LR 0.023500    Time 0.052377    
2022-01-29 11:01:45,247 - Epoch: [124][  391/  391]    Overall Loss 0.702857    Objective Loss 0.702857    Top1 78.846154    Top5 97.596154    LR 0.023500    Time 0.052277    
2022-01-29 11:01:45,307 - --- validate (epoch=124)-----------
2022-01-29 11:01:45,307 - 10000 samples (128 per mini-batch)
2022-01-29 11:01:46,988 - Epoch: [124][   79/   79]    Loss 1.390367    Top1 62.410000    Top5 88.210000    
2022-01-29 11:01:47,046 - ==> Top1: 62.410    Top5: 88.210    Loss: 1.390

2022-01-29 11:01:47,052 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:01:47,052 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:01:47,097 - 

2022-01-29 11:01:47,097 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:01:52,612 - Epoch: [125][  100/  391]    Overall Loss 0.686192    Objective Loss 0.686192                                        LR 0.023500    Time 0.055120    
2022-01-29 11:01:57,833 - Epoch: [125][  200/  391]    Overall Loss 0.686903    Objective Loss 0.686903                                        LR 0.023500    Time 0.053661    
2022-01-29 11:02:03,050 - Epoch: [125][  300/  391]    Overall Loss 0.693930    Objective Loss 0.693930                                        LR 0.023500    Time 0.053162    
2022-01-29 11:02:07,793 - Epoch: [125][  391/  391]    Overall Loss 0.702182    Objective Loss 0.702182    Top1 77.403846    Top5 97.115385    LR 0.023500    Time 0.052915    
2022-01-29 11:02:07,850 - --- validate (epoch=125)-----------
2022-01-29 11:02:07,850 - 10000 samples (128 per mini-batch)
2022-01-29 11:02:09,524 - Epoch: [125][   79/   79]    Loss 1.409990    Top1 61.530000    Top5 87.430000    
2022-01-29 11:02:09,575 - ==> Top1: 61.530    Top5: 87.430    Loss: 1.410

2022-01-29 11:02:09,580 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:02:09,580 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:02:09,626 - 

2022-01-29 11:02:09,626 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:02:15,064 - Epoch: [126][  100/  391]    Overall Loss 0.658200    Objective Loss 0.658200                                        LR 0.023500    Time 0.054347    
2022-01-29 11:02:20,297 - Epoch: [126][  200/  391]    Overall Loss 0.680050    Objective Loss 0.680050                                        LR 0.023500    Time 0.053333    
2022-01-29 11:02:25,530 - Epoch: [126][  300/  391]    Overall Loss 0.691175    Objective Loss 0.691175                                        LR 0.023500    Time 0.052997    
2022-01-29 11:02:30,288 - Epoch: [126][  391/  391]    Overall Loss 0.694776    Objective Loss 0.694776    Top1 74.519231    Top5 97.596154    LR 0.023500    Time 0.052827    
2022-01-29 11:02:30,346 - --- validate (epoch=126)-----------
2022-01-29 11:02:30,346 - 10000 samples (128 per mini-batch)
2022-01-29 11:02:32,129 - Epoch: [126][   79/   79]    Loss 1.412934    Top1 62.270000    Top5 87.940000    
2022-01-29 11:02:32,187 - ==> Top1: 62.270    Top5: 87.940    Loss: 1.413

2022-01-29 11:02:32,193 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:02:32,193 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:02:32,230 - 

2022-01-29 11:02:32,230 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:02:37,703 - Epoch: [127][  100/  391]    Overall Loss 0.677424    Objective Loss 0.677424                                        LR 0.023500    Time 0.054702    
2022-01-29 11:02:42,925 - Epoch: [127][  200/  391]    Overall Loss 0.685915    Objective Loss 0.685915                                        LR 0.023500    Time 0.053455    
2022-01-29 11:02:48,122 - Epoch: [127][  300/  391]    Overall Loss 0.693440    Objective Loss 0.693440                                        LR 0.023500    Time 0.052959    
2022-01-29 11:02:52,786 - Epoch: [127][  391/  391]    Overall Loss 0.702132    Objective Loss 0.702132    Top1 75.000000    Top5 97.115385    LR 0.023500    Time 0.052558    
2022-01-29 11:02:52,844 - --- validate (epoch=127)-----------
2022-01-29 11:02:52,845 - 10000 samples (128 per mini-batch)
2022-01-29 11:02:54,489 - Epoch: [127][   79/   79]    Loss 1.397941    Top1 61.630000    Top5 88.080000    
2022-01-29 11:02:54,543 - ==> Top1: 61.630    Top5: 88.080    Loss: 1.398

2022-01-29 11:02:54,548 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:02:54,548 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:02:54,592 - 

2022-01-29 11:02:54,592 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:02:59,983 - Epoch: [128][  100/  391]    Overall Loss 0.646205    Objective Loss 0.646205                                        LR 0.023500    Time 0.053884    
2022-01-29 11:03:05,077 - Epoch: [128][  200/  391]    Overall Loss 0.669399    Objective Loss 0.669399                                        LR 0.023500    Time 0.052405    
2022-01-29 11:03:10,177 - Epoch: [128][  300/  391]    Overall Loss 0.687519    Objective Loss 0.687519                                        LR 0.023500    Time 0.051931    
2022-01-29 11:03:14,809 - Epoch: [128][  391/  391]    Overall Loss 0.692793    Objective Loss 0.692793    Top1 76.923077    Top5 95.192308    LR 0.023500    Time 0.051692    
2022-01-29 11:03:14,870 - --- validate (epoch=128)-----------
2022-01-29 11:03:14,870 - 10000 samples (128 per mini-batch)
2022-01-29 11:03:16,523 - Epoch: [128][   79/   79]    Loss 1.393027    Top1 62.160000    Top5 88.350000    
2022-01-29 11:03:16,579 - ==> Top1: 62.160    Top5: 88.350    Loss: 1.393

2022-01-29 11:03:16,585 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:03:16,585 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:03:16,629 - 

2022-01-29 11:03:16,629 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:03:22,064 - Epoch: [129][  100/  391]    Overall Loss 0.667901    Objective Loss 0.667901                                        LR 0.023500    Time 0.054319    
2022-01-29 11:03:27,222 - Epoch: [129][  200/  391]    Overall Loss 0.672162    Objective Loss 0.672162                                        LR 0.023500    Time 0.052947    
2022-01-29 11:03:32,376 - Epoch: [129][  300/  391]    Overall Loss 0.687224    Objective Loss 0.687224                                        LR 0.023500    Time 0.052475    
2022-01-29 11:03:36,999 - Epoch: [129][  391/  391]    Overall Loss 0.688595    Objective Loss 0.688595    Top1 78.365385    Top5 97.115385    LR 0.023500    Time 0.052082    
2022-01-29 11:03:37,054 - --- validate (epoch=129)-----------
2022-01-29 11:03:37,055 - 10000 samples (128 per mini-batch)
2022-01-29 11:03:38,722 - Epoch: [129][   79/   79]    Loss 1.425830    Top1 61.650000    Top5 87.240000    
2022-01-29 11:03:38,778 - ==> Top1: 61.650    Top5: 87.240    Loss: 1.426

2022-01-29 11:03:38,783 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:03:38,783 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:03:38,820 - 

2022-01-29 11:03:38,820 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:03:44,167 - Epoch: [130][  100/  391]    Overall Loss 0.643423    Objective Loss 0.643423                                        LR 0.023500    Time 0.053443    
2022-01-29 11:03:49,389 - Epoch: [130][  200/  391]    Overall Loss 0.662413    Objective Loss 0.662413                                        LR 0.023500    Time 0.052829    
2022-01-29 11:03:54,606 - Epoch: [130][  300/  391]    Overall Loss 0.672460    Objective Loss 0.672460                                        LR 0.023500    Time 0.052606    
2022-01-29 11:03:59,349 - Epoch: [130][  391/  391]    Overall Loss 0.682894    Objective Loss 0.682894    Top1 76.923077    Top5 96.153846    LR 0.023500    Time 0.052490    
2022-01-29 11:03:59,407 - --- validate (epoch=130)-----------
2022-01-29 11:03:59,407 - 10000 samples (128 per mini-batch)
2022-01-29 11:04:01,068 - Epoch: [130][   79/   79]    Loss 1.374787    Top1 62.840000    Top5 88.420000    
2022-01-29 11:04:01,120 - ==> Top1: 62.840    Top5: 88.420    Loss: 1.375

2022-01-29 11:04:01,126 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:04:01,126 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:04:01,171 - 

2022-01-29 11:04:01,171 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:04:06,655 - Epoch: [131][  100/  391]    Overall Loss 0.642062    Objective Loss 0.642062                                        LR 0.023500    Time 0.054812    
2022-01-29 11:04:11,856 - Epoch: [131][  200/  391]    Overall Loss 0.656634    Objective Loss 0.656634                                        LR 0.023500    Time 0.053408    
2022-01-29 11:04:17,083 - Epoch: [131][  300/  391]    Overall Loss 0.672619    Objective Loss 0.672619                                        LR 0.023500    Time 0.053024    
2022-01-29 11:04:21,768 - Epoch: [131][  391/  391]    Overall Loss 0.682709    Objective Loss 0.682709    Top1 80.288462    Top5 95.673077    LR 0.023500    Time 0.052664    
2022-01-29 11:04:21,828 - --- validate (epoch=131)-----------
2022-01-29 11:04:21,828 - 10000 samples (128 per mini-batch)
2022-01-29 11:04:23,560 - Epoch: [131][   79/   79]    Loss 1.401224    Top1 62.300000    Top5 87.860000    
2022-01-29 11:04:23,612 - ==> Top1: 62.300    Top5: 87.860    Loss: 1.401

2022-01-29 11:04:23,618 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:04:23,618 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:04:23,663 - 

2022-01-29 11:04:23,663 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:04:29,109 - Epoch: [132][  100/  391]    Overall Loss 0.646485    Objective Loss 0.646485                                        LR 0.023500    Time 0.054430    
2022-01-29 11:04:34,203 - Epoch: [132][  200/  391]    Overall Loss 0.653266    Objective Loss 0.653266                                        LR 0.023500    Time 0.052681    
2022-01-29 11:04:39,296 - Epoch: [132][  300/  391]    Overall Loss 0.665169    Objective Loss 0.665169                                        LR 0.023500    Time 0.052094    
2022-01-29 11:04:43,922 - Epoch: [132][  391/  391]    Overall Loss 0.679361    Objective Loss 0.679361    Top1 78.365385    Top5 95.192308    LR 0.023500    Time 0.051797    
2022-01-29 11:04:43,980 - --- validate (epoch=132)-----------
2022-01-29 11:04:43,981 - 10000 samples (128 per mini-batch)
2022-01-29 11:04:45,660 - Epoch: [132][   79/   79]    Loss 1.407401    Top1 61.930000    Top5 88.020000    
2022-01-29 11:04:45,712 - ==> Top1: 61.930    Top5: 88.020    Loss: 1.407

2022-01-29 11:04:45,718 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:04:45,718 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:04:45,762 - 

2022-01-29 11:04:45,762 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:04:51,065 - Epoch: [133][  100/  391]    Overall Loss 0.635753    Objective Loss 0.635753                                        LR 0.023500    Time 0.053005    
2022-01-29 11:04:56,153 - Epoch: [133][  200/  391]    Overall Loss 0.660266    Objective Loss 0.660266                                        LR 0.023500    Time 0.051939    
2022-01-29 11:05:01,245 - Epoch: [133][  300/  391]    Overall Loss 0.675547    Objective Loss 0.675547                                        LR 0.023500    Time 0.051596    
2022-01-29 11:05:05,869 - Epoch: [133][  391/  391]    Overall Loss 0.682488    Objective Loss 0.682488    Top1 78.846154    Top5 98.557692    LR 0.023500    Time 0.051411    
2022-01-29 11:05:05,928 - --- validate (epoch=133)-----------
2022-01-29 11:05:05,928 - 10000 samples (128 per mini-batch)
2022-01-29 11:05:07,645 - Epoch: [133][   79/   79]    Loss 1.386730    Top1 62.170000    Top5 87.960000    
2022-01-29 11:05:07,698 - ==> Top1: 62.170    Top5: 87.960    Loss: 1.387

2022-01-29 11:05:07,704 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:05:07,704 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:05:07,747 - 

2022-01-29 11:05:07,748 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:05:13,086 - Epoch: [134][  100/  391]    Overall Loss 0.641016    Objective Loss 0.641016                                        LR 0.023500    Time 0.053355    
2022-01-29 11:05:18,290 - Epoch: [134][  200/  391]    Overall Loss 0.660744    Objective Loss 0.660744                                        LR 0.023500    Time 0.052695    
2022-01-29 11:05:23,489 - Epoch: [134][  300/  391]    Overall Loss 0.667821    Objective Loss 0.667821                                        LR 0.023500    Time 0.052456    
2022-01-29 11:05:28,217 - Epoch: [134][  391/  391]    Overall Loss 0.679145    Objective Loss 0.679145    Top1 76.923077    Top5 94.711538    LR 0.023500    Time 0.052339    
2022-01-29 11:05:28,276 - --- validate (epoch=134)-----------
2022-01-29 11:05:28,277 - 10000 samples (128 per mini-batch)
2022-01-29 11:05:29,952 - Epoch: [134][   79/   79]    Loss 1.470747    Top1 61.370000    Top5 87.400000    
2022-01-29 11:05:30,008 - ==> Top1: 61.370    Top5: 87.400    Loss: 1.471

2022-01-29 11:05:30,014 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:05:30,014 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:05:30,059 - 

2022-01-29 11:05:30,059 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:05:35,409 - Epoch: [135][  100/  391]    Overall Loss 0.646528    Objective Loss 0.646528                                        LR 0.023500    Time 0.053471    
2022-01-29 11:05:40,496 - Epoch: [135][  200/  391]    Overall Loss 0.660099    Objective Loss 0.660099                                        LR 0.023500    Time 0.052164    
2022-01-29 11:05:45,511 - Epoch: [135][  300/  391]    Overall Loss 0.673705    Objective Loss 0.673705                                        LR 0.023500    Time 0.051488    
2022-01-29 11:05:50,066 - Epoch: [135][  391/  391]    Overall Loss 0.678620    Objective Loss 0.678620    Top1 81.250000    Top5 96.634615    LR 0.023500    Time 0.051152    
2022-01-29 11:05:50,123 - --- validate (epoch=135)-----------
2022-01-29 11:05:50,123 - 10000 samples (128 per mini-batch)
2022-01-29 11:05:51,782 - Epoch: [135][   79/   79]    Loss 1.417256    Top1 61.930000    Top5 87.560000    
2022-01-29 11:05:51,835 - ==> Top1: 61.930    Top5: 87.560    Loss: 1.417

2022-01-29 11:05:51,840 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:05:51,840 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:05:51,884 - 

2022-01-29 11:05:51,884 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:05:57,322 - Epoch: [136][  100/  391]    Overall Loss 0.637554    Objective Loss 0.637554                                        LR 0.023500    Time 0.054356    
2022-01-29 11:06:02,562 - Epoch: [136][  200/  391]    Overall Loss 0.642226    Objective Loss 0.642226                                        LR 0.023500    Time 0.053370    
2022-01-29 11:06:07,787 - Epoch: [136][  300/  391]    Overall Loss 0.660667    Objective Loss 0.660667                                        LR 0.023500    Time 0.052996    
2022-01-29 11:06:12,531 - Epoch: [136][  391/  391]    Overall Loss 0.677500    Objective Loss 0.677500    Top1 78.365385    Top5 97.596154    LR 0.023500    Time 0.052793    
2022-01-29 11:06:12,589 - --- validate (epoch=136)-----------
2022-01-29 11:06:12,590 - 10000 samples (128 per mini-batch)
2022-01-29 11:06:14,294 - Epoch: [136][   79/   79]    Loss 1.374274    Top1 62.550000    Top5 88.170000    
2022-01-29 11:06:14,349 - ==> Top1: 62.550    Top5: 88.170    Loss: 1.374

2022-01-29 11:06:14,355 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:06:14,355 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:06:14,400 - 

2022-01-29 11:06:14,400 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:06:19,835 - Epoch: [137][  100/  391]    Overall Loss 0.637391    Objective Loss 0.637391                                        LR 0.023500    Time 0.054326    
2022-01-29 11:06:25,052 - Epoch: [137][  200/  391]    Overall Loss 0.649221    Objective Loss 0.649221                                        LR 0.023500    Time 0.053243    
2022-01-29 11:06:30,274 - Epoch: [137][  300/  391]    Overall Loss 0.662629    Objective Loss 0.662629                                        LR 0.023500    Time 0.052899    
2022-01-29 11:06:35,019 - Epoch: [137][  391/  391]    Overall Loss 0.672235    Objective Loss 0.672235    Top1 78.365385    Top5 97.596154    LR 0.023500    Time 0.052723    
2022-01-29 11:06:35,079 - --- validate (epoch=137)-----------
2022-01-29 11:06:35,079 - 10000 samples (128 per mini-batch)
2022-01-29 11:06:36,779 - Epoch: [137][   79/   79]    Loss 1.431337    Top1 61.870000    Top5 87.890000    
2022-01-29 11:06:36,837 - ==> Top1: 61.870    Top5: 87.890    Loss: 1.431

2022-01-29 11:06:36,842 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:06:36,842 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:06:36,887 - 

2022-01-29 11:06:36,887 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:06:42,401 - Epoch: [138][  100/  391]    Overall Loss 0.637604    Objective Loss 0.637604                                        LR 0.023500    Time 0.055113    
2022-01-29 11:06:47,684 - Epoch: [138][  200/  391]    Overall Loss 0.652590    Objective Loss 0.652590                                        LR 0.023500    Time 0.053968    
2022-01-29 11:06:52,978 - Epoch: [138][  300/  391]    Overall Loss 0.660685    Objective Loss 0.660685                                        LR 0.023500    Time 0.053621    
2022-01-29 11:06:57,792 - Epoch: [138][  391/  391]    Overall Loss 0.669182    Objective Loss 0.669182    Top1 74.038462    Top5 94.711538    LR 0.023500    Time 0.053452    
2022-01-29 11:06:57,851 - --- validate (epoch=138)-----------
2022-01-29 11:06:57,852 - 10000 samples (128 per mini-batch)
2022-01-29 11:06:59,645 - Epoch: [138][   79/   79]    Loss 1.450870    Top1 61.280000    Top5 87.310000    
2022-01-29 11:06:59,705 - ==> Top1: 61.280    Top5: 87.310    Loss: 1.451

2022-01-29 11:06:59,710 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:06:59,710 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:06:59,755 - 

2022-01-29 11:06:59,756 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:07:05,262 - Epoch: [139][  100/  391]    Overall Loss 0.634769    Objective Loss 0.634769                                        LR 0.023500    Time 0.055031    
2022-01-29 11:07:10,470 - Epoch: [139][  200/  391]    Overall Loss 0.649478    Objective Loss 0.649478                                        LR 0.023500    Time 0.053552    
2022-01-29 11:07:15,684 - Epoch: [139][  300/  391]    Overall Loss 0.652973    Objective Loss 0.652973                                        LR 0.023500    Time 0.053079    
2022-01-29 11:07:20,428 - Epoch: [139][  391/  391]    Overall Loss 0.663398    Objective Loss 0.663398    Top1 77.403846    Top5 97.596154    LR 0.023500    Time 0.052855    
2022-01-29 11:07:20,486 - --- validate (epoch=139)-----------
2022-01-29 11:07:20,486 - 10000 samples (128 per mini-batch)
2022-01-29 11:07:22,179 - Epoch: [139][   79/   79]    Loss 1.442212    Top1 61.420000    Top5 87.310000    
2022-01-29 11:07:22,230 - ==> Top1: 61.420    Top5: 87.310    Loss: 1.442

2022-01-29 11:07:22,236 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:07:22,236 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:07:22,281 - 

2022-01-29 11:07:22,281 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:07:27,747 - Epoch: [140][  100/  391]    Overall Loss 0.636102    Objective Loss 0.636102                                        LR 0.023500    Time 0.054637    
2022-01-29 11:07:33,013 - Epoch: [140][  200/  391]    Overall Loss 0.646258    Objective Loss 0.646258                                        LR 0.023500    Time 0.053643    
2022-01-29 11:07:38,254 - Epoch: [140][  300/  391]    Overall Loss 0.652576    Objective Loss 0.652576                                        LR 0.023500    Time 0.053227    
2022-01-29 11:07:42,994 - Epoch: [140][  391/  391]    Overall Loss 0.663492    Objective Loss 0.663492    Top1 75.961538    Top5 99.038462    LR 0.023500    Time 0.052945    
2022-01-29 11:07:43,053 - --- validate (epoch=140)-----------
2022-01-29 11:07:43,053 - 10000 samples (128 per mini-batch)
2022-01-29 11:07:44,751 - Epoch: [140][   79/   79]    Loss 1.470503    Top1 60.810000    Top5 86.890000    
2022-01-29 11:07:44,810 - ==> Top1: 60.810    Top5: 86.890    Loss: 1.471

2022-01-29 11:07:44,815 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:07:44,815 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:07:44,859 - 

2022-01-29 11:07:44,859 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:07:50,294 - Epoch: [141][  100/  391]    Overall Loss 0.644099    Objective Loss 0.644099                                        LR 0.023500    Time 0.054323    
2022-01-29 11:07:55,513 - Epoch: [141][  200/  391]    Overall Loss 0.648578    Objective Loss 0.648578                                        LR 0.023500    Time 0.053251    
2022-01-29 11:08:00,741 - Epoch: [141][  300/  391]    Overall Loss 0.656395    Objective Loss 0.656395                                        LR 0.023500    Time 0.052923    
2022-01-29 11:08:05,495 - Epoch: [141][  391/  391]    Overall Loss 0.662186    Objective Loss 0.662186    Top1 77.403846    Top5 97.596154    LR 0.023500    Time 0.052760    
2022-01-29 11:08:05,553 - --- validate (epoch=141)-----------
2022-01-29 11:08:05,553 - 10000 samples (128 per mini-batch)
2022-01-29 11:08:07,252 - Epoch: [141][   79/   79]    Loss 1.406375    Top1 62.370000    Top5 88.210000    
2022-01-29 11:08:07,312 - ==> Top1: 62.370    Top5: 88.210    Loss: 1.406

2022-01-29 11:08:07,318 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:08:07,318 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:08:07,363 - 

2022-01-29 11:08:07,364 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:08:12,891 - Epoch: [142][  100/  391]    Overall Loss 0.629781    Objective Loss 0.629781                                        LR 0.023500    Time 0.055246    
2022-01-29 11:08:18,119 - Epoch: [142][  200/  391]    Overall Loss 0.640902    Objective Loss 0.640902                                        LR 0.023500    Time 0.053757    
2022-01-29 11:08:23,328 - Epoch: [142][  300/  391]    Overall Loss 0.653960    Objective Loss 0.653960                                        LR 0.023500    Time 0.053198    
2022-01-29 11:08:28,068 - Epoch: [142][  391/  391]    Overall Loss 0.663051    Objective Loss 0.663051    Top1 79.326923    Top5 95.673077    LR 0.023500    Time 0.052939    
2022-01-29 11:08:28,128 - --- validate (epoch=142)-----------
2022-01-29 11:08:28,128 - 10000 samples (128 per mini-batch)
2022-01-29 11:08:29,770 - Epoch: [142][   79/   79]    Loss 1.436093    Top1 61.710000    Top5 87.930000    
2022-01-29 11:08:29,827 - ==> Top1: 61.710    Top5: 87.930    Loss: 1.436

2022-01-29 11:08:29,832 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:08:29,832 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:08:29,877 - 

2022-01-29 11:08:29,877 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:08:35,376 - Epoch: [143][  100/  391]    Overall Loss 0.624871    Objective Loss 0.624871                                        LR 0.023500    Time 0.054963    
2022-01-29 11:08:40,617 - Epoch: [143][  200/  391]    Overall Loss 0.638411    Objective Loss 0.638411                                        LR 0.023500    Time 0.053683    
2022-01-29 11:08:45,843 - Epoch: [143][  300/  391]    Overall Loss 0.650341    Objective Loss 0.650341                                        LR 0.023500    Time 0.053206    
2022-01-29 11:08:50,591 - Epoch: [143][  391/  391]    Overall Loss 0.659893    Objective Loss 0.659893    Top1 79.326923    Top5 98.076923    LR 0.023500    Time 0.052964    
2022-01-29 11:08:50,649 - --- validate (epoch=143)-----------
2022-01-29 11:08:50,650 - 10000 samples (128 per mini-batch)
2022-01-29 11:08:52,354 - Epoch: [143][   79/   79]    Loss 1.444022    Top1 61.630000    Top5 87.160000    
2022-01-29 11:08:52,405 - ==> Top1: 61.630    Top5: 87.160    Loss: 1.444

2022-01-29 11:08:52,410 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:08:52,411 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:08:52,456 - 

2022-01-29 11:08:52,456 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:08:57,916 - Epoch: [144][  100/  391]    Overall Loss 0.619313    Objective Loss 0.619313                                        LR 0.023500    Time 0.054576    
2022-01-29 11:09:03,186 - Epoch: [144][  200/  391]    Overall Loss 0.629271    Objective Loss 0.629271                                        LR 0.023500    Time 0.053635    
2022-01-29 11:09:08,412 - Epoch: [144][  300/  391]    Overall Loss 0.643931    Objective Loss 0.643931                                        LR 0.023500    Time 0.053173    
2022-01-29 11:09:13,214 - Epoch: [144][  391/  391]    Overall Loss 0.656301    Objective Loss 0.656301    Top1 78.846154    Top5 98.076923    LR 0.023500    Time 0.053077    
2022-01-29 11:09:13,274 - --- validate (epoch=144)-----------
2022-01-29 11:09:13,274 - 10000 samples (128 per mini-batch)
2022-01-29 11:09:14,951 - Epoch: [144][   79/   79]    Loss 1.430701    Top1 61.800000    Top5 87.710000    
2022-01-29 11:09:15,003 - ==> Top1: 61.800    Top5: 87.710    Loss: 1.431

2022-01-29 11:09:15,009 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:09:15,009 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:09:15,054 - 

2022-01-29 11:09:15,054 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:09:20,675 - Epoch: [145][  100/  391]    Overall Loss 0.608633    Objective Loss 0.608633                                        LR 0.023500    Time 0.056181    
2022-01-29 11:09:25,978 - Epoch: [145][  200/  391]    Overall Loss 0.628033    Objective Loss 0.628033                                        LR 0.023500    Time 0.054601    
2022-01-29 11:09:31,215 - Epoch: [145][  300/  391]    Overall Loss 0.643627    Objective Loss 0.643627                                        LR 0.023500    Time 0.053855    
2022-01-29 11:09:35,977 - Epoch: [145][  391/  391]    Overall Loss 0.656456    Objective Loss 0.656456    Top1 79.807692    Top5 98.557692    LR 0.023500    Time 0.053497    
2022-01-29 11:09:36,037 - --- validate (epoch=145)-----------
2022-01-29 11:09:36,037 - 10000 samples (128 per mini-batch)
2022-01-29 11:09:37,732 - Epoch: [145][   79/   79]    Loss 1.447012    Top1 61.150000    Top5 87.680000    
2022-01-29 11:09:37,791 - ==> Top1: 61.150    Top5: 87.680    Loss: 1.447

2022-01-29 11:09:37,796 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:09:37,797 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:09:37,841 - 

2022-01-29 11:09:37,842 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:09:43,388 - Epoch: [146][  100/  391]    Overall Loss 0.637820    Objective Loss 0.637820                                        LR 0.023500    Time 0.055430    
2022-01-29 11:09:48,615 - Epoch: [146][  200/  391]    Overall Loss 0.637418    Objective Loss 0.637418                                        LR 0.023500    Time 0.053846    
2022-01-29 11:09:53,836 - Epoch: [146][  300/  391]    Overall Loss 0.643729    Objective Loss 0.643729                                        LR 0.023500    Time 0.053298    
2022-01-29 11:09:58,586 - Epoch: [146][  391/  391]    Overall Loss 0.651728    Objective Loss 0.651728    Top1 82.211538    Top5 97.115385    LR 0.023500    Time 0.053038    
2022-01-29 11:09:58,652 - --- validate (epoch=146)-----------
2022-01-29 11:09:58,652 - 10000 samples (128 per mini-batch)
2022-01-29 11:10:00,315 - Epoch: [146][   79/   79]    Loss 1.453327    Top1 61.490000    Top5 87.650000    
2022-01-29 11:10:00,367 - ==> Top1: 61.490    Top5: 87.650    Loss: 1.453

2022-01-29 11:10:00,373 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:10:00,373 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:10:00,416 - 

2022-01-29 11:10:00,416 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:10:05,862 - Epoch: [147][  100/  391]    Overall Loss 0.635460    Objective Loss 0.635460                                        LR 0.023500    Time 0.054429    
2022-01-29 11:10:11,095 - Epoch: [147][  200/  391]    Overall Loss 0.646860    Objective Loss 0.646860                                        LR 0.023500    Time 0.053374    
2022-01-29 11:10:16,320 - Epoch: [147][  300/  391]    Overall Loss 0.652298    Objective Loss 0.652298                                        LR 0.023500    Time 0.052997    
2022-01-29 11:10:21,095 - Epoch: [147][  391/  391]    Overall Loss 0.657245    Objective Loss 0.657245    Top1 82.211538    Top5 98.076923    LR 0.023500    Time 0.052875    
2022-01-29 11:10:21,154 - --- validate (epoch=147)-----------
2022-01-29 11:10:21,154 - 10000 samples (128 per mini-batch)
2022-01-29 11:10:23,021 - Epoch: [147][   79/   79]    Loss 1.399535    Top1 62.000000    Top5 88.310000    
2022-01-29 11:10:23,077 - ==> Top1: 62.000    Top5: 88.310    Loss: 1.400

2022-01-29 11:10:23,083 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:10:23,083 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:10:23,128 - 

2022-01-29 11:10:23,128 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:10:28,649 - Epoch: [148][  100/  391]    Overall Loss 0.611046    Objective Loss 0.611046                                        LR 0.023500    Time 0.055189    
2022-01-29 11:10:33,884 - Epoch: [148][  200/  391]    Overall Loss 0.631724    Objective Loss 0.631724                                        LR 0.023500    Time 0.053765    
2022-01-29 11:10:39,070 - Epoch: [148][  300/  391]    Overall Loss 0.643007    Objective Loss 0.643007                                        LR 0.023500    Time 0.053127    
2022-01-29 11:10:43,707 - Epoch: [148][  391/  391]    Overall Loss 0.651370    Objective Loss 0.651370    Top1 81.250000    Top5 96.153846    LR 0.023500    Time 0.052618    
2022-01-29 11:10:43,766 - --- validate (epoch=148)-----------
2022-01-29 11:10:43,766 - 10000 samples (128 per mini-batch)
2022-01-29 11:10:45,481 - Epoch: [148][   79/   79]    Loss 1.423073    Top1 61.720000    Top5 87.800000    
2022-01-29 11:10:45,534 - ==> Top1: 61.720    Top5: 87.800    Loss: 1.423

2022-01-29 11:10:45,539 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:10:45,540 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:10:45,583 - 

2022-01-29 11:10:45,583 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:10:50,980 - Epoch: [149][  100/  391]    Overall Loss 0.628184    Objective Loss 0.628184                                        LR 0.023500    Time 0.053936    
2022-01-29 11:10:56,072 - Epoch: [149][  200/  391]    Overall Loss 0.634871    Objective Loss 0.634871                                        LR 0.023500    Time 0.052424    
2022-01-29 11:11:01,170 - Epoch: [149][  300/  391]    Overall Loss 0.647260    Objective Loss 0.647260                                        LR 0.023500    Time 0.051940    
2022-01-29 11:11:05,796 - Epoch: [149][  391/  391]    Overall Loss 0.654693    Objective Loss 0.654693    Top1 74.519231    Top5 96.153846    LR 0.023500    Time 0.051679    
2022-01-29 11:11:05,855 - --- validate (epoch=149)-----------
2022-01-29 11:11:05,855 - 10000 samples (128 per mini-batch)
2022-01-29 11:11:07,586 - Epoch: [149][   79/   79]    Loss 1.435578    Top1 61.740000    Top5 87.580000    
2022-01-29 11:11:07,641 - ==> Top1: 61.740    Top5: 87.580    Loss: 1.436

2022-01-29 11:11:07,647 - ==> Best [Top1: 64.070   Top5: 89.240   Sparsity:0.00   Params: 627712 on epoch: 106]
2022-01-29 11:11:07,647 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:11:07,688 - 

2022-01-29 11:11:07,688 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:11:13,220 - Epoch: [150][  100/  391]    Overall Loss 0.540682    Objective Loss 0.540682                                        LR 0.005522    Time 0.055288    
2022-01-29 11:11:18,461 - Epoch: [150][  200/  391]    Overall Loss 0.525764    Objective Loss 0.525764                                        LR 0.005522    Time 0.053846    
2022-01-29 11:11:23,699 - Epoch: [150][  300/  391]    Overall Loss 0.510246    Objective Loss 0.510246                                        LR 0.005522    Time 0.053355    
2022-01-29 11:11:28,459 - Epoch: [150][  391/  391]    Overall Loss 0.505062    Objective Loss 0.505062    Top1 87.980769    Top5 99.038462    LR 0.005522    Time 0.053108    
2022-01-29 11:11:28,516 - --- validate (epoch=150)-----------
2022-01-29 11:11:28,517 - 10000 samples (128 per mini-batch)
2022-01-29 11:11:30,182 - Epoch: [150][   79/   79]    Loss 1.302761    Top1 65.140000    Top5 89.450000    
2022-01-29 11:11:30,236 - ==> Top1: 65.140    Top5: 89.450    Loss: 1.303

2022-01-29 11:11:30,242 - ==> Best [Top1: 65.140   Top5: 89.450   Sparsity:0.00   Params: 627712 on epoch: 150]
2022-01-29 11:11:30,242 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:11:30,289 - 

2022-01-29 11:11:30,290 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:11:35,668 - Epoch: [151][  100/  391]    Overall Loss 0.455149    Objective Loss 0.455149                                        LR 0.005522    Time 0.053761    
2022-01-29 11:11:40,909 - Epoch: [151][  200/  391]    Overall Loss 0.451585    Objective Loss 0.451585                                        LR 0.005522    Time 0.053077    
2022-01-29 11:11:46,136 - Epoch: [151][  300/  391]    Overall Loss 0.455029    Objective Loss 0.455029                                        LR 0.005522    Time 0.052808    
2022-01-29 11:11:50,862 - Epoch: [151][  391/  391]    Overall Loss 0.457282    Objective Loss 0.457282    Top1 86.538462    Top5 98.557692    LR 0.005522    Time 0.052602    
2022-01-29 11:11:50,915 - --- validate (epoch=151)-----------
2022-01-29 11:11:50,915 - 10000 samples (128 per mini-batch)
2022-01-29 11:11:52,556 - Epoch: [151][   79/   79]    Loss 1.285687    Top1 65.380000    Top5 89.340000    
2022-01-29 11:11:52,612 - ==> Top1: 65.380    Top5: 89.340    Loss: 1.286

2022-01-29 11:11:52,617 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:11:52,617 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:11:52,665 - 

2022-01-29 11:11:52,666 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:11:58,142 - Epoch: [152][  100/  391]    Overall Loss 0.433800    Objective Loss 0.433800                                        LR 0.005522    Time 0.054733    
2022-01-29 11:12:03,375 - Epoch: [152][  200/  391]    Overall Loss 0.431865    Objective Loss 0.431865                                        LR 0.005522    Time 0.053530    
2022-01-29 11:12:08,590 - Epoch: [152][  300/  391]    Overall Loss 0.438764    Objective Loss 0.438764                                        LR 0.005522    Time 0.053068    
2022-01-29 11:12:13,333 - Epoch: [152][  391/  391]    Overall Loss 0.442311    Objective Loss 0.442311    Top1 79.326923    Top5 98.557692    LR 0.005522    Time 0.052845    
2022-01-29 11:12:13,391 - --- validate (epoch=152)-----------
2022-01-29 11:12:13,391 - 10000 samples (128 per mini-batch)
2022-01-29 11:12:15,176 - Epoch: [152][   79/   79]    Loss 1.300154    Top1 65.320000    Top5 89.170000    
2022-01-29 11:12:15,229 - ==> Top1: 65.320    Top5: 89.170    Loss: 1.300

2022-01-29 11:12:15,235 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:12:15,235 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:12:15,281 - 

2022-01-29 11:12:15,281 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:12:20,794 - Epoch: [153][  100/  391]    Overall Loss 0.428583    Objective Loss 0.428583                                        LR 0.005522    Time 0.055109    
2022-01-29 11:12:26,022 - Epoch: [153][  200/  391]    Overall Loss 0.428935    Objective Loss 0.428935                                        LR 0.005522    Time 0.053690    
2022-01-29 11:12:31,248 - Epoch: [153][  300/  391]    Overall Loss 0.427422    Objective Loss 0.427422                                        LR 0.005522    Time 0.053211    
2022-01-29 11:12:36,001 - Epoch: [153][  391/  391]    Overall Loss 0.431277    Objective Loss 0.431277    Top1 88.461538    Top5 100.000000    LR 0.005522    Time 0.052981    
2022-01-29 11:12:36,060 - --- validate (epoch=153)-----------
2022-01-29 11:12:36,061 - 10000 samples (128 per mini-batch)
2022-01-29 11:12:37,843 - Epoch: [153][   79/   79]    Loss 1.295452    Top1 65.350000    Top5 89.240000    
2022-01-29 11:12:37,902 - ==> Top1: 65.350    Top5: 89.240    Loss: 1.295

2022-01-29 11:12:37,907 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:12:37,907 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:12:37,945 - 

2022-01-29 11:12:37,945 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:12:43,378 - Epoch: [154][  100/  391]    Overall Loss 0.408175    Objective Loss 0.408175                                        LR 0.005522    Time 0.054307    
2022-01-29 11:12:48,570 - Epoch: [154][  200/  391]    Overall Loss 0.418917    Objective Loss 0.418917                                        LR 0.005522    Time 0.053106    
2022-01-29 11:12:53,789 - Epoch: [154][  300/  391]    Overall Loss 0.421214    Objective Loss 0.421214                                        LR 0.005522    Time 0.052798    
2022-01-29 11:12:58,531 - Epoch: [154][  391/  391]    Overall Loss 0.421490    Objective Loss 0.421490    Top1 87.980769    Top5 99.038462    LR 0.005522    Time 0.052636    
2022-01-29 11:12:58,592 - --- validate (epoch=154)-----------
2022-01-29 11:12:58,592 - 10000 samples (128 per mini-batch)
2022-01-29 11:13:00,338 - Epoch: [154][   79/   79]    Loss 1.303122    Top1 65.360000    Top5 89.310000    
2022-01-29 11:13:00,393 - ==> Top1: 65.360    Top5: 89.310    Loss: 1.303

2022-01-29 11:13:00,398 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:13:00,399 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:13:00,435 - 

2022-01-29 11:13:00,435 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:13:05,746 - Epoch: [155][  100/  391]    Overall Loss 0.398735    Objective Loss 0.398735                                        LR 0.005522    Time 0.053084    
2022-01-29 11:13:10,774 - Epoch: [155][  200/  391]    Overall Loss 0.408330    Objective Loss 0.408330                                        LR 0.005522    Time 0.051679    
2022-01-29 11:13:15,923 - Epoch: [155][  300/  391]    Overall Loss 0.412980    Objective Loss 0.412980                                        LR 0.005522    Time 0.051612    
2022-01-29 11:13:20,645 - Epoch: [155][  391/  391]    Overall Loss 0.414529    Objective Loss 0.414529    Top1 88.461538    Top5 99.038462    LR 0.005522    Time 0.051674    
2022-01-29 11:13:20,704 - --- validate (epoch=155)-----------
2022-01-29 11:13:20,704 - 10000 samples (128 per mini-batch)
2022-01-29 11:13:22,382 - Epoch: [155][   79/   79]    Loss 1.322712    Top1 65.110000    Top5 88.900000    
2022-01-29 11:13:22,442 - ==> Top1: 65.110    Top5: 88.900    Loss: 1.323

2022-01-29 11:13:22,448 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:13:22,448 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:13:22,493 - 

2022-01-29 11:13:22,493 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:13:27,861 - Epoch: [156][  100/  391]    Overall Loss 0.399414    Objective Loss 0.399414                                        LR 0.005522    Time 0.053656    
2022-01-29 11:13:32,970 - Epoch: [156][  200/  391]    Overall Loss 0.401488    Objective Loss 0.401488                                        LR 0.005522    Time 0.052365    
2022-01-29 11:13:38,108 - Epoch: [156][  300/  391]    Overall Loss 0.407862    Objective Loss 0.407862                                        LR 0.005522    Time 0.052037    
2022-01-29 11:13:42,835 - Epoch: [156][  391/  391]    Overall Loss 0.408470    Objective Loss 0.408470    Top1 90.384615    Top5 99.038462    LR 0.005522    Time 0.052011    
2022-01-29 11:13:42,894 - --- validate (epoch=156)-----------
2022-01-29 11:13:42,894 - 10000 samples (128 per mini-batch)
2022-01-29 11:13:44,571 - Epoch: [156][   79/   79]    Loss 1.306678    Top1 65.140000    Top5 89.100000    
2022-01-29 11:13:44,625 - ==> Top1: 65.140    Top5: 89.100    Loss: 1.307

2022-01-29 11:13:44,630 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:13:44,630 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:13:44,675 - 

2022-01-29 11:13:44,676 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:13:50,162 - Epoch: [157][  100/  391]    Overall Loss 0.386022    Objective Loss 0.386022                                        LR 0.005522    Time 0.054839    
2022-01-29 11:13:55,400 - Epoch: [157][  200/  391]    Overall Loss 0.396773    Objective Loss 0.396773                                        LR 0.005522    Time 0.053605    
2022-01-29 11:14:00,644 - Epoch: [157][  300/  391]    Overall Loss 0.402456    Objective Loss 0.402456                                        LR 0.005522    Time 0.053213    
2022-01-29 11:14:05,408 - Epoch: [157][  391/  391]    Overall Loss 0.406151    Objective Loss 0.406151    Top1 87.980769    Top5 99.519231    LR 0.005522    Time 0.053011    
2022-01-29 11:14:05,466 - --- validate (epoch=157)-----------
2022-01-29 11:14:05,466 - 10000 samples (128 per mini-batch)
2022-01-29 11:14:07,233 - Epoch: [157][   79/   79]    Loss 1.315505    Top1 65.020000    Top5 89.110000    
2022-01-29 11:14:07,289 - ==> Top1: 65.020    Top5: 89.110    Loss: 1.316

2022-01-29 11:14:07,295 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:14:07,295 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:14:07,334 - 

2022-01-29 11:14:07,334 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:14:12,806 - Epoch: [158][  100/  391]    Overall Loss 0.390580    Objective Loss 0.390580                                        LR 0.005522    Time 0.054695    
2022-01-29 11:14:18,056 - Epoch: [158][  200/  391]    Overall Loss 0.398854    Objective Loss 0.398854                                        LR 0.005522    Time 0.053591    
2022-01-29 11:14:23,287 - Epoch: [158][  300/  391]    Overall Loss 0.400197    Objective Loss 0.400197                                        LR 0.005522    Time 0.053160    
2022-01-29 11:14:28,038 - Epoch: [158][  391/  391]    Overall Loss 0.398882    Objective Loss 0.398882    Top1 90.384615    Top5 99.038462    LR 0.005522    Time 0.052938    
2022-01-29 11:14:28,097 - --- validate (epoch=158)-----------
2022-01-29 11:14:28,098 - 10000 samples (128 per mini-batch)
2022-01-29 11:14:29,741 - Epoch: [158][   79/   79]    Loss 1.324767    Top1 64.900000    Top5 89.050000    
2022-01-29 11:14:29,801 - ==> Top1: 64.900    Top5: 89.050    Loss: 1.325

2022-01-29 11:14:29,807 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:14:29,807 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:14:29,850 - 

2022-01-29 11:14:29,851 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:14:35,389 - Epoch: [159][  100/  391]    Overall Loss 0.386595    Objective Loss 0.386595                                        LR 0.005522    Time 0.055353    
2022-01-29 11:14:40,626 - Epoch: [159][  200/  391]    Overall Loss 0.395077    Objective Loss 0.395077                                        LR 0.005522    Time 0.053861    
2022-01-29 11:14:45,867 - Epoch: [159][  300/  391]    Overall Loss 0.398154    Objective Loss 0.398154                                        LR 0.005522    Time 0.053373    
2022-01-29 11:14:50,634 - Epoch: [159][  391/  391]    Overall Loss 0.396611    Objective Loss 0.396611    Top1 87.019231    Top5 98.076923    LR 0.005522    Time 0.053143    
2022-01-29 11:14:50,693 - --- validate (epoch=159)-----------
2022-01-29 11:14:50,693 - 10000 samples (128 per mini-batch)
2022-01-29 11:14:52,357 - Epoch: [159][   79/   79]    Loss 1.334412    Top1 64.660000    Top5 89.200000    
2022-01-29 11:14:52,415 - ==> Top1: 64.660    Top5: 89.200    Loss: 1.334

2022-01-29 11:14:52,420 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:14:52,420 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:14:52,466 - 

2022-01-29 11:14:52,466 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:14:57,995 - Epoch: [160][  100/  391]    Overall Loss 0.368696    Objective Loss 0.368696                                        LR 0.005522    Time 0.055265    
2022-01-29 11:15:03,246 - Epoch: [160][  200/  391]    Overall Loss 0.379975    Objective Loss 0.379975                                        LR 0.005522    Time 0.053884    
2022-01-29 11:15:08,489 - Epoch: [160][  300/  391]    Overall Loss 0.385459    Objective Loss 0.385459                                        LR 0.005522    Time 0.053396    
2022-01-29 11:15:13,230 - Epoch: [160][  391/  391]    Overall Loss 0.388931    Objective Loss 0.388931    Top1 90.384615    Top5 100.000000    LR 0.005522    Time 0.053092    
2022-01-29 11:15:13,289 - --- validate (epoch=160)-----------
2022-01-29 11:15:13,289 - 10000 samples (128 per mini-batch)
2022-01-29 11:15:14,947 - Epoch: [160][   79/   79]    Loss 1.332059    Top1 64.880000    Top5 89.150000    
2022-01-29 11:15:15,002 - ==> Top1: 64.880    Top5: 89.150    Loss: 1.332

2022-01-29 11:15:15,007 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:15:15,007 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:15:15,052 - 

2022-01-29 11:15:15,052 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:15:20,556 - Epoch: [161][  100/  391]    Overall Loss 0.373692    Objective Loss 0.373692                                        LR 0.005522    Time 0.055013    
2022-01-29 11:15:25,811 - Epoch: [161][  200/  391]    Overall Loss 0.377612    Objective Loss 0.377612                                        LR 0.005522    Time 0.053775    
2022-01-29 11:15:31,118 - Epoch: [161][  300/  391]    Overall Loss 0.385073    Objective Loss 0.385073                                        LR 0.005522    Time 0.053538    
2022-01-29 11:15:35,939 - Epoch: [161][  391/  391]    Overall Loss 0.385069    Objective Loss 0.385069    Top1 91.826923    Top5 98.076923    LR 0.005522    Time 0.053406    
2022-01-29 11:15:35,998 - --- validate (epoch=161)-----------
2022-01-29 11:15:35,998 - 10000 samples (128 per mini-batch)
2022-01-29 11:15:37,653 - Epoch: [161][   79/   79]    Loss 1.339185    Top1 65.070000    Top5 89.130000    
2022-01-29 11:15:37,708 - ==> Top1: 65.070    Top5: 89.130    Loss: 1.339

2022-01-29 11:15:37,714 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:15:37,714 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:15:37,759 - 

2022-01-29 11:15:37,759 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:15:43,335 - Epoch: [162][  100/  391]    Overall Loss 0.373655    Objective Loss 0.373655                                        LR 0.005522    Time 0.055738    
2022-01-29 11:15:48,569 - Epoch: [162][  200/  391]    Overall Loss 0.377677    Objective Loss 0.377677                                        LR 0.005522    Time 0.054033    
2022-01-29 11:15:53,810 - Epoch: [162][  300/  391]    Overall Loss 0.378858    Objective Loss 0.378858                                        LR 0.005522    Time 0.053489    
2022-01-29 11:15:58,573 - Epoch: [162][  391/  391]    Overall Loss 0.382848    Objective Loss 0.382848    Top1 86.538462    Top5 98.557692    LR 0.005522    Time 0.053219    
2022-01-29 11:15:58,634 - --- validate (epoch=162)-----------
2022-01-29 11:15:58,634 - 10000 samples (128 per mini-batch)
2022-01-29 11:16:00,315 - Epoch: [162][   79/   79]    Loss 1.331868    Top1 64.710000    Top5 89.040000    
2022-01-29 11:16:00,370 - ==> Top1: 64.710    Top5: 89.040    Loss: 1.332

2022-01-29 11:16:00,375 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:16:00,375 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:16:00,420 - 

2022-01-29 11:16:00,420 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:16:05,893 - Epoch: [163][  100/  391]    Overall Loss 0.369433    Objective Loss 0.369433                                        LR 0.005522    Time 0.054694    
2022-01-29 11:16:11,130 - Epoch: [163][  200/  391]    Overall Loss 0.372183    Objective Loss 0.372183                                        LR 0.005522    Time 0.053530    
2022-01-29 11:16:16,368 - Epoch: [163][  300/  391]    Overall Loss 0.378164    Objective Loss 0.378164                                        LR 0.005522    Time 0.053140    
2022-01-29 11:16:21,113 - Epoch: [163][  391/  391]    Overall Loss 0.382608    Objective Loss 0.382608    Top1 89.423077    Top5 99.519231    LR 0.005522    Time 0.052908    
2022-01-29 11:16:21,171 - --- validate (epoch=163)-----------
2022-01-29 11:16:21,172 - 10000 samples (128 per mini-batch)
2022-01-29 11:16:22,904 - Epoch: [163][   79/   79]    Loss 1.351350    Top1 64.690000    Top5 88.900000    
2022-01-29 11:16:22,966 - ==> Top1: 64.690    Top5: 88.900    Loss: 1.351

2022-01-29 11:16:22,972 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:16:22,972 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:16:23,009 - 

2022-01-29 11:16:23,009 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:16:28,502 - Epoch: [164][  100/  391]    Overall Loss 0.360946    Objective Loss 0.360946                                        LR 0.005522    Time 0.054902    
2022-01-29 11:16:33,717 - Epoch: [164][  200/  391]    Overall Loss 0.362961    Objective Loss 0.362961                                        LR 0.005522    Time 0.053521    
2022-01-29 11:16:38,947 - Epoch: [164][  300/  391]    Overall Loss 0.369785    Objective Loss 0.369785                                        LR 0.005522    Time 0.053110    
2022-01-29 11:16:43,695 - Epoch: [164][  391/  391]    Overall Loss 0.372367    Objective Loss 0.372367    Top1 86.538462    Top5 99.038462    LR 0.005522    Time 0.052891    
2022-01-29 11:16:43,751 - --- validate (epoch=164)-----------
2022-01-29 11:16:43,752 - 10000 samples (128 per mini-batch)
2022-01-29 11:16:45,452 - Epoch: [164][   79/   79]    Loss 1.359106    Top1 65.360000    Top5 89.180000    
2022-01-29 11:16:45,504 - ==> Top1: 65.360    Top5: 89.180    Loss: 1.359

2022-01-29 11:16:45,510 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:16:45,510 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:16:45,555 - 

2022-01-29 11:16:45,555 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:16:51,112 - Epoch: [165][  100/  391]    Overall Loss 0.360536    Objective Loss 0.360536                                        LR 0.005522    Time 0.055538    
2022-01-29 11:16:56,362 - Epoch: [165][  200/  391]    Overall Loss 0.366713    Objective Loss 0.366713                                        LR 0.005522    Time 0.054016    
2022-01-29 11:17:01,610 - Epoch: [165][  300/  391]    Overall Loss 0.372842    Objective Loss 0.372842                                        LR 0.005522    Time 0.053501    
2022-01-29 11:17:06,387 - Epoch: [165][  391/  391]    Overall Loss 0.375448    Objective Loss 0.375448    Top1 85.576923    Top5 98.557692    LR 0.005522    Time 0.053266    
2022-01-29 11:17:06,444 - --- validate (epoch=165)-----------
2022-01-29 11:17:06,444 - 10000 samples (128 per mini-batch)
2022-01-29 11:17:08,144 - Epoch: [165][   79/   79]    Loss 1.339342    Top1 64.880000    Top5 89.260000    
2022-01-29 11:17:08,197 - ==> Top1: 64.880    Top5: 89.260    Loss: 1.339

2022-01-29 11:17:08,203 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:17:08,203 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:17:08,239 - 

2022-01-29 11:17:08,239 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:17:13,695 - Epoch: [166][  100/  391]    Overall Loss 0.349373    Objective Loss 0.349373                                        LR 0.005522    Time 0.054533    
2022-01-29 11:17:18,940 - Epoch: [166][  200/  391]    Overall Loss 0.361545    Objective Loss 0.361545                                        LR 0.005522    Time 0.053487    
2022-01-29 11:17:24,212 - Epoch: [166][  300/  391]    Overall Loss 0.365292    Objective Loss 0.365292                                        LR 0.005522    Time 0.053226    
2022-01-29 11:17:29,012 - Epoch: [166][  391/  391]    Overall Loss 0.369614    Objective Loss 0.369614    Top1 88.461538    Top5 99.038462    LR 0.005522    Time 0.053112    
2022-01-29 11:17:29,071 - --- validate (epoch=166)-----------
2022-01-29 11:17:29,071 - 10000 samples (128 per mini-batch)
2022-01-29 11:17:30,767 - Epoch: [166][   79/   79]    Loss 1.356169    Top1 64.910000    Top5 89.010000    
2022-01-29 11:17:30,824 - ==> Top1: 64.910    Top5: 89.010    Loss: 1.356

2022-01-29 11:17:30,891 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:17:30,891 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:17:30,936 - 

2022-01-29 11:17:30,936 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:17:36,399 - Epoch: [167][  100/  391]    Overall Loss 0.361449    Objective Loss 0.361449                                        LR 0.005522    Time 0.054596    
2022-01-29 11:17:41,642 - Epoch: [167][  200/  391]    Overall Loss 0.361252    Objective Loss 0.361252                                        LR 0.005522    Time 0.053508    
2022-01-29 11:17:46,947 - Epoch: [167][  300/  391]    Overall Loss 0.362843    Objective Loss 0.362843                                        LR 0.005522    Time 0.053354    
2022-01-29 11:17:51,632 - Epoch: [167][  391/  391]    Overall Loss 0.367943    Objective Loss 0.367943    Top1 87.500000    Top5 99.519231    LR 0.005522    Time 0.052915    
2022-01-29 11:17:51,688 - --- validate (epoch=167)-----------
2022-01-29 11:17:51,689 - 10000 samples (128 per mini-batch)
2022-01-29 11:17:53,336 - Epoch: [167][   79/   79]    Loss 1.360980    Top1 64.610000    Top5 88.890000    
2022-01-29 11:17:53,395 - ==> Top1: 64.610    Top5: 88.890    Loss: 1.361

2022-01-29 11:17:53,401 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:17:53,401 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:17:53,444 - 

2022-01-29 11:17:53,445 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:17:58,756 - Epoch: [168][  100/  391]    Overall Loss 0.351800    Objective Loss 0.351800                                        LR 0.005522    Time 0.053081    
2022-01-29 11:18:03,856 - Epoch: [168][  200/  391]    Overall Loss 0.356215    Objective Loss 0.356215                                        LR 0.005522    Time 0.052036    
2022-01-29 11:18:09,064 - Epoch: [168][  300/  391]    Overall Loss 0.365039    Objective Loss 0.365039                                        LR 0.005522    Time 0.052048    
2022-01-29 11:18:13,797 - Epoch: [168][  391/  391]    Overall Loss 0.365032    Objective Loss 0.365032    Top1 89.423077    Top5 99.519231    LR 0.005522    Time 0.052038    
2022-01-29 11:18:13,856 - --- validate (epoch=168)-----------
2022-01-29 11:18:13,856 - 10000 samples (128 per mini-batch)
2022-01-29 11:18:15,543 - Epoch: [168][   79/   79]    Loss 1.342382    Top1 65.070000    Top5 89.130000    
2022-01-29 11:18:15,594 - ==> Top1: 65.070    Top5: 89.130    Loss: 1.342

2022-01-29 11:18:15,600 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:18:15,600 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:18:15,645 - 

2022-01-29 11:18:15,645 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:18:21,120 - Epoch: [169][  100/  391]    Overall Loss 0.355515    Objective Loss 0.355515                                        LR 0.005522    Time 0.054726    
2022-01-29 11:18:26,300 - Epoch: [169][  200/  391]    Overall Loss 0.358370    Objective Loss 0.358370                                        LR 0.005522    Time 0.053257    
2022-01-29 11:18:31,479 - Epoch: [169][  300/  391]    Overall Loss 0.362135    Objective Loss 0.362135                                        LR 0.005522    Time 0.052766    
2022-01-29 11:18:36,191 - Epoch: [169][  391/  391]    Overall Loss 0.364732    Objective Loss 0.364732    Top1 85.576923    Top5 98.557692    LR 0.005522    Time 0.052535    
2022-01-29 11:18:36,252 - --- validate (epoch=169)-----------
2022-01-29 11:18:36,252 - 10000 samples (128 per mini-batch)
2022-01-29 11:18:37,919 - Epoch: [169][   79/   79]    Loss 1.367259    Top1 64.770000    Top5 88.910000    
2022-01-29 11:18:37,970 - ==> Top1: 64.770    Top5: 88.910    Loss: 1.367

2022-01-29 11:18:37,976 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:18:37,976 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:18:38,020 - 

2022-01-29 11:18:38,021 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:18:43,425 - Epoch: [170][  100/  391]    Overall Loss 0.340930    Objective Loss 0.340930                                        LR 0.005522    Time 0.054015    
2022-01-29 11:18:48,605 - Epoch: [170][  200/  391]    Overall Loss 0.351825    Objective Loss 0.351825                                        LR 0.005522    Time 0.052903    
2022-01-29 11:18:53,786 - Epoch: [170][  300/  391]    Overall Loss 0.354830    Objective Loss 0.354830                                        LR 0.005522    Time 0.052537    
2022-01-29 11:18:58,500 - Epoch: [170][  391/  391]    Overall Loss 0.360485    Objective Loss 0.360485    Top1 87.500000    Top5 99.519231    LR 0.005522    Time 0.052364    
2022-01-29 11:18:58,557 - --- validate (epoch=170)-----------
2022-01-29 11:18:58,557 - 10000 samples (128 per mini-batch)
2022-01-29 11:19:00,233 - Epoch: [170][   79/   79]    Loss 1.377880    Top1 64.680000    Top5 89.000000    
2022-01-29 11:19:00,291 - ==> Top1: 64.680    Top5: 89.000    Loss: 1.378

2022-01-29 11:19:00,297 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:19:00,297 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:19:00,342 - 

2022-01-29 11:19:00,342 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:19:05,838 - Epoch: [171][  100/  391]    Overall Loss 0.349227    Objective Loss 0.349227                                        LR 0.005522    Time 0.054931    
2022-01-29 11:19:11,048 - Epoch: [171][  200/  391]    Overall Loss 0.353108    Objective Loss 0.353108                                        LR 0.005522    Time 0.053512    
2022-01-29 11:19:16,198 - Epoch: [171][  300/  391]    Overall Loss 0.357521    Objective Loss 0.357521                                        LR 0.005522    Time 0.052839    
2022-01-29 11:19:20,888 - Epoch: [171][  391/  391]    Overall Loss 0.358200    Objective Loss 0.358200    Top1 88.942308    Top5 100.000000    LR 0.005522    Time 0.052535    
2022-01-29 11:19:20,948 - --- validate (epoch=171)-----------
2022-01-29 11:19:20,948 - 10000 samples (128 per mini-batch)
2022-01-29 11:19:22,688 - Epoch: [171][   79/   79]    Loss 1.373472    Top1 64.490000    Top5 88.900000    
2022-01-29 11:19:22,752 - ==> Top1: 64.490    Top5: 88.900    Loss: 1.373

2022-01-29 11:19:22,758 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:19:22,758 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:19:22,795 - 

2022-01-29 11:19:22,796 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:19:28,291 - Epoch: [172][  100/  391]    Overall Loss 0.347823    Objective Loss 0.347823                                        LR 0.005522    Time 0.054929    
2022-01-29 11:19:33,473 - Epoch: [172][  200/  391]    Overall Loss 0.352721    Objective Loss 0.352721                                        LR 0.005522    Time 0.053373    
2022-01-29 11:19:38,610 - Epoch: [172][  300/  391]    Overall Loss 0.357695    Objective Loss 0.357695                                        LR 0.005522    Time 0.052700    
2022-01-29 11:19:43,276 - Epoch: [172][  391/  391]    Overall Loss 0.361484    Objective Loss 0.361484    Top1 90.865385    Top5 99.038462    LR 0.005522    Time 0.052368    
2022-01-29 11:19:43,337 - --- validate (epoch=172)-----------
2022-01-29 11:19:43,337 - 10000 samples (128 per mini-batch)
2022-01-29 11:19:44,993 - Epoch: [172][   79/   79]    Loss 1.372952    Top1 64.580000    Top5 88.910000    
2022-01-29 11:19:45,051 - ==> Top1: 64.580    Top5: 88.910    Loss: 1.373

2022-01-29 11:19:45,056 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:19:45,056 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:19:45,100 - 

2022-01-29 11:19:45,100 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:19:50,488 - Epoch: [173][  100/  391]    Overall Loss 0.344122    Objective Loss 0.344122                                        LR 0.005522    Time 0.053849    
2022-01-29 11:19:55,706 - Epoch: [173][  200/  391]    Overall Loss 0.350061    Objective Loss 0.350061                                        LR 0.005522    Time 0.053011    
2022-01-29 11:20:00,913 - Epoch: [173][  300/  391]    Overall Loss 0.352031    Objective Loss 0.352031                                        LR 0.005522    Time 0.052696    
2022-01-29 11:20:05,652 - Epoch: [173][  391/  391]    Overall Loss 0.354215    Objective Loss 0.354215    Top1 93.269231    Top5 100.000000    LR 0.005522    Time 0.052549    
2022-01-29 11:20:05,712 - --- validate (epoch=173)-----------
2022-01-29 11:20:05,712 - 10000 samples (128 per mini-batch)
2022-01-29 11:20:07,459 - Epoch: [173][   79/   79]    Loss 1.380816    Top1 64.420000    Top5 88.990000    
2022-01-29 11:20:07,510 - ==> Top1: 64.420    Top5: 88.990    Loss: 1.381

2022-01-29 11:20:07,516 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:20:07,516 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:20:07,561 - 

2022-01-29 11:20:07,561 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:20:12,926 - Epoch: [174][  100/  391]    Overall Loss 0.338810    Objective Loss 0.338810                                        LR 0.005522    Time 0.053618    
2022-01-29 11:20:18,053 - Epoch: [174][  200/  391]    Overall Loss 0.348268    Objective Loss 0.348268                                        LR 0.005522    Time 0.052438    
2022-01-29 11:20:23,203 - Epoch: [174][  300/  391]    Overall Loss 0.351612    Objective Loss 0.351612                                        LR 0.005522    Time 0.052124    
2022-01-29 11:20:27,953 - Epoch: [174][  391/  391]    Overall Loss 0.356711    Objective Loss 0.356711    Top1 87.980769    Top5 100.000000    LR 0.005522    Time 0.052139    
2022-01-29 11:20:28,010 - --- validate (epoch=174)-----------
2022-01-29 11:20:28,010 - 10000 samples (128 per mini-batch)
2022-01-29 11:20:29,666 - Epoch: [174][   79/   79]    Loss 1.375747    Top1 64.650000    Top5 88.930000    
2022-01-29 11:20:29,722 - ==> Top1: 64.650    Top5: 88.930    Loss: 1.376

2022-01-29 11:20:29,727 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:20:29,727 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:20:29,767 - 

2022-01-29 11:20:29,767 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:20:35,277 - Epoch: [175][  100/  391]    Overall Loss 0.318334    Objective Loss 0.318334                                        LR 0.001298    Time 0.055070    
2022-01-29 11:20:40,499 - Epoch: [175][  200/  391]    Overall Loss 0.322069    Objective Loss 0.322069                                        LR 0.001298    Time 0.053638    
2022-01-29 11:20:45,712 - Epoch: [175][  300/  391]    Overall Loss 0.322237    Objective Loss 0.322237                                        LR 0.001298    Time 0.053133    
2022-01-29 11:20:50,444 - Epoch: [175][  391/  391]    Overall Loss 0.322592    Objective Loss 0.322592    Top1 90.384615    Top5 99.519231    LR 0.001298    Time 0.052867    
2022-01-29 11:20:50,501 - --- validate (epoch=175)-----------
2022-01-29 11:20:50,501 - 10000 samples (128 per mini-batch)
2022-01-29 11:20:52,283 - Epoch: [175][   79/   79]    Loss 1.345580    Top1 65.190000    Top5 89.250000    
2022-01-29 11:20:52,336 - ==> Top1: 65.190    Top5: 89.250    Loss: 1.346

2022-01-29 11:20:52,342 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:20:52,342 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:20:52,387 - 

2022-01-29 11:20:52,387 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:20:57,889 - Epoch: [176][  100/  391]    Overall Loss 0.319145    Objective Loss 0.319145                                        LR 0.001298    Time 0.054993    
2022-01-29 11:21:03,163 - Epoch: [176][  200/  391]    Overall Loss 0.310174    Objective Loss 0.310174                                        LR 0.001298    Time 0.053862    
2022-01-29 11:21:08,375 - Epoch: [176][  300/  391]    Overall Loss 0.309340    Objective Loss 0.309340                                        LR 0.001298    Time 0.053280    
2022-01-29 11:21:13,109 - Epoch: [176][  391/  391]    Overall Loss 0.312480    Objective Loss 0.312480    Top1 92.788462    Top5 100.000000    LR 0.001298    Time 0.052985    
2022-01-29 11:21:13,169 - --- validate (epoch=176)-----------
2022-01-29 11:21:13,169 - 10000 samples (128 per mini-batch)
2022-01-29 11:21:14,946 - Epoch: [176][   79/   79]    Loss 1.350901    Top1 65.130000    Top5 89.110000    
2022-01-29 11:21:14,998 - ==> Top1: 65.130    Top5: 89.110    Loss: 1.351

2022-01-29 11:21:15,003 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:21:15,003 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:21:15,047 - 

2022-01-29 11:21:15,047 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:21:20,407 - Epoch: [177][  100/  391]    Overall Loss 0.297461    Objective Loss 0.297461                                        LR 0.001298    Time 0.053567    
2022-01-29 11:21:25,616 - Epoch: [177][  200/  391]    Overall Loss 0.303694    Objective Loss 0.303694                                        LR 0.001298    Time 0.052828    
2022-01-29 11:21:30,711 - Epoch: [177][  300/  391]    Overall Loss 0.306109    Objective Loss 0.306109                                        LR 0.001298    Time 0.052198    
2022-01-29 11:21:35,336 - Epoch: [177][  391/  391]    Overall Loss 0.309454    Objective Loss 0.309454    Top1 89.423077    Top5 99.038462    LR 0.001298    Time 0.051875    
2022-01-29 11:21:35,396 - --- validate (epoch=177)-----------
2022-01-29 11:21:35,396 - 10000 samples (128 per mini-batch)
2022-01-29 11:21:37,043 - Epoch: [177][   79/   79]    Loss 1.351500    Top1 65.140000    Top5 89.250000    
2022-01-29 11:21:37,096 - ==> Top1: 65.140    Top5: 89.250    Loss: 1.351

2022-01-29 11:21:37,101 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:21:37,101 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:21:37,145 - 

2022-01-29 11:21:37,145 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:21:42,612 - Epoch: [178][  100/  391]    Overall Loss 0.296334    Objective Loss 0.296334                                        LR 0.001298    Time 0.054645    
2022-01-29 11:21:47,845 - Epoch: [178][  200/  391]    Overall Loss 0.303966    Objective Loss 0.303966                                        LR 0.001298    Time 0.053483    
2022-01-29 11:21:53,090 - Epoch: [178][  300/  391]    Overall Loss 0.303763    Objective Loss 0.303763                                        LR 0.001298    Time 0.053136    
2022-01-29 11:21:57,857 - Epoch: [178][  391/  391]    Overall Loss 0.304357    Objective Loss 0.304357    Top1 93.269231    Top5 99.519231    LR 0.001298    Time 0.052959    
2022-01-29 11:21:57,916 - --- validate (epoch=178)-----------
2022-01-29 11:21:57,916 - 10000 samples (128 per mini-batch)
2022-01-29 11:21:59,622 - Epoch: [178][   79/   79]    Loss 1.359416    Top1 64.900000    Top5 89.130000    
2022-01-29 11:21:59,679 - ==> Top1: 64.900    Top5: 89.130    Loss: 1.359

2022-01-29 11:21:59,684 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:21:59,684 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:21:59,726 - 

2022-01-29 11:21:59,726 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:22:05,249 - Epoch: [179][  100/  391]    Overall Loss 0.304891    Objective Loss 0.304891                                        LR 0.001298    Time 0.055198    
2022-01-29 11:22:10,401 - Epoch: [179][  200/  391]    Overall Loss 0.303138    Objective Loss 0.303138                                        LR 0.001298    Time 0.053356    
2022-01-29 11:22:15,663 - Epoch: [179][  300/  391]    Overall Loss 0.300227    Objective Loss 0.300227                                        LR 0.001298    Time 0.053109    
2022-01-29 11:22:20,449 - Epoch: [179][  391/  391]    Overall Loss 0.301988    Objective Loss 0.301988    Top1 91.346154    Top5 100.000000    LR 0.001298    Time 0.052988    
2022-01-29 11:22:20,515 - --- validate (epoch=179)-----------
2022-01-29 11:22:20,516 - 10000 samples (128 per mini-batch)
2022-01-29 11:22:22,199 - Epoch: [179][   79/   79]    Loss 1.359661    Top1 65.090000    Top5 89.080000    
2022-01-29 11:22:22,250 - ==> Top1: 65.090    Top5: 89.080    Loss: 1.360

2022-01-29 11:22:22,255 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:22:22,255 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:22:22,301 - 

2022-01-29 11:22:22,301 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:22:27,757 - Epoch: [180][  100/  391]    Overall Loss 0.298528    Objective Loss 0.298528                                        LR 0.001298    Time 0.054533    
2022-01-29 11:22:32,918 - Epoch: [180][  200/  391]    Overall Loss 0.300311    Objective Loss 0.300311                                        LR 0.001298    Time 0.053067    
2022-01-29 11:22:38,025 - Epoch: [180][  300/  391]    Overall Loss 0.302458    Objective Loss 0.302458                                        LR 0.001298    Time 0.052398    
2022-01-29 11:22:42,670 - Epoch: [180][  391/  391]    Overall Loss 0.304875    Objective Loss 0.304875    Top1 89.423077    Top5 99.519231    LR 0.001298    Time 0.052081    
2022-01-29 11:22:42,730 - --- validate (epoch=180)-----------
2022-01-29 11:22:42,730 - 10000 samples (128 per mini-batch)
2022-01-29 11:22:44,525 - Epoch: [180][   79/   79]    Loss 1.358144    Top1 65.190000    Top5 89.210000    
2022-01-29 11:22:44,584 - ==> Top1: 65.190    Top5: 89.210    Loss: 1.358

2022-01-29 11:22:44,589 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:22:44,589 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:22:44,633 - 

2022-01-29 11:22:44,633 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:22:49,960 - Epoch: [181][  100/  391]    Overall Loss 0.296576    Objective Loss 0.296576                                        LR 0.001298    Time 0.053233    
2022-01-29 11:22:55,084 - Epoch: [181][  200/  391]    Overall Loss 0.291410    Objective Loss 0.291410                                        LR 0.001298    Time 0.052232    
2022-01-29 11:23:00,220 - Epoch: [181][  300/  391]    Overall Loss 0.297385    Objective Loss 0.297385                                        LR 0.001298    Time 0.051939    
2022-01-29 11:23:04,889 - Epoch: [181][  391/  391]    Overall Loss 0.298319    Objective Loss 0.298319    Top1 94.230769    Top5 99.519231    LR 0.001298    Time 0.051790    
2022-01-29 11:23:04,946 - --- validate (epoch=181)-----------
2022-01-29 11:23:04,947 - 10000 samples (128 per mini-batch)
2022-01-29 11:23:06,605 - Epoch: [181][   79/   79]    Loss 1.366970    Top1 65.200000    Top5 88.990000    
2022-01-29 11:23:06,657 - ==> Top1: 65.200    Top5: 88.990    Loss: 1.367

2022-01-29 11:23:06,663 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:23:06,663 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:23:06,706 - 

2022-01-29 11:23:06,706 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:23:12,019 - Epoch: [182][  100/  391]    Overall Loss 0.286379    Objective Loss 0.286379                                        LR 0.001298    Time 0.053093    
2022-01-29 11:23:17,033 - Epoch: [182][  200/  391]    Overall Loss 0.290177    Objective Loss 0.290177                                        LR 0.001298    Time 0.051614    
2022-01-29 11:23:22,116 - Epoch: [182][  300/  391]    Overall Loss 0.293308    Objective Loss 0.293308                                        LR 0.001298    Time 0.051348    
2022-01-29 11:23:26,850 - Epoch: [182][  391/  391]    Overall Loss 0.295608    Objective Loss 0.295608    Top1 89.903846    Top5 100.000000    LR 0.001298    Time 0.051504    
2022-01-29 11:23:26,910 - --- validate (epoch=182)-----------
2022-01-29 11:23:26,911 - 10000 samples (128 per mini-batch)
2022-01-29 11:23:28,615 - Epoch: [182][   79/   79]    Loss 1.360734    Top1 65.180000    Top5 89.090000    
2022-01-29 11:23:28,674 - ==> Top1: 65.180    Top5: 89.090    Loss: 1.361

2022-01-29 11:23:28,679 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:23:28,679 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:23:28,725 - 

2022-01-29 11:23:28,725 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:23:34,223 - Epoch: [183][  100/  391]    Overall Loss 0.292610    Objective Loss 0.292610                                        LR 0.001298    Time 0.054948    
2022-01-29 11:23:39,433 - Epoch: [183][  200/  391]    Overall Loss 0.292170    Objective Loss 0.292170                                        LR 0.001298    Time 0.053524    
2022-01-29 11:23:44,648 - Epoch: [183][  300/  391]    Overall Loss 0.295780    Objective Loss 0.295780                                        LR 0.001298    Time 0.053063    
2022-01-29 11:23:49,398 - Epoch: [183][  391/  391]    Overall Loss 0.295636    Objective Loss 0.295636    Top1 89.423077    Top5 99.038462    LR 0.001298    Time 0.052860    
2022-01-29 11:23:49,459 - --- validate (epoch=183)-----------
2022-01-29 11:23:49,459 - 10000 samples (128 per mini-batch)
2022-01-29 11:23:51,160 - Epoch: [183][   79/   79]    Loss 1.362660    Top1 64.950000    Top5 89.040000    
2022-01-29 11:23:51,214 - ==> Top1: 64.950    Top5: 89.040    Loss: 1.363

2022-01-29 11:23:51,219 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:23:51,219 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:23:51,264 - 

2022-01-29 11:23:51,264 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:23:56,672 - Epoch: [184][  100/  391]    Overall Loss 0.286448    Objective Loss 0.286448                                        LR 0.001298    Time 0.054052    
2022-01-29 11:24:01,875 - Epoch: [184][  200/  391]    Overall Loss 0.291096    Objective Loss 0.291096                                        LR 0.001298    Time 0.053038    
2022-01-29 11:24:07,083 - Epoch: [184][  300/  391]    Overall Loss 0.294521    Objective Loss 0.294521                                        LR 0.001298    Time 0.052714    
2022-01-29 11:24:11,821 - Epoch: [184][  391/  391]    Overall Loss 0.295273    Objective Loss 0.295273    Top1 91.346154    Top5 99.519231    LR 0.001298    Time 0.052561    
2022-01-29 11:24:11,880 - --- validate (epoch=184)-----------
2022-01-29 11:24:11,880 - 10000 samples (128 per mini-batch)
2022-01-29 11:24:13,556 - Epoch: [184][   79/   79]    Loss 1.361794    Top1 65.200000    Top5 89.140000    
2022-01-29 11:24:13,619 - ==> Top1: 65.200    Top5: 89.140    Loss: 1.362

2022-01-29 11:24:13,624 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:24:13,624 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:24:13,661 - 

2022-01-29 11:24:13,661 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:24:19,242 - Epoch: [185][  100/  391]    Overall Loss 0.299942    Objective Loss 0.299942                                        LR 0.001298    Time 0.055776    
2022-01-29 11:24:24,547 - Epoch: [185][  200/  391]    Overall Loss 0.300423    Objective Loss 0.300423                                        LR 0.001298    Time 0.054410    
2022-01-29 11:24:29,856 - Epoch: [185][  300/  391]    Overall Loss 0.294899    Objective Loss 0.294899                                        LR 0.001298    Time 0.053967    
2022-01-29 11:24:34,605 - Epoch: [185][  391/  391]    Overall Loss 0.295264    Objective Loss 0.295264    Top1 93.269231    Top5 100.000000    LR 0.001298    Time 0.053551    
2022-01-29 11:24:34,670 - --- validate (epoch=185)-----------
2022-01-29 11:24:34,670 - 10000 samples (128 per mini-batch)
2022-01-29 11:24:36,440 - Epoch: [185][   79/   79]    Loss 1.364675    Top1 64.900000    Top5 89.130000    
2022-01-29 11:24:36,492 - ==> Top1: 64.900    Top5: 89.130    Loss: 1.365

2022-01-29 11:24:36,498 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:24:36,498 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:24:36,543 - 

2022-01-29 11:24:36,543 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:24:42,022 - Epoch: [186][  100/  391]    Overall Loss 0.293075    Objective Loss 0.293075                                        LR 0.001298    Time 0.054770    
2022-01-29 11:24:47,206 - Epoch: [186][  200/  391]    Overall Loss 0.295830    Objective Loss 0.295830                                        LR 0.001298    Time 0.053297    
2022-01-29 11:24:52,401 - Epoch: [186][  300/  391]    Overall Loss 0.292693    Objective Loss 0.292693                                        LR 0.001298    Time 0.052846    
2022-01-29 11:24:57,121 - Epoch: [186][  391/  391]    Overall Loss 0.293725    Objective Loss 0.293725    Top1 91.826923    Top5 99.519231    LR 0.001298    Time 0.052616    
2022-01-29 11:24:57,181 - --- validate (epoch=186)-----------
2022-01-29 11:24:57,181 - 10000 samples (128 per mini-batch)
2022-01-29 11:24:58,983 - Epoch: [186][   79/   79]    Loss 1.370252    Top1 65.000000    Top5 89.190000    
2022-01-29 11:24:59,048 - ==> Top1: 65.000    Top5: 89.190    Loss: 1.370

2022-01-29 11:24:59,053 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:24:59,053 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:24:59,091 - 

2022-01-29 11:24:59,091 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:25:04,444 - Epoch: [187][  100/  391]    Overall Loss 0.293052    Objective Loss 0.293052                                        LR 0.001298    Time 0.053499    
2022-01-29 11:25:09,554 - Epoch: [187][  200/  391]    Overall Loss 0.290664    Objective Loss 0.290664                                        LR 0.001298    Time 0.052293    
2022-01-29 11:25:14,694 - Epoch: [187][  300/  391]    Overall Loss 0.292502    Objective Loss 0.292502                                        LR 0.001298    Time 0.051994    
2022-01-29 11:25:19,371 - Epoch: [187][  391/  391]    Overall Loss 0.294295    Objective Loss 0.294295    Top1 90.865385    Top5 100.000000    LR 0.001298    Time 0.051852    
2022-01-29 11:25:19,430 - --- validate (epoch=187)-----------
2022-01-29 11:25:19,430 - 10000 samples (128 per mini-batch)
2022-01-29 11:25:21,130 - Epoch: [187][   79/   79]    Loss 1.366613    Top1 65.060000    Top5 88.960000    
2022-01-29 11:25:21,184 - ==> Top1: 65.060    Top5: 88.960    Loss: 1.367

2022-01-29 11:25:21,190 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:25:21,190 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:25:21,234 - 

2022-01-29 11:25:21,234 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:25:26,563 - Epoch: [188][  100/  391]    Overall Loss 0.291025    Objective Loss 0.291025                                        LR 0.001298    Time 0.053260    
2022-01-29 11:25:31,702 - Epoch: [188][  200/  391]    Overall Loss 0.289163    Objective Loss 0.289163                                        LR 0.001298    Time 0.052321    
2022-01-29 11:25:36,948 - Epoch: [188][  300/  391]    Overall Loss 0.290107    Objective Loss 0.290107                                        LR 0.001298    Time 0.052364    
2022-01-29 11:25:41,716 - Epoch: [188][  391/  391]    Overall Loss 0.291217    Objective Loss 0.291217    Top1 89.423077    Top5 99.519231    LR 0.001298    Time 0.052370    
2022-01-29 11:25:41,774 - --- validate (epoch=188)-----------
2022-01-29 11:25:41,775 - 10000 samples (128 per mini-batch)
2022-01-29 11:25:43,530 - Epoch: [188][   79/   79]    Loss 1.367487    Top1 65.340000    Top5 88.930000    
2022-01-29 11:25:43,590 - ==> Top1: 65.340    Top5: 88.930    Loss: 1.367

2022-01-29 11:25:43,596 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:25:43,596 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:25:43,641 - 

2022-01-29 11:25:43,641 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:25:49,145 - Epoch: [189][  100/  391]    Overall Loss 0.287415    Objective Loss 0.287415                                        LR 0.001298    Time 0.055010    
2022-01-29 11:25:54,368 - Epoch: [189][  200/  391]    Overall Loss 0.287335    Objective Loss 0.287335                                        LR 0.001298    Time 0.053618    
2022-01-29 11:25:59,570 - Epoch: [189][  300/  391]    Overall Loss 0.288102    Objective Loss 0.288102                                        LR 0.001298    Time 0.053080    
2022-01-29 11:26:04,299 - Epoch: [189][  391/  391]    Overall Loss 0.293069    Objective Loss 0.293069    Top1 92.307692    Top5 99.519231    LR 0.001298    Time 0.052821    
2022-01-29 11:26:04,364 - --- validate (epoch=189)-----------
2022-01-29 11:26:04,365 - 10000 samples (128 per mini-batch)
2022-01-29 11:26:06,131 - Epoch: [189][   79/   79]    Loss 1.368473    Top1 65.010000    Top5 89.270000    
2022-01-29 11:26:06,191 - ==> Top1: 65.010    Top5: 89.270    Loss: 1.368

2022-01-29 11:26:06,196 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:26:06,196 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:26:06,242 - 

2022-01-29 11:26:06,242 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:26:11,767 - Epoch: [190][  100/  391]    Overall Loss 0.282174    Objective Loss 0.282174                                        LR 0.001298    Time 0.055222    
2022-01-29 11:26:16,981 - Epoch: [190][  200/  391]    Overall Loss 0.284581    Objective Loss 0.284581                                        LR 0.001298    Time 0.053680    
2022-01-29 11:26:22,208 - Epoch: [190][  300/  391]    Overall Loss 0.287684    Objective Loss 0.287684                                        LR 0.001298    Time 0.053205    
2022-01-29 11:26:26,959 - Epoch: [190][  391/  391]    Overall Loss 0.288653    Objective Loss 0.288653    Top1 88.942308    Top5 98.557692    LR 0.001298    Time 0.052971    
2022-01-29 11:26:27,018 - --- validate (epoch=190)-----------
2022-01-29 11:26:27,019 - 10000 samples (128 per mini-batch)
2022-01-29 11:26:28,668 - Epoch: [190][   79/   79]    Loss 1.370880    Top1 65.050000    Top5 89.220000    
2022-01-29 11:26:28,732 - ==> Top1: 65.050    Top5: 89.220    Loss: 1.371

2022-01-29 11:26:28,738 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:26:28,738 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:26:28,776 - 

2022-01-29 11:26:28,776 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:26:34,194 - Epoch: [191][  100/  391]    Overall Loss 0.291452    Objective Loss 0.291452                                        LR 0.001298    Time 0.054153    
2022-01-29 11:26:39,400 - Epoch: [191][  200/  391]    Overall Loss 0.286574    Objective Loss 0.286574                                        LR 0.001298    Time 0.053103    
2022-01-29 11:26:44,607 - Epoch: [191][  300/  391]    Overall Loss 0.290141    Objective Loss 0.290141                                        LR 0.001298    Time 0.052754    
2022-01-29 11:26:49,348 - Epoch: [191][  391/  391]    Overall Loss 0.289282    Objective Loss 0.289282    Top1 92.788462    Top5 99.519231    LR 0.001298    Time 0.052599    
2022-01-29 11:26:49,405 - --- validate (epoch=191)-----------
2022-01-29 11:26:49,405 - 10000 samples (128 per mini-batch)
2022-01-29 11:26:51,090 - Epoch: [191][   79/   79]    Loss 1.386028    Top1 65.110000    Top5 89.100000    
2022-01-29 11:26:51,143 - ==> Top1: 65.110    Top5: 89.100    Loss: 1.386

2022-01-29 11:26:51,149 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:26:51,149 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:26:51,256 - 

2022-01-29 11:26:51,256 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:26:56,754 - Epoch: [192][  100/  391]    Overall Loss 0.287973    Objective Loss 0.287973                                        LR 0.001298    Time 0.054947    
2022-01-29 11:27:02,025 - Epoch: [192][  200/  391]    Overall Loss 0.286415    Objective Loss 0.286415                                        LR 0.001298    Time 0.053826    
2022-01-29 11:27:07,271 - Epoch: [192][  300/  391]    Overall Loss 0.285998    Objective Loss 0.285998                                        LR 0.001298    Time 0.053367    
2022-01-29 11:27:12,044 - Epoch: [192][  391/  391]    Overall Loss 0.284942    Objective Loss 0.284942    Top1 96.634615    Top5 99.038462    LR 0.001298    Time 0.053152    
2022-01-29 11:27:12,103 - --- validate (epoch=192)-----------
2022-01-29 11:27:12,103 - 10000 samples (128 per mini-batch)
2022-01-29 11:27:13,734 - Epoch: [192][   79/   79]    Loss 1.364321    Top1 65.170000    Top5 89.120000    
2022-01-29 11:27:13,793 - ==> Top1: 65.170    Top5: 89.120    Loss: 1.364

2022-01-29 11:27:13,799 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:27:13,799 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:27:13,843 - 

2022-01-29 11:27:13,843 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:27:19,389 - Epoch: [193][  100/  391]    Overall Loss 0.286490    Objective Loss 0.286490                                        LR 0.001298    Time 0.055440    
2022-01-29 11:27:24,640 - Epoch: [193][  200/  391]    Overall Loss 0.282978    Objective Loss 0.282978                                        LR 0.001298    Time 0.053971    
2022-01-29 11:27:29,893 - Epoch: [193][  300/  391]    Overall Loss 0.285059    Objective Loss 0.285059                                        LR 0.001298    Time 0.053488    
2022-01-29 11:27:34,705 - Epoch: [193][  391/  391]    Overall Loss 0.287069    Objective Loss 0.287069    Top1 93.269231    Top5 99.519231    LR 0.001298    Time 0.053345    
2022-01-29 11:27:34,761 - --- validate (epoch=193)-----------
2022-01-29 11:27:34,761 - 10000 samples (128 per mini-batch)
2022-01-29 11:27:36,467 - Epoch: [193][   79/   79]    Loss 1.370825    Top1 65.040000    Top5 89.130000    
2022-01-29 11:27:36,520 - ==> Top1: 65.040    Top5: 89.130    Loss: 1.371

2022-01-29 11:27:36,526 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:27:36,526 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:27:36,571 - 

2022-01-29 11:27:36,572 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:27:42,153 - Epoch: [194][  100/  391]    Overall Loss 0.283347    Objective Loss 0.283347                                        LR 0.001298    Time 0.055788    
2022-01-29 11:27:47,390 - Epoch: [194][  200/  391]    Overall Loss 0.283521    Objective Loss 0.283521                                        LR 0.001298    Time 0.054073    
2022-01-29 11:27:52,631 - Epoch: [194][  300/  391]    Overall Loss 0.283576    Objective Loss 0.283576                                        LR 0.001298    Time 0.053515    
2022-01-29 11:27:57,399 - Epoch: [194][  391/  391]    Overall Loss 0.283997    Objective Loss 0.283997    Top1 93.750000    Top5 99.519231    LR 0.001298    Time 0.053252    
2022-01-29 11:27:57,458 - --- validate (epoch=194)-----------
2022-01-29 11:27:57,458 - 10000 samples (128 per mini-batch)
2022-01-29 11:27:59,167 - Epoch: [194][   79/   79]    Loss 1.365828    Top1 65.200000    Top5 89.150000    
2022-01-29 11:27:59,222 - ==> Top1: 65.200    Top5: 89.150    Loss: 1.366

2022-01-29 11:27:59,228 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:27:59,228 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:27:59,273 - 

2022-01-29 11:27:59,273 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:28:04,754 - Epoch: [195][  100/  391]    Overall Loss 0.281927    Objective Loss 0.281927                                        LR 0.001298    Time 0.054778    
2022-01-29 11:28:10,059 - Epoch: [195][  200/  391]    Overall Loss 0.282449    Objective Loss 0.282449                                        LR 0.001298    Time 0.053912    
2022-01-29 11:28:15,386 - Epoch: [195][  300/  391]    Overall Loss 0.285180    Objective Loss 0.285180                                        LR 0.001298    Time 0.053696    
2022-01-29 11:28:20,236 - Epoch: [195][  391/  391]    Overall Loss 0.285489    Objective Loss 0.285489    Top1 93.750000    Top5 99.038462    LR 0.001298    Time 0.053601    
2022-01-29 11:28:20,295 - --- validate (epoch=195)-----------
2022-01-29 11:28:20,296 - 10000 samples (128 per mini-batch)
2022-01-29 11:28:21,946 - Epoch: [195][   79/   79]    Loss 1.367330    Top1 65.050000    Top5 89.140000    
2022-01-29 11:28:22,011 - ==> Top1: 65.050    Top5: 89.140    Loss: 1.367

2022-01-29 11:28:22,016 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:28:22,016 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:28:22,054 - 

2022-01-29 11:28:22,054 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:28:27,634 - Epoch: [196][  100/  391]    Overall Loss 0.284757    Objective Loss 0.284757                                        LR 0.001298    Time 0.055777    
2022-01-29 11:28:32,928 - Epoch: [196][  200/  391]    Overall Loss 0.284580    Objective Loss 0.284580                                        LR 0.001298    Time 0.054355    
2022-01-29 11:28:38,234 - Epoch: [196][  300/  391]    Overall Loss 0.284286    Objective Loss 0.284286                                        LR 0.001298    Time 0.053921    
2022-01-29 11:28:43,057 - Epoch: [196][  391/  391]    Overall Loss 0.285959    Objective Loss 0.285959    Top1 93.750000    Top5 99.038462    LR 0.001298    Time 0.053704    
2022-01-29 11:28:43,116 - --- validate (epoch=196)-----------
2022-01-29 11:28:43,117 - 10000 samples (128 per mini-batch)
2022-01-29 11:28:44,820 - Epoch: [196][   79/   79]    Loss 1.384849    Top1 64.900000    Top5 89.070000    
2022-01-29 11:28:44,878 - ==> Top1: 64.900    Top5: 89.070    Loss: 1.385

2022-01-29 11:28:44,884 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:28:44,884 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:28:44,922 - 

2022-01-29 11:28:44,922 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:28:50,494 - Epoch: [197][  100/  391]    Overall Loss 0.275857    Objective Loss 0.275857                                        LR 0.001298    Time 0.055698    
2022-01-29 11:28:55,795 - Epoch: [197][  200/  391]    Overall Loss 0.279403    Objective Loss 0.279403                                        LR 0.001298    Time 0.054348    
2022-01-29 11:29:01,060 - Epoch: [197][  300/  391]    Overall Loss 0.283316    Objective Loss 0.283316                                        LR 0.001298    Time 0.053780    
2022-01-29 11:29:05,871 - Epoch: [197][  391/  391]    Overall Loss 0.285084    Objective Loss 0.285084    Top1 90.384615    Top5 99.519231    LR 0.001298    Time 0.053564    
2022-01-29 11:29:05,928 - --- validate (epoch=197)-----------
2022-01-29 11:29:05,928 - 10000 samples (128 per mini-batch)
2022-01-29 11:29:07,622 - Epoch: [197][   79/   79]    Loss 1.380082    Top1 65.000000    Top5 88.980000    
2022-01-29 11:29:07,683 - ==> Top1: 65.000    Top5: 88.980    Loss: 1.380

2022-01-29 11:29:07,689 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:29:07,689 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:29:07,726 - 

2022-01-29 11:29:07,726 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:29:13,183 - Epoch: [198][  100/  391]    Overall Loss 0.273559    Objective Loss 0.273559                                        LR 0.001298    Time 0.054538    
2022-01-29 11:29:18,417 - Epoch: [198][  200/  391]    Overall Loss 0.278707    Objective Loss 0.278707                                        LR 0.001298    Time 0.053436    
2022-01-29 11:29:23,670 - Epoch: [198][  300/  391]    Overall Loss 0.280495    Objective Loss 0.280495                                        LR 0.001298    Time 0.053129    
2022-01-29 11:29:28,472 - Epoch: [198][  391/  391]    Overall Loss 0.282104    Objective Loss 0.282104    Top1 92.307692    Top5 100.000000    LR 0.001298    Time 0.053043    
2022-01-29 11:29:28,527 - --- validate (epoch=198)-----------
2022-01-29 11:29:28,527 - 10000 samples (128 per mini-batch)
2022-01-29 11:29:30,222 - Epoch: [198][   79/   79]    Loss 1.388732    Top1 65.060000    Top5 88.870000    
2022-01-29 11:29:30,277 - ==> Top1: 65.060    Top5: 88.870    Loss: 1.389

2022-01-29 11:29:30,283 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:29:30,283 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:29:30,328 - 

2022-01-29 11:29:30,328 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:29:35,894 - Epoch: [199][  100/  391]    Overall Loss 0.279108    Objective Loss 0.279108                                        LR 0.001298    Time 0.055635    
2022-01-29 11:29:41,098 - Epoch: [199][  200/  391]    Overall Loss 0.280812    Objective Loss 0.280812                                        LR 0.001298    Time 0.053833    
2022-01-29 11:29:46,300 - Epoch: [199][  300/  391]    Overall Loss 0.282264    Objective Loss 0.282264                                        LR 0.001298    Time 0.053223    
2022-01-29 11:29:51,033 - Epoch: [199][  391/  391]    Overall Loss 0.283476    Objective Loss 0.283476    Top1 92.788462    Top5 99.519231    LR 0.001298    Time 0.052939    
2022-01-29 11:29:51,090 - --- validate (epoch=199)-----------
2022-01-29 11:29:51,091 - 10000 samples (128 per mini-batch)
2022-01-29 11:29:52,772 - Epoch: [199][   79/   79]    Loss 1.381144    Top1 64.960000    Top5 88.840000    
2022-01-29 11:29:52,830 - ==> Top1: 64.960    Top5: 88.840    Loss: 1.381

2022-01-29 11:29:52,835 - ==> Best [Top1: 65.380   Top5: 89.340   Sparsity:0.00   Params: 627712 on epoch: 151]
2022-01-29 11:29:52,836 - Saving checkpoint to: logs/2022.01.29-101451/checkpoint.pth.tar
2022-01-29 11:29:52,957 - 

2022-01-29 11:29:52,957 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:30:00,647 - Epoch: [200][  100/  391]    Overall Loss 1.235119    Objective Loss 1.235119                                        LR 0.001298    Time 0.076878    
2022-01-29 11:30:08,002 - Epoch: [200][  200/  391]    Overall Loss 1.177877    Objective Loss 1.177877                                        LR 0.001298    Time 0.075214    
2022-01-29 11:30:15,341 - Epoch: [200][  300/  391]    Overall Loss 1.149306    Objective Loss 1.149306                                        LR 0.001298    Time 0.074603    
2022-01-29 11:30:22,026 - Epoch: [200][  391/  391]    Overall Loss 1.133412    Objective Loss 1.133412    Top1 76.442308    Top5 95.192308    LR 0.001298    Time 0.074333    
2022-01-29 11:30:22,085 - --- validate (epoch=200)-----------
2022-01-29 11:30:22,085 - 10000 samples (128 per mini-batch)
2022-01-29 11:30:25,490 - Epoch: [200][   79/   79]    Loss 1.658414    Top1 55.440000    Top5 83.130000    
2022-01-29 11:30:25,545 - ==> Top1: 55.440    Top5: 83.130    Loss: 1.658

2022-01-29 11:30:25,550 - ==> Best [Top1: 55.440   Top5: 83.130   Sparsity:0.00   Params: 627712 on epoch: 200]
2022-01-29 11:30:25,550 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:30:25,581 - 

2022-01-29 11:30:25,581 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:30:33,145 - Epoch: [201][  100/  391]    Overall Loss 1.049084    Objective Loss 1.049084                                        LR 0.001298    Time 0.075611    
2022-01-29 11:30:40,549 - Epoch: [201][  200/  391]    Overall Loss 1.031743    Objective Loss 1.031743                                        LR 0.001298    Time 0.074819    
2022-01-29 11:30:47,758 - Epoch: [201][  300/  391]    Overall Loss 1.029352    Objective Loss 1.029352                                        LR 0.001298    Time 0.073907    
2022-01-29 11:30:54,308 - Epoch: [201][  391/  391]    Overall Loss 1.035521    Objective Loss 1.035521    Top1 69.711538    Top5 93.750000    LR 0.001298    Time 0.073458    
2022-01-29 11:30:54,366 - --- validate (epoch=201)-----------
2022-01-29 11:30:54,366 - 10000 samples (128 per mini-batch)
2022-01-29 11:30:57,637 - Epoch: [201][   79/   79]    Loss 1.513062    Top1 58.370000    Top5 85.650000    
2022-01-29 11:30:57,693 - ==> Top1: 58.370    Top5: 85.650    Loss: 1.513

2022-01-29 11:30:57,698 - ==> Best [Top1: 58.370   Top5: 85.650   Sparsity:0.00   Params: 627712 on epoch: 201]
2022-01-29 11:30:57,698 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:30:57,738 - 

2022-01-29 11:30:57,738 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:31:05,154 - Epoch: [202][  100/  391]    Overall Loss 0.985539    Objective Loss 0.985539                                        LR 0.001298    Time 0.074135    
2022-01-29 11:31:12,426 - Epoch: [202][  200/  391]    Overall Loss 0.981584    Objective Loss 0.981584                                        LR 0.001298    Time 0.073420    
2022-01-29 11:31:19,734 - Epoch: [202][  300/  391]    Overall Loss 0.995085    Objective Loss 0.995085                                        LR 0.001298    Time 0.073307    
2022-01-29 11:31:26,393 - Epoch: [202][  391/  391]    Overall Loss 1.003047    Objective Loss 1.003047    Top1 71.153846    Top5 97.115385    LR 0.001298    Time 0.073274    
2022-01-29 11:31:26,451 - --- validate (epoch=202)-----------
2022-01-29 11:31:26,451 - 10000 samples (128 per mini-batch)
2022-01-29 11:31:29,748 - Epoch: [202][   79/   79]    Loss 1.435499    Top1 60.760000    Top5 86.850000    
2022-01-29 11:31:29,802 - ==> Top1: 60.760    Top5: 86.850    Loss: 1.435

2022-01-29 11:31:29,808 - ==> Best [Top1: 60.760   Top5: 86.850   Sparsity:0.00   Params: 627712 on epoch: 202]
2022-01-29 11:31:29,808 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:31:29,848 - 

2022-01-29 11:31:29,848 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:31:37,495 - Epoch: [203][  100/  391]    Overall Loss 0.969017    Objective Loss 0.969017                                        LR 0.001298    Time 0.076443    
2022-01-29 11:31:44,805 - Epoch: [203][  200/  391]    Overall Loss 0.985739    Objective Loss 0.985739                                        LR 0.001298    Time 0.074767    
2022-01-29 11:31:52,120 - Epoch: [203][  300/  391]    Overall Loss 0.983582    Objective Loss 0.983582                                        LR 0.001298    Time 0.074224    
2022-01-29 11:31:58,777 - Epoch: [203][  391/  391]    Overall Loss 0.986368    Objective Loss 0.986368    Top1 70.192308    Top5 96.634615    LR 0.001298    Time 0.073974    
2022-01-29 11:31:58,835 - --- validate (epoch=203)-----------
2022-01-29 11:31:58,836 - 10000 samples (128 per mini-batch)
2022-01-29 11:32:02,161 - Epoch: [203][   79/   79]    Loss 1.512499    Top1 58.770000    Top5 85.700000    
2022-01-29 11:32:02,216 - ==> Top1: 58.770    Top5: 85.700    Loss: 1.512

2022-01-29 11:32:02,221 - ==> Best [Top1: 60.760   Top5: 86.850   Sparsity:0.00   Params: 627712 on epoch: 202]
2022-01-29 11:32:02,221 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:32:02,258 - 

2022-01-29 11:32:02,258 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:32:09,855 - Epoch: [204][  100/  391]    Overall Loss 0.962151    Objective Loss 0.962151                                        LR 0.001298    Time 0.075940    
2022-01-29 11:32:17,115 - Epoch: [204][  200/  391]    Overall Loss 0.967019    Objective Loss 0.967019                                        LR 0.001298    Time 0.074267    
2022-01-29 11:32:24,433 - Epoch: [204][  300/  391]    Overall Loss 0.966790    Objective Loss 0.966790                                        LR 0.001298    Time 0.073900    
2022-01-29 11:32:31,091 - Epoch: [204][  391/  391]    Overall Loss 0.974475    Objective Loss 0.974475    Top1 75.480769    Top5 96.634615    LR 0.001298    Time 0.073729    
2022-01-29 11:32:31,150 - --- validate (epoch=204)-----------
2022-01-29 11:32:31,151 - 10000 samples (128 per mini-batch)
2022-01-29 11:32:34,465 - Epoch: [204][   79/   79]    Loss 1.443249    Top1 60.250000    Top5 86.810000    
2022-01-29 11:32:34,519 - ==> Top1: 60.250    Top5: 86.810    Loss: 1.443

2022-01-29 11:32:34,524 - ==> Best [Top1: 60.760   Top5: 86.850   Sparsity:0.00   Params: 627712 on epoch: 202]
2022-01-29 11:32:34,525 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:32:34,557 - 

2022-01-29 11:32:34,557 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:32:42,099 - Epoch: [205][  100/  391]    Overall Loss 0.950816    Objective Loss 0.950816                                        LR 0.001298    Time 0.075389    
2022-01-29 11:32:49,447 - Epoch: [205][  200/  391]    Overall Loss 0.949726    Objective Loss 0.949726                                        LR 0.001298    Time 0.074431    
2022-01-29 11:32:56,866 - Epoch: [205][  300/  391]    Overall Loss 0.958573    Objective Loss 0.958573                                        LR 0.001298    Time 0.074346    
2022-01-29 11:33:03,592 - Epoch: [205][  391/  391]    Overall Loss 0.961360    Objective Loss 0.961360    Top1 75.000000    Top5 93.269231    LR 0.001298    Time 0.074244    
2022-01-29 11:33:03,656 - --- validate (epoch=205)-----------
2022-01-29 11:33:03,656 - 10000 samples (128 per mini-batch)
2022-01-29 11:33:06,936 - Epoch: [205][   79/   79]    Loss 1.524633    Top1 58.580000    Top5 84.950000    
2022-01-29 11:33:06,990 - ==> Top1: 58.580    Top5: 84.950    Loss: 1.525

2022-01-29 11:33:06,995 - ==> Best [Top1: 60.760   Top5: 86.850   Sparsity:0.00   Params: 627712 on epoch: 202]
2022-01-29 11:33:06,995 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:33:07,032 - 

2022-01-29 11:33:07,032 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:33:14,769 - Epoch: [206][  100/  391]    Overall Loss 0.916248    Objective Loss 0.916248                                        LR 0.001298    Time 0.077340    
2022-01-29 11:33:22,200 - Epoch: [206][  200/  391]    Overall Loss 0.926401    Objective Loss 0.926401                                        LR 0.001298    Time 0.075824    
2022-01-29 11:33:29,619 - Epoch: [206][  300/  391]    Overall Loss 0.939206    Objective Loss 0.939206                                        LR 0.001298    Time 0.075274    
2022-01-29 11:33:36,353 - Epoch: [206][  391/  391]    Overall Loss 0.950664    Objective Loss 0.950664    Top1 70.192308    Top5 91.346154    LR 0.001298    Time 0.074975    
2022-01-29 11:33:36,410 - --- validate (epoch=206)-----------
2022-01-29 11:33:36,410 - 10000 samples (128 per mini-batch)
2022-01-29 11:33:39,688 - Epoch: [206][   79/   79]    Loss 1.473227    Top1 59.120000    Top5 86.480000    
2022-01-29 11:33:39,744 - ==> Top1: 59.120    Top5: 86.480    Loss: 1.473

2022-01-29 11:33:39,749 - ==> Best [Top1: 60.760   Top5: 86.850   Sparsity:0.00   Params: 627712 on epoch: 202]
2022-01-29 11:33:39,749 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:33:39,780 - 

2022-01-29 11:33:39,781 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:33:47,390 - Epoch: [207][  100/  391]    Overall Loss 0.908198    Objective Loss 0.908198                                        LR 0.001298    Time 0.076073    
2022-01-29 11:33:54,804 - Epoch: [207][  200/  391]    Overall Loss 0.911934    Objective Loss 0.911934                                        LR 0.001298    Time 0.075099    
2022-01-29 11:34:02,221 - Epoch: [207][  300/  391]    Overall Loss 0.924211    Objective Loss 0.924211                                        LR 0.001298    Time 0.074788    
2022-01-29 11:34:08,869 - Epoch: [207][  391/  391]    Overall Loss 0.927769    Objective Loss 0.927769    Top1 71.634615    Top5 92.788462    LR 0.001298    Time 0.074381    
2022-01-29 11:34:08,925 - --- validate (epoch=207)-----------
2022-01-29 11:34:08,925 - 10000 samples (128 per mini-batch)
2022-01-29 11:34:12,184 - Epoch: [207][   79/   79]    Loss 1.446653    Top1 59.980000    Top5 86.550000    
2022-01-29 11:34:12,244 - ==> Top1: 59.980    Top5: 86.550    Loss: 1.447

2022-01-29 11:34:12,249 - ==> Best [Top1: 60.760   Top5: 86.850   Sparsity:0.00   Params: 627712 on epoch: 202]
2022-01-29 11:34:12,249 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:34:12,286 - 

2022-01-29 11:34:12,286 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:34:19,914 - Epoch: [208][  100/  391]    Overall Loss 0.916025    Objective Loss 0.916025                                        LR 0.001298    Time 0.076250    
2022-01-29 11:34:27,298 - Epoch: [208][  200/  391]    Overall Loss 0.919247    Objective Loss 0.919247                                        LR 0.001298    Time 0.075040    
2022-01-29 11:34:34,686 - Epoch: [208][  300/  391]    Overall Loss 0.928502    Objective Loss 0.928502                                        LR 0.001298    Time 0.074649    
2022-01-29 11:34:41,409 - Epoch: [208][  391/  391]    Overall Loss 0.926356    Objective Loss 0.926356    Top1 76.923077    Top5 94.711538    LR 0.001298    Time 0.074468    
2022-01-29 11:34:41,468 - --- validate (epoch=208)-----------
2022-01-29 11:34:41,468 - 10000 samples (128 per mini-batch)
2022-01-29 11:34:44,849 - Epoch: [208][   79/   79]    Loss 1.471205    Top1 59.670000    Top5 86.140000    
2022-01-29 11:34:44,902 - ==> Top1: 59.670    Top5: 86.140    Loss: 1.471

2022-01-29 11:34:44,907 - ==> Best [Top1: 60.760   Top5: 86.850   Sparsity:0.00   Params: 627712 on epoch: 202]
2022-01-29 11:34:44,907 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:34:44,943 - 

2022-01-29 11:34:44,943 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:34:52,599 - Epoch: [209][  100/  391]    Overall Loss 0.886888    Objective Loss 0.886888                                        LR 0.001298    Time 0.076531    
2022-01-29 11:34:59,958 - Epoch: [209][  200/  391]    Overall Loss 0.890457    Objective Loss 0.890457                                        LR 0.001298    Time 0.075056    
2022-01-29 11:35:07,259 - Epoch: [209][  300/  391]    Overall Loss 0.910616    Objective Loss 0.910616                                        LR 0.001298    Time 0.074371    
2022-01-29 11:35:13,908 - Epoch: [209][  391/  391]    Overall Loss 0.913973    Objective Loss 0.913973    Top1 74.519231    Top5 96.634615    LR 0.001298    Time 0.074064    
2022-01-29 11:35:13,967 - --- validate (epoch=209)-----------
2022-01-29 11:35:13,967 - 10000 samples (128 per mini-batch)
2022-01-29 11:35:17,213 - Epoch: [209][   79/   79]    Loss 1.426096    Top1 60.830000    Top5 86.820000    
2022-01-29 11:35:17,266 - ==> Top1: 60.830    Top5: 86.820    Loss: 1.426

2022-01-29 11:35:17,271 - ==> Best [Top1: 60.830   Top5: 86.820   Sparsity:0.00   Params: 627712 on epoch: 209]
2022-01-29 11:35:17,271 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:35:17,310 - 

2022-01-29 11:35:17,311 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:35:24,847 - Epoch: [210][  100/  391]    Overall Loss 0.870332    Objective Loss 0.870332                                        LR 0.001298    Time 0.075343    
2022-01-29 11:35:32,161 - Epoch: [210][  200/  391]    Overall Loss 0.901071    Objective Loss 0.901071                                        LR 0.001298    Time 0.074233    
2022-01-29 11:35:39,516 - Epoch: [210][  300/  391]    Overall Loss 0.910965    Objective Loss 0.910965                                        LR 0.001298    Time 0.074005    
2022-01-29 11:35:46,169 - Epoch: [210][  391/  391]    Overall Loss 0.916196    Objective Loss 0.916196    Top1 74.519231    Top5 94.711538    LR 0.001298    Time 0.073794    
2022-01-29 11:35:46,225 - --- validate (epoch=210)-----------
2022-01-29 11:35:46,225 - 10000 samples (128 per mini-batch)
2022-01-29 11:35:49,498 - Epoch: [210][   79/   79]    Loss 1.474649    Top1 59.610000    Top5 86.020000    
2022-01-29 11:35:49,553 - ==> Top1: 59.610    Top5: 86.020    Loss: 1.475

2022-01-29 11:35:49,558 - ==> Best [Top1: 60.830   Top5: 86.820   Sparsity:0.00   Params: 627712 on epoch: 209]
2022-01-29 11:35:49,558 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:35:49,593 - 

2022-01-29 11:35:49,593 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:35:57,115 - Epoch: [211][  100/  391]    Overall Loss 0.909620    Objective Loss 0.909620                                        LR 0.001298    Time 0.075194    
2022-01-29 11:36:04,420 - Epoch: [211][  200/  391]    Overall Loss 0.903146    Objective Loss 0.903146                                        LR 0.001298    Time 0.074117    
2022-01-29 11:36:11,722 - Epoch: [211][  300/  391]    Overall Loss 0.910406    Objective Loss 0.910406                                        LR 0.001298    Time 0.073746    
2022-01-29 11:36:18,266 - Epoch: [211][  391/  391]    Overall Loss 0.916021    Objective Loss 0.916021    Top1 75.961538    Top5 96.634615    LR 0.001298    Time 0.073319    
2022-01-29 11:36:18,325 - --- validate (epoch=211)-----------
2022-01-29 11:36:18,326 - 10000 samples (128 per mini-batch)
2022-01-29 11:36:21,513 - Epoch: [211][   79/   79]    Loss 1.417788    Top1 61.280000    Top5 87.290000    
2022-01-29 11:36:21,566 - ==> Top1: 61.280    Top5: 87.290    Loss: 1.418

2022-01-29 11:36:21,572 - ==> Best [Top1: 61.280   Top5: 87.290   Sparsity:0.00   Params: 627712 on epoch: 211]
2022-01-29 11:36:21,572 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:36:21,612 - 

2022-01-29 11:36:21,612 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:36:29,199 - Epoch: [212][  100/  391]    Overall Loss 0.891897    Objective Loss 0.891897                                        LR 0.001298    Time 0.075840    
2022-01-29 11:36:36,543 - Epoch: [212][  200/  391]    Overall Loss 0.898176    Objective Loss 0.898176                                        LR 0.001298    Time 0.074639    
2022-01-29 11:36:43,903 - Epoch: [212][  300/  391]    Overall Loss 0.905740    Objective Loss 0.905740                                        LR 0.001298    Time 0.074288    
2022-01-29 11:36:50,598 - Epoch: [212][  391/  391]    Overall Loss 0.912327    Objective Loss 0.912327    Top1 75.000000    Top5 95.192308    LR 0.001298    Time 0.074121    
2022-01-29 11:36:50,657 - --- validate (epoch=212)-----------
2022-01-29 11:36:50,657 - 10000 samples (128 per mini-batch)
2022-01-29 11:36:53,901 - Epoch: [212][   79/   79]    Loss 1.431384    Top1 61.060000    Top5 87.030000    
2022-01-29 11:36:53,959 - ==> Top1: 61.060    Top5: 87.030    Loss: 1.431

2022-01-29 11:36:53,964 - ==> Best [Top1: 61.280   Top5: 87.290   Sparsity:0.00   Params: 627712 on epoch: 211]
2022-01-29 11:36:53,964 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:36:53,992 - 

2022-01-29 11:36:53,992 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:37:01,506 - Epoch: [213][  100/  391]    Overall Loss 0.905142    Objective Loss 0.905142                                        LR 0.001298    Time 0.075112    
2022-01-29 11:37:08,695 - Epoch: [213][  200/  391]    Overall Loss 0.893920    Objective Loss 0.893920                                        LR 0.001298    Time 0.073499    
2022-01-29 11:37:15,887 - Epoch: [213][  300/  391]    Overall Loss 0.893215    Objective Loss 0.893215                                        LR 0.001298    Time 0.072969    
2022-01-29 11:37:22,430 - Epoch: [213][  391/  391]    Overall Loss 0.898286    Objective Loss 0.898286    Top1 80.288462    Top5 98.557692    LR 0.001298    Time 0.072719    
2022-01-29 11:37:22,487 - --- validate (epoch=213)-----------
2022-01-29 11:37:22,487 - 10000 samples (128 per mini-batch)
2022-01-29 11:37:25,773 - Epoch: [213][   79/   79]    Loss 1.402760    Top1 60.960000    Top5 87.230000    
2022-01-29 11:37:25,827 - ==> Top1: 60.960    Top5: 87.230    Loss: 1.403

2022-01-29 11:37:25,832 - ==> Best [Top1: 61.280   Top5: 87.290   Sparsity:0.00   Params: 627712 on epoch: 211]
2022-01-29 11:37:25,833 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:37:25,869 - 

2022-01-29 11:37:25,869 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:37:33,473 - Epoch: [214][  100/  391]    Overall Loss 0.869738    Objective Loss 0.869738                                        LR 0.001298    Time 0.076006    
2022-01-29 11:37:40,835 - Epoch: [214][  200/  391]    Overall Loss 0.875446    Objective Loss 0.875446                                        LR 0.001298    Time 0.074808    
2022-01-29 11:37:48,279 - Epoch: [214][  300/  391]    Overall Loss 0.884883    Objective Loss 0.884883                                        LR 0.001298    Time 0.074684    
2022-01-29 11:37:55,052 - Epoch: [214][  391/  391]    Overall Loss 0.891232    Objective Loss 0.891232    Top1 74.038462    Top5 95.192308    LR 0.001298    Time 0.074623    
2022-01-29 11:37:55,113 - --- validate (epoch=214)-----------
2022-01-29 11:37:55,113 - 10000 samples (128 per mini-batch)
2022-01-29 11:37:58,419 - Epoch: [214][   79/   79]    Loss 1.533475    Top1 57.970000    Top5 85.080000    
2022-01-29 11:37:58,475 - ==> Top1: 57.970    Top5: 85.080    Loss: 1.533

2022-01-29 11:37:58,479 - ==> Best [Top1: 61.280   Top5: 87.290   Sparsity:0.00   Params: 627712 on epoch: 211]
2022-01-29 11:37:58,480 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:37:58,513 - 

2022-01-29 11:37:58,513 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:38:06,193 - Epoch: [215][  100/  391]    Overall Loss 0.856427    Objective Loss 0.856427                                        LR 0.001298    Time 0.076774    
2022-01-29 11:38:13,544 - Epoch: [215][  200/  391]    Overall Loss 0.888806    Objective Loss 0.888806                                        LR 0.001298    Time 0.075139    
2022-01-29 11:38:20,925 - Epoch: [215][  300/  391]    Overall Loss 0.888554    Objective Loss 0.888554                                        LR 0.001298    Time 0.074692    
2022-01-29 11:38:27,642 - Epoch: [215][  391/  391]    Overall Loss 0.886083    Objective Loss 0.886083    Top1 73.076923    Top5 92.788462    LR 0.001298    Time 0.074484    
2022-01-29 11:38:27,700 - --- validate (epoch=215)-----------
2022-01-29 11:38:27,700 - 10000 samples (128 per mini-batch)
2022-01-29 11:38:30,968 - Epoch: [215][   79/   79]    Loss 1.420625    Top1 60.890000    Top5 86.560000    
2022-01-29 11:38:31,020 - ==> Top1: 60.890    Top5: 86.560    Loss: 1.421

2022-01-29 11:38:31,025 - ==> Best [Top1: 61.280   Top5: 87.290   Sparsity:0.00   Params: 627712 on epoch: 211]
2022-01-29 11:38:31,025 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:38:31,061 - 

2022-01-29 11:38:31,061 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:38:38,644 - Epoch: [216][  100/  391]    Overall Loss 0.877779    Objective Loss 0.877779                                        LR 0.001298    Time 0.075808    
2022-01-29 11:38:45,892 - Epoch: [216][  200/  391]    Overall Loss 0.874841    Objective Loss 0.874841                                        LR 0.001298    Time 0.074141    
2022-01-29 11:38:53,060 - Epoch: [216][  300/  391]    Overall Loss 0.879165    Objective Loss 0.879165                                        LR 0.001298    Time 0.073317    
2022-01-29 11:38:59,614 - Epoch: [216][  391/  391]    Overall Loss 0.881456    Objective Loss 0.881456    Top1 77.884615    Top5 95.673077    LR 0.001298    Time 0.073013    
2022-01-29 11:38:59,674 - --- validate (epoch=216)-----------
2022-01-29 11:38:59,674 - 10000 samples (128 per mini-batch)
2022-01-29 11:39:02,936 - Epoch: [216][   79/   79]    Loss 1.420714    Top1 61.240000    Top5 86.940000    
2022-01-29 11:39:02,994 - ==> Top1: 61.240    Top5: 86.940    Loss: 1.421

2022-01-29 11:39:02,999 - ==> Best [Top1: 61.280   Top5: 87.290   Sparsity:0.00   Params: 627712 on epoch: 211]
2022-01-29 11:39:02,999 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:39:03,032 - 

2022-01-29 11:39:03,032 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:39:10,551 - Epoch: [217][  100/  391]    Overall Loss 0.842876    Objective Loss 0.842876                                        LR 0.001298    Time 0.075164    
2022-01-29 11:39:17,714 - Epoch: [217][  200/  391]    Overall Loss 0.855196    Objective Loss 0.855196                                        LR 0.001298    Time 0.073392    
2022-01-29 11:39:24,878 - Epoch: [217][  300/  391]    Overall Loss 0.866000    Objective Loss 0.866000                                        LR 0.001298    Time 0.072802    
2022-01-29 11:39:31,405 - Epoch: [217][  391/  391]    Overall Loss 0.871153    Objective Loss 0.871153    Top1 71.153846    Top5 96.153846    LR 0.001298    Time 0.072550    
2022-01-29 11:39:31,465 - --- validate (epoch=217)-----------
2022-01-29 11:39:31,466 - 10000 samples (128 per mini-batch)
2022-01-29 11:39:34,653 - Epoch: [217][   79/   79]    Loss 1.418801    Top1 60.320000    Top5 87.250000    
2022-01-29 11:39:34,712 - ==> Top1: 60.320    Top5: 87.250    Loss: 1.419

2022-01-29 11:39:34,716 - ==> Best [Top1: 61.280   Top5: 87.290   Sparsity:0.00   Params: 627712 on epoch: 211]
2022-01-29 11:39:34,716 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:39:34,744 - 

2022-01-29 11:39:34,745 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:39:42,239 - Epoch: [218][  100/  391]    Overall Loss 0.865845    Objective Loss 0.865845                                        LR 0.001298    Time 0.074925    
2022-01-29 11:39:49,536 - Epoch: [218][  200/  391]    Overall Loss 0.863070    Objective Loss 0.863070                                        LR 0.001298    Time 0.073939    
2022-01-29 11:39:56,859 - Epoch: [218][  300/  391]    Overall Loss 0.874499    Objective Loss 0.874499                                        LR 0.001298    Time 0.073702    
2022-01-29 11:40:03,456 - Epoch: [218][  391/  391]    Overall Loss 0.879016    Objective Loss 0.879016    Top1 71.634615    Top5 96.634615    LR 0.001298    Time 0.073418    
2022-01-29 11:40:03,521 - --- validate (epoch=218)-----------
2022-01-29 11:40:03,521 - 10000 samples (128 per mini-batch)
2022-01-29 11:40:06,737 - Epoch: [218][   79/   79]    Loss 1.469416    Top1 59.530000    Top5 86.160000    
2022-01-29 11:40:06,791 - ==> Top1: 59.530    Top5: 86.160    Loss: 1.469

2022-01-29 11:40:06,796 - ==> Best [Top1: 61.280   Top5: 87.290   Sparsity:0.00   Params: 627712 on epoch: 211]
2022-01-29 11:40:06,796 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:40:06,832 - 

2022-01-29 11:40:06,833 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:40:14,331 - Epoch: [219][  100/  391]    Overall Loss 0.845281    Objective Loss 0.845281                                        LR 0.001298    Time 0.074959    
2022-01-29 11:40:21,506 - Epoch: [219][  200/  391]    Overall Loss 0.860007    Objective Loss 0.860007                                        LR 0.001298    Time 0.073348    
2022-01-29 11:40:28,681 - Epoch: [219][  300/  391]    Overall Loss 0.859358    Objective Loss 0.859358                                        LR 0.001298    Time 0.072812    
2022-01-29 11:40:35,210 - Epoch: [219][  391/  391]    Overall Loss 0.860225    Objective Loss 0.860225    Top1 75.961538    Top5 96.153846    LR 0.001298    Time 0.072560    
2022-01-29 11:40:35,266 - --- validate (epoch=219)-----------
2022-01-29 11:40:35,266 - 10000 samples (128 per mini-batch)
2022-01-29 11:40:38,519 - Epoch: [219][   79/   79]    Loss 1.431197    Top1 60.220000    Top5 86.990000    
2022-01-29 11:40:38,574 - ==> Top1: 60.220    Top5: 86.990    Loss: 1.431

2022-01-29 11:40:38,579 - ==> Best [Top1: 61.280   Top5: 87.290   Sparsity:0.00   Params: 627712 on epoch: 211]
2022-01-29 11:40:38,579 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:40:38,615 - 

2022-01-29 11:40:38,615 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:40:46,149 - Epoch: [220][  100/  391]    Overall Loss 0.843483    Objective Loss 0.843483                                        LR 0.001298    Time 0.075313    
2022-01-29 11:40:53,339 - Epoch: [220][  200/  391]    Overall Loss 0.839440    Objective Loss 0.839440                                        LR 0.001298    Time 0.073597    
2022-01-29 11:41:00,524 - Epoch: [220][  300/  391]    Overall Loss 0.848680    Objective Loss 0.848680                                        LR 0.001298    Time 0.073012    
2022-01-29 11:41:07,060 - Epoch: [220][  391/  391]    Overall Loss 0.863806    Objective Loss 0.863806    Top1 74.519231    Top5 96.153846    LR 0.001298    Time 0.072735    
2022-01-29 11:41:07,120 - --- validate (epoch=220)-----------
2022-01-29 11:41:07,120 - 10000 samples (128 per mini-batch)
2022-01-29 11:41:10,467 - Epoch: [220][   79/   79]    Loss 1.460512    Top1 60.080000    Top5 86.140000    
2022-01-29 11:41:10,525 - ==> Top1: 60.080    Top5: 86.140    Loss: 1.461

2022-01-29 11:41:10,530 - ==> Best [Top1: 61.280   Top5: 87.290   Sparsity:0.00   Params: 627712 on epoch: 211]
2022-01-29 11:41:10,530 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:41:10,561 - 

2022-01-29 11:41:10,562 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:41:17,982 - Epoch: [221][  100/  391]    Overall Loss 0.829630    Objective Loss 0.829630                                        LR 0.001298    Time 0.074177    
2022-01-29 11:41:25,283 - Epoch: [221][  200/  391]    Overall Loss 0.839147    Objective Loss 0.839147                                        LR 0.001298    Time 0.073590    
2022-01-29 11:41:32,651 - Epoch: [221][  300/  391]    Overall Loss 0.852403    Objective Loss 0.852403                                        LR 0.001298    Time 0.073617    
2022-01-29 11:41:39,350 - Epoch: [221][  391/  391]    Overall Loss 0.858926    Objective Loss 0.858926    Top1 77.403846    Top5 96.634615    LR 0.001298    Time 0.073613    
2022-01-29 11:41:39,409 - --- validate (epoch=221)-----------
2022-01-29 11:41:39,409 - 10000 samples (128 per mini-batch)
2022-01-29 11:41:42,675 - Epoch: [221][   79/   79]    Loss 1.448326    Top1 60.450000    Top5 86.510000    
2022-01-29 11:41:42,728 - ==> Top1: 60.450    Top5: 86.510    Loss: 1.448

2022-01-29 11:41:42,733 - ==> Best [Top1: 61.280   Top5: 87.290   Sparsity:0.00   Params: 627712 on epoch: 211]
2022-01-29 11:41:42,733 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:41:42,769 - 

2022-01-29 11:41:42,769 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:41:50,251 - Epoch: [222][  100/  391]    Overall Loss 0.805284    Objective Loss 0.805284                                        LR 0.001298    Time 0.074803    
2022-01-29 11:41:57,450 - Epoch: [222][  200/  391]    Overall Loss 0.826884    Objective Loss 0.826884                                        LR 0.001298    Time 0.073390    
2022-01-29 11:42:04,664 - Epoch: [222][  300/  391]    Overall Loss 0.824321    Objective Loss 0.824321                                        LR 0.001298    Time 0.072969    
2022-01-29 11:42:11,225 - Epoch: [222][  391/  391]    Overall Loss 0.836063    Objective Loss 0.836063    Top1 70.192308    Top5 92.307692    LR 0.001298    Time 0.072765    
2022-01-29 11:42:11,282 - --- validate (epoch=222)-----------
2022-01-29 11:42:11,282 - 10000 samples (128 per mini-batch)
2022-01-29 11:42:14,525 - Epoch: [222][   79/   79]    Loss 1.463648    Top1 59.540000    Top5 85.750000    
2022-01-29 11:42:14,576 - ==> Top1: 59.540    Top5: 85.750    Loss: 1.464

2022-01-29 11:42:14,581 - ==> Best [Top1: 61.280   Top5: 87.290   Sparsity:0.00   Params: 627712 on epoch: 211]
2022-01-29 11:42:14,581 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:42:14,616 - 

2022-01-29 11:42:14,616 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:42:22,140 - Epoch: [223][  100/  391]    Overall Loss 0.810846    Objective Loss 0.810846                                        LR 0.001298    Time 0.075210    
2022-01-29 11:42:29,498 - Epoch: [223][  200/  391]    Overall Loss 0.823827    Objective Loss 0.823827                                        LR 0.001298    Time 0.074390    
2022-01-29 11:42:36,863 - Epoch: [223][  300/  391]    Overall Loss 0.839675    Objective Loss 0.839675                                        LR 0.001298    Time 0.074140    
2022-01-29 11:42:43,569 - Epoch: [223][  391/  391]    Overall Loss 0.846371    Objective Loss 0.846371    Top1 71.634615    Top5 96.153846    LR 0.001298    Time 0.074035    
2022-01-29 11:42:43,626 - --- validate (epoch=223)-----------
2022-01-29 11:42:43,626 - 10000 samples (128 per mini-batch)
2022-01-29 11:42:46,952 - Epoch: [223][   79/   79]    Loss 1.439438    Top1 60.540000    Top5 86.560000    
2022-01-29 11:42:47,004 - ==> Top1: 60.540    Top5: 86.560    Loss: 1.439

2022-01-29 11:42:47,009 - ==> Best [Top1: 61.280   Top5: 87.290   Sparsity:0.00   Params: 627712 on epoch: 211]
2022-01-29 11:42:47,009 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:42:47,038 - 

2022-01-29 11:42:47,038 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:42:54,623 - Epoch: [224][  100/  391]    Overall Loss 0.795535    Objective Loss 0.795535                                        LR 0.001298    Time 0.075823    
2022-01-29 11:43:01,967 - Epoch: [224][  200/  391]    Overall Loss 0.819651    Objective Loss 0.819651                                        LR 0.001298    Time 0.074629    
2022-01-29 11:43:09,314 - Epoch: [224][  300/  391]    Overall Loss 0.824317    Objective Loss 0.824317                                        LR 0.001298    Time 0.074241    
2022-01-29 11:43:16,037 - Epoch: [224][  391/  391]    Overall Loss 0.834540    Objective Loss 0.834540    Top1 72.115385    Top5 96.153846    LR 0.001298    Time 0.074152    
2022-01-29 11:43:16,096 - --- validate (epoch=224)-----------
2022-01-29 11:43:16,096 - 10000 samples (128 per mini-batch)
2022-01-29 11:43:19,378 - Epoch: [224][   79/   79]    Loss 1.435612    Top1 60.420000    Top5 86.900000    
2022-01-29 11:43:19,437 - ==> Top1: 60.420    Top5: 86.900    Loss: 1.436

2022-01-29 11:43:19,442 - ==> Best [Top1: 61.280   Top5: 87.290   Sparsity:0.00   Params: 627712 on epoch: 211]
2022-01-29 11:43:19,442 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:43:19,477 - 

2022-01-29 11:43:19,477 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:43:27,098 - Epoch: [225][  100/  391]    Overall Loss 0.798783    Objective Loss 0.798783                                        LR 0.001298    Time 0.076188    
2022-01-29 11:43:34,527 - Epoch: [225][  200/  391]    Overall Loss 0.806683    Objective Loss 0.806683                                        LR 0.001298    Time 0.075230    
2022-01-29 11:43:41,955 - Epoch: [225][  300/  391]    Overall Loss 0.821487    Objective Loss 0.821487                                        LR 0.001298    Time 0.074911    
2022-01-29 11:43:48,718 - Epoch: [225][  391/  391]    Overall Loss 0.829833    Objective Loss 0.829833    Top1 74.519231    Top5 93.269231    LR 0.001298    Time 0.074772    
2022-01-29 11:43:48,778 - --- validate (epoch=225)-----------
2022-01-29 11:43:48,778 - 10000 samples (128 per mini-batch)
2022-01-29 11:43:52,090 - Epoch: [225][   79/   79]    Loss 1.447299    Top1 60.160000    Top5 86.310000    
2022-01-29 11:43:52,144 - ==> Top1: 60.160    Top5: 86.310    Loss: 1.447

2022-01-29 11:43:52,149 - ==> Best [Top1: 61.280   Top5: 87.290   Sparsity:0.00   Params: 627712 on epoch: 211]
2022-01-29 11:43:52,149 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:43:52,185 - 

2022-01-29 11:43:52,185 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:43:59,836 - Epoch: [226][  100/  391]    Overall Loss 0.796047    Objective Loss 0.796047                                        LR 0.001298    Time 0.076473    
2022-01-29 11:44:07,271 - Epoch: [226][  200/  391]    Overall Loss 0.809922    Objective Loss 0.809922                                        LR 0.001298    Time 0.075410    
2022-01-29 11:44:14,690 - Epoch: [226][  300/  391]    Overall Loss 0.818490    Objective Loss 0.818490                                        LR 0.001298    Time 0.075000    
2022-01-29 11:44:21,341 - Epoch: [226][  391/  391]    Overall Loss 0.825923    Objective Loss 0.825923    Top1 78.846154    Top5 97.115385    LR 0.001298    Time 0.074552    
2022-01-29 11:44:21,398 - --- validate (epoch=226)-----------
2022-01-29 11:44:21,399 - 10000 samples (128 per mini-batch)
2022-01-29 11:44:24,739 - Epoch: [226][   79/   79]    Loss 1.385951    Top1 62.050000    Top5 87.380000    
2022-01-29 11:44:24,797 - ==> Top1: 62.050    Top5: 87.380    Loss: 1.386

2022-01-29 11:44:24,802 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:44:24,802 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:44:24,841 - 

2022-01-29 11:44:24,841 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:44:32,406 - Epoch: [227][  100/  391]    Overall Loss 0.773238    Objective Loss 0.773238                                        LR 0.001298    Time 0.075627    
2022-01-29 11:44:39,614 - Epoch: [227][  200/  391]    Overall Loss 0.778237    Objective Loss 0.778237                                        LR 0.001298    Time 0.073848    
2022-01-29 11:44:46,790 - Epoch: [227][  300/  391]    Overall Loss 0.794144    Objective Loss 0.794144                                        LR 0.001298    Time 0.073149    
2022-01-29 11:44:53,349 - Epoch: [227][  391/  391]    Overall Loss 0.808283    Objective Loss 0.808283    Top1 76.923077    Top5 96.153846    LR 0.001298    Time 0.072897    
2022-01-29 11:44:53,408 - --- validate (epoch=227)-----------
2022-01-29 11:44:53,408 - 10000 samples (128 per mini-batch)
2022-01-29 11:44:56,706 - Epoch: [227][   79/   79]    Loss 1.433720    Top1 60.670000    Top5 86.360000    
2022-01-29 11:44:56,765 - ==> Top1: 60.670    Top5: 86.360    Loss: 1.434

2022-01-29 11:44:56,770 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:44:56,770 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:44:56,806 - 

2022-01-29 11:44:56,807 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:45:04,454 - Epoch: [228][  100/  391]    Overall Loss 0.797981    Objective Loss 0.797981                                        LR 0.001298    Time 0.076447    
2022-01-29 11:45:11,848 - Epoch: [228][  200/  391]    Overall Loss 0.815008    Objective Loss 0.815008                                        LR 0.001298    Time 0.075188    
2022-01-29 11:45:19,236 - Epoch: [228][  300/  391]    Overall Loss 0.816119    Objective Loss 0.816119                                        LR 0.001298    Time 0.074750    
2022-01-29 11:45:25,909 - Epoch: [228][  391/  391]    Overall Loss 0.818045    Objective Loss 0.818045    Top1 75.480769    Top5 96.153846    LR 0.001298    Time 0.074417    
2022-01-29 11:45:25,967 - --- validate (epoch=228)-----------
2022-01-29 11:45:25,968 - 10000 samples (128 per mini-batch)
2022-01-29 11:45:29,238 - Epoch: [228][   79/   79]    Loss 1.425077    Top1 61.060000    Top5 86.810000    
2022-01-29 11:45:29,290 - ==> Top1: 61.060    Top5: 86.810    Loss: 1.425

2022-01-29 11:45:29,295 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:45:29,295 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:45:29,330 - 

2022-01-29 11:45:29,330 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:45:36,929 - Epoch: [229][  100/  391]    Overall Loss 0.836550    Objective Loss 0.836550                                        LR 0.001298    Time 0.075961    
2022-01-29 11:45:44,240 - Epoch: [229][  200/  391]    Overall Loss 0.810947    Objective Loss 0.810947                                        LR 0.001298    Time 0.074532    
2022-01-29 11:45:51,604 - Epoch: [229][  300/  391]    Overall Loss 0.820130    Objective Loss 0.820130                                        LR 0.001298    Time 0.074232    
2022-01-29 11:45:58,369 - Epoch: [229][  391/  391]    Overall Loss 0.826440    Objective Loss 0.826440    Top1 80.769231    Top5 94.711538    LR 0.001298    Time 0.074255    
2022-01-29 11:45:58,429 - --- validate (epoch=229)-----------
2022-01-29 11:45:58,429 - 10000 samples (128 per mini-batch)
2022-01-29 11:46:01,680 - Epoch: [229][   79/   79]    Loss 1.459701    Top1 60.650000    Top5 86.210000    
2022-01-29 11:46:01,735 - ==> Top1: 60.650    Top5: 86.210    Loss: 1.460

2022-01-29 11:46:01,740 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:46:01,740 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:46:01,776 - 

2022-01-29 11:46:01,776 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:46:09,280 - Epoch: [230][  100/  391]    Overall Loss 0.768877    Objective Loss 0.768877                                        LR 0.001298    Time 0.075010    
2022-01-29 11:46:16,563 - Epoch: [230][  200/  391]    Overall Loss 0.770614    Objective Loss 0.770614                                        LR 0.001298    Time 0.073919    
2022-01-29 11:46:23,864 - Epoch: [230][  300/  391]    Overall Loss 0.786175    Objective Loss 0.786175                                        LR 0.001298    Time 0.073612    
2022-01-29 11:46:30,609 - Epoch: [230][  391/  391]    Overall Loss 0.795773    Objective Loss 0.795773    Top1 75.961538    Top5 95.192308    LR 0.001298    Time 0.073730    
2022-01-29 11:46:30,667 - --- validate (epoch=230)-----------
2022-01-29 11:46:30,667 - 10000 samples (128 per mini-batch)
2022-01-29 11:46:33,973 - Epoch: [230][   79/   79]    Loss 1.479150    Top1 59.530000    Top5 86.560000    
2022-01-29 11:46:34,025 - ==> Top1: 59.530    Top5: 86.560    Loss: 1.479

2022-01-29 11:46:34,030 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:46:34,030 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:46:34,067 - 

2022-01-29 11:46:34,067 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:46:41,700 - Epoch: [231][  100/  391]    Overall Loss 0.816811    Objective Loss 0.816811                                        LR 0.001298    Time 0.076307    
2022-01-29 11:46:49,038 - Epoch: [231][  200/  391]    Overall Loss 0.818887    Objective Loss 0.818887                                        LR 0.001298    Time 0.074838    
2022-01-29 11:46:56,380 - Epoch: [231][  300/  391]    Overall Loss 0.820905    Objective Loss 0.820905                                        LR 0.001298    Time 0.074360    
2022-01-29 11:47:03,055 - Epoch: [231][  391/  391]    Overall Loss 0.823149    Objective Loss 0.823149    Top1 76.442308    Top5 98.557692    LR 0.001298    Time 0.074122    
2022-01-29 11:47:03,112 - --- validate (epoch=231)-----------
2022-01-29 11:47:03,112 - 10000 samples (128 per mini-batch)
2022-01-29 11:47:06,385 - Epoch: [231][   79/   79]    Loss 1.429795    Top1 60.940000    Top5 86.460000    
2022-01-29 11:47:06,439 - ==> Top1: 60.940    Top5: 86.460    Loss: 1.430

2022-01-29 11:47:06,443 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:47:06,443 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:47:06,479 - 

2022-01-29 11:47:06,479 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:47:14,142 - Epoch: [232][  100/  391]    Overall Loss 0.815669    Objective Loss 0.815669                                        LR 0.001298    Time 0.076600    
2022-01-29 11:47:21,546 - Epoch: [232][  200/  391]    Overall Loss 0.796163    Objective Loss 0.796163                                        LR 0.001298    Time 0.075318    
2022-01-29 11:47:28,950 - Epoch: [232][  300/  391]    Overall Loss 0.810702    Objective Loss 0.810702                                        LR 0.001298    Time 0.074887    
2022-01-29 11:47:35,650 - Epoch: [232][  391/  391]    Overall Loss 0.806041    Objective Loss 0.806041    Top1 70.673077    Top5 96.634615    LR 0.001298    Time 0.074592    
2022-01-29 11:47:35,706 - --- validate (epoch=232)-----------
2022-01-29 11:47:35,706 - 10000 samples (128 per mini-batch)
2022-01-29 11:47:39,066 - Epoch: [232][   79/   79]    Loss 1.462066    Top1 60.310000    Top5 85.570000    
2022-01-29 11:47:39,117 - ==> Top1: 60.310    Top5: 85.570    Loss: 1.462

2022-01-29 11:47:39,122 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:47:39,122 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:47:39,151 - 

2022-01-29 11:47:39,151 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:47:46,815 - Epoch: [233][  100/  391]    Overall Loss 0.782232    Objective Loss 0.782232                                        LR 0.001298    Time 0.076613    
2022-01-29 11:47:54,239 - Epoch: [233][  200/  391]    Overall Loss 0.804616    Objective Loss 0.804616                                        LR 0.001298    Time 0.075424    
2022-01-29 11:48:01,671 - Epoch: [233][  300/  391]    Overall Loss 0.798261    Objective Loss 0.798261                                        LR 0.001298    Time 0.075050    
2022-01-29 11:48:08,430 - Epoch: [233][  391/  391]    Overall Loss 0.802254    Objective Loss 0.802254    Top1 74.519231    Top5 96.153846    LR 0.001298    Time 0.074869    
2022-01-29 11:48:08,481 - --- validate (epoch=233)-----------
2022-01-29 11:48:08,481 - 10000 samples (128 per mini-batch)
2022-01-29 11:48:11,818 - Epoch: [233][   79/   79]    Loss 1.415546    Top1 61.250000    Top5 87.180000    
2022-01-29 11:48:11,878 - ==> Top1: 61.250    Top5: 87.180    Loss: 1.416

2022-01-29 11:48:11,883 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:48:11,883 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:48:11,920 - 

2022-01-29 11:48:11,920 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:48:19,573 - Epoch: [234][  100/  391]    Overall Loss 0.752968    Objective Loss 0.752968                                        LR 0.001298    Time 0.076502    
2022-01-29 11:48:26,994 - Epoch: [234][  200/  391]    Overall Loss 0.783861    Objective Loss 0.783861                                        LR 0.001298    Time 0.075355    
2022-01-29 11:48:34,404 - Epoch: [234][  300/  391]    Overall Loss 0.790510    Objective Loss 0.790510                                        LR 0.001298    Time 0.074933    
2022-01-29 11:48:41,152 - Epoch: [234][  391/  391]    Overall Loss 0.803766    Objective Loss 0.803766    Top1 75.480769    Top5 94.711538    LR 0.001298    Time 0.074749    
2022-01-29 11:48:41,207 - --- validate (epoch=234)-----------
2022-01-29 11:48:41,207 - 10000 samples (128 per mini-batch)
2022-01-29 11:48:44,501 - Epoch: [234][   79/   79]    Loss 1.462112    Top1 60.010000    Top5 86.410000    
2022-01-29 11:48:44,560 - ==> Top1: 60.010    Top5: 86.410    Loss: 1.462

2022-01-29 11:48:44,565 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:48:44,566 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:48:44,602 - 

2022-01-29 11:48:44,602 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:48:52,288 - Epoch: [235][  100/  391]    Overall Loss 0.783825    Objective Loss 0.783825                                        LR 0.001298    Time 0.076828    
2022-01-29 11:48:59,664 - Epoch: [235][  200/  391]    Overall Loss 0.784865    Objective Loss 0.784865                                        LR 0.001298    Time 0.075292    
2022-01-29 11:49:07,054 - Epoch: [235][  300/  391]    Overall Loss 0.791754    Objective Loss 0.791754                                        LR 0.001298    Time 0.074825    
2022-01-29 11:49:13,780 - Epoch: [235][  391/  391]    Overall Loss 0.804999    Objective Loss 0.804999    Top1 76.923077    Top5 95.673077    LR 0.001298    Time 0.074609    
2022-01-29 11:49:13,841 - --- validate (epoch=235)-----------
2022-01-29 11:49:13,841 - 10000 samples (128 per mini-batch)
2022-01-29 11:49:17,055 - Epoch: [235][   79/   79]    Loss 1.459285    Top1 60.260000    Top5 86.290000    
2022-01-29 11:49:17,119 - ==> Top1: 60.260    Top5: 86.290    Loss: 1.459

2022-01-29 11:49:17,124 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:49:17,124 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:49:17,156 - 

2022-01-29 11:49:17,156 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:49:24,742 - Epoch: [236][  100/  391]    Overall Loss 0.768993    Objective Loss 0.768993                                        LR 0.001298    Time 0.075834    
2022-01-29 11:49:32,129 - Epoch: [236][  200/  391]    Overall Loss 0.762523    Objective Loss 0.762523                                        LR 0.001298    Time 0.074845    
2022-01-29 11:49:39,526 - Epoch: [236][  300/  391]    Overall Loss 0.777618    Objective Loss 0.777618                                        LR 0.001298    Time 0.074550    
2022-01-29 11:49:46,252 - Epoch: [236][  391/  391]    Overall Loss 0.786224    Objective Loss 0.786224    Top1 81.730769    Top5 96.153846    LR 0.001298    Time 0.074400    
2022-01-29 11:49:46,318 - --- validate (epoch=236)-----------
2022-01-29 11:49:46,319 - 10000 samples (128 per mini-batch)
2022-01-29 11:49:49,652 - Epoch: [236][   79/   79]    Loss 1.481129    Top1 59.970000    Top5 85.930000    
2022-01-29 11:49:49,706 - ==> Top1: 59.970    Top5: 85.930    Loss: 1.481

2022-01-29 11:49:49,711 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:49:49,711 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:49:49,747 - 

2022-01-29 11:49:49,747 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:49:57,280 - Epoch: [237][  100/  391]    Overall Loss 0.761389    Objective Loss 0.761389                                        LR 0.001298    Time 0.075300    
2022-01-29 11:50:04,488 - Epoch: [237][  200/  391]    Overall Loss 0.763424    Objective Loss 0.763424                                        LR 0.001298    Time 0.073683    
2022-01-29 11:50:11,680 - Epoch: [237][  300/  391]    Overall Loss 0.785957    Objective Loss 0.785957                                        LR 0.001298    Time 0.073094    
2022-01-29 11:50:18,220 - Epoch: [237][  391/  391]    Overall Loss 0.790753    Objective Loss 0.790753    Top1 76.923077    Top5 94.711538    LR 0.001298    Time 0.072807    
2022-01-29 11:50:18,280 - --- validate (epoch=237)-----------
2022-01-29 11:50:18,280 - 10000 samples (128 per mini-batch)
2022-01-29 11:50:21,452 - Epoch: [237][   79/   79]    Loss 1.461803    Top1 60.320000    Top5 86.700000    
2022-01-29 11:50:21,506 - ==> Top1: 60.320    Top5: 86.700    Loss: 1.462

2022-01-29 11:50:21,511 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:50:21,511 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:50:21,547 - 

2022-01-29 11:50:21,547 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:50:29,034 - Epoch: [238][  100/  391]    Overall Loss 0.752666    Objective Loss 0.752666                                        LR 0.001298    Time 0.074849    
2022-01-29 11:50:36,238 - Epoch: [238][  200/  391]    Overall Loss 0.772933    Objective Loss 0.772933                                        LR 0.001298    Time 0.073436    
2022-01-29 11:50:43,436 - Epoch: [238][  300/  391]    Overall Loss 0.782347    Objective Loss 0.782347                                        LR 0.001298    Time 0.072950    
2022-01-29 11:50:49,983 - Epoch: [238][  391/  391]    Overall Loss 0.778523    Objective Loss 0.778523    Top1 77.403846    Top5 97.115385    LR 0.001298    Time 0.072715    
2022-01-29 11:50:50,043 - --- validate (epoch=238)-----------
2022-01-29 11:50:50,044 - 10000 samples (128 per mini-batch)
2022-01-29 11:50:53,371 - Epoch: [238][   79/   79]    Loss 1.448132    Top1 60.700000    Top5 86.440000    
2022-01-29 11:50:53,427 - ==> Top1: 60.700    Top5: 86.440    Loss: 1.448

2022-01-29 11:50:53,433 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:50:53,433 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:50:53,468 - 

2022-01-29 11:50:53,468 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:51:01,025 - Epoch: [239][  100/  391]    Overall Loss 0.766561    Objective Loss 0.766561                                        LR 0.001298    Time 0.075544    
2022-01-29 11:51:08,316 - Epoch: [239][  200/  391]    Overall Loss 0.781696    Objective Loss 0.781696                                        LR 0.001298    Time 0.074221    
2022-01-29 11:51:15,510 - Epoch: [239][  300/  391]    Overall Loss 0.790651    Objective Loss 0.790651                                        LR 0.001298    Time 0.073460    
2022-01-29 11:51:22,063 - Epoch: [239][  391/  391]    Overall Loss 0.802527    Objective Loss 0.802527    Top1 70.192308    Top5 95.192308    LR 0.001298    Time 0.073121    
2022-01-29 11:51:22,122 - --- validate (epoch=239)-----------
2022-01-29 11:51:22,122 - 10000 samples (128 per mini-batch)
2022-01-29 11:51:25,319 - Epoch: [239][   79/   79]    Loss 1.448298    Top1 60.570000    Top5 86.640000    
2022-01-29 11:51:25,378 - ==> Top1: 60.570    Top5: 86.640    Loss: 1.448

2022-01-29 11:51:25,383 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:51:25,383 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:51:25,418 - 

2022-01-29 11:51:25,418 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:51:32,874 - Epoch: [240][  100/  391]    Overall Loss 0.735139    Objective Loss 0.735139                                        LR 0.001298    Time 0.074525    
2022-01-29 11:51:40,075 - Epoch: [240][  200/  391]    Overall Loss 0.752976    Objective Loss 0.752976                                        LR 0.001298    Time 0.073266    
2022-01-29 11:51:47,221 - Epoch: [240][  300/  391]    Overall Loss 0.761488    Objective Loss 0.761488                                        LR 0.001298    Time 0.072661    
2022-01-29 11:51:53,722 - Epoch: [240][  391/  391]    Overall Loss 0.766578    Objective Loss 0.766578    Top1 78.365385    Top5 98.076923    LR 0.001298    Time 0.072373    
2022-01-29 11:51:53,785 - --- validate (epoch=240)-----------
2022-01-29 11:51:53,785 - 10000 samples (128 per mini-batch)
2022-01-29 11:51:57,028 - Epoch: [240][   79/   79]    Loss 1.468121    Top1 59.960000    Top5 86.360000    
2022-01-29 11:51:57,079 - ==> Top1: 59.960    Top5: 86.360    Loss: 1.468

2022-01-29 11:51:57,084 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:51:57,084 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:51:57,120 - 

2022-01-29 11:51:57,121 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:52:04,633 - Epoch: [241][  100/  391]    Overall Loss 0.752515    Objective Loss 0.752515                                        LR 0.001298    Time 0.075099    
2022-01-29 11:52:11,930 - Epoch: [241][  200/  391]    Overall Loss 0.774862    Objective Loss 0.774862                                        LR 0.001298    Time 0.074029    
2022-01-29 11:52:19,284 - Epoch: [241][  300/  391]    Overall Loss 0.782886    Objective Loss 0.782886                                        LR 0.001298    Time 0.073863    
2022-01-29 11:52:25,949 - Epoch: [241][  391/  391]    Overall Loss 0.788331    Objective Loss 0.788331    Top1 75.480769    Top5 94.711538    LR 0.001298    Time 0.073716    
2022-01-29 11:52:26,008 - --- validate (epoch=241)-----------
2022-01-29 11:52:26,008 - 10000 samples (128 per mini-batch)
2022-01-29 11:52:29,321 - Epoch: [241][   79/   79]    Loss 1.405880    Top1 60.900000    Top5 87.180000    
2022-01-29 11:52:29,378 - ==> Top1: 60.900    Top5: 87.180    Loss: 1.406

2022-01-29 11:52:29,383 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:52:29,383 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:52:29,415 - 

2022-01-29 11:52:29,415 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:52:36,957 - Epoch: [242][  100/  391]    Overall Loss 0.739478    Objective Loss 0.739478                                        LR 0.001298    Time 0.075384    
2022-01-29 11:52:44,276 - Epoch: [242][  200/  391]    Overall Loss 0.761952    Objective Loss 0.761952                                        LR 0.001298    Time 0.074284    
2022-01-29 11:52:51,599 - Epoch: [242][  300/  391]    Overall Loss 0.772722    Objective Loss 0.772722                                        LR 0.001298    Time 0.073930    
2022-01-29 11:52:58,266 - Epoch: [242][  391/  391]    Overall Loss 0.772491    Objective Loss 0.772491    Top1 77.884615    Top5 97.596154    LR 0.001298    Time 0.073774    
2022-01-29 11:52:58,323 - --- validate (epoch=242)-----------
2022-01-29 11:52:58,323 - 10000 samples (128 per mini-batch)
2022-01-29 11:53:01,720 - Epoch: [242][   79/   79]    Loss 1.419997    Top1 60.420000    Top5 87.110000    
2022-01-29 11:53:01,776 - ==> Top1: 60.420    Top5: 87.110    Loss: 1.420

2022-01-29 11:53:01,781 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:53:01,781 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:53:01,817 - 

2022-01-29 11:53:01,818 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:53:09,354 - Epoch: [243][  100/  391]    Overall Loss 0.768948    Objective Loss 0.768948                                        LR 0.001298    Time 0.075341    
2022-01-29 11:53:16,649 - Epoch: [243][  200/  391]    Overall Loss 0.783187    Objective Loss 0.783187                                        LR 0.001298    Time 0.074139    
2022-01-29 11:53:23,847 - Epoch: [243][  300/  391]    Overall Loss 0.786271    Objective Loss 0.786271                                        LR 0.001298    Time 0.073417    
2022-01-29 11:53:30,402 - Epoch: [243][  391/  391]    Overall Loss 0.787399    Objective Loss 0.787399    Top1 75.000000    Top5 93.750000    LR 0.001298    Time 0.073092    
2022-01-29 11:53:30,461 - --- validate (epoch=243)-----------
2022-01-29 11:53:30,461 - 10000 samples (128 per mini-batch)
2022-01-29 11:53:33,712 - Epoch: [243][   79/   79]    Loss 1.423494    Top1 61.340000    Top5 87.080000    
2022-01-29 11:53:33,770 - ==> Top1: 61.340    Top5: 87.080    Loss: 1.423

2022-01-29 11:53:33,775 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:53:33,776 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:53:33,812 - 

2022-01-29 11:53:33,812 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:53:41,487 - Epoch: [244][  100/  391]    Overall Loss 0.764741    Objective Loss 0.764741                                        LR 0.001298    Time 0.076725    
2022-01-29 11:53:48,870 - Epoch: [244][  200/  391]    Overall Loss 0.772195    Objective Loss 0.772195                                        LR 0.001298    Time 0.075272    
2022-01-29 11:53:56,233 - Epoch: [244][  300/  391]    Overall Loss 0.769825    Objective Loss 0.769825                                        LR 0.001298    Time 0.074721    
2022-01-29 11:54:02,851 - Epoch: [244][  391/  391]    Overall Loss 0.780227    Objective Loss 0.780227    Top1 78.846154    Top5 97.596154    LR 0.001298    Time 0.074255    
2022-01-29 11:54:02,911 - --- validate (epoch=244)-----------
2022-01-29 11:54:02,911 - 10000 samples (128 per mini-batch)
2022-01-29 11:54:06,163 - Epoch: [244][   79/   79]    Loss 1.528310    Top1 58.290000    Top5 85.290000    
2022-01-29 11:54:06,221 - ==> Top1: 58.290    Top5: 85.290    Loss: 1.528

2022-01-29 11:54:06,226 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:54:06,226 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:54:06,262 - 

2022-01-29 11:54:06,262 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:54:13,775 - Epoch: [245][  100/  391]    Overall Loss 0.759842    Objective Loss 0.759842                                        LR 0.001298    Time 0.075105    
2022-01-29 11:54:20,973 - Epoch: [245][  200/  391]    Overall Loss 0.769397    Objective Loss 0.769397                                        LR 0.001298    Time 0.073540    
2022-01-29 11:54:28,178 - Epoch: [245][  300/  391]    Overall Loss 0.768603    Objective Loss 0.768603                                        LR 0.001298    Time 0.073039    
2022-01-29 11:54:34,732 - Epoch: [245][  391/  391]    Overall Loss 0.775896    Objective Loss 0.775896    Top1 76.442308    Top5 98.076923    LR 0.001298    Time 0.072800    
2022-01-29 11:54:34,792 - --- validate (epoch=245)-----------
2022-01-29 11:54:34,792 - 10000 samples (128 per mini-batch)
2022-01-29 11:54:38,042 - Epoch: [245][   79/   79]    Loss 1.468771    Top1 60.360000    Top5 86.170000    
2022-01-29 11:54:38,096 - ==> Top1: 60.360    Top5: 86.170    Loss: 1.469

2022-01-29 11:54:38,101 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:54:38,101 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:54:38,138 - 

2022-01-29 11:54:38,138 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:54:45,687 - Epoch: [246][  100/  391]    Overall Loss 0.768622    Objective Loss 0.768622                                        LR 0.001298    Time 0.075459    
2022-01-29 11:54:53,022 - Epoch: [246][  200/  391]    Overall Loss 0.770091    Objective Loss 0.770091                                        LR 0.001298    Time 0.074400    
2022-01-29 11:55:00,351 - Epoch: [246][  300/  391]    Overall Loss 0.767390    Objective Loss 0.767390                                        LR 0.001298    Time 0.074028    
2022-01-29 11:55:07,021 - Epoch: [246][  391/  391]    Overall Loss 0.765678    Objective Loss 0.765678    Top1 75.961538    Top5 94.230769    LR 0.001298    Time 0.073856    
2022-01-29 11:55:07,078 - --- validate (epoch=246)-----------
2022-01-29 11:55:07,078 - 10000 samples (128 per mini-batch)
2022-01-29 11:55:10,281 - Epoch: [246][   79/   79]    Loss 1.415248    Top1 61.570000    Top5 87.300000    
2022-01-29 11:55:10,339 - ==> Top1: 61.570    Top5: 87.300    Loss: 1.415

2022-01-29 11:55:10,344 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:55:10,344 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:55:10,375 - 

2022-01-29 11:55:10,375 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:55:17,876 - Epoch: [247][  100/  391]    Overall Loss 0.741936    Objective Loss 0.741936                                        LR 0.001298    Time 0.074983    
2022-01-29 11:55:24,965 - Epoch: [247][  200/  391]    Overall Loss 0.750655    Objective Loss 0.750655                                        LR 0.001298    Time 0.072931    
2022-01-29 11:55:32,244 - Epoch: [247][  300/  391]    Overall Loss 0.764201    Objective Loss 0.764201                                        LR 0.001298    Time 0.072880    
2022-01-29 11:55:38,941 - Epoch: [247][  391/  391]    Overall Loss 0.776498    Objective Loss 0.776498    Top1 80.769231    Top5 99.038462    LR 0.001298    Time 0.073044    
2022-01-29 11:55:39,002 - --- validate (epoch=247)-----------
2022-01-29 11:55:39,002 - 10000 samples (128 per mini-batch)
2022-01-29 11:55:42,270 - Epoch: [247][   79/   79]    Loss 1.429800    Top1 60.600000    Top5 86.620000    
2022-01-29 11:55:42,327 - ==> Top1: 60.600    Top5: 86.620    Loss: 1.430

2022-01-29 11:55:42,332 - ==> Best [Top1: 62.050   Top5: 87.380   Sparsity:0.00   Params: 627712 on epoch: 226]
2022-01-29 11:55:42,332 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:55:42,369 - 

2022-01-29 11:55:42,369 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:55:49,972 - Epoch: [248][  100/  391]    Overall Loss 0.735182    Objective Loss 0.735182                                        LR 0.001298    Time 0.076003    
2022-01-29 11:55:57,239 - Epoch: [248][  200/  391]    Overall Loss 0.754901    Objective Loss 0.754901                                        LR 0.001298    Time 0.074332    
2022-01-29 11:56:04,426 - Epoch: [248][  300/  391]    Overall Loss 0.757990    Objective Loss 0.757990                                        LR 0.001298    Time 0.073509    
2022-01-29 11:56:10,966 - Epoch: [248][  391/  391]    Overall Loss 0.765617    Objective Loss 0.765617    Top1 75.000000    Top5 97.115385    LR 0.001298    Time 0.073124    
2022-01-29 11:56:11,025 - --- validate (epoch=248)-----------
2022-01-29 11:56:11,026 - 10000 samples (128 per mini-batch)
2022-01-29 11:56:14,254 - Epoch: [248][   79/   79]    Loss 1.379156    Top1 62.550000    Top5 87.350000    
2022-01-29 11:56:14,308 - ==> Top1: 62.550    Top5: 87.350    Loss: 1.379

2022-01-29 11:56:14,313 - ==> Best [Top1: 62.550   Top5: 87.350   Sparsity:0.00   Params: 627712 on epoch: 248]
2022-01-29 11:56:14,313 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:56:14,353 - 

2022-01-29 11:56:14,353 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:56:21,815 - Epoch: [249][  100/  391]    Overall Loss 0.749933    Objective Loss 0.749933                                        LR 0.001298    Time 0.074600    
2022-01-29 11:56:29,120 - Epoch: [249][  200/  391]    Overall Loss 0.752596    Objective Loss 0.752596                                        LR 0.001298    Time 0.073817    
2022-01-29 11:56:36,448 - Epoch: [249][  300/  391]    Overall Loss 0.757293    Objective Loss 0.757293                                        LR 0.001298    Time 0.073636    
2022-01-29 11:56:43,101 - Epoch: [249][  391/  391]    Overall Loss 0.764875    Objective Loss 0.764875    Top1 80.288462    Top5 96.153846    LR 0.001298    Time 0.073512    
2022-01-29 11:56:43,160 - --- validate (epoch=249)-----------
2022-01-29 11:56:43,161 - 10000 samples (128 per mini-batch)
2022-01-29 11:56:46,482 - Epoch: [249][   79/   79]    Loss 1.397915    Top1 61.570000    Top5 87.400000    
2022-01-29 11:56:46,542 - ==> Top1: 61.570    Top5: 87.400    Loss: 1.398

2022-01-29 11:56:46,547 - ==> Best [Top1: 62.550   Top5: 87.350   Sparsity:0.00   Params: 627712 on epoch: 248]
2022-01-29 11:56:46,547 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:56:46,580 - 

2022-01-29 11:56:46,580 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:56:54,241 - Epoch: [250][  100/  391]    Overall Loss 0.606509    Objective Loss 0.606509                                        LR 0.000305    Time 0.076580    
2022-01-29 11:57:01,561 - Epoch: [250][  200/  391]    Overall Loss 0.599899    Objective Loss 0.599899                                        LR 0.000305    Time 0.074883    
2022-01-29 11:57:08,745 - Epoch: [250][  300/  391]    Overall Loss 0.595100    Objective Loss 0.595100                                        LR 0.000305    Time 0.073868    
2022-01-29 11:57:15,277 - Epoch: [250][  391/  391]    Overall Loss 0.589116    Objective Loss 0.589116    Top1 80.288462    Top5 99.038462    LR 0.000305    Time 0.073379    
2022-01-29 11:57:15,337 - --- validate (epoch=250)-----------
2022-01-29 11:57:15,337 - 10000 samples (128 per mini-batch)
2022-01-29 11:57:18,634 - Epoch: [250][   79/   79]    Loss 1.321749    Top1 63.730000    Top5 88.110000    
2022-01-29 11:57:18,692 - ==> Top1: 63.730    Top5: 88.110    Loss: 1.322

2022-01-29 11:57:18,698 - ==> Best [Top1: 63.730   Top5: 88.110   Sparsity:0.00   Params: 627712 on epoch: 250]
2022-01-29 11:57:18,698 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:57:18,738 - 

2022-01-29 11:57:18,738 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:57:26,368 - Epoch: [251][  100/  391]    Overall Loss 0.558907    Objective Loss 0.558907                                        LR 0.000305    Time 0.076273    
2022-01-29 11:57:33,693 - Epoch: [251][  200/  391]    Overall Loss 0.554089    Objective Loss 0.554089                                        LR 0.000305    Time 0.074757    
2022-01-29 11:57:40,898 - Epoch: [251][  300/  391]    Overall Loss 0.558262    Objective Loss 0.558262                                        LR 0.000305    Time 0.073851    
2022-01-29 11:57:47,419 - Epoch: [251][  391/  391]    Overall Loss 0.559284    Objective Loss 0.559284    Top1 87.500000    Top5 99.519231    LR 0.000305    Time 0.073337    
2022-01-29 11:57:47,478 - --- validate (epoch=251)-----------
2022-01-29 11:57:47,478 - 10000 samples (128 per mini-batch)
2022-01-29 11:57:50,698 - Epoch: [251][   79/   79]    Loss 1.310526    Top1 63.480000    Top5 88.360000    
2022-01-29 11:57:50,754 - ==> Top1: 63.480    Top5: 88.360    Loss: 1.311

2022-01-29 11:57:50,760 - ==> Best [Top1: 63.730   Top5: 88.110   Sparsity:0.00   Params: 627712 on epoch: 250]
2022-01-29 11:57:50,760 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:57:50,795 - 

2022-01-29 11:57:50,795 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:57:58,357 - Epoch: [252][  100/  391]    Overall Loss 0.538919    Objective Loss 0.538919                                        LR 0.000305    Time 0.075585    
2022-01-29 11:58:05,695 - Epoch: [252][  200/  391]    Overall Loss 0.542027    Objective Loss 0.542027                                        LR 0.000305    Time 0.074479    
2022-01-29 11:58:13,036 - Epoch: [252][  300/  391]    Overall Loss 0.547228    Objective Loss 0.547228                                        LR 0.000305    Time 0.074121    
2022-01-29 11:58:19,550 - Epoch: [252][  391/  391]    Overall Loss 0.548145    Objective Loss 0.548145    Top1 83.173077    Top5 99.038462    LR 0.000305    Time 0.073527    
2022-01-29 11:58:19,611 - --- validate (epoch=252)-----------
2022-01-29 11:58:19,611 - 10000 samples (128 per mini-batch)
2022-01-29 11:58:22,786 - Epoch: [252][   79/   79]    Loss 1.299781    Top1 63.560000    Top5 88.930000    
2022-01-29 11:58:22,846 - ==> Top1: 63.560    Top5: 88.930    Loss: 1.300

2022-01-29 11:58:22,851 - ==> Best [Top1: 63.730   Top5: 88.110   Sparsity:0.00   Params: 627712 on epoch: 250]
2022-01-29 11:58:22,851 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:58:22,879 - 

2022-01-29 11:58:22,879 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:58:30,324 - Epoch: [253][  100/  391]    Overall Loss 0.532558    Objective Loss 0.532558                                        LR 0.000305    Time 0.074431    
2022-01-29 11:58:37,629 - Epoch: [253][  200/  391]    Overall Loss 0.537560    Objective Loss 0.537560                                        LR 0.000305    Time 0.073735    
2022-01-29 11:58:45,002 - Epoch: [253][  300/  391]    Overall Loss 0.539828    Objective Loss 0.539828                                        LR 0.000305    Time 0.073727    
2022-01-29 11:58:51,705 - Epoch: [253][  391/  391]    Overall Loss 0.539963    Objective Loss 0.539963    Top1 82.211538    Top5 96.634615    LR 0.000305    Time 0.073710    
2022-01-29 11:58:51,762 - --- validate (epoch=253)-----------
2022-01-29 11:58:51,763 - 10000 samples (128 per mini-batch)
2022-01-29 11:58:55,008 - Epoch: [253][   79/   79]    Loss 1.295557    Top1 64.710000    Top5 88.740000    
2022-01-29 11:58:55,059 - ==> Top1: 64.710    Top5: 88.740    Loss: 1.296

2022-01-29 11:58:55,064 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 11:58:55,064 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:58:55,103 - 

2022-01-29 11:58:55,103 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:59:02,765 - Epoch: [254][  100/  391]    Overall Loss 0.535947    Objective Loss 0.535947                                        LR 0.000305    Time 0.076596    
2022-01-29 11:59:10,112 - Epoch: [254][  200/  391]    Overall Loss 0.538125    Objective Loss 0.538125                                        LR 0.000305    Time 0.075030    
2022-01-29 11:59:17,401 - Epoch: [254][  300/  391]    Overall Loss 0.536820    Objective Loss 0.536820                                        LR 0.000305    Time 0.074314    
2022-01-29 11:59:24,039 - Epoch: [254][  391/  391]    Overall Loss 0.536257    Objective Loss 0.536257    Top1 84.134615    Top5 99.038462    LR 0.000305    Time 0.073992    
2022-01-29 11:59:24,095 - --- validate (epoch=254)-----------
2022-01-29 11:59:24,095 - 10000 samples (128 per mini-batch)
2022-01-29 11:59:27,414 - Epoch: [254][   79/   79]    Loss 1.301478    Top1 64.120000    Top5 88.370000    
2022-01-29 11:59:27,480 - ==> Top1: 64.120    Top5: 88.370    Loss: 1.301

2022-01-29 11:59:27,485 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 11:59:27,485 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:59:27,517 - 

2022-01-29 11:59:27,517 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 11:59:35,000 - Epoch: [255][  100/  391]    Overall Loss 0.535659    Objective Loss 0.535659                                        LR 0.000305    Time 0.074801    
2022-01-29 11:59:42,199 - Epoch: [255][  200/  391]    Overall Loss 0.539014    Objective Loss 0.539014                                        LR 0.000305    Time 0.073396    
2022-01-29 11:59:49,406 - Epoch: [255][  300/  391]    Overall Loss 0.535382    Objective Loss 0.535382                                        LR 0.000305    Time 0.072950    
2022-01-29 11:59:56,070 - Epoch: [255][  391/  391]    Overall Loss 0.534616    Objective Loss 0.534616    Top1 85.096154    Top5 99.038462    LR 0.000305    Time 0.073013    
2022-01-29 11:59:56,129 - --- validate (epoch=255)-----------
2022-01-29 11:59:56,129 - 10000 samples (128 per mini-batch)
2022-01-29 11:59:59,545 - Epoch: [255][   79/   79]    Loss 1.317530    Top1 63.740000    Top5 88.360000    
2022-01-29 11:59:59,602 - ==> Top1: 63.740    Top5: 88.360    Loss: 1.318

2022-01-29 11:59:59,607 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 11:59:59,607 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 11:59:59,644 - 

2022-01-29 11:59:59,644 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:00:07,287 - Epoch: [256][  100/  391]    Overall Loss 0.521378    Objective Loss 0.521378                                        LR 0.000305    Time 0.076402    
2022-01-29 12:00:14,616 - Epoch: [256][  200/  391]    Overall Loss 0.526077    Objective Loss 0.526077                                        LR 0.000305    Time 0.074841    
2022-01-29 12:00:21,941 - Epoch: [256][  300/  391]    Overall Loss 0.524887    Objective Loss 0.524887                                        LR 0.000305    Time 0.074309    
2022-01-29 12:00:28,606 - Epoch: [256][  391/  391]    Overall Loss 0.524898    Objective Loss 0.524898    Top1 90.865385    Top5 99.519231    LR 0.000305    Time 0.074058    
2022-01-29 12:00:28,671 - --- validate (epoch=256)-----------
2022-01-29 12:00:28,671 - 10000 samples (128 per mini-batch)
2022-01-29 12:00:31,970 - Epoch: [256][   79/   79]    Loss 1.306002    Top1 63.530000    Top5 88.350000    
2022-01-29 12:00:32,022 - ==> Top1: 63.530    Top5: 88.350    Loss: 1.306

2022-01-29 12:00:32,027 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:00:32,027 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:00:32,064 - 

2022-01-29 12:00:32,064 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:00:39,683 - Epoch: [257][  100/  391]    Overall Loss 0.519240    Objective Loss 0.519240                                        LR 0.000305    Time 0.076164    
2022-01-29 12:00:46,881 - Epoch: [257][  200/  391]    Overall Loss 0.522058    Objective Loss 0.522058                                        LR 0.000305    Time 0.074069    
2022-01-29 12:00:54,174 - Epoch: [257][  300/  391]    Overall Loss 0.519938    Objective Loss 0.519938                                        LR 0.000305    Time 0.073685    
2022-01-29 12:01:00,835 - Epoch: [257][  391/  391]    Overall Loss 0.522129    Objective Loss 0.522129    Top1 87.500000    Top5 98.557692    LR 0.000305    Time 0.073571    
2022-01-29 12:01:00,896 - --- validate (epoch=257)-----------
2022-01-29 12:01:00,896 - 10000 samples (128 per mini-batch)
2022-01-29 12:01:04,219 - Epoch: [257][   79/   79]    Loss 1.315912    Top1 64.160000    Top5 88.420000    
2022-01-29 12:01:04,278 - ==> Top1: 64.160    Top5: 88.420    Loss: 1.316

2022-01-29 12:01:04,283 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:01:04,283 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:01:04,312 - 

2022-01-29 12:01:04,313 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:01:11,942 - Epoch: [258][  100/  391]    Overall Loss 0.507807    Objective Loss 0.507807                                        LR 0.000305    Time 0.076268    
2022-01-29 12:01:19,295 - Epoch: [258][  200/  391]    Overall Loss 0.518818    Objective Loss 0.518818                                        LR 0.000305    Time 0.074892    
2022-01-29 12:01:26,644 - Epoch: [258][  300/  391]    Overall Loss 0.518043    Objective Loss 0.518043                                        LR 0.000305    Time 0.074423    
2022-01-29 12:01:33,331 - Epoch: [258][  391/  391]    Overall Loss 0.521283    Objective Loss 0.521283    Top1 90.384615    Top5 100.000000    LR 0.000305    Time 0.074204    
2022-01-29 12:01:33,391 - --- validate (epoch=258)-----------
2022-01-29 12:01:33,391 - 10000 samples (128 per mini-batch)
2022-01-29 12:01:36,697 - Epoch: [258][   79/   79]    Loss 1.309646    Top1 64.510000    Top5 88.420000    
2022-01-29 12:01:36,748 - ==> Top1: 64.510    Top5: 88.420    Loss: 1.310

2022-01-29 12:01:36,753 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:01:36,753 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:01:36,789 - 

2022-01-29 12:01:36,789 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:01:44,202 - Epoch: [259][  100/  391]    Overall Loss 0.510704    Objective Loss 0.510704                                        LR 0.000305    Time 0.074103    
2022-01-29 12:01:51,382 - Epoch: [259][  200/  391]    Overall Loss 0.515964    Objective Loss 0.515964                                        LR 0.000305    Time 0.072947    
2022-01-29 12:01:58,560 - Epoch: [259][  300/  391]    Overall Loss 0.512612    Objective Loss 0.512612                                        LR 0.000305    Time 0.072555    
2022-01-29 12:02:05,194 - Epoch: [259][  391/  391]    Overall Loss 0.516657    Objective Loss 0.516657    Top1 88.942308    Top5 99.519231    LR 0.000305    Time 0.072634    
2022-01-29 12:02:05,253 - --- validate (epoch=259)-----------
2022-01-29 12:02:05,254 - 10000 samples (128 per mini-batch)
2022-01-29 12:02:08,559 - Epoch: [259][   79/   79]    Loss 1.312953    Top1 63.740000    Top5 88.230000    
2022-01-29 12:02:08,617 - ==> Top1: 63.740    Top5: 88.230    Loss: 1.313

2022-01-29 12:02:08,623 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:02:08,623 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:02:08,659 - 

2022-01-29 12:02:08,660 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:02:16,329 - Epoch: [260][  100/  391]    Overall Loss 0.501969    Objective Loss 0.501969                                        LR 0.000305    Time 0.076667    
2022-01-29 12:02:23,660 - Epoch: [260][  200/  391]    Overall Loss 0.511261    Objective Loss 0.511261                                        LR 0.000305    Time 0.074987    
2022-01-29 12:02:30,862 - Epoch: [260][  300/  391]    Overall Loss 0.512072    Objective Loss 0.512072                                        LR 0.000305    Time 0.073995    
2022-01-29 12:02:37,409 - Epoch: [260][  391/  391]    Overall Loss 0.513217    Objective Loss 0.513217    Top1 87.980769    Top5 99.519231    LR 0.000305    Time 0.073515    
2022-01-29 12:02:37,466 - --- validate (epoch=260)-----------
2022-01-29 12:02:37,466 - 10000 samples (128 per mini-batch)
2022-01-29 12:02:40,663 - Epoch: [260][   79/   79]    Loss 1.314292    Top1 64.010000    Top5 88.320000    
2022-01-29 12:02:40,721 - ==> Top1: 64.010    Top5: 88.320    Loss: 1.314

2022-01-29 12:02:40,726 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:02:40,726 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:02:40,761 - 

2022-01-29 12:02:40,762 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:02:48,415 - Epoch: [261][  100/  391]    Overall Loss 0.519423    Objective Loss 0.519423                                        LR 0.000305    Time 0.076511    
2022-01-29 12:02:55,693 - Epoch: [261][  200/  391]    Overall Loss 0.510559    Objective Loss 0.510559                                        LR 0.000305    Time 0.074639    
2022-01-29 12:03:02,879 - Epoch: [261][  300/  391]    Overall Loss 0.509103    Objective Loss 0.509103                                        LR 0.000305    Time 0.073709    
2022-01-29 12:03:09,408 - Epoch: [261][  391/  391]    Overall Loss 0.512037    Objective Loss 0.512037    Top1 85.096154    Top5 99.038462    LR 0.000305    Time 0.073249    
2022-01-29 12:03:09,467 - --- validate (epoch=261)-----------
2022-01-29 12:03:09,467 - 10000 samples (128 per mini-batch)
2022-01-29 12:03:12,671 - Epoch: [261][   79/   79]    Loss 1.327238    Top1 63.320000    Top5 88.310000    
2022-01-29 12:03:12,733 - ==> Top1: 63.320    Top5: 88.310    Loss: 1.327

2022-01-29 12:03:12,738 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:03:12,739 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:03:12,770 - 

2022-01-29 12:03:12,771 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:03:20,132 - Epoch: [262][  100/  391]    Overall Loss 0.498165    Objective Loss 0.498165                                        LR 0.000305    Time 0.073581    
2022-01-29 12:03:27,260 - Epoch: [262][  200/  391]    Overall Loss 0.506137    Objective Loss 0.506137                                        LR 0.000305    Time 0.072425    
2022-01-29 12:03:34,387 - Epoch: [262][  300/  391]    Overall Loss 0.507356    Objective Loss 0.507356                                        LR 0.000305    Time 0.072036    
2022-01-29 12:03:40,873 - Epoch: [262][  391/  391]    Overall Loss 0.508517    Objective Loss 0.508517    Top1 84.615385    Top5 97.115385    LR 0.000305    Time 0.071858    
2022-01-29 12:03:40,932 - --- validate (epoch=262)-----------
2022-01-29 12:03:40,932 - 10000 samples (128 per mini-batch)
2022-01-29 12:03:44,196 - Epoch: [262][   79/   79]    Loss 1.320104    Top1 63.830000    Top5 88.140000    
2022-01-29 12:03:44,248 - ==> Top1: 63.830    Top5: 88.140    Loss: 1.320

2022-01-29 12:03:44,253 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:03:44,253 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:03:44,289 - 

2022-01-29 12:03:44,289 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:03:51,665 - Epoch: [263][  100/  391]    Overall Loss 0.499713    Objective Loss 0.499713                                        LR 0.000305    Time 0.073729    
2022-01-29 12:03:58,828 - Epoch: [263][  200/  391]    Overall Loss 0.498128    Objective Loss 0.498128                                        LR 0.000305    Time 0.072673    
2022-01-29 12:04:06,013 - Epoch: [263][  300/  391]    Overall Loss 0.502303    Objective Loss 0.502303                                        LR 0.000305    Time 0.072396    
2022-01-29 12:04:12,552 - Epoch: [263][  391/  391]    Overall Loss 0.501499    Objective Loss 0.501499    Top1 87.500000    Top5 99.519231    LR 0.000305    Time 0.072268    
2022-01-29 12:04:12,610 - --- validate (epoch=263)-----------
2022-01-29 12:04:12,610 - 10000 samples (128 per mini-batch)
2022-01-29 12:04:15,796 - Epoch: [263][   79/   79]    Loss 1.296934    Top1 64.390000    Top5 88.760000    
2022-01-29 12:04:15,853 - ==> Top1: 64.390    Top5: 88.760    Loss: 1.297

2022-01-29 12:04:15,858 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:04:15,858 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:04:15,894 - 

2022-01-29 12:04:15,894 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:04:23,355 - Epoch: [264][  100/  391]    Overall Loss 0.499189    Objective Loss 0.499189                                        LR 0.000305    Time 0.074582    
2022-01-29 12:04:30,520 - Epoch: [264][  200/  391]    Overall Loss 0.497255    Objective Loss 0.497255                                        LR 0.000305    Time 0.073111    
2022-01-29 12:04:37,679 - Epoch: [264][  300/  391]    Overall Loss 0.504931    Objective Loss 0.504931                                        LR 0.000305    Time 0.072601    
2022-01-29 12:04:44,190 - Epoch: [264][  391/  391]    Overall Loss 0.506847    Objective Loss 0.506847    Top1 87.980769    Top5 98.557692    LR 0.000305    Time 0.072354    
2022-01-29 12:04:44,250 - --- validate (epoch=264)-----------
2022-01-29 12:04:44,250 - 10000 samples (128 per mini-batch)
2022-01-29 12:04:47,475 - Epoch: [264][   79/   79]    Loss 1.300263    Top1 64.560000    Top5 88.270000    
2022-01-29 12:04:47,535 - ==> Top1: 64.560    Top5: 88.270    Loss: 1.300

2022-01-29 12:04:47,540 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:04:47,540 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:04:47,575 - 

2022-01-29 12:04:47,575 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:04:55,146 - Epoch: [265][  100/  391]    Overall Loss 0.494089    Objective Loss 0.494089                                        LR 0.000305    Time 0.075679    
2022-01-29 12:05:02,454 - Epoch: [265][  200/  391]    Overall Loss 0.493498    Objective Loss 0.493498                                        LR 0.000305    Time 0.074377    
2022-01-29 12:05:09,744 - Epoch: [265][  300/  391]    Overall Loss 0.499987    Objective Loss 0.499987                                        LR 0.000305    Time 0.073880    
2022-01-29 12:05:16,381 - Epoch: [265][  391/  391]    Overall Loss 0.498669    Objective Loss 0.498669    Top1 84.134615    Top5 99.038462    LR 0.000305    Time 0.073660    
2022-01-29 12:05:16,440 - --- validate (epoch=265)-----------
2022-01-29 12:05:16,441 - 10000 samples (128 per mini-batch)
2022-01-29 12:05:19,635 - Epoch: [265][   79/   79]    Loss 1.320000    Top1 64.080000    Top5 88.250000    
2022-01-29 12:05:19,694 - ==> Top1: 64.080    Top5: 88.250    Loss: 1.320

2022-01-29 12:05:19,699 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:05:19,699 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:05:19,734 - 

2022-01-29 12:05:19,734 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:05:27,309 - Epoch: [266][  100/  391]    Overall Loss 0.497213    Objective Loss 0.497213                                        LR 0.000305    Time 0.075720    
2022-01-29 12:05:34,655 - Epoch: [266][  200/  391]    Overall Loss 0.492642    Objective Loss 0.492642                                        LR 0.000305    Time 0.074586    
2022-01-29 12:05:41,991 - Epoch: [266][  300/  391]    Overall Loss 0.494714    Objective Loss 0.494714                                        LR 0.000305    Time 0.074175    
2022-01-29 12:05:48,637 - Epoch: [266][  391/  391]    Overall Loss 0.497795    Objective Loss 0.497795    Top1 85.096154    Top5 99.038462    LR 0.000305    Time 0.073907    
2022-01-29 12:05:48,695 - --- validate (epoch=266)-----------
2022-01-29 12:05:48,695 - 10000 samples (128 per mini-batch)
2022-01-29 12:05:51,887 - Epoch: [266][   79/   79]    Loss 1.318574    Top1 64.000000    Top5 88.550000    
2022-01-29 12:05:51,941 - ==> Top1: 64.000    Top5: 88.550    Loss: 1.319

2022-01-29 12:05:51,945 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:05:51,945 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:05:51,981 - 

2022-01-29 12:05:51,981 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:05:59,606 - Epoch: [267][  100/  391]    Overall Loss 0.492546    Objective Loss 0.492546                                        LR 0.000305    Time 0.076226    
2022-01-29 12:06:06,998 - Epoch: [267][  200/  391]    Overall Loss 0.498179    Objective Loss 0.498179                                        LR 0.000305    Time 0.075064    
2022-01-29 12:06:14,298 - Epoch: [267][  300/  391]    Overall Loss 0.495242    Objective Loss 0.495242                                        LR 0.000305    Time 0.074376    
2022-01-29 12:06:20,951 - Epoch: [267][  391/  391]    Overall Loss 0.496757    Objective Loss 0.496757    Top1 86.057692    Top5 99.519231    LR 0.000305    Time 0.074078    
2022-01-29 12:06:21,011 - --- validate (epoch=267)-----------
2022-01-29 12:06:21,011 - 10000 samples (128 per mini-batch)
2022-01-29 12:06:24,286 - Epoch: [267][   79/   79]    Loss 1.326711    Top1 63.910000    Top5 88.280000    
2022-01-29 12:06:24,340 - ==> Top1: 63.910    Top5: 88.280    Loss: 1.327

2022-01-29 12:06:24,345 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:06:24,345 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:06:24,382 - 

2022-01-29 12:06:24,382 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:06:32,079 - Epoch: [268][  100/  391]    Overall Loss 0.480873    Objective Loss 0.480873                                        LR 0.000305    Time 0.076940    
2022-01-29 12:06:39,468 - Epoch: [268][  200/  391]    Overall Loss 0.491605    Objective Loss 0.491605                                        LR 0.000305    Time 0.075413    
2022-01-29 12:06:46,808 - Epoch: [268][  300/  391]    Overall Loss 0.492287    Objective Loss 0.492287                                        LR 0.000305    Time 0.074740    
2022-01-29 12:06:53,475 - Epoch: [268][  391/  391]    Overall Loss 0.491060    Objective Loss 0.491060    Top1 83.653846    Top5 99.038462    LR 0.000305    Time 0.074393    
2022-01-29 12:06:53,533 - --- validate (epoch=268)-----------
2022-01-29 12:06:53,533 - 10000 samples (128 per mini-batch)
2022-01-29 12:06:56,945 - Epoch: [268][   79/   79]    Loss 1.313362    Top1 64.180000    Top5 88.370000    
2022-01-29 12:06:57,002 - ==> Top1: 64.180    Top5: 88.370    Loss: 1.313

2022-01-29 12:06:57,007 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:06:57,007 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:06:57,044 - 

2022-01-29 12:06:57,044 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:07:04,573 - Epoch: [269][  100/  391]    Overall Loss 0.493351    Objective Loss 0.493351                                        LR 0.000305    Time 0.075269    
2022-01-29 12:07:11,915 - Epoch: [269][  200/  391]    Overall Loss 0.485794    Objective Loss 0.485794                                        LR 0.000305    Time 0.074336    
2022-01-29 12:07:19,341 - Epoch: [269][  300/  391]    Overall Loss 0.489870    Objective Loss 0.489870                                        LR 0.000305    Time 0.074309    
2022-01-29 12:07:26,060 - Epoch: [269][  391/  391]    Overall Loss 0.489566    Objective Loss 0.489566    Top1 89.903846    Top5 99.519231    LR 0.000305    Time 0.074196    
2022-01-29 12:07:26,116 - --- validate (epoch=269)-----------
2022-01-29 12:07:26,116 - 10000 samples (128 per mini-batch)
2022-01-29 12:07:29,397 - Epoch: [269][   79/   79]    Loss 1.325008    Top1 63.750000    Top5 88.040000    
2022-01-29 12:07:29,457 - ==> Top1: 63.750    Top5: 88.040    Loss: 1.325

2022-01-29 12:07:29,462 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:07:29,463 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:07:29,499 - 

2022-01-29 12:07:29,499 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:07:37,110 - Epoch: [270][  100/  391]    Overall Loss 0.481800    Objective Loss 0.481800                                        LR 0.000305    Time 0.076078    
2022-01-29 12:07:44,426 - Epoch: [270][  200/  391]    Overall Loss 0.486639    Objective Loss 0.486639                                        LR 0.000305    Time 0.074614    
2022-01-29 12:07:51,739 - Epoch: [270][  300/  391]    Overall Loss 0.490312    Objective Loss 0.490312                                        LR 0.000305    Time 0.074119    
2022-01-29 12:07:58,346 - Epoch: [270][  391/  391]    Overall Loss 0.489803    Objective Loss 0.489803    Top1 87.980769    Top5 99.519231    LR 0.000305    Time 0.073764    
2022-01-29 12:07:58,406 - --- validate (epoch=270)-----------
2022-01-29 12:07:58,407 - 10000 samples (128 per mini-batch)
2022-01-29 12:08:01,626 - Epoch: [270][   79/   79]    Loss 1.312736    Top1 64.250000    Top5 88.390000    
2022-01-29 12:08:01,682 - ==> Top1: 64.250    Top5: 88.390    Loss: 1.313

2022-01-29 12:08:01,687 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:08:01,687 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:08:01,722 - 

2022-01-29 12:08:01,722 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:08:09,377 - Epoch: [271][  100/  391]    Overall Loss 0.467376    Objective Loss 0.467376                                        LR 0.000305    Time 0.076514    
2022-01-29 12:08:16,672 - Epoch: [271][  200/  391]    Overall Loss 0.481430    Objective Loss 0.481430                                        LR 0.000305    Time 0.074728    
2022-01-29 12:08:23,968 - Epoch: [271][  300/  391]    Overall Loss 0.483290    Objective Loss 0.483290                                        LR 0.000305    Time 0.074139    
2022-01-29 12:08:30,602 - Epoch: [271][  391/  391]    Overall Loss 0.485501    Objective Loss 0.485501    Top1 87.500000    Top5 99.038462    LR 0.000305    Time 0.073849    
2022-01-29 12:08:30,661 - --- validate (epoch=271)-----------
2022-01-29 12:08:30,661 - 10000 samples (128 per mini-batch)
2022-01-29 12:08:33,886 - Epoch: [271][   79/   79]    Loss 1.331487    Top1 64.080000    Top5 88.170000    
2022-01-29 12:08:33,937 - ==> Top1: 64.080    Top5: 88.170    Loss: 1.331

2022-01-29 12:08:33,942 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:08:33,942 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:08:33,977 - 

2022-01-29 12:08:33,977 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:08:41,493 - Epoch: [272][  100/  391]    Overall Loss 0.473460    Objective Loss 0.473460                                        LR 0.000305    Time 0.075134    
2022-01-29 12:08:48,802 - Epoch: [272][  200/  391]    Overall Loss 0.480705    Objective Loss 0.480705                                        LR 0.000305    Time 0.074105    
2022-01-29 12:08:56,264 - Epoch: [272][  300/  391]    Overall Loss 0.483939    Objective Loss 0.483939                                        LR 0.000305    Time 0.074276    
2022-01-29 12:09:03,057 - Epoch: [272][  391/  391]    Overall Loss 0.483972    Objective Loss 0.483972    Top1 87.019231    Top5 99.519231    LR 0.000305    Time 0.074361    
2022-01-29 12:09:03,116 - --- validate (epoch=272)-----------
2022-01-29 12:09:03,117 - 10000 samples (128 per mini-batch)
2022-01-29 12:09:06,393 - Epoch: [272][   79/   79]    Loss 1.302220    Top1 64.420000    Top5 88.460000    
2022-01-29 12:09:06,445 - ==> Top1: 64.420    Top5: 88.460    Loss: 1.302

2022-01-29 12:09:06,451 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:09:06,451 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:09:06,486 - 

2022-01-29 12:09:06,486 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:09:14,087 - Epoch: [273][  100/  391]    Overall Loss 0.474252    Objective Loss 0.474252                                        LR 0.000305    Time 0.075979    
2022-01-29 12:09:21,387 - Epoch: [273][  200/  391]    Overall Loss 0.478398    Objective Loss 0.478398                                        LR 0.000305    Time 0.074490    
2022-01-29 12:09:28,695 - Epoch: [273][  300/  391]    Overall Loss 0.479516    Objective Loss 0.479516                                        LR 0.000305    Time 0.074015    
2022-01-29 12:09:35,335 - Epoch: [273][  391/  391]    Overall Loss 0.480354    Objective Loss 0.480354    Top1 88.461538    Top5 99.038462    LR 0.000305    Time 0.073770    
2022-01-29 12:09:35,396 - --- validate (epoch=273)-----------
2022-01-29 12:09:35,396 - 10000 samples (128 per mini-batch)
2022-01-29 12:09:38,640 - Epoch: [273][   79/   79]    Loss 1.304100    Top1 64.070000    Top5 88.420000    
2022-01-29 12:09:38,698 - ==> Top1: 64.070    Top5: 88.420    Loss: 1.304

2022-01-29 12:09:38,703 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:09:38,703 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:09:38,739 - 

2022-01-29 12:09:38,739 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:09:46,332 - Epoch: [274][  100/  391]    Overall Loss 0.471240    Objective Loss 0.471240                                        LR 0.000305    Time 0.075912    
2022-01-29 12:09:53,633 - Epoch: [274][  200/  391]    Overall Loss 0.478008    Objective Loss 0.478008                                        LR 0.000305    Time 0.074457    
2022-01-29 12:10:00,941 - Epoch: [274][  300/  391]    Overall Loss 0.477110    Objective Loss 0.477110                                        LR 0.000305    Time 0.073995    
2022-01-29 12:10:07,728 - Epoch: [274][  391/  391]    Overall Loss 0.477776    Objective Loss 0.477776    Top1 89.423077    Top5 100.000000    LR 0.000305    Time 0.074129    
2022-01-29 12:10:07,785 - --- validate (epoch=274)-----------
2022-01-29 12:10:07,785 - 10000 samples (128 per mini-batch)
2022-01-29 12:10:11,061 - Epoch: [274][   79/   79]    Loss 1.308802    Top1 64.030000    Top5 88.400000    
2022-01-29 12:10:11,113 - ==> Top1: 64.030    Top5: 88.400    Loss: 1.309

2022-01-29 12:10:11,118 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:10:11,118 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:10:11,146 - 

2022-01-29 12:10:11,146 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:10:18,706 - Epoch: [275][  100/  391]    Overall Loss 0.479953    Objective Loss 0.479953                                        LR 0.000305    Time 0.075571    
2022-01-29 12:10:26,017 - Epoch: [275][  200/  391]    Overall Loss 0.471936    Objective Loss 0.471936                                        LR 0.000305    Time 0.074334    
2022-01-29 12:10:33,323 - Epoch: [275][  300/  391]    Overall Loss 0.477122    Objective Loss 0.477122                                        LR 0.000305    Time 0.073909    
2022-01-29 12:10:40,014 - Epoch: [275][  391/  391]    Overall Loss 0.481173    Objective Loss 0.481173    Top1 85.576923    Top5 99.038462    LR 0.000305    Time 0.073817    
2022-01-29 12:10:40,073 - --- validate (epoch=275)-----------
2022-01-29 12:10:40,073 - 10000 samples (128 per mini-batch)
2022-01-29 12:10:43,426 - Epoch: [275][   79/   79]    Loss 1.315472    Top1 63.790000    Top5 88.540000    
2022-01-29 12:10:43,485 - ==> Top1: 63.790    Top5: 88.540    Loss: 1.315

2022-01-29 12:10:43,491 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:10:43,491 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:10:43,526 - 

2022-01-29 12:10:43,526 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:10:51,058 - Epoch: [276][  100/  391]    Overall Loss 0.485329    Objective Loss 0.485329                                        LR 0.000305    Time 0.075292    
2022-01-29 12:10:58,367 - Epoch: [276][  200/  391]    Overall Loss 0.477325    Objective Loss 0.477325                                        LR 0.000305    Time 0.074188    
2022-01-29 12:11:05,696 - Epoch: [276][  300/  391]    Overall Loss 0.474980    Objective Loss 0.474980                                        LR 0.000305    Time 0.073886    
2022-01-29 12:11:12,409 - Epoch: [276][  391/  391]    Overall Loss 0.477688    Objective Loss 0.477688    Top1 85.096154    Top5 99.519231    LR 0.000305    Time 0.073854    
2022-01-29 12:11:12,462 - --- validate (epoch=276)-----------
2022-01-29 12:11:12,462 - 10000 samples (128 per mini-batch)
2022-01-29 12:11:15,804 - Epoch: [276][   79/   79]    Loss 1.321738    Top1 63.940000    Top5 88.270000    
2022-01-29 12:11:15,856 - ==> Top1: 63.940    Top5: 88.270    Loss: 1.322

2022-01-29 12:11:15,860 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:11:15,860 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:11:15,896 - 

2022-01-29 12:11:15,896 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:11:23,535 - Epoch: [277][  100/  391]    Overall Loss 0.468338    Objective Loss 0.468338                                        LR 0.000305    Time 0.076363    
2022-01-29 12:11:30,951 - Epoch: [277][  200/  391]    Overall Loss 0.472818    Objective Loss 0.472818                                        LR 0.000305    Time 0.075256    
2022-01-29 12:11:38,197 - Epoch: [277][  300/  391]    Overall Loss 0.473839    Objective Loss 0.473839                                        LR 0.000305    Time 0.074323    
2022-01-29 12:11:44,735 - Epoch: [277][  391/  391]    Overall Loss 0.474876    Objective Loss 0.474876    Top1 89.903846    Top5 97.596154    LR 0.000305    Time 0.073743    
2022-01-29 12:11:44,800 - --- validate (epoch=277)-----------
2022-01-29 12:11:44,801 - 10000 samples (128 per mini-batch)
2022-01-29 12:11:48,093 - Epoch: [277][   79/   79]    Loss 1.331083    Top1 63.790000    Top5 87.930000    
2022-01-29 12:11:48,149 - ==> Top1: 63.790    Top5: 87.930    Loss: 1.331

2022-01-29 12:11:48,154 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:11:48,154 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:11:48,190 - 

2022-01-29 12:11:48,191 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:11:55,826 - Epoch: [278][  100/  391]    Overall Loss 0.472081    Objective Loss 0.472081                                        LR 0.000305    Time 0.076332    
2022-01-29 12:12:03,162 - Epoch: [278][  200/  391]    Overall Loss 0.470987    Objective Loss 0.470987                                        LR 0.000305    Time 0.074842    
2022-01-29 12:12:10,471 - Epoch: [278][  300/  391]    Overall Loss 0.473837    Objective Loss 0.473837                                        LR 0.000305    Time 0.074255    
2022-01-29 12:12:17,136 - Epoch: [278][  391/  391]    Overall Loss 0.473122    Objective Loss 0.473122    Top1 86.538462    Top5 96.153846    LR 0.000305    Time 0.074015    
2022-01-29 12:12:17,194 - --- validate (epoch=278)-----------
2022-01-29 12:12:17,194 - 10000 samples (128 per mini-batch)
2022-01-29 12:12:20,445 - Epoch: [278][   79/   79]    Loss 1.310786    Top1 64.480000    Top5 88.290000    
2022-01-29 12:12:20,500 - ==> Top1: 64.480    Top5: 88.290    Loss: 1.311

2022-01-29 12:12:20,505 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:12:20,505 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:12:20,538 - 

2022-01-29 12:12:20,538 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:12:28,151 - Epoch: [279][  100/  391]    Overall Loss 0.474093    Objective Loss 0.474093                                        LR 0.000305    Time 0.076106    
2022-01-29 12:12:35,537 - Epoch: [279][  200/  391]    Overall Loss 0.472602    Objective Loss 0.472602                                        LR 0.000305    Time 0.074974    
2022-01-29 12:12:42,899 - Epoch: [279][  300/  391]    Overall Loss 0.474167    Objective Loss 0.474167                                        LR 0.000305    Time 0.074520    
2022-01-29 12:12:49,589 - Epoch: [279][  391/  391]    Overall Loss 0.475983    Objective Loss 0.475983    Top1 87.980769    Top5 98.076923    LR 0.000305    Time 0.074284    
2022-01-29 12:12:49,647 - --- validate (epoch=279)-----------
2022-01-29 12:12:49,648 - 10000 samples (128 per mini-batch)
2022-01-29 12:12:52,971 - Epoch: [279][   79/   79]    Loss 1.339718    Top1 64.070000    Top5 88.220000    
2022-01-29 12:12:53,025 - ==> Top1: 64.070    Top5: 88.220    Loss: 1.340

2022-01-29 12:12:53,030 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:12:53,030 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:12:53,067 - 

2022-01-29 12:12:53,067 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:13:00,729 - Epoch: [280][  100/  391]    Overall Loss 0.459814    Objective Loss 0.459814                                        LR 0.000305    Time 0.076592    
2022-01-29 12:13:08,084 - Epoch: [280][  200/  391]    Overall Loss 0.462124    Objective Loss 0.462124                                        LR 0.000305    Time 0.075068    
2022-01-29 12:13:15,448 - Epoch: [280][  300/  391]    Overall Loss 0.467486    Objective Loss 0.467486                                        LR 0.000305    Time 0.074588    
2022-01-29 12:13:22,095 - Epoch: [280][  391/  391]    Overall Loss 0.470996    Objective Loss 0.470996    Top1 83.173077    Top5 98.076923    LR 0.000305    Time 0.074227    
2022-01-29 12:13:22,153 - --- validate (epoch=280)-----------
2022-01-29 12:13:22,153 - 10000 samples (128 per mini-batch)
2022-01-29 12:13:25,381 - Epoch: [280][   79/   79]    Loss 1.311811    Top1 64.180000    Top5 88.390000    
2022-01-29 12:13:25,432 - ==> Top1: 64.180    Top5: 88.390    Loss: 1.312

2022-01-29 12:13:25,437 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:13:25,437 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:13:25,472 - 

2022-01-29 12:13:25,472 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:13:32,779 - Epoch: [281][  100/  391]    Overall Loss 0.460206    Objective Loss 0.460206                                        LR 0.000305    Time 0.073040    
2022-01-29 12:13:39,869 - Epoch: [281][  200/  391]    Overall Loss 0.460450    Objective Loss 0.460450                                        LR 0.000305    Time 0.071963    
2022-01-29 12:13:46,967 - Epoch: [281][  300/  391]    Overall Loss 0.465190    Objective Loss 0.465190                                        LR 0.000305    Time 0.071633    
2022-01-29 12:13:53,422 - Epoch: [281][  391/  391]    Overall Loss 0.468142    Objective Loss 0.468142    Top1 86.538462    Top5 99.519231    LR 0.000305    Time 0.071467    
2022-01-29 12:13:53,481 - --- validate (epoch=281)-----------
2022-01-29 12:13:53,481 - 10000 samples (128 per mini-batch)
2022-01-29 12:13:56,788 - Epoch: [281][   79/   79]    Loss 1.309402    Top1 64.340000    Top5 88.390000    
2022-01-29 12:13:56,840 - ==> Top1: 64.340    Top5: 88.390    Loss: 1.309

2022-01-29 12:13:56,846 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:13:56,846 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:13:56,881 - 

2022-01-29 12:13:56,881 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:14:04,395 - Epoch: [282][  100/  391]    Overall Loss 0.469907    Objective Loss 0.469907                                        LR 0.000305    Time 0.075111    
2022-01-29 12:14:11,793 - Epoch: [282][  200/  391]    Overall Loss 0.465162    Objective Loss 0.465162                                        LR 0.000305    Time 0.074539    
2022-01-29 12:14:19,188 - Epoch: [282][  300/  391]    Overall Loss 0.468823    Objective Loss 0.468823                                        LR 0.000305    Time 0.074339    
2022-01-29 12:14:25,921 - Epoch: [282][  391/  391]    Overall Loss 0.470382    Objective Loss 0.470382    Top1 87.500000    Top5 98.557692    LR 0.000305    Time 0.074255    
2022-01-29 12:14:25,980 - --- validate (epoch=282)-----------
2022-01-29 12:14:25,980 - 10000 samples (128 per mini-batch)
2022-01-29 12:14:29,304 - Epoch: [282][   79/   79]    Loss 1.323455    Top1 63.780000    Top5 88.370000    
2022-01-29 12:14:29,359 - ==> Top1: 63.780    Top5: 88.370    Loss: 1.323

2022-01-29 12:14:29,364 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:14:29,364 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:14:29,393 - 

2022-01-29 12:14:29,393 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:14:37,104 - Epoch: [283][  100/  391]    Overall Loss 0.464643    Objective Loss 0.464643                                        LR 0.000305    Time 0.077074    
2022-01-29 12:14:44,507 - Epoch: [283][  200/  391]    Overall Loss 0.465892    Objective Loss 0.465892                                        LR 0.000305    Time 0.075552    
2022-01-29 12:14:51,915 - Epoch: [283][  300/  391]    Overall Loss 0.468091    Objective Loss 0.468091                                        LR 0.000305    Time 0.075058    
2022-01-29 12:14:58,656 - Epoch: [283][  391/  391]    Overall Loss 0.470181    Objective Loss 0.470181    Top1 91.346154    Top5 100.000000    LR 0.000305    Time 0.074826    
2022-01-29 12:14:58,721 - --- validate (epoch=283)-----------
2022-01-29 12:14:58,721 - 10000 samples (128 per mini-batch)
2022-01-29 12:15:01,995 - Epoch: [283][   79/   79]    Loss 1.332904    Top1 63.950000    Top5 88.180000    
2022-01-29 12:15:02,052 - ==> Top1: 63.950    Top5: 88.180    Loss: 1.333

2022-01-29 12:15:02,058 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:15:02,058 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:15:02,094 - 

2022-01-29 12:15:02,095 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:15:09,854 - Epoch: [284][  100/  391]    Overall Loss 0.449320    Objective Loss 0.449320                                        LR 0.000305    Time 0.077565    
2022-01-29 12:15:17,052 - Epoch: [284][  200/  391]    Overall Loss 0.460767    Objective Loss 0.460767                                        LR 0.000305    Time 0.074771    
2022-01-29 12:15:24,230 - Epoch: [284][  300/  391]    Overall Loss 0.462842    Objective Loss 0.462842                                        LR 0.000305    Time 0.073768    
2022-01-29 12:15:30,765 - Epoch: [284][  391/  391]    Overall Loss 0.464560    Objective Loss 0.464560    Top1 88.461538    Top5 99.519231    LR 0.000305    Time 0.073312    
2022-01-29 12:15:30,824 - --- validate (epoch=284)-----------
2022-01-29 12:15:30,824 - 10000 samples (128 per mini-batch)
2022-01-29 12:15:34,064 - Epoch: [284][   79/   79]    Loss 1.367076    Top1 63.050000    Top5 87.980000    
2022-01-29 12:15:34,121 - ==> Top1: 63.050    Top5: 87.980    Loss: 1.367

2022-01-29 12:15:34,126 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:15:34,126 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:15:34,158 - 

2022-01-29 12:15:34,159 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:15:41,714 - Epoch: [285][  100/  391]    Overall Loss 0.468247    Objective Loss 0.468247                                        LR 0.000305    Time 0.075531    
2022-01-29 12:15:49,041 - Epoch: [285][  200/  391]    Overall Loss 0.465788    Objective Loss 0.465788                                        LR 0.000305    Time 0.074395    
2022-01-29 12:15:56,347 - Epoch: [285][  300/  391]    Overall Loss 0.467341    Objective Loss 0.467341                                        LR 0.000305    Time 0.073948    
2022-01-29 12:16:03,011 - Epoch: [285][  391/  391]    Overall Loss 0.471029    Objective Loss 0.471029    Top1 86.538462    Top5 98.076923    LR 0.000305    Time 0.073779    
2022-01-29 12:16:03,070 - --- validate (epoch=285)-----------
2022-01-29 12:16:03,071 - 10000 samples (128 per mini-batch)
2022-01-29 12:16:06,329 - Epoch: [285][   79/   79]    Loss 1.313139    Top1 64.200000    Top5 88.550000    
2022-01-29 12:16:06,379 - ==> Top1: 64.200    Top5: 88.550    Loss: 1.313

2022-01-29 12:16:06,384 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:16:06,384 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:16:06,420 - 

2022-01-29 12:16:06,421 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:16:13,977 - Epoch: [286][  100/  391]    Overall Loss 0.467312    Objective Loss 0.467312                                        LR 0.000305    Time 0.075540    
2022-01-29 12:16:21,178 - Epoch: [286][  200/  391]    Overall Loss 0.464691    Objective Loss 0.464691                                        LR 0.000305    Time 0.073766    
2022-01-29 12:16:28,375 - Epoch: [286][  300/  391]    Overall Loss 0.461543    Objective Loss 0.461543                                        LR 0.000305    Time 0.073166    
2022-01-29 12:16:34,915 - Epoch: [286][  391/  391]    Overall Loss 0.462820    Objective Loss 0.462820    Top1 86.057692    Top5 99.519231    LR 0.000305    Time 0.072859    
2022-01-29 12:16:34,967 - --- validate (epoch=286)-----------
2022-01-29 12:16:34,967 - 10000 samples (128 per mini-batch)
2022-01-29 12:16:38,252 - Epoch: [286][   79/   79]    Loss 1.321689    Top1 63.750000    Top5 88.100000    
2022-01-29 12:16:38,303 - ==> Top1: 63.750    Top5: 88.100    Loss: 1.322

2022-01-29 12:16:38,308 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:16:38,308 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:16:38,345 - 

2022-01-29 12:16:38,345 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:16:45,984 - Epoch: [287][  100/  391]    Overall Loss 0.470047    Objective Loss 0.470047                                        LR 0.000305    Time 0.076362    
2022-01-29 12:16:53,327 - Epoch: [287][  200/  391]    Overall Loss 0.461820    Objective Loss 0.461820                                        LR 0.000305    Time 0.074891    
2022-01-29 12:17:00,668 - Epoch: [287][  300/  391]    Overall Loss 0.464448    Objective Loss 0.464448                                        LR 0.000305    Time 0.074396    
2022-01-29 12:17:07,357 - Epoch: [287][  391/  391]    Overall Loss 0.464257    Objective Loss 0.464257    Top1 83.173077    Top5 99.519231    LR 0.000305    Time 0.074185    
2022-01-29 12:17:07,417 - --- validate (epoch=287)-----------
2022-01-29 12:17:07,417 - 10000 samples (128 per mini-batch)
2022-01-29 12:17:10,736 - Epoch: [287][   79/   79]    Loss 1.303205    Top1 64.510000    Top5 88.710000    
2022-01-29 12:17:10,791 - ==> Top1: 64.510    Top5: 88.710    Loss: 1.303

2022-01-29 12:17:10,796 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:17:10,796 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:17:10,825 - 

2022-01-29 12:17:10,825 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:17:18,408 - Epoch: [288][  100/  391]    Overall Loss 0.450738    Objective Loss 0.450738                                        LR 0.000305    Time 0.075809    
2022-01-29 12:17:25,763 - Epoch: [288][  200/  391]    Overall Loss 0.458614    Objective Loss 0.458614                                        LR 0.000305    Time 0.074670    
2022-01-29 12:17:33,113 - Epoch: [288][  300/  391]    Overall Loss 0.465863    Objective Loss 0.465863                                        LR 0.000305    Time 0.074279    
2022-01-29 12:17:39,810 - Epoch: [288][  391/  391]    Overall Loss 0.466061    Objective Loss 0.466061    Top1 87.500000    Top5 98.076923    LR 0.000305    Time 0.074118    
2022-01-29 12:17:39,863 - --- validate (epoch=288)-----------
2022-01-29 12:17:39,863 - 10000 samples (128 per mini-batch)
2022-01-29 12:17:43,224 - Epoch: [288][   79/   79]    Loss 1.329889    Top1 63.700000    Top5 88.180000    
2022-01-29 12:17:43,283 - ==> Top1: 63.700    Top5: 88.180    Loss: 1.330

2022-01-29 12:17:43,289 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:17:43,289 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:17:43,325 - 

2022-01-29 12:17:43,325 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:17:51,282 - Epoch: [289][  100/  391]    Overall Loss 0.454924    Objective Loss 0.454924                                        LR 0.000305    Time 0.079541    
2022-01-29 12:17:59,120 - Epoch: [289][  200/  391]    Overall Loss 0.461248    Objective Loss 0.461248                                        LR 0.000305    Time 0.078954    
2022-01-29 12:18:06,919 - Epoch: [289][  300/  391]    Overall Loss 0.466538    Objective Loss 0.466538                                        LR 0.000305    Time 0.078629    
2022-01-29 12:18:13,980 - Epoch: [289][  391/  391]    Overall Loss 0.466953    Objective Loss 0.466953    Top1 89.423077    Top5 99.038462    LR 0.000305    Time 0.078386    
2022-01-29 12:18:14,040 - --- validate (epoch=289)-----------
2022-01-29 12:18:14,040 - 10000 samples (128 per mini-batch)
2022-01-29 12:18:17,447 - Epoch: [289][   79/   79]    Loss 1.318594    Top1 63.940000    Top5 88.410000    
2022-01-29 12:18:17,506 - ==> Top1: 63.940    Top5: 88.410    Loss: 1.319

2022-01-29 12:18:17,511 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:18:17,511 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:18:17,539 - 

2022-01-29 12:18:17,539 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:18:25,117 - Epoch: [290][  100/  391]    Overall Loss 0.449631    Objective Loss 0.449631                                        LR 0.000305    Time 0.075753    
2022-01-29 12:18:32,335 - Epoch: [290][  200/  391]    Overall Loss 0.454852    Objective Loss 0.454852                                        LR 0.000305    Time 0.073964    
2022-01-29 12:18:39,514 - Epoch: [290][  300/  391]    Overall Loss 0.457484    Objective Loss 0.457484                                        LR 0.000305    Time 0.073238    
2022-01-29 12:18:46,036 - Epoch: [290][  391/  391]    Overall Loss 0.459855    Objective Loss 0.459855    Top1 85.096154    Top5 99.519231    LR 0.000305    Time 0.072870    
2022-01-29 12:18:46,094 - --- validate (epoch=290)-----------
2022-01-29 12:18:46,094 - 10000 samples (128 per mini-batch)
2022-01-29 12:18:49,275 - Epoch: [290][   79/   79]    Loss 1.320897    Top1 63.920000    Top5 88.450000    
2022-01-29 12:18:49,334 - ==> Top1: 63.920    Top5: 88.450    Loss: 1.321

2022-01-29 12:18:49,339 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:18:49,339 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:18:49,374 - 

2022-01-29 12:18:49,375 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:18:56,922 - Epoch: [291][  100/  391]    Overall Loss 0.476002    Objective Loss 0.476002                                        LR 0.000305    Time 0.075442    
2022-01-29 12:19:04,313 - Epoch: [291][  200/  391]    Overall Loss 0.477996    Objective Loss 0.477996                                        LR 0.000305    Time 0.074673    
2022-01-29 12:19:11,728 - Epoch: [291][  300/  391]    Overall Loss 0.473345    Objective Loss 0.473345                                        LR 0.000305    Time 0.074496    
2022-01-29 12:19:18,366 - Epoch: [291][  391/  391]    Overall Loss 0.472795    Objective Loss 0.472795    Top1 89.423077    Top5 98.557692    LR 0.000305    Time 0.074133    
2022-01-29 12:19:18,426 - --- validate (epoch=291)-----------
2022-01-29 12:19:18,426 - 10000 samples (128 per mini-batch)
2022-01-29 12:19:21,620 - Epoch: [291][   79/   79]    Loss 1.337317    Top1 63.590000    Top5 88.120000    
2022-01-29 12:19:21,684 - ==> Top1: 63.590    Top5: 88.120    Loss: 1.337

2022-01-29 12:19:21,689 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:19:21,689 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:19:21,718 - 

2022-01-29 12:19:21,718 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:19:29,269 - Epoch: [292][  100/  391]    Overall Loss 0.463148    Objective Loss 0.463148                                        LR 0.000305    Time 0.075483    
2022-01-29 12:19:36,618 - Epoch: [292][  200/  391]    Overall Loss 0.459538    Objective Loss 0.459538                                        LR 0.000305    Time 0.074479    
2022-01-29 12:19:43,957 - Epoch: [292][  300/  391]    Overall Loss 0.465455    Objective Loss 0.465455                                        LR 0.000305    Time 0.074114    
2022-01-29 12:19:50,732 - Epoch: [292][  391/  391]    Overall Loss 0.463508    Objective Loss 0.463508    Top1 89.903846    Top5 99.519231    LR 0.000305    Time 0.074191    
2022-01-29 12:19:50,790 - --- validate (epoch=292)-----------
2022-01-29 12:19:50,790 - 10000 samples (128 per mini-batch)
2022-01-29 12:19:54,077 - Epoch: [292][   79/   79]    Loss 1.332174    Top1 63.390000    Top5 88.240000    
2022-01-29 12:19:54,129 - ==> Top1: 63.390    Top5: 88.240    Loss: 1.332

2022-01-29 12:19:54,134 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:19:54,134 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:19:54,163 - 

2022-01-29 12:19:54,164 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:20:01,863 - Epoch: [293][  100/  391]    Overall Loss 0.452285    Objective Loss 0.452285                                        LR 0.000305    Time 0.076969    
2022-01-29 12:20:09,253 - Epoch: [293][  200/  391]    Overall Loss 0.453798    Objective Loss 0.453798                                        LR 0.000305    Time 0.075429    
2022-01-29 12:20:16,640 - Epoch: [293][  300/  391]    Overall Loss 0.456506    Objective Loss 0.456506                                        LR 0.000305    Time 0.074906    
2022-01-29 12:20:23,367 - Epoch: [293][  391/  391]    Overall Loss 0.461790    Objective Loss 0.461790    Top1 87.019231    Top5 99.038462    LR 0.000305    Time 0.074675    
2022-01-29 12:20:23,426 - --- validate (epoch=293)-----------
2022-01-29 12:20:23,426 - 10000 samples (128 per mini-batch)
2022-01-29 12:20:26,723 - Epoch: [293][   79/   79]    Loss 1.347455    Top1 63.780000    Top5 87.940000    
2022-01-29 12:20:26,776 - ==> Top1: 63.780    Top5: 87.940    Loss: 1.347

2022-01-29 12:20:26,781 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:20:26,781 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:20:26,810 - 

2022-01-29 12:20:26,810 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:20:34,377 - Epoch: [294][  100/  391]    Overall Loss 0.470116    Objective Loss 0.470116                                        LR 0.000305    Time 0.075635    
2022-01-29 12:20:41,636 - Epoch: [294][  200/  391]    Overall Loss 0.466254    Objective Loss 0.466254                                        LR 0.000305    Time 0.074113    
2022-01-29 12:20:48,952 - Epoch: [294][  300/  391]    Overall Loss 0.465071    Objective Loss 0.465071                                        LR 0.000305    Time 0.073790    
2022-01-29 12:20:55,659 - Epoch: [294][  391/  391]    Overall Loss 0.466727    Objective Loss 0.466727    Top1 89.423077    Top5 99.038462    LR 0.000305    Time 0.073768    
2022-01-29 12:20:55,723 - --- validate (epoch=294)-----------
2022-01-29 12:20:55,723 - 10000 samples (128 per mini-batch)
2022-01-29 12:20:59,042 - Epoch: [294][   79/   79]    Loss 1.367752    Top1 62.730000    Top5 87.430000    
2022-01-29 12:20:59,101 - ==> Top1: 62.730    Top5: 87.430    Loss: 1.368

2022-01-29 12:20:59,106 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:20:59,107 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:20:59,142 - 

2022-01-29 12:20:59,142 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:21:06,660 - Epoch: [295][  100/  391]    Overall Loss 0.448954    Objective Loss 0.448954                                        LR 0.000305    Time 0.075153    
2022-01-29 12:21:13,982 - Epoch: [295][  200/  391]    Overall Loss 0.455040    Objective Loss 0.455040                                        LR 0.000305    Time 0.074183    
2022-01-29 12:21:21,295 - Epoch: [295][  300/  391]    Overall Loss 0.458984    Objective Loss 0.458984                                        LR 0.000305    Time 0.073830    
2022-01-29 12:21:27,947 - Epoch: [295][  391/  391]    Overall Loss 0.462581    Objective Loss 0.462581    Top1 88.942308    Top5 98.557692    LR 0.000305    Time 0.073657    
2022-01-29 12:21:28,006 - --- validate (epoch=295)-----------
2022-01-29 12:21:28,006 - 10000 samples (128 per mini-batch)
2022-01-29 12:21:31,302 - Epoch: [295][   79/   79]    Loss 1.365379    Top1 62.790000    Top5 87.990000    
2022-01-29 12:21:31,367 - ==> Top1: 62.790    Top5: 87.990    Loss: 1.365

2022-01-29 12:21:31,372 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:21:31,372 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:21:31,403 - 

2022-01-29 12:21:31,404 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:21:39,029 - Epoch: [296][  100/  391]    Overall Loss 0.452981    Objective Loss 0.452981                                        LR 0.000305    Time 0.076226    
2022-01-29 12:21:46,393 - Epoch: [296][  200/  391]    Overall Loss 0.454381    Objective Loss 0.454381                                        LR 0.000305    Time 0.074927    
2022-01-29 12:21:53,694 - Epoch: [296][  300/  391]    Overall Loss 0.456629    Objective Loss 0.456629                                        LR 0.000305    Time 0.074287    
2022-01-29 12:22:00,310 - Epoch: [296][  391/  391]    Overall Loss 0.457160    Objective Loss 0.457160    Top1 89.903846    Top5 100.000000    LR 0.000305    Time 0.073917    
2022-01-29 12:22:00,369 - --- validate (epoch=296)-----------
2022-01-29 12:22:00,370 - 10000 samples (128 per mini-batch)
2022-01-29 12:22:03,693 - Epoch: [296][   79/   79]    Loss 1.312523    Top1 64.310000    Top5 88.690000    
2022-01-29 12:22:03,743 - ==> Top1: 64.310    Top5: 88.690    Loss: 1.313

2022-01-29 12:22:03,748 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:22:03,748 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:22:03,785 - 

2022-01-29 12:22:03,785 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:22:11,361 - Epoch: [297][  100/  391]    Overall Loss 0.452016    Objective Loss 0.452016                                        LR 0.000305    Time 0.075735    
2022-01-29 12:22:18,540 - Epoch: [297][  200/  391]    Overall Loss 0.454243    Objective Loss 0.454243                                        LR 0.000305    Time 0.073754    
2022-01-29 12:22:25,707 - Epoch: [297][  300/  391]    Overall Loss 0.459037    Objective Loss 0.459037                                        LR 0.000305    Time 0.073058    
2022-01-29 12:22:32,375 - Epoch: [297][  391/  391]    Overall Loss 0.459349    Objective Loss 0.459349    Top1 89.423077    Top5 99.519231    LR 0.000305    Time 0.073106    
2022-01-29 12:22:32,434 - --- validate (epoch=297)-----------
2022-01-29 12:22:32,434 - 10000 samples (128 per mini-batch)
2022-01-29 12:22:35,763 - Epoch: [297][   79/   79]    Loss 1.338069    Top1 63.830000    Top5 88.210000    
2022-01-29 12:22:35,820 - ==> Top1: 63.830    Top5: 88.210    Loss: 1.338

2022-01-29 12:22:35,826 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:22:35,826 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:22:35,862 - 

2022-01-29 12:22:35,862 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:22:43,422 - Epoch: [298][  100/  391]    Overall Loss 0.444001    Objective Loss 0.444001                                        LR 0.000305    Time 0.075567    
2022-01-29 12:22:50,717 - Epoch: [298][  200/  391]    Overall Loss 0.451360    Objective Loss 0.451360                                        LR 0.000305    Time 0.074257    
2022-01-29 12:22:57,928 - Epoch: [298][  300/  391]    Overall Loss 0.453559    Objective Loss 0.453559                                        LR 0.000305    Time 0.073539    
2022-01-29 12:23:04,495 - Epoch: [298][  391/  391]    Overall Loss 0.456845    Objective Loss 0.456845    Top1 91.826923    Top5 99.519231    LR 0.000305    Time 0.073216    
2022-01-29 12:23:04,556 - --- validate (epoch=298)-----------
2022-01-29 12:23:04,556 - 10000 samples (128 per mini-batch)
2022-01-29 12:23:07,754 - Epoch: [298][   79/   79]    Loss 1.317407    Top1 63.980000    Top5 88.730000    
2022-01-29 12:23:07,807 - ==> Top1: 63.980    Top5: 88.730    Loss: 1.317

2022-01-29 12:23:07,812 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:23:07,812 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:23:07,840 - 

2022-01-29 12:23:07,840 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 12:23:15,420 - Epoch: [299][  100/  391]    Overall Loss 0.457169    Objective Loss 0.457169                                        LR 0.000305    Time 0.075775    
2022-01-29 12:23:22,713 - Epoch: [299][  200/  391]    Overall Loss 0.454600    Objective Loss 0.454600                                        LR 0.000305    Time 0.074347    
2022-01-29 12:23:30,009 - Epoch: [299][  300/  391]    Overall Loss 0.457661    Objective Loss 0.457661                                        LR 0.000305    Time 0.073884    
2022-01-29 12:23:36,642 - Epoch: [299][  391/  391]    Overall Loss 0.456290    Objective Loss 0.456290    Top1 89.903846    Top5 100.000000    LR 0.000305    Time 0.073649    
2022-01-29 12:23:36,706 - --- validate (epoch=299)-----------
2022-01-29 12:23:36,706 - 10000 samples (128 per mini-batch)
2022-01-29 12:23:39,963 - Epoch: [299][   79/   79]    Loss 1.319360    Top1 63.970000    Top5 88.320000    
2022-01-29 12:23:40,015 - ==> Top1: 63.970    Top5: 88.320    Loss: 1.319

2022-01-29 12:23:40,020 - ==> Best [Top1: 64.710   Top5: 88.740   Sparsity:0.00   Params: 627712 on epoch: 253]
2022-01-29 12:23:40,021 - Saving checkpoint to: logs/2022.01.29-101451/qat_checkpoint.pth.tar
2022-01-29 12:23:40,056 - --- test ---------------------
2022-01-29 12:23:40,056 - 10000 samples (128 per mini-batch)
2022-01-29 12:23:43,419 - Test: [   79/   79]    Loss 1.322630    Top1 63.970000    Top5 88.320000    
2022-01-29 12:23:43,473 - ==> Top1: 63.970    Top5: 88.320    Loss: 1.323

2022-01-29 12:23:43,476 - 
2022-01-29 12:23:43,477 - Log file for this run: /home/ermanokman/repos/github/ai8x-training/logs/2022.01.29-101451/2022.01.29-101451.log
