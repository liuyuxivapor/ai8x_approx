2022-01-29 03:17:18,015 - Log file for this run: /home/ermanokman/repos/github/ai8x-training/logs/2022.01.29-031718/2022.01.29-031718.log
2022-01-29 03:17:18,015 - Number of CPUs: 24
2022-01-29 03:17:18,033 - Number of GPUs: 1
2022-01-29 03:17:18,033 - CUDA version: 11.1
2022-01-29 03:17:18,033 - CUDNN version: 8005
2022-01-29 03:17:18,033 - Kernel: 5.4.0-90-generic
2022-01-29 03:17:18,033 - Python: 3.8.11 (default, Aug  3 2021, 15:09:35) 
[GCC 7.5.0]
2022-01-29 03:17:18,033 - pip freeze: {'absl-py': '0.13.0', 'aiohttp': '3.7.4', 'appdirs': '1.4.4', 'argon2-cffi': '20.1.0', 'astroid': '2.5', 'async-generator': '1.10', 'async-timeout': '3.0.1', 'atomicwrites': '1.4.0', 'attrs': '21.2.0', 'audioread': '2.1.9', 'backcall': '0.2.0', 'bleach': '3.3.0', 'blinker': '1.4', 'bottleneck': '1.3.2', 'bqplot': '0.11.5', 'brotlipy': '0.7.0', 'cachetools': '4.2.2', 'certifi': '2021.5.30', 'cffi': '1.14.6', 'chardet': '4.0.0', 'click': '8.0.1', 'cloudpickle': '1.6.0', 'colorama': '0.4.4', 'coverage': '5.5', 'cryptography': '3.4.7', 'cycler': '0.10.0', 'cython': '0.29.24', 'cytoolz': '0.11.0', 'dask': '2021.7.2', 'decorator': '5.0.9', 'defusedxml': '0.7.1', 'deprecated': '1.2.12', 'distiller': '0.4.0rc0', 'entrypoints': '0.3', 'fsspec': '2021.7.0', 'gitdb': '4.0.7', 'gitpython': '3.1.0', 'google-auth': '1.33.0', 'google-auth-oauthlib': '0.4.4', 'graphviz': '0.10.1', 'grpcio': '1.36.1', 'gym': '0.12.5', 'idna': '2.10', 'imageio': '2.9.0', 'importlib-metadata': '3.10.0', 'ipykernel': '5.5.3', 'ipython': '7.22.0', 'ipython-genutils': '0.2.0', 'ipywidgets': '7.4.2', 'isort': '5.9.2', 'jedi': '0.17.0', 'jinja2': '2.11.3', 'joblib': '1.0.1', 'jsonpatch': '1.32', 'jsonpointer': '2.1', 'jsonschema': '3.2.0', 'jupyter': '1.0.0', 'jupyter-client': '6.1.12', 'jupyter-console': '6.4.0', 'jupyter-core': '4.7.1', 'jupyterlab-pygments': '0.1.2', 'kiwisolver': '1.3.1', 'lazy-object-proxy': '1.6.0', 'librosa': '0.8.1', 'llvmlite': '0.36.0', 'locket': '0.2.1', 'markdown': '3.3.4', 'markupsafe': '1.1.1', 'matplotlib': '3.3.4', 'mccabe': '0.6.1', 'mistune': '0.8.4', 'mkl-fft': '1.3.0', 'mkl-random': '1.1.1', 'mkl-service': '2.3.0', 'more-itertools': '8.7.0', 'multidict': '5.1.0', 'munch': '2.5.0', 'nbclient': '0.5.3', 'nbconvert': '6.0.7', 'nbformat': '5.1.3', 'nest-asyncio': '1.5.1', 'networkx': '2.6.2', 'notebook': '6.3.0', 'numba': '0.53.1', 'numexpr': '2.7.3', 'numpy': '1.20.2', 'oauthlib': '3.1.1', 'olefile': '0.46', 'onnx': '1.10.1', 'packaging': '21.0', 'pandas': '1.3.1', 'pandocfilters': '1.4.3', 'parso': '0.8.2', 'partd': '1.2.0', 'pathspec': '0.7.0', 'pexpect': '4.8.0', 'pickleshare': '0.7.5', 'pillow': '8.3.1', 'pip': '21.2.2', 'pluggy': '0.13.1', 'pooch': '1.4.0', 'pretrainedmodels': '0.7.4', 'prometheus-client': '0.10.0', 'prompt-toolkit': '3.0.17', 'protobuf': '3.16.0', 'ptyprocess': '0.7.0', 'py': '1.10.0', 'pyasn1': '0.4.8', 'pyasn1-modules': '0.2.8', 'pycparser': '2.20', 'pydot': '1.4.1', 'pygithub': '1.55', 'pyglet': '1.5.15', 'pygments': '2.9.0', 'pyjwt': '2.1.0', 'pylint': '2.6.0', 'pynacl': '1.4.0', 'pyopenssl': '20.0.1', 'pyparsing': '2.4.7', 'pyqt5': '5.12.3', 'pyqt5-sip': '4.19.18', 'pyqtchart': '5.12', 'pyqtwebengine': '5.12.1', 'pyrsistent': '0.17.3', 'pysocks': '1.7.1', 'pytest': '4.6.11', 'python-dateutil': '2.8.2', 'pytsmod': '0.3.3', 'pytz': '2021.1', 'pywavelets': '1.1.1', 'pyyaml': '5.4.1', 'pyzmq': '22.0.3', 'qgrid': '1.1.1', 'qtconsole': '5.0.3', 'qtpy': '1.9.0', 'requests': '2.25.1', 'requests-oauthlib': '1.3.0', 'resampy': '0.2.2', 'rsa': '4.7.2', 'scikit-image': '0.18.1', 'scikit-learn': '0.23.2', 'scipy': '1.6.2', 'send2trash': '1.5.0', 'setuptools': '52.0.0.post20210125', 'shap': '0.37.0', 'six': '1.16.0', 'slicer': '0.0.7', 'smmap': '4.0.0', 'soundfile': '0.10.3.post1', 'tabulate': '0.8.3', 'tensorboard': '2.4.0', 'tensorboard-plugin-wit': '1.6.0', 'terminado': '0.9.4', 'testpath': '0.4.4', 'threadpoolctl': '2.2.0', 'tifffile': '2020.10.1', 'toml': '0.10.2', 'toolz': '0.11.1', 'torch': '1.8.1+cu111', 'torchaudio': '0.8.0a0+e4e171a', 'torchfile': '0.1.0', 'torchnet': '0.0.4', 'torchvision': '0.9.1+cu111', 'tornado': '6.1', 'tqdm': '4.33.0', 'traitlets': '5.0.5', 'traittypes': '0.2.1', 'typing-extensions': '3.10.0.0', 'urllib3': '1.26.6', 'visdom': '0.1.8.9', 'wcwidth': '0.2.5', 'webencodings': '0.5.1', 'websocket-client': '0.58.0', 'werkzeug': '1.0.1', 'wheel': '0.36.2', 'widgetsnbextension': '3.4.2', 'wrapt': '1.12.1', 'xlsxwriter': '1.3.8', 'yamllint': '1.26.1', 'yarl': '1.6.3', 'zipp': '3.5.0'}
2022-01-29 03:17:18,033 - Command line: train.py --deterministic --epochs 300 --optimizer SGD --lr 0.1 --compress schedule-cifar100-mobilenetv2.yaml --model ai87netmobilenetv2cifar100_m0_75 --dataset CIFAR100 --device MAX78002 --batch-size 128 --print-freq 100 --validation-split 0 --use-bias --qat-policy qat_policy_cifar100_mobilenetv2.yaml
2022-01-29 03:17:18,033 - Distiller: 0.4.0rc0
2022-01-29 03:17:18,033 - set_deterministic was invoked
2022-01-29 03:17:19,858 - Optimizer Type: <class 'torch.optim.sgd.SGD'>
2022-01-29 03:17:19,859 - Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False}
2022-01-29 03:17:20,938 - set_deterministic was invoked
2022-01-29 03:17:20,941 - Dataset sizes:
	training=50000
	validation=10000
	test=10000
2022-01-29 03:17:20,941 - Reading compression schedule from: schedule-cifar100-mobilenetv2.yaml
2022-01-29 03:17:20,943 - Schedule contents:
{
  "lr_schedulers": {
    "training_lr": {
      "class": "MultiStepLR",
      "milestones": [
        100,
        150,
        175,
        250
      ],
      "gamma": 0.235
    }
  },
  "policies": [
    {
      "lr_scheduler": {
        "instance_name": "training_lr"
      },
      "starting_epoch": 0,
      "ending_epoch": 250,
      "frequency": 1
    }
  ]
}
2022-01-29 03:17:20,948 - 

2022-01-29 03:17:20,948 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:17:26,817 - Epoch: [0][  100/  391]    Overall Loss 4.377823    Objective Loss 4.377823                                        LR 0.100000    Time 0.058657    
2022-01-29 03:17:32,179 - Epoch: [0][  200/  391]    Overall Loss 4.242043    Objective Loss 4.242043                                        LR 0.100000    Time 0.056137    
2022-01-29 03:17:37,538 - Epoch: [0][  300/  391]    Overall Loss 4.156688    Objective Loss 4.156688                                        LR 0.100000    Time 0.055283    
2022-01-29 03:17:42,407 - Epoch: [0][  391/  391]    Overall Loss 4.099108    Objective Loss 4.099108    Top1 6.730769    Top5 30.769231    LR 0.100000    Time 0.054869    
2022-01-29 03:17:42,464 - --- validate (epoch=0)-----------
2022-01-29 03:17:42,464 - 10000 samples (128 per mini-batch)
2022-01-29 03:17:44,271 - Epoch: [0][   79/   79]    Loss 3.966646    Top1 6.530000    Top5 26.090000    
2022-01-29 03:17:44,322 - ==> Top1: 6.530    Top5: 26.090    Loss: 3.967

2022-01-29 03:17:44,328 - ==> Best [Top1: 6.530   Top5: 26.090   Sparsity:0.00   Params: 1341960 on epoch: 0]
2022-01-29 03:17:44,328 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:17:44,369 - 

2022-01-29 03:17:44,369 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:17:49,962 - Epoch: [1][  100/  391]    Overall Loss 3.821791    Objective Loss 3.821791                                        LR 0.100000    Time 0.055912    
2022-01-29 03:17:55,345 - Epoch: [1][  200/  391]    Overall Loss 3.787197    Objective Loss 3.787197                                        LR 0.100000    Time 0.054867    
2022-01-29 03:18:00,733 - Epoch: [1][  300/  391]    Overall Loss 3.750955    Objective Loss 3.750955                                        LR 0.100000    Time 0.054532    
2022-01-29 03:18:05,635 - Epoch: [1][  391/  391]    Overall Loss 3.723834    Objective Loss 3.723834    Top1 10.576923    Top5 34.615385    LR 0.100000    Time 0.054377    
2022-01-29 03:18:05,690 - --- validate (epoch=1)-----------
2022-01-29 03:18:05,690 - 10000 samples (128 per mini-batch)
2022-01-29 03:18:07,509 - Epoch: [1][   79/   79]    Loss 3.655275    Top1 12.110000    Top5 36.910000    
2022-01-29 03:18:07,566 - ==> Top1: 12.110    Top5: 36.910    Loss: 3.655

2022-01-29 03:18:07,571 - ==> Best [Top1: 12.110   Top5: 36.910   Sparsity:0.00   Params: 1341960 on epoch: 1]
2022-01-29 03:18:07,571 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:18:07,626 - 

2022-01-29 03:18:07,627 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:18:13,310 - Epoch: [2][  100/  391]    Overall Loss 3.535066    Objective Loss 3.535066                                        LR 0.100000    Time 0.056809    
2022-01-29 03:18:18,707 - Epoch: [2][  200/  391]    Overall Loss 3.504002    Objective Loss 3.504002                                        LR 0.100000    Time 0.055382    
2022-01-29 03:18:24,099 - Epoch: [2][  300/  391]    Overall Loss 3.489512    Objective Loss 3.489512                                        LR 0.100000    Time 0.054892    
2022-01-29 03:18:28,998 - Epoch: [2][  391/  391]    Overall Loss 3.465107    Objective Loss 3.465107    Top1 16.346154    Top5 43.269231    LR 0.100000    Time 0.054644    
2022-01-29 03:18:29,063 - --- validate (epoch=2)-----------
2022-01-29 03:18:29,063 - 10000 samples (128 per mini-batch)
2022-01-29 03:18:30,915 - Epoch: [2][   79/   79]    Loss 3.339796    Top1 16.930000    Top5 44.940000    
2022-01-29 03:18:30,972 - ==> Top1: 16.930    Top5: 44.940    Loss: 3.340

2022-01-29 03:18:30,978 - ==> Best [Top1: 16.930   Top5: 44.940   Sparsity:0.00   Params: 1341960 on epoch: 2]
2022-01-29 03:18:30,978 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:18:31,034 - 

2022-01-29 03:18:31,034 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:18:36,737 - Epoch: [3][  100/  391]    Overall Loss 3.308381    Objective Loss 3.308381                                        LR 0.100000    Time 0.057002    
2022-01-29 03:18:42,136 - Epoch: [3][  200/  391]    Overall Loss 3.287303    Objective Loss 3.287303                                        LR 0.100000    Time 0.055489    
2022-01-29 03:18:47,549 - Epoch: [3][  300/  391]    Overall Loss 3.271732    Objective Loss 3.271732                                        LR 0.100000    Time 0.055033    
2022-01-29 03:18:52,473 - Epoch: [3][  391/  391]    Overall Loss 3.244716    Objective Loss 3.244716    Top1 20.192308    Top5 51.442308    LR 0.100000    Time 0.054817    
2022-01-29 03:18:52,538 - --- validate (epoch=3)-----------
2022-01-29 03:18:52,538 - 10000 samples (128 per mini-batch)
2022-01-29 03:18:54,378 - Epoch: [3][   79/   79]    Loss 3.206561    Top1 19.120000    Top5 49.390000    
2022-01-29 03:18:54,427 - ==> Top1: 19.120    Top5: 49.390    Loss: 3.207

2022-01-29 03:18:54,432 - ==> Best [Top1: 19.120   Top5: 49.390   Sparsity:0.00   Params: 1341960 on epoch: 3]
2022-01-29 03:18:54,432 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:18:54,483 - 

2022-01-29 03:18:54,483 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:19:00,110 - Epoch: [4][  100/  391]    Overall Loss 3.114290    Objective Loss 3.114290                                        LR 0.100000    Time 0.056242    
2022-01-29 03:19:05,479 - Epoch: [4][  200/  391]    Overall Loss 3.086028    Objective Loss 3.086028                                        LR 0.100000    Time 0.054959    
2022-01-29 03:19:10,872 - Epoch: [4][  300/  391]    Overall Loss 3.062469    Objective Loss 3.062469                                        LR 0.100000    Time 0.054614    
2022-01-29 03:19:15,772 - Epoch: [4][  391/  391]    Overall Loss 3.041663    Objective Loss 3.041663    Top1 17.788462    Top5 52.884615    LR 0.100000    Time 0.054432    
2022-01-29 03:19:15,829 - --- validate (epoch=4)-----------
2022-01-29 03:19:15,829 - 10000 samples (128 per mini-batch)
2022-01-29 03:19:17,758 - Epoch: [4][   79/   79]    Loss 3.224435    Top1 20.060000    Top5 50.340000    
2022-01-29 03:19:17,806 - ==> Top1: 20.060    Top5: 50.340    Loss: 3.224

2022-01-29 03:19:17,811 - ==> Best [Top1: 20.060   Top5: 50.340   Sparsity:0.00   Params: 1341960 on epoch: 4]
2022-01-29 03:19:17,811 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:19:17,868 - 

2022-01-29 03:19:17,868 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:19:23,457 - Epoch: [5][  100/  391]    Overall Loss 2.902992    Objective Loss 2.902992                                        LR 0.100000    Time 0.055863    
2022-01-29 03:19:28,816 - Epoch: [5][  200/  391]    Overall Loss 2.906735    Objective Loss 2.906735                                        LR 0.100000    Time 0.054720    
2022-01-29 03:19:34,197 - Epoch: [5][  300/  391]    Overall Loss 2.894135    Objective Loss 2.894135                                        LR 0.100000    Time 0.054415    
2022-01-29 03:19:39,096 - Epoch: [5][  391/  391]    Overall Loss 2.879331    Objective Loss 2.879331    Top1 25.480769    Top5 57.211538    LR 0.100000    Time 0.054278    
2022-01-29 03:19:39,154 - --- validate (epoch=5)-----------
2022-01-29 03:19:39,155 - 10000 samples (128 per mini-batch)
2022-01-29 03:19:41,084 - Epoch: [5][   79/   79]    Loss 2.915092    Top1 25.870000    Top5 56.860000    
2022-01-29 03:19:41,140 - ==> Top1: 25.870    Top5: 56.860    Loss: 2.915

2022-01-29 03:19:41,146 - ==> Best [Top1: 25.870   Top5: 56.860   Sparsity:0.00   Params: 1341960 on epoch: 5]
2022-01-29 03:19:41,146 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:19:41,203 - 

2022-01-29 03:19:41,203 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:19:46,895 - Epoch: [6][  100/  391]    Overall Loss 2.773250    Objective Loss 2.773250                                        LR 0.100000    Time 0.056893    
2022-01-29 03:19:52,262 - Epoch: [6][  200/  391]    Overall Loss 2.772517    Objective Loss 2.772517                                        LR 0.100000    Time 0.055278    
2022-01-29 03:19:57,630 - Epoch: [6][  300/  391]    Overall Loss 2.764036    Objective Loss 2.764036                                        LR 0.100000    Time 0.054744    
2022-01-29 03:20:02,515 - Epoch: [6][  391/  391]    Overall Loss 2.752060    Objective Loss 2.752060    Top1 27.403846    Top5 57.211538    LR 0.100000    Time 0.054493    
2022-01-29 03:20:02,573 - --- validate (epoch=6)-----------
2022-01-29 03:20:02,573 - 10000 samples (128 per mini-batch)
2022-01-29 03:20:04,426 - Epoch: [6][   79/   79]    Loss 2.754638    Top1 27.320000    Top5 61.920000    
2022-01-29 03:20:04,484 - ==> Top1: 27.320    Top5: 61.920    Loss: 2.755

2022-01-29 03:20:04,490 - ==> Best [Top1: 27.320   Top5: 61.920   Sparsity:0.00   Params: 1341960 on epoch: 6]
2022-01-29 03:20:04,490 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:20:04,547 - 

2022-01-29 03:20:04,547 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:20:10,237 - Epoch: [7][  100/  391]    Overall Loss 2.665800    Objective Loss 2.665800                                        LR 0.100000    Time 0.056879    
2022-01-29 03:20:15,651 - Epoch: [7][  200/  391]    Overall Loss 2.665533    Objective Loss 2.665533                                        LR 0.100000    Time 0.055505    
2022-01-29 03:20:21,059 - Epoch: [7][  300/  391]    Overall Loss 2.651814    Objective Loss 2.651814                                        LR 0.100000    Time 0.055027    
2022-01-29 03:20:25,982 - Epoch: [7][  391/  391]    Overall Loss 2.643437    Objective Loss 2.643437    Top1 26.442308    Top5 58.173077    LR 0.100000    Time 0.054808    
2022-01-29 03:20:26,043 - --- validate (epoch=7)-----------
2022-01-29 03:20:26,043 - 10000 samples (128 per mini-batch)
2022-01-29 03:20:27,890 - Epoch: [7][   79/   79]    Loss 2.713666    Top1 28.850000    Top5 62.060000    
2022-01-29 03:20:27,940 - ==> Top1: 28.850    Top5: 62.060    Loss: 2.714

2022-01-29 03:20:27,945 - ==> Best [Top1: 28.850   Top5: 62.060   Sparsity:0.00   Params: 1341960 on epoch: 7]
2022-01-29 03:20:27,945 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:20:28,002 - 

2022-01-29 03:20:28,002 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:20:33,652 - Epoch: [8][  100/  391]    Overall Loss 2.554677    Objective Loss 2.554677                                        LR 0.100000    Time 0.056475    
2022-01-29 03:20:39,043 - Epoch: [8][  200/  391]    Overall Loss 2.552704    Objective Loss 2.552704                                        LR 0.100000    Time 0.055188    
2022-01-29 03:20:44,428 - Epoch: [8][  300/  391]    Overall Loss 2.558829    Objective Loss 2.558829                                        LR 0.100000    Time 0.054739    
2022-01-29 03:20:49,319 - Epoch: [8][  391/  391]    Overall Loss 2.549669    Objective Loss 2.549669    Top1 29.326923    Top5 66.826923    LR 0.100000    Time 0.054505    
2022-01-29 03:20:49,383 - --- validate (epoch=8)-----------
2022-01-29 03:20:49,383 - 10000 samples (128 per mini-batch)
2022-01-29 03:20:51,198 - Epoch: [8][   79/   79]    Loss 2.798930    Top1 27.470000    Top5 61.410000    
2022-01-29 03:20:51,252 - ==> Top1: 27.470    Top5: 61.410    Loss: 2.799

2022-01-29 03:20:51,257 - ==> Best [Top1: 28.850   Top5: 62.060   Sparsity:0.00   Params: 1341960 on epoch: 7]
2022-01-29 03:20:51,257 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:20:51,300 - 

2022-01-29 03:20:51,300 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:20:56,987 - Epoch: [9][  100/  391]    Overall Loss 2.469972    Objective Loss 2.469972                                        LR 0.100000    Time 0.056843    
2022-01-29 03:21:02,368 - Epoch: [9][  200/  391]    Overall Loss 2.473902    Objective Loss 2.473902                                        LR 0.100000    Time 0.055322    
2022-01-29 03:21:07,745 - Epoch: [9][  300/  391]    Overall Loss 2.468783    Objective Loss 2.468783                                        LR 0.100000    Time 0.054804    
2022-01-29 03:21:12,637 - Epoch: [9][  391/  391]    Overall Loss 2.468683    Objective Loss 2.468683    Top1 33.173077    Top5 64.423077    LR 0.100000    Time 0.054557    
2022-01-29 03:21:12,696 - --- validate (epoch=9)-----------
2022-01-29 03:21:12,696 - 10000 samples (128 per mini-batch)
2022-01-29 03:21:14,546 - Epoch: [9][   79/   79]    Loss 2.912770    Top1 26.440000    Top5 59.310000    
2022-01-29 03:21:14,599 - ==> Top1: 26.440    Top5: 59.310    Loss: 2.913

2022-01-29 03:21:14,604 - ==> Best [Top1: 28.850   Top5: 62.060   Sparsity:0.00   Params: 1341960 on epoch: 7]
2022-01-29 03:21:14,604 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:21:14,652 - 

2022-01-29 03:21:14,652 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:21:20,368 - Epoch: [10][  100/  391]    Overall Loss 2.436758    Objective Loss 2.436758                                        LR 0.100000    Time 0.057130    
2022-01-29 03:21:25,775 - Epoch: [10][  200/  391]    Overall Loss 2.409225    Objective Loss 2.409225                                        LR 0.100000    Time 0.055596    
2022-01-29 03:21:31,191 - Epoch: [10][  300/  391]    Overall Loss 2.402533    Objective Loss 2.402533                                        LR 0.100000    Time 0.055114    
2022-01-29 03:21:36,117 - Epoch: [10][  391/  391]    Overall Loss 2.402784    Objective Loss 2.402784    Top1 37.500000    Top5 70.673077    LR 0.100000    Time 0.054883    
2022-01-29 03:21:36,172 - --- validate (epoch=10)-----------
2022-01-29 03:21:36,172 - 10000 samples (128 per mini-batch)
2022-01-29 03:21:38,000 - Epoch: [10][   79/   79]    Loss 2.712072    Top1 29.800000    Top5 62.600000    
2022-01-29 03:21:38,050 - ==> Top1: 29.800    Top5: 62.600    Loss: 2.712

2022-01-29 03:21:38,055 - ==> Best [Top1: 29.800   Top5: 62.600   Sparsity:0.00   Params: 1341960 on epoch: 10]
2022-01-29 03:21:38,055 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:21:38,106 - 

2022-01-29 03:21:38,106 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:21:43,776 - Epoch: [11][  100/  391]    Overall Loss 2.361646    Objective Loss 2.361646                                        LR 0.100000    Time 0.056667    
2022-01-29 03:21:49,183 - Epoch: [11][  200/  391]    Overall Loss 2.346820    Objective Loss 2.346820                                        LR 0.100000    Time 0.055365    
2022-01-29 03:21:54,621 - Epoch: [11][  300/  391]    Overall Loss 2.339015    Objective Loss 2.339015                                        LR 0.100000    Time 0.055034    
2022-01-29 03:21:59,563 - Epoch: [11][  391/  391]    Overall Loss 2.340494    Objective Loss 2.340494    Top1 39.903846    Top5 67.788462    LR 0.100000    Time 0.054863    
2022-01-29 03:21:59,621 - --- validate (epoch=11)-----------
2022-01-29 03:21:59,621 - 10000 samples (128 per mini-batch)
2022-01-29 03:22:01,469 - Epoch: [11][   79/   79]    Loss 2.805497    Top1 28.200000    Top5 63.090000    
2022-01-29 03:22:01,527 - ==> Top1: 28.200    Top5: 63.090    Loss: 2.805

2022-01-29 03:22:01,533 - ==> Best [Top1: 29.800   Top5: 62.600   Sparsity:0.00   Params: 1341960 on epoch: 10]
2022-01-29 03:22:01,533 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:22:01,581 - 

2022-01-29 03:22:01,581 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:22:07,327 - Epoch: [12][  100/  391]    Overall Loss 2.296811    Objective Loss 2.296811                                        LR 0.100000    Time 0.057434    
2022-01-29 03:22:12,732 - Epoch: [12][  200/  391]    Overall Loss 2.288343    Objective Loss 2.288343                                        LR 0.100000    Time 0.055738    
2022-01-29 03:22:18,168 - Epoch: [12][  300/  391]    Overall Loss 2.287601    Objective Loss 2.287601                                        LR 0.100000    Time 0.055278    
2022-01-29 03:22:23,095 - Epoch: [12][  391/  391]    Overall Loss 2.284167    Objective Loss 2.284167    Top1 36.057692    Top5 69.230769    LR 0.100000    Time 0.055011    
2022-01-29 03:22:23,155 - --- validate (epoch=12)-----------
2022-01-29 03:22:23,155 - 10000 samples (128 per mini-batch)
2022-01-29 03:22:24,996 - Epoch: [12][   79/   79]    Loss 2.316665    Top1 37.950000    Top5 71.060000    
2022-01-29 03:22:25,049 - ==> Top1: 37.950    Top5: 71.060    Loss: 2.317

2022-01-29 03:22:25,054 - ==> Best [Top1: 37.950   Top5: 71.060   Sparsity:0.00   Params: 1341960 on epoch: 12]
2022-01-29 03:22:25,054 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:22:25,109 - 

2022-01-29 03:22:25,109 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:22:30,854 - Epoch: [13][  100/  391]    Overall Loss 2.219423    Objective Loss 2.219423                                        LR 0.100000    Time 0.057415    
2022-01-29 03:22:36,276 - Epoch: [13][  200/  391]    Overall Loss 2.215151    Objective Loss 2.215151                                        LR 0.100000    Time 0.055815    
2022-01-29 03:22:41,707 - Epoch: [13][  300/  391]    Overall Loss 2.220848    Objective Loss 2.220848                                        LR 0.100000    Time 0.055310    
2022-01-29 03:22:46,620 - Epoch: [13][  391/  391]    Overall Loss 2.225458    Objective Loss 2.225458    Top1 39.903846    Top5 67.307692    LR 0.100000    Time 0.055000    
2022-01-29 03:22:46,682 - --- validate (epoch=13)-----------
2022-01-29 03:22:46,682 - 10000 samples (128 per mini-batch)
2022-01-29 03:22:48,529 - Epoch: [13][   79/   79]    Loss 2.670977    Top1 33.020000    Top5 64.080000    
2022-01-29 03:22:48,581 - ==> Top1: 33.020    Top5: 64.080    Loss: 2.671

2022-01-29 03:22:48,587 - ==> Best [Top1: 37.950   Top5: 71.060   Sparsity:0.00   Params: 1341960 on epoch: 12]
2022-01-29 03:22:48,587 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:22:48,629 - 

2022-01-29 03:22:48,629 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:22:54,282 - Epoch: [14][  100/  391]    Overall Loss 2.172422    Objective Loss 2.172422                                        LR 0.100000    Time 0.056492    
2022-01-29 03:22:59,667 - Epoch: [14][  200/  391]    Overall Loss 2.181182    Objective Loss 2.181182                                        LR 0.100000    Time 0.055167    
2022-01-29 03:23:05,046 - Epoch: [14][  300/  391]    Overall Loss 2.181975    Objective Loss 2.181975                                        LR 0.100000    Time 0.054708    
2022-01-29 03:23:09,931 - Epoch: [14][  391/  391]    Overall Loss 2.182947    Objective Loss 2.182947    Top1 37.019231    Top5 71.634615    LR 0.100000    Time 0.054465    
2022-01-29 03:23:09,988 - --- validate (epoch=14)-----------
2022-01-29 03:23:09,988 - 10000 samples (128 per mini-batch)
2022-01-29 03:23:11,784 - Epoch: [14][   79/   79]    Loss 2.628926    Top1 32.260000    Top5 66.410000    
2022-01-29 03:23:11,832 - ==> Top1: 32.260    Top5: 66.410    Loss: 2.629

2022-01-29 03:23:11,837 - ==> Best [Top1: 37.950   Top5: 71.060   Sparsity:0.00   Params: 1341960 on epoch: 12]
2022-01-29 03:23:11,837 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:23:11,884 - 

2022-01-29 03:23:11,884 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:23:17,577 - Epoch: [15][  100/  391]    Overall Loss 2.142978    Objective Loss 2.142978                                        LR 0.100000    Time 0.056901    
2022-01-29 03:23:22,973 - Epoch: [15][  200/  391]    Overall Loss 2.135173    Objective Loss 2.135173                                        LR 0.100000    Time 0.055428    
2022-01-29 03:23:28,366 - Epoch: [15][  300/  391]    Overall Loss 2.132474    Objective Loss 2.132474                                        LR 0.100000    Time 0.054925    
2022-01-29 03:23:33,313 - Epoch: [15][  391/  391]    Overall Loss 2.134571    Objective Loss 2.134571    Top1 47.596154    Top5 74.519231    LR 0.100000    Time 0.054793    
2022-01-29 03:23:33,373 - --- validate (epoch=15)-----------
2022-01-29 03:23:33,373 - 10000 samples (128 per mini-batch)
2022-01-29 03:23:35,223 - Epoch: [15][   79/   79]    Loss 2.287636    Top1 38.970000    Top5 71.730000    
2022-01-29 03:23:35,278 - ==> Top1: 38.970    Top5: 71.730    Loss: 2.288

2022-01-29 03:23:35,283 - ==> Best [Top1: 38.970   Top5: 71.730   Sparsity:0.00   Params: 1341960 on epoch: 15]
2022-01-29 03:23:35,283 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:23:35,334 - 

2022-01-29 03:23:35,334 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:23:41,064 - Epoch: [16][  100/  391]    Overall Loss 2.054071    Objective Loss 2.054071                                        LR 0.100000    Time 0.057271    
2022-01-29 03:23:46,485 - Epoch: [16][  200/  391]    Overall Loss 2.076955    Objective Loss 2.076955                                        LR 0.100000    Time 0.055735    
2022-01-29 03:23:51,882 - Epoch: [16][  300/  391]    Overall Loss 2.079082    Objective Loss 2.079082                                        LR 0.100000    Time 0.055144    
2022-01-29 03:23:56,811 - Epoch: [16][  391/  391]    Overall Loss 2.076232    Objective Loss 2.076232    Top1 39.423077    Top5 76.442308    LR 0.100000    Time 0.054914    
2022-01-29 03:23:56,868 - --- validate (epoch=16)-----------
2022-01-29 03:23:56,868 - 10000 samples (128 per mini-batch)
2022-01-29 03:23:58,696 - Epoch: [16][   79/   79]    Loss 2.675219    Top1 32.290000    Top5 65.040000    
2022-01-29 03:23:58,749 - ==> Top1: 32.290    Top5: 65.040    Loss: 2.675

2022-01-29 03:23:58,754 - ==> Best [Top1: 38.970   Top5: 71.730   Sparsity:0.00   Params: 1341960 on epoch: 15]
2022-01-29 03:23:58,754 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:23:58,795 - 

2022-01-29 03:23:58,795 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:24:04,417 - Epoch: [17][  100/  391]    Overall Loss 2.035999    Objective Loss 2.035999                                        LR 0.100000    Time 0.056192    
2022-01-29 03:24:09,837 - Epoch: [17][  200/  391]    Overall Loss 2.040318    Objective Loss 2.040318                                        LR 0.100000    Time 0.055191    
2022-01-29 03:24:15,255 - Epoch: [17][  300/  391]    Overall Loss 2.046333    Objective Loss 2.046333                                        LR 0.100000    Time 0.054852    
2022-01-29 03:24:20,183 - Epoch: [17][  391/  391]    Overall Loss 2.051270    Objective Loss 2.051270    Top1 42.788462    Top5 77.403846    LR 0.100000    Time 0.054685    
2022-01-29 03:24:20,240 - --- validate (epoch=17)-----------
2022-01-29 03:24:20,240 - 10000 samples (128 per mini-batch)
2022-01-29 03:24:22,072 - Epoch: [17][   79/   79]    Loss 2.582381    Top1 35.240000    Top5 67.550000    
2022-01-29 03:24:22,132 - ==> Top1: 35.240    Top5: 67.550    Loss: 2.582

2022-01-29 03:24:22,137 - ==> Best [Top1: 38.970   Top5: 71.730   Sparsity:0.00   Params: 1341960 on epoch: 15]
2022-01-29 03:24:22,137 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:24:22,177 - 

2022-01-29 03:24:22,177 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:24:27,847 - Epoch: [18][  100/  391]    Overall Loss 2.014619    Objective Loss 2.014619                                        LR 0.100000    Time 0.056671    
2022-01-29 03:24:33,219 - Epoch: [18][  200/  391]    Overall Loss 2.018931    Objective Loss 2.018931                                        LR 0.100000    Time 0.055193    
2022-01-29 03:24:38,604 - Epoch: [18][  300/  391]    Overall Loss 2.025929    Objective Loss 2.025929                                        LR 0.100000    Time 0.054741    
2022-01-29 03:24:43,505 - Epoch: [18][  391/  391]    Overall Loss 2.022827    Objective Loss 2.022827    Top1 44.711538    Top5 74.519231    LR 0.100000    Time 0.054534    
2022-01-29 03:24:43,564 - --- validate (epoch=18)-----------
2022-01-29 03:24:43,564 - 10000 samples (128 per mini-batch)
2022-01-29 03:24:45,425 - Epoch: [18][   79/   79]    Loss 2.171218    Top1 40.990000    Top5 74.020000    
2022-01-29 03:24:45,479 - ==> Top1: 40.990    Top5: 74.020    Loss: 2.171

2022-01-29 03:24:45,484 - ==> Best [Top1: 40.990   Top5: 74.020   Sparsity:0.00   Params: 1341960 on epoch: 18]
2022-01-29 03:24:45,484 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:24:45,537 - 

2022-01-29 03:24:45,537 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:24:51,237 - Epoch: [19][  100/  391]    Overall Loss 1.972324    Objective Loss 1.972324                                        LR 0.100000    Time 0.056975    
2022-01-29 03:24:56,603 - Epoch: [19][  200/  391]    Overall Loss 1.967302    Objective Loss 1.967302                                        LR 0.100000    Time 0.055310    
2022-01-29 03:25:02,056 - Epoch: [19][  300/  391]    Overall Loss 1.974227    Objective Loss 1.974227                                        LR 0.100000    Time 0.055047    
2022-01-29 03:25:06,980 - Epoch: [19][  391/  391]    Overall Loss 1.972354    Objective Loss 1.972354    Top1 49.519231    Top5 78.846154    LR 0.100000    Time 0.054829    
2022-01-29 03:25:07,044 - --- validate (epoch=19)-----------
2022-01-29 03:25:07,044 - 10000 samples (128 per mini-batch)
2022-01-29 03:25:08,850 - Epoch: [19][   79/   79]    Loss 2.285103    Top1 38.730000    Top5 71.930000    
2022-01-29 03:25:08,903 - ==> Top1: 38.730    Top5: 71.930    Loss: 2.285

2022-01-29 03:25:08,908 - ==> Best [Top1: 40.990   Top5: 74.020   Sparsity:0.00   Params: 1341960 on epoch: 18]
2022-01-29 03:25:08,908 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:25:08,955 - 

2022-01-29 03:25:08,955 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:25:14,560 - Epoch: [20][  100/  391]    Overall Loss 1.931160    Objective Loss 1.931160                                        LR 0.100000    Time 0.056023    
2022-01-29 03:25:19,942 - Epoch: [20][  200/  391]    Overall Loss 1.932467    Objective Loss 1.932467                                        LR 0.100000    Time 0.054918    
2022-01-29 03:25:25,326 - Epoch: [20][  300/  391]    Overall Loss 1.937530    Objective Loss 1.937530                                        LR 0.100000    Time 0.054557    
2022-01-29 03:25:30,222 - Epoch: [20][  391/  391]    Overall Loss 1.936341    Objective Loss 1.936341    Top1 44.711538    Top5 81.250000    LR 0.100000    Time 0.054378    
2022-01-29 03:25:30,278 - --- validate (epoch=20)-----------
2022-01-29 03:25:30,278 - 10000 samples (128 per mini-batch)
2022-01-29 03:25:32,094 - Epoch: [20][   79/   79]    Loss 2.200286    Top1 41.180000    Top5 74.170000    
2022-01-29 03:25:32,149 - ==> Top1: 41.180    Top5: 74.170    Loss: 2.200

2022-01-29 03:25:32,154 - ==> Best [Top1: 41.180   Top5: 74.170   Sparsity:0.00   Params: 1341960 on epoch: 20]
2022-01-29 03:25:32,154 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:25:32,201 - 

2022-01-29 03:25:32,202 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:25:37,924 - Epoch: [21][  100/  391]    Overall Loss 1.929758    Objective Loss 1.929758                                        LR 0.100000    Time 0.057193    
2022-01-29 03:25:43,355 - Epoch: [21][  200/  391]    Overall Loss 1.917330    Objective Loss 1.917330                                        LR 0.100000    Time 0.055749    
2022-01-29 03:25:48,757 - Epoch: [21][  300/  391]    Overall Loss 1.916157    Objective Loss 1.916157                                        LR 0.100000    Time 0.055169    
2022-01-29 03:25:53,654 - Epoch: [21][  391/  391]    Overall Loss 1.916990    Objective Loss 1.916990    Top1 46.153846    Top5 81.730769    LR 0.100000    Time 0.054853    
2022-01-29 03:25:53,713 - --- validate (epoch=21)-----------
2022-01-29 03:25:53,713 - 10000 samples (128 per mini-batch)
2022-01-29 03:25:55,520 - Epoch: [21][   79/   79]    Loss 2.155780    Top1 42.150000    Top5 74.090000    
2022-01-29 03:25:55,573 - ==> Top1: 42.150    Top5: 74.090    Loss: 2.156

2022-01-29 03:25:55,579 - ==> Best [Top1: 42.150   Top5: 74.090   Sparsity:0.00   Params: 1341960 on epoch: 21]
2022-01-29 03:25:55,579 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:25:55,634 - 

2022-01-29 03:25:55,634 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:26:01,311 - Epoch: [22][  100/  391]    Overall Loss 1.873611    Objective Loss 1.873611                                        LR 0.100000    Time 0.056739    
2022-01-29 03:26:06,702 - Epoch: [22][  200/  391]    Overall Loss 1.866110    Objective Loss 1.866110                                        LR 0.100000    Time 0.055320    
2022-01-29 03:26:12,102 - Epoch: [22][  300/  391]    Overall Loss 1.880875    Objective Loss 1.880875                                        LR 0.100000    Time 0.054878    
2022-01-29 03:26:17,014 - Epoch: [22][  391/  391]    Overall Loss 1.881138    Objective Loss 1.881138    Top1 46.153846    Top5 80.288462    LR 0.100000    Time 0.054664    
2022-01-29 03:26:17,071 - --- validate (epoch=22)-----------
2022-01-29 03:26:17,071 - 10000 samples (128 per mini-batch)
2022-01-29 03:26:18,934 - Epoch: [22][   79/   79]    Loss 2.155043    Top1 42.100000    Top5 74.790000    
2022-01-29 03:26:18,986 - ==> Top1: 42.100    Top5: 74.790    Loss: 2.155

2022-01-29 03:26:18,992 - ==> Best [Top1: 42.150   Top5: 74.090   Sparsity:0.00   Params: 1341960 on epoch: 21]
2022-01-29 03:26:18,992 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:26:19,039 - 

2022-01-29 03:26:19,040 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:26:24,722 - Epoch: [23][  100/  391]    Overall Loss 1.832587    Objective Loss 1.832587                                        LR 0.100000    Time 0.056794    
2022-01-29 03:26:30,133 - Epoch: [23][  200/  391]    Overall Loss 1.847023    Objective Loss 1.847023                                        LR 0.100000    Time 0.055449    
2022-01-29 03:26:35,545 - Epoch: [23][  300/  391]    Overall Loss 1.853224    Objective Loss 1.853224                                        LR 0.100000    Time 0.055003    
2022-01-29 03:26:40,488 - Epoch: [23][  391/  391]    Overall Loss 1.859181    Objective Loss 1.859181    Top1 44.230769    Top5 75.000000    LR 0.100000    Time 0.054841    
2022-01-29 03:26:40,551 - --- validate (epoch=23)-----------
2022-01-29 03:26:40,552 - 10000 samples (128 per mini-batch)
2022-01-29 03:26:42,402 - Epoch: [23][   79/   79]    Loss 2.128199    Top1 42.580000    Top5 75.590000    
2022-01-29 03:26:42,459 - ==> Top1: 42.580    Top5: 75.590    Loss: 2.128

2022-01-29 03:26:42,464 - ==> Best [Top1: 42.580   Top5: 75.590   Sparsity:0.00   Params: 1341960 on epoch: 23]
2022-01-29 03:26:42,464 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:26:42,515 - 

2022-01-29 03:26:42,515 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:26:48,223 - Epoch: [24][  100/  391]    Overall Loss 1.800611    Objective Loss 1.800611                                        LR 0.100000    Time 0.057047    
2022-01-29 03:26:53,619 - Epoch: [24][  200/  391]    Overall Loss 1.810458    Objective Loss 1.810458                                        LR 0.100000    Time 0.055503    
2022-01-29 03:26:59,008 - Epoch: [24][  300/  391]    Overall Loss 1.829245    Objective Loss 1.829245                                        LR 0.100000    Time 0.054961    
2022-01-29 03:27:03,912 - Epoch: [24][  391/  391]    Overall Loss 1.832990    Objective Loss 1.832990    Top1 49.038462    Top5 83.173077    LR 0.100000    Time 0.054709    
2022-01-29 03:27:03,970 - --- validate (epoch=24)-----------
2022-01-29 03:27:03,971 - 10000 samples (128 per mini-batch)
2022-01-29 03:27:05,805 - Epoch: [24][   79/   79]    Loss 2.228778    Top1 41.270000    Top5 74.280000    
2022-01-29 03:27:05,854 - ==> Top1: 41.270    Top5: 74.280    Loss: 2.229

2022-01-29 03:27:05,859 - ==> Best [Top1: 42.580   Top5: 75.590   Sparsity:0.00   Params: 1341960 on epoch: 23]
2022-01-29 03:27:05,859 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:27:05,906 - 

2022-01-29 03:27:05,907 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:27:11,609 - Epoch: [25][  100/  391]    Overall Loss 1.783659    Objective Loss 1.783659                                        LR 0.100000    Time 0.056995    
2022-01-29 03:27:16,974 - Epoch: [25][  200/  391]    Overall Loss 1.791648    Objective Loss 1.791648                                        LR 0.100000    Time 0.055318    
2022-01-29 03:27:22,346 - Epoch: [25][  300/  391]    Overall Loss 1.800258    Objective Loss 1.800258                                        LR 0.100000    Time 0.054782    
2022-01-29 03:27:27,238 - Epoch: [25][  391/  391]    Overall Loss 1.806472    Objective Loss 1.806472    Top1 49.519231    Top5 80.769231    LR 0.100000    Time 0.054544    
2022-01-29 03:27:27,298 - --- validate (epoch=25)-----------
2022-01-29 03:27:27,298 - 10000 samples (128 per mini-batch)
2022-01-29 03:27:29,158 - Epoch: [25][   79/   79]    Loss 1.978519    Top1 45.760000    Top5 77.980000    
2022-01-29 03:27:29,214 - ==> Top1: 45.760    Top5: 77.980    Loss: 1.979

2022-01-29 03:27:29,220 - ==> Best [Top1: 45.760   Top5: 77.980   Sparsity:0.00   Params: 1341960 on epoch: 25]
2022-01-29 03:27:29,220 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:27:29,271 - 

2022-01-29 03:27:29,271 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:27:34,884 - Epoch: [26][  100/  391]    Overall Loss 1.735152    Objective Loss 1.735152                                        LR 0.100000    Time 0.056106    
2022-01-29 03:27:40,269 - Epoch: [26][  200/  391]    Overall Loss 1.760433    Objective Loss 1.760433                                        LR 0.100000    Time 0.054972    
2022-01-29 03:27:45,655 - Epoch: [26][  300/  391]    Overall Loss 1.775478    Objective Loss 1.775478                                        LR 0.100000    Time 0.054599    
2022-01-29 03:27:50,551 - Epoch: [26][  391/  391]    Overall Loss 1.780075    Objective Loss 1.780075    Top1 49.038462    Top5 82.211538    LR 0.100000    Time 0.054412    
2022-01-29 03:27:50,610 - --- validate (epoch=26)-----------
2022-01-29 03:27:50,610 - 10000 samples (128 per mini-batch)
2022-01-29 03:27:52,447 - Epoch: [26][   79/   79]    Loss 2.410364    Top1 38.690000    Top5 70.450000    
2022-01-29 03:27:52,501 - ==> Top1: 38.690    Top5: 70.450    Loss: 2.410

2022-01-29 03:27:52,506 - ==> Best [Top1: 45.760   Top5: 77.980   Sparsity:0.00   Params: 1341960 on epoch: 25]
2022-01-29 03:27:52,506 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:27:52,554 - 

2022-01-29 03:27:52,554 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:27:58,253 - Epoch: [27][  100/  391]    Overall Loss 1.728256    Objective Loss 1.728256                                        LR 0.100000    Time 0.056960    
2022-01-29 03:28:03,648 - Epoch: [27][  200/  391]    Overall Loss 1.745220    Objective Loss 1.745220                                        LR 0.100000    Time 0.055449    
2022-01-29 03:28:09,049 - Epoch: [27][  300/  391]    Overall Loss 1.752855    Objective Loss 1.752855                                        LR 0.100000    Time 0.054968    
2022-01-29 03:28:13,957 - Epoch: [27][  391/  391]    Overall Loss 1.758872    Objective Loss 1.758872    Top1 52.403846    Top5 87.500000    LR 0.100000    Time 0.054726    
2022-01-29 03:28:14,016 - --- validate (epoch=27)-----------
2022-01-29 03:28:14,016 - 10000 samples (128 per mini-batch)
2022-01-29 03:28:15,843 - Epoch: [27][   79/   79]    Loss 2.106543    Top1 43.040000    Top5 76.480000    
2022-01-29 03:28:15,902 - ==> Top1: 43.040    Top5: 76.480    Loss: 2.107

2022-01-29 03:28:15,907 - ==> Best [Top1: 45.760   Top5: 77.980   Sparsity:0.00   Params: 1341960 on epoch: 25]
2022-01-29 03:28:15,907 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:28:15,955 - 

2022-01-29 03:28:15,955 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:28:21,618 - Epoch: [28][  100/  391]    Overall Loss 1.732524    Objective Loss 1.732524                                        LR 0.100000    Time 0.056602    
2022-01-29 03:28:27,013 - Epoch: [28][  200/  391]    Overall Loss 1.729984    Objective Loss 1.729984                                        LR 0.100000    Time 0.055274    
2022-01-29 03:28:32,409 - Epoch: [28][  300/  391]    Overall Loss 1.738973    Objective Loss 1.738973                                        LR 0.100000    Time 0.054834    
2022-01-29 03:28:37,349 - Epoch: [28][  391/  391]    Overall Loss 1.738005    Objective Loss 1.738005    Top1 54.807692    Top5 82.211538    LR 0.100000    Time 0.054703    
2022-01-29 03:28:37,413 - --- validate (epoch=28)-----------
2022-01-29 03:28:37,413 - 10000 samples (128 per mini-batch)
2022-01-29 03:28:39,265 - Epoch: [28][   79/   79]    Loss 2.336304    Top1 40.190000    Top5 72.820000    
2022-01-29 03:28:39,318 - ==> Top1: 40.190    Top5: 72.820    Loss: 2.336

2022-01-29 03:28:39,323 - ==> Best [Top1: 45.760   Top5: 77.980   Sparsity:0.00   Params: 1341960 on epoch: 25]
2022-01-29 03:28:39,323 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:28:39,371 - 

2022-01-29 03:28:39,371 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:28:45,038 - Epoch: [29][  100/  391]    Overall Loss 1.708636    Objective Loss 1.708636                                        LR 0.100000    Time 0.056640    
2022-01-29 03:28:50,428 - Epoch: [29][  200/  391]    Overall Loss 1.722762    Objective Loss 1.722762                                        LR 0.100000    Time 0.055263    
2022-01-29 03:28:55,823 - Epoch: [29][  300/  391]    Overall Loss 1.718645    Objective Loss 1.718645                                        LR 0.100000    Time 0.054826    
2022-01-29 03:29:00,744 - Epoch: [29][  391/  391]    Overall Loss 1.725386    Objective Loss 1.725386    Top1 53.846154    Top5 83.173077    LR 0.100000    Time 0.054648    
2022-01-29 03:29:00,799 - --- validate (epoch=29)-----------
2022-01-29 03:29:00,799 - 10000 samples (128 per mini-batch)
2022-01-29 03:29:02,634 - Epoch: [29][   79/   79]    Loss 2.011470    Top1 45.510000    Top5 77.180000    
2022-01-29 03:29:02,689 - ==> Top1: 45.510    Top5: 77.180    Loss: 2.011

2022-01-29 03:29:02,694 - ==> Best [Top1: 45.760   Top5: 77.980   Sparsity:0.00   Params: 1341960 on epoch: 25]
2022-01-29 03:29:02,694 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:29:02,741 - 

2022-01-29 03:29:02,741 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:29:08,428 - Epoch: [30][  100/  391]    Overall Loss 1.663216    Objective Loss 1.663216                                        LR 0.100000    Time 0.056845    
2022-01-29 03:29:13,869 - Epoch: [30][  200/  391]    Overall Loss 1.681935    Objective Loss 1.681935                                        LR 0.100000    Time 0.055621    
2022-01-29 03:29:19,292 - Epoch: [30][  300/  391]    Overall Loss 1.694246    Objective Loss 1.694246                                        LR 0.100000    Time 0.055155    
2022-01-29 03:29:24,223 - Epoch: [30][  391/  391]    Overall Loss 1.694899    Objective Loss 1.694899    Top1 50.480769    Top5 83.653846    LR 0.100000    Time 0.054927    
2022-01-29 03:29:24,282 - --- validate (epoch=30)-----------
2022-01-29 03:29:24,282 - 10000 samples (128 per mini-batch)
2022-01-29 03:29:26,136 - Epoch: [30][   79/   79]    Loss 2.179194    Top1 43.340000    Top5 75.160000    
2022-01-29 03:29:26,195 - ==> Top1: 43.340    Top5: 75.160    Loss: 2.179

2022-01-29 03:29:26,200 - ==> Best [Top1: 45.760   Top5: 77.980   Sparsity:0.00   Params: 1341960 on epoch: 25]
2022-01-29 03:29:26,200 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:29:26,242 - 

2022-01-29 03:29:26,242 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:29:31,939 - Epoch: [31][  100/  391]    Overall Loss 1.675211    Objective Loss 1.675211                                        LR 0.100000    Time 0.056938    
2022-01-29 03:29:37,338 - Epoch: [31][  200/  391]    Overall Loss 1.667144    Objective Loss 1.667144                                        LR 0.100000    Time 0.055464    
2022-01-29 03:29:42,735 - Epoch: [31][  300/  391]    Overall Loss 1.680678    Objective Loss 1.680678                                        LR 0.100000    Time 0.054962    
2022-01-29 03:29:47,635 - Epoch: [31][  391/  391]    Overall Loss 1.689432    Objective Loss 1.689432    Top1 54.326923    Top5 84.134615    LR 0.100000    Time 0.054700    
2022-01-29 03:29:47,694 - --- validate (epoch=31)-----------
2022-01-29 03:29:47,694 - 10000 samples (128 per mini-batch)
2022-01-29 03:29:49,553 - Epoch: [31][   79/   79]    Loss 2.386085    Top1 39.690000    Top5 71.390000    
2022-01-29 03:29:49,606 - ==> Top1: 39.690    Top5: 71.390    Loss: 2.386

2022-01-29 03:29:49,611 - ==> Best [Top1: 45.760   Top5: 77.980   Sparsity:0.00   Params: 1341960 on epoch: 25]
2022-01-29 03:29:49,612 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:29:49,659 - 

2022-01-29 03:29:49,660 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:29:55,261 - Epoch: [32][  100/  391]    Overall Loss 1.644601    Objective Loss 1.644601                                        LR 0.100000    Time 0.055988    
2022-01-29 03:30:00,631 - Epoch: [32][  200/  391]    Overall Loss 1.657884    Objective Loss 1.657884                                        LR 0.100000    Time 0.054841    
2022-01-29 03:30:06,009 - Epoch: [32][  300/  391]    Overall Loss 1.658665    Objective Loss 1.658665                                        LR 0.100000    Time 0.054485    
2022-01-29 03:30:10,906 - Epoch: [32][  391/  391]    Overall Loss 1.662540    Objective Loss 1.662540    Top1 53.846154    Top5 85.576923    LR 0.100000    Time 0.054327    
2022-01-29 03:30:10,959 - --- validate (epoch=32)-----------
2022-01-29 03:30:10,959 - 10000 samples (128 per mini-batch)
2022-01-29 03:30:12,812 - Epoch: [32][   79/   79]    Loss 2.517945    Top1 37.780000    Top5 70.480000    
2022-01-29 03:30:12,863 - ==> Top1: 37.780    Top5: 70.480    Loss: 2.518

2022-01-29 03:30:12,868 - ==> Best [Top1: 45.760   Top5: 77.980   Sparsity:0.00   Params: 1341960 on epoch: 25]
2022-01-29 03:30:12,868 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:30:12,916 - 

2022-01-29 03:30:12,916 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:30:18,629 - Epoch: [33][  100/  391]    Overall Loss 1.611248    Objective Loss 1.611248                                        LR 0.100000    Time 0.057098    
2022-01-29 03:30:23,990 - Epoch: [33][  200/  391]    Overall Loss 1.622848    Objective Loss 1.622848                                        LR 0.100000    Time 0.055352    
2022-01-29 03:30:29,316 - Epoch: [33][  300/  391]    Overall Loss 1.633576    Objective Loss 1.633576                                        LR 0.100000    Time 0.054650    
2022-01-29 03:30:34,147 - Epoch: [33][  391/  391]    Overall Loss 1.642604    Objective Loss 1.642604    Top1 50.000000    Top5 82.211538    LR 0.100000    Time 0.054285    
2022-01-29 03:30:34,210 - --- validate (epoch=33)-----------
2022-01-29 03:30:34,210 - 10000 samples (128 per mini-batch)
2022-01-29 03:30:36,028 - Epoch: [33][   79/   79]    Loss 1.889203    Top1 48.670000    Top5 79.940000    
2022-01-29 03:30:36,080 - ==> Top1: 48.670    Top5: 79.940    Loss: 1.889

2022-01-29 03:30:36,086 - ==> Best [Top1: 48.670   Top5: 79.940   Sparsity:0.00   Params: 1341960 on epoch: 33]
2022-01-29 03:30:36,086 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:30:36,141 - 

2022-01-29 03:30:36,141 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:30:41,765 - Epoch: [34][  100/  391]    Overall Loss 1.608581    Objective Loss 1.608581                                        LR 0.100000    Time 0.056213    
2022-01-29 03:30:47,090 - Epoch: [34][  200/  391]    Overall Loss 1.615576    Objective Loss 1.615576                                        LR 0.100000    Time 0.054723    
2022-01-29 03:30:52,462 - Epoch: [34][  300/  391]    Overall Loss 1.621480    Objective Loss 1.621480                                        LR 0.100000    Time 0.054386    
2022-01-29 03:30:57,388 - Epoch: [34][  391/  391]    Overall Loss 1.627256    Objective Loss 1.627256    Top1 53.846154    Top5 86.538462    LR 0.100000    Time 0.054325    
2022-01-29 03:30:57,452 - --- validate (epoch=34)-----------
2022-01-29 03:30:57,452 - 10000 samples (128 per mini-batch)
2022-01-29 03:30:59,285 - Epoch: [34][   79/   79]    Loss 2.075504    Top1 44.810000    Top5 77.520000    
2022-01-29 03:30:59,339 - ==> Top1: 44.810    Top5: 77.520    Loss: 2.076

2022-01-29 03:30:59,344 - ==> Best [Top1: 48.670   Top5: 79.940   Sparsity:0.00   Params: 1341960 on epoch: 33]
2022-01-29 03:30:59,344 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:30:59,387 - 

2022-01-29 03:30:59,387 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:31:04,998 - Epoch: [35][  100/  391]    Overall Loss 1.588722    Objective Loss 1.588722                                        LR 0.100000    Time 0.056081    
2022-01-29 03:31:10,334 - Epoch: [35][  200/  391]    Overall Loss 1.599977    Objective Loss 1.599977                                        LR 0.100000    Time 0.054716    
2022-01-29 03:31:15,744 - Epoch: [35][  300/  391]    Overall Loss 1.619292    Objective Loss 1.619292                                        LR 0.100000    Time 0.054509    
2022-01-29 03:31:20,643 - Epoch: [35][  391/  391]    Overall Loss 1.625960    Objective Loss 1.625960    Top1 50.961538    Top5 85.096154    LR 0.100000    Time 0.054349    
2022-01-29 03:31:20,698 - --- validate (epoch=35)-----------
2022-01-29 03:31:20,699 - 10000 samples (128 per mini-batch)
2022-01-29 03:31:22,651 - Epoch: [35][   79/   79]    Loss 1.986174    Top1 46.490000    Top5 78.850000    
2022-01-29 03:31:22,707 - ==> Top1: 46.490    Top5: 78.850    Loss: 1.986

2022-01-29 03:31:22,712 - ==> Best [Top1: 48.670   Top5: 79.940   Sparsity:0.00   Params: 1341960 on epoch: 33]
2022-01-29 03:31:22,712 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:31:22,760 - 

2022-01-29 03:31:22,760 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:31:28,463 - Epoch: [36][  100/  391]    Overall Loss 1.559261    Objective Loss 1.559261                                        LR 0.100000    Time 0.057001    
2022-01-29 03:31:33,850 - Epoch: [36][  200/  391]    Overall Loss 1.589343    Objective Loss 1.589343                                        LR 0.100000    Time 0.055432    
2022-01-29 03:31:39,202 - Epoch: [36][  300/  391]    Overall Loss 1.591627    Objective Loss 1.591627                                        LR 0.100000    Time 0.054791    
2022-01-29 03:31:44,089 - Epoch: [36][  391/  391]    Overall Loss 1.599594    Objective Loss 1.599594    Top1 58.653846    Top5 89.903846    LR 0.100000    Time 0.054536    
2022-01-29 03:31:44,145 - --- validate (epoch=36)-----------
2022-01-29 03:31:44,146 - 10000 samples (128 per mini-batch)
2022-01-29 03:31:45,978 - Epoch: [36][   79/   79]    Loss 1.940719    Top1 47.580000    Top5 79.060000    
2022-01-29 03:31:46,034 - ==> Top1: 47.580    Top5: 79.060    Loss: 1.941

2022-01-29 03:31:46,039 - ==> Best [Top1: 48.670   Top5: 79.940   Sparsity:0.00   Params: 1341960 on epoch: 33]
2022-01-29 03:31:46,039 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:31:46,083 - 

2022-01-29 03:31:46,083 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:31:51,767 - Epoch: [37][  100/  391]    Overall Loss 1.573776    Objective Loss 1.573776                                        LR 0.100000    Time 0.056815    
2022-01-29 03:31:57,167 - Epoch: [37][  200/  391]    Overall Loss 1.584677    Objective Loss 1.584677                                        LR 0.100000    Time 0.055403    
2022-01-29 03:32:02,544 - Epoch: [37][  300/  391]    Overall Loss 1.581606    Objective Loss 1.581606                                        LR 0.100000    Time 0.054857    
2022-01-29 03:32:07,422 - Epoch: [37][  391/  391]    Overall Loss 1.588271    Objective Loss 1.588271    Top1 54.807692    Top5 80.288462    LR 0.100000    Time 0.054562    
2022-01-29 03:32:07,478 - --- validate (epoch=37)-----------
2022-01-29 03:32:07,478 - 10000 samples (128 per mini-batch)
2022-01-29 03:32:09,349 - Epoch: [37][   79/   79]    Loss 1.914861    Top1 48.150000    Top5 79.380000    
2022-01-29 03:32:09,404 - ==> Top1: 48.150    Top5: 79.380    Loss: 1.915

2022-01-29 03:32:09,410 - ==> Best [Top1: 48.670   Top5: 79.940   Sparsity:0.00   Params: 1341960 on epoch: 33]
2022-01-29 03:32:09,410 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:32:09,458 - 

2022-01-29 03:32:09,458 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:32:15,068 - Epoch: [38][  100/  391]    Overall Loss 1.533660    Objective Loss 1.533660                                        LR 0.100000    Time 0.056074    
2022-01-29 03:32:20,419 - Epoch: [38][  200/  391]    Overall Loss 1.551949    Objective Loss 1.551949                                        LR 0.100000    Time 0.054788    
2022-01-29 03:32:25,791 - Epoch: [38][  300/  391]    Overall Loss 1.565107    Objective Loss 1.565107                                        LR 0.100000    Time 0.054428    
2022-01-29 03:32:30,636 - Epoch: [38][  391/  391]    Overall Loss 1.567172    Objective Loss 1.567172    Top1 56.730769    Top5 86.538462    LR 0.100000    Time 0.054150    
2022-01-29 03:32:30,689 - --- validate (epoch=38)-----------
2022-01-29 03:32:30,690 - 10000 samples (128 per mini-batch)
2022-01-29 03:32:32,650 - Epoch: [38][   79/   79]    Loss 2.014562    Top1 46.680000    Top5 77.370000    
2022-01-29 03:32:32,704 - ==> Top1: 46.680    Top5: 77.370    Loss: 2.015

2022-01-29 03:32:32,709 - ==> Best [Top1: 48.670   Top5: 79.940   Sparsity:0.00   Params: 1341960 on epoch: 33]
2022-01-29 03:32:32,709 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:32:32,756 - 

2022-01-29 03:32:32,756 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:32:38,388 - Epoch: [39][  100/  391]    Overall Loss 1.520610    Objective Loss 1.520610                                        LR 0.100000    Time 0.056292    
2022-01-29 03:32:43,725 - Epoch: [39][  200/  391]    Overall Loss 1.543616    Objective Loss 1.543616                                        LR 0.100000    Time 0.054830    
2022-01-29 03:32:49,084 - Epoch: [39][  300/  391]    Overall Loss 1.556284    Objective Loss 1.556284                                        LR 0.100000    Time 0.054413    
2022-01-29 03:32:53,941 - Epoch: [39][  391/  391]    Overall Loss 1.562500    Objective Loss 1.562500    Top1 58.173077    Top5 86.057692    LR 0.100000    Time 0.054168    
2022-01-29 03:32:53,993 - --- validate (epoch=39)-----------
2022-01-29 03:32:53,993 - 10000 samples (128 per mini-batch)
2022-01-29 03:32:55,806 - Epoch: [39][   79/   79]    Loss 1.904851    Top1 47.790000    Top5 79.570000    
2022-01-29 03:32:55,864 - ==> Top1: 47.790    Top5: 79.570    Loss: 1.905

2022-01-29 03:32:55,869 - ==> Best [Top1: 48.670   Top5: 79.940   Sparsity:0.00   Params: 1341960 on epoch: 33]
2022-01-29 03:32:55,869 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:32:55,915 - 

2022-01-29 03:32:55,915 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:33:01,539 - Epoch: [40][  100/  391]    Overall Loss 1.508263    Objective Loss 1.508263                                        LR 0.100000    Time 0.056208    
2022-01-29 03:33:06,849 - Epoch: [40][  200/  391]    Overall Loss 1.530686    Objective Loss 1.530686                                        LR 0.100000    Time 0.054649    
2022-01-29 03:33:12,195 - Epoch: [40][  300/  391]    Overall Loss 1.541798    Objective Loss 1.541798                                        LR 0.100000    Time 0.054249    
2022-01-29 03:33:17,056 - Epoch: [40][  391/  391]    Overall Loss 1.536930    Objective Loss 1.536930    Top1 56.730769    Top5 87.980769    LR 0.100000    Time 0.054054    
2022-01-29 03:33:17,113 - --- validate (epoch=40)-----------
2022-01-29 03:33:17,113 - 10000 samples (128 per mini-batch)
2022-01-29 03:33:18,929 - Epoch: [40][   79/   79]    Loss 1.975185    Top1 47.710000    Top5 78.020000    
2022-01-29 03:33:18,978 - ==> Top1: 47.710    Top5: 78.020    Loss: 1.975

2022-01-29 03:33:18,983 - ==> Best [Top1: 48.670   Top5: 79.940   Sparsity:0.00   Params: 1341960 on epoch: 33]
2022-01-29 03:33:18,983 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:33:19,031 - 

2022-01-29 03:33:19,031 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:33:24,644 - Epoch: [41][  100/  391]    Overall Loss 1.510951    Objective Loss 1.510951                                        LR 0.100000    Time 0.056098    
2022-01-29 03:33:30,022 - Epoch: [41][  200/  391]    Overall Loss 1.513002    Objective Loss 1.513002                                        LR 0.100000    Time 0.054938    
2022-01-29 03:33:35,402 - Epoch: [41][  300/  391]    Overall Loss 1.525381    Objective Loss 1.525381                                        LR 0.100000    Time 0.054556    
2022-01-29 03:33:40,274 - Epoch: [41][  391/  391]    Overall Loss 1.530906    Objective Loss 1.530906    Top1 61.538462    Top5 85.096154    LR 0.100000    Time 0.054317    
2022-01-29 03:33:40,335 - --- validate (epoch=41)-----------
2022-01-29 03:33:40,336 - 10000 samples (128 per mini-batch)
2022-01-29 03:33:42,187 - Epoch: [41][   79/   79]    Loss 2.055439    Top1 45.640000    Top5 77.970000    
2022-01-29 03:33:42,240 - ==> Top1: 45.640    Top5: 77.970    Loss: 2.055

2022-01-29 03:33:42,246 - ==> Best [Top1: 48.670   Top5: 79.940   Sparsity:0.00   Params: 1341960 on epoch: 33]
2022-01-29 03:33:42,246 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:33:42,294 - 

2022-01-29 03:33:42,294 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:33:47,999 - Epoch: [42][  100/  391]    Overall Loss 1.503838    Objective Loss 1.503838                                        LR 0.100000    Time 0.057023    
2022-01-29 03:33:53,343 - Epoch: [42][  200/  391]    Overall Loss 1.494431    Objective Loss 1.494431                                        LR 0.100000    Time 0.055227    
2022-01-29 03:33:58,668 - Epoch: [42][  300/  391]    Overall Loss 1.510199    Objective Loss 1.510199                                        LR 0.100000    Time 0.054563    
2022-01-29 03:34:03,535 - Epoch: [42][  391/  391]    Overall Loss 1.520239    Objective Loss 1.520239    Top1 55.288462    Top5 88.942308    LR 0.100000    Time 0.054311    
2022-01-29 03:34:03,593 - --- validate (epoch=42)-----------
2022-01-29 03:34:03,594 - 10000 samples (128 per mini-batch)
2022-01-29 03:34:05,407 - Epoch: [42][   79/   79]    Loss 1.897511    Top1 48.440000    Top5 79.770000    
2022-01-29 03:34:05,463 - ==> Top1: 48.440    Top5: 79.770    Loss: 1.898

2022-01-29 03:34:05,468 - ==> Best [Top1: 48.670   Top5: 79.940   Sparsity:0.00   Params: 1341960 on epoch: 33]
2022-01-29 03:34:05,468 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:34:05,515 - 

2022-01-29 03:34:05,515 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:34:11,185 - Epoch: [43][  100/  391]    Overall Loss 1.453833    Objective Loss 1.453833                                        LR 0.100000    Time 0.056667    
2022-01-29 03:34:16,579 - Epoch: [43][  200/  391]    Overall Loss 1.479406    Objective Loss 1.479406                                        LR 0.100000    Time 0.055300    
2022-01-29 03:34:21,977 - Epoch: [43][  300/  391]    Overall Loss 1.494850    Objective Loss 1.494850                                        LR 0.100000    Time 0.054857    
2022-01-29 03:34:26,889 - Epoch: [43][  391/  391]    Overall Loss 1.504968    Objective Loss 1.504968    Top1 61.538462    Top5 86.538462    LR 0.100000    Time 0.054652    
2022-01-29 03:34:26,948 - --- validate (epoch=43)-----------
2022-01-29 03:34:26,948 - 10000 samples (128 per mini-batch)
2022-01-29 03:34:28,791 - Epoch: [43][   79/   79]    Loss 1.907894    Top1 48.320000    Top5 80.270000    
2022-01-29 03:34:28,843 - ==> Top1: 48.320    Top5: 80.270    Loss: 1.908

2022-01-29 03:34:28,848 - ==> Best [Top1: 48.670   Top5: 79.940   Sparsity:0.00   Params: 1341960 on epoch: 33]
2022-01-29 03:34:28,848 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:34:28,896 - 

2022-01-29 03:34:28,896 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:34:34,498 - Epoch: [44][  100/  391]    Overall Loss 1.477546    Objective Loss 1.477546                                        LR 0.100000    Time 0.055994    
2022-01-29 03:34:39,908 - Epoch: [44][  200/  391]    Overall Loss 1.487112    Objective Loss 1.487112                                        LR 0.100000    Time 0.055040    
2022-01-29 03:34:45,322 - Epoch: [44][  300/  391]    Overall Loss 1.495287    Objective Loss 1.495287                                        LR 0.100000    Time 0.054739    
2022-01-29 03:34:50,242 - Epoch: [44][  391/  391]    Overall Loss 1.499830    Objective Loss 1.499830    Top1 52.884615    Top5 83.173077    LR 0.100000    Time 0.054581    
2022-01-29 03:34:50,300 - --- validate (epoch=44)-----------
2022-01-29 03:34:50,300 - 10000 samples (128 per mini-batch)
2022-01-29 03:34:52,150 - Epoch: [44][   79/   79]    Loss 1.825724    Top1 50.870000    Top5 81.250000    
2022-01-29 03:34:52,205 - ==> Top1: 50.870    Top5: 81.250    Loss: 1.826

2022-01-29 03:34:52,211 - ==> Best [Top1: 50.870   Top5: 81.250   Sparsity:0.00   Params: 1341960 on epoch: 44]
2022-01-29 03:34:52,211 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:34:52,268 - 

2022-01-29 03:34:52,268 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:34:57,972 - Epoch: [45][  100/  391]    Overall Loss 1.418489    Objective Loss 1.418489                                        LR 0.100000    Time 0.057012    
2022-01-29 03:35:03,341 - Epoch: [45][  200/  391]    Overall Loss 1.443238    Objective Loss 1.443238                                        LR 0.100000    Time 0.055346    
2022-01-29 03:35:08,671 - Epoch: [45][  300/  391]    Overall Loss 1.460571    Objective Loss 1.460571                                        LR 0.100000    Time 0.054662    
2022-01-29 03:35:13,509 - Epoch: [45][  391/  391]    Overall Loss 1.477119    Objective Loss 1.477119    Top1 54.326923    Top5 84.615385    LR 0.100000    Time 0.054313    
2022-01-29 03:35:13,565 - --- validate (epoch=45)-----------
2022-01-29 03:35:13,566 - 10000 samples (128 per mini-batch)
2022-01-29 03:35:15,385 - Epoch: [45][   79/   79]    Loss 1.742023    Top1 51.730000    Top5 82.520000    
2022-01-29 03:35:15,441 - ==> Top1: 51.730    Top5: 82.520    Loss: 1.742

2022-01-29 03:35:15,447 - ==> Best [Top1: 51.730   Top5: 82.520   Sparsity:0.00   Params: 1341960 on epoch: 45]
2022-01-29 03:35:15,447 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:35:15,504 - 

2022-01-29 03:35:15,504 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:35:21,190 - Epoch: [46][  100/  391]    Overall Loss 1.417172    Objective Loss 1.417172                                        LR 0.100000    Time 0.056839    
2022-01-29 03:35:26,512 - Epoch: [46][  200/  391]    Overall Loss 1.442989    Objective Loss 1.442989                                        LR 0.100000    Time 0.055023    
2022-01-29 03:35:31,855 - Epoch: [46][  300/  391]    Overall Loss 1.453436    Objective Loss 1.453436                                        LR 0.100000    Time 0.054489    
2022-01-29 03:35:36,731 - Epoch: [46][  391/  391]    Overall Loss 1.465828    Objective Loss 1.465828    Top1 66.346154    Top5 92.307692    LR 0.100000    Time 0.054277    
2022-01-29 03:35:36,790 - --- validate (epoch=46)-----------
2022-01-29 03:35:36,790 - 10000 samples (128 per mini-batch)
2022-01-29 03:35:38,643 - Epoch: [46][   79/   79]    Loss 1.825759    Top1 50.440000    Top5 81.270000    
2022-01-29 03:35:38,702 - ==> Top1: 50.440    Top5: 81.270    Loss: 1.826

2022-01-29 03:35:38,707 - ==> Best [Top1: 51.730   Top5: 82.520   Sparsity:0.00   Params: 1341960 on epoch: 45]
2022-01-29 03:35:38,707 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:35:38,755 - 

2022-01-29 03:35:38,755 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:35:44,364 - Epoch: [47][  100/  391]    Overall Loss 1.421021    Objective Loss 1.421021                                        LR 0.100000    Time 0.056062    
2022-01-29 03:35:49,743 - Epoch: [47][  200/  391]    Overall Loss 1.434453    Objective Loss 1.434453                                        LR 0.100000    Time 0.054923    
2022-01-29 03:35:55,123 - Epoch: [47][  300/  391]    Overall Loss 1.452337    Objective Loss 1.452337                                        LR 0.100000    Time 0.054548    
2022-01-29 03:36:00,023 - Epoch: [47][  391/  391]    Overall Loss 1.457535    Objective Loss 1.457535    Top1 60.096154    Top5 86.538462    LR 0.100000    Time 0.054380    
2022-01-29 03:36:00,081 - --- validate (epoch=47)-----------
2022-01-29 03:36:00,081 - 10000 samples (128 per mini-batch)
2022-01-29 03:36:01,918 - Epoch: [47][   79/   79]    Loss 1.732677    Top1 53.300000    Top5 82.080000    
2022-01-29 03:36:01,969 - ==> Top1: 53.300    Top5: 82.080    Loss: 1.733

2022-01-29 03:36:01,974 - ==> Best [Top1: 53.300   Top5: 82.080   Sparsity:0.00   Params: 1341960 on epoch: 47]
2022-01-29 03:36:01,974 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:36:02,031 - 

2022-01-29 03:36:02,031 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:36:07,731 - Epoch: [48][  100/  391]    Overall Loss 1.429396    Objective Loss 1.429396                                        LR 0.100000    Time 0.056978    
2022-01-29 03:36:13,134 - Epoch: [48][  200/  391]    Overall Loss 1.447427    Objective Loss 1.447427                                        LR 0.100000    Time 0.055501    
2022-01-29 03:36:18,552 - Epoch: [48][  300/  391]    Overall Loss 1.437781    Objective Loss 1.437781                                        LR 0.100000    Time 0.055057    
2022-01-29 03:36:23,495 - Epoch: [48][  391/  391]    Overall Loss 1.449323    Objective Loss 1.449323    Top1 55.769231    Top5 83.653846    LR 0.100000    Time 0.054882    
2022-01-29 03:36:23,559 - --- validate (epoch=48)-----------
2022-01-29 03:36:23,559 - 10000 samples (128 per mini-batch)
2022-01-29 03:36:25,406 - Epoch: [48][   79/   79]    Loss 1.911718    Top1 48.590000    Top5 79.010000    
2022-01-29 03:36:25,458 - ==> Top1: 48.590    Top5: 79.010    Loss: 1.912

2022-01-29 03:36:25,463 - ==> Best [Top1: 53.300   Top5: 82.080   Sparsity:0.00   Params: 1341960 on epoch: 47]
2022-01-29 03:36:25,463 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:36:25,510 - 

2022-01-29 03:36:25,511 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:36:31,241 - Epoch: [49][  100/  391]    Overall Loss 1.418226    Objective Loss 1.418226                                        LR 0.100000    Time 0.057273    
2022-01-29 03:36:36,661 - Epoch: [49][  200/  391]    Overall Loss 1.419296    Objective Loss 1.419296                                        LR 0.100000    Time 0.055736    
2022-01-29 03:36:42,084 - Epoch: [49][  300/  391]    Overall Loss 1.433115    Objective Loss 1.433115                                        LR 0.100000    Time 0.055228    
2022-01-29 03:36:47,035 - Epoch: [49][  391/  391]    Overall Loss 1.440610    Objective Loss 1.440610    Top1 53.365385    Top5 84.134615    LR 0.100000    Time 0.055036    
2022-01-29 03:36:47,094 - --- validate (epoch=49)-----------
2022-01-29 03:36:47,094 - 10000 samples (128 per mini-batch)
2022-01-29 03:36:48,939 - Epoch: [49][   79/   79]    Loss 1.792043    Top1 51.420000    Top5 81.860000    
2022-01-29 03:36:48,997 - ==> Top1: 51.420    Top5: 81.860    Loss: 1.792

2022-01-29 03:36:49,002 - ==> Best [Top1: 53.300   Top5: 82.080   Sparsity:0.00   Params: 1341960 on epoch: 47]
2022-01-29 03:36:49,002 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:36:49,045 - 

2022-01-29 03:36:49,046 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:36:54,702 - Epoch: [50][  100/  391]    Overall Loss 1.399587    Objective Loss 1.399587                                        LR 0.100000    Time 0.056537    
2022-01-29 03:37:00,115 - Epoch: [50][  200/  391]    Overall Loss 1.418586    Objective Loss 1.418586                                        LR 0.100000    Time 0.055329    
2022-01-29 03:37:05,503 - Epoch: [50][  300/  391]    Overall Loss 1.420084    Objective Loss 1.420084                                        LR 0.100000    Time 0.054843    
2022-01-29 03:37:10,414 - Epoch: [50][  391/  391]    Overall Loss 1.431114    Objective Loss 1.431114    Top1 54.326923    Top5 83.173077    LR 0.100000    Time 0.054636    
2022-01-29 03:37:10,468 - --- validate (epoch=50)-----------
2022-01-29 03:37:10,469 - 10000 samples (128 per mini-batch)
2022-01-29 03:37:12,335 - Epoch: [50][   79/   79]    Loss 1.803158    Top1 51.300000    Top5 81.030000    
2022-01-29 03:37:12,393 - ==> Top1: 51.300    Top5: 81.030    Loss: 1.803

2022-01-29 03:37:12,398 - ==> Best [Top1: 53.300   Top5: 82.080   Sparsity:0.00   Params: 1341960 on epoch: 47]
2022-01-29 03:37:12,398 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:37:12,441 - 

2022-01-29 03:37:12,441 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:37:18,195 - Epoch: [51][  100/  391]    Overall Loss 1.355083    Objective Loss 1.355083                                        LR 0.100000    Time 0.057503    
2022-01-29 03:37:23,641 - Epoch: [51][  200/  391]    Overall Loss 1.394568    Objective Loss 1.394568                                        LR 0.100000    Time 0.055978    
2022-01-29 03:37:29,085 - Epoch: [51][  300/  391]    Overall Loss 1.404383    Objective Loss 1.404383                                        LR 0.100000    Time 0.055465    
2022-01-29 03:37:33,993 - Epoch: [51][  391/  391]    Overall Loss 1.416149    Objective Loss 1.416149    Top1 55.288462    Top5 85.576923    LR 0.100000    Time 0.055104    
2022-01-29 03:37:34,051 - --- validate (epoch=51)-----------
2022-01-29 03:37:34,051 - 10000 samples (128 per mini-batch)
2022-01-29 03:37:35,925 - Epoch: [51][   79/   79]    Loss 1.860158    Top1 49.490000    Top5 80.340000    
2022-01-29 03:37:35,977 - ==> Top1: 49.490    Top5: 80.340    Loss: 1.860

2022-01-29 03:37:35,982 - ==> Best [Top1: 53.300   Top5: 82.080   Sparsity:0.00   Params: 1341960 on epoch: 47]
2022-01-29 03:37:35,982 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:37:36,030 - 

2022-01-29 03:37:36,030 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:37:41,670 - Epoch: [52][  100/  391]    Overall Loss 1.343962    Objective Loss 1.343962                                        LR 0.100000    Time 0.056372    
2022-01-29 03:37:47,045 - Epoch: [52][  200/  391]    Overall Loss 1.364224    Objective Loss 1.364224                                        LR 0.100000    Time 0.055055    
2022-01-29 03:37:52,449 - Epoch: [52][  300/  391]    Overall Loss 1.387724    Objective Loss 1.387724                                        LR 0.100000    Time 0.054715    
2022-01-29 03:37:57,376 - Epoch: [52][  391/  391]    Overall Loss 1.400013    Objective Loss 1.400013    Top1 61.057692    Top5 87.500000    LR 0.100000    Time 0.054579    
2022-01-29 03:37:57,437 - --- validate (epoch=52)-----------
2022-01-29 03:37:57,437 - 10000 samples (128 per mini-batch)
2022-01-29 03:37:59,247 - Epoch: [52][   79/   79]    Loss 1.904376    Top1 48.380000    Top5 79.400000    
2022-01-29 03:37:59,297 - ==> Top1: 48.380    Top5: 79.400    Loss: 1.904

2022-01-29 03:37:59,302 - ==> Best [Top1: 53.300   Top5: 82.080   Sparsity:0.00   Params: 1341960 on epoch: 47]
2022-01-29 03:37:59,302 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:37:59,349 - 

2022-01-29 03:37:59,349 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:38:04,917 - Epoch: [53][  100/  391]    Overall Loss 1.369210    Objective Loss 1.369210                                        LR 0.100000    Time 0.055651    
2022-01-29 03:38:10,292 - Epoch: [53][  200/  391]    Overall Loss 1.386610    Objective Loss 1.386610                                        LR 0.100000    Time 0.054697    
2022-01-29 03:38:15,672 - Epoch: [53][  300/  391]    Overall Loss 1.391613    Objective Loss 1.391613                                        LR 0.100000    Time 0.054394    
2022-01-29 03:38:20,559 - Epoch: [53][  391/  391]    Overall Loss 1.401664    Objective Loss 1.401664    Top1 60.096154    Top5 92.307692    LR 0.100000    Time 0.054232    
2022-01-29 03:38:20,623 - --- validate (epoch=53)-----------
2022-01-29 03:38:20,623 - 10000 samples (128 per mini-batch)
2022-01-29 03:38:22,478 - Epoch: [53][   79/   79]    Loss 2.066110    Top1 46.530000    Top5 77.350000    
2022-01-29 03:38:22,532 - ==> Top1: 46.530    Top5: 77.350    Loss: 2.066

2022-01-29 03:38:22,537 - ==> Best [Top1: 53.300   Top5: 82.080   Sparsity:0.00   Params: 1341960 on epoch: 47]
2022-01-29 03:38:22,537 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:38:22,585 - 

2022-01-29 03:38:22,585 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:38:28,255 - Epoch: [54][  100/  391]    Overall Loss 1.342355    Objective Loss 1.342355                                        LR 0.100000    Time 0.056672    
2022-01-29 03:38:33,639 - Epoch: [54][  200/  391]    Overall Loss 1.367504    Objective Loss 1.367504                                        LR 0.100000    Time 0.055252    
2022-01-29 03:38:39,043 - Epoch: [54][  300/  391]    Overall Loss 1.380838    Objective Loss 1.380838                                        LR 0.100000    Time 0.054843    
2022-01-29 03:38:43,961 - Epoch: [54][  391/  391]    Overall Loss 1.383016    Objective Loss 1.383016    Top1 60.576923    Top5 88.461538    LR 0.100000    Time 0.054657    
2022-01-29 03:38:44,015 - --- validate (epoch=54)-----------
2022-01-29 03:38:44,015 - 10000 samples (128 per mini-batch)
2022-01-29 03:38:45,869 - Epoch: [54][   79/   79]    Loss 1.833483    Top1 50.870000    Top5 81.170000    
2022-01-29 03:38:45,920 - ==> Top1: 50.870    Top5: 81.170    Loss: 1.833

2022-01-29 03:38:45,925 - ==> Best [Top1: 53.300   Top5: 82.080   Sparsity:0.00   Params: 1341960 on epoch: 47]
2022-01-29 03:38:45,925 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:38:45,973 - 

2022-01-29 03:38:45,973 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:38:51,706 - Epoch: [55][  100/  391]    Overall Loss 1.356204    Objective Loss 1.356204                                        LR 0.100000    Time 0.057306    
2022-01-29 03:38:57,095 - Epoch: [55][  200/  391]    Overall Loss 1.375716    Objective Loss 1.375716                                        LR 0.100000    Time 0.055590    
2022-01-29 03:39:02,492 - Epoch: [55][  300/  391]    Overall Loss 1.383248    Objective Loss 1.383248                                        LR 0.100000    Time 0.055047    
2022-01-29 03:39:07,387 - Epoch: [55][  391/  391]    Overall Loss 1.387083    Objective Loss 1.387083    Top1 60.096154    Top5 88.942308    LR 0.100000    Time 0.054754    
2022-01-29 03:39:07,446 - --- validate (epoch=55)-----------
2022-01-29 03:39:07,446 - 10000 samples (128 per mini-batch)
2022-01-29 03:39:09,261 - Epoch: [55][   79/   79]    Loss 1.899690    Top1 49.550000    Top5 79.480000    
2022-01-29 03:39:09,313 - ==> Top1: 49.550    Top5: 79.480    Loss: 1.900

2022-01-29 03:39:09,318 - ==> Best [Top1: 53.300   Top5: 82.080   Sparsity:0.00   Params: 1341960 on epoch: 47]
2022-01-29 03:39:09,318 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:39:09,365 - 

2022-01-29 03:39:09,366 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:39:14,986 - Epoch: [56][  100/  391]    Overall Loss 1.331645    Objective Loss 1.331645                                        LR 0.100000    Time 0.056179    
2022-01-29 03:39:20,397 - Epoch: [56][  200/  391]    Overall Loss 1.343413    Objective Loss 1.343413                                        LR 0.100000    Time 0.055135    
2022-01-29 03:39:25,812 - Epoch: [56][  300/  391]    Overall Loss 1.363657    Objective Loss 1.363657                                        LR 0.100000    Time 0.054803    
2022-01-29 03:39:30,732 - Epoch: [56][  391/  391]    Overall Loss 1.376168    Objective Loss 1.376168    Top1 61.538462    Top5 90.384615    LR 0.100000    Time 0.054630    
2022-01-29 03:39:30,791 - --- validate (epoch=56)-----------
2022-01-29 03:39:30,791 - 10000 samples (128 per mini-batch)
2022-01-29 03:39:32,638 - Epoch: [56][   79/   79]    Loss 1.917860    Top1 48.710000    Top5 80.150000    
2022-01-29 03:39:32,694 - ==> Top1: 48.710    Top5: 80.150    Loss: 1.918

2022-01-29 03:39:32,700 - ==> Best [Top1: 53.300   Top5: 82.080   Sparsity:0.00   Params: 1341960 on epoch: 47]
2022-01-29 03:39:32,700 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:39:32,747 - 

2022-01-29 03:39:32,748 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:39:38,466 - Epoch: [57][  100/  391]    Overall Loss 1.325602    Objective Loss 1.325602                                        LR 0.100000    Time 0.057155    
2022-01-29 03:39:43,888 - Epoch: [57][  200/  391]    Overall Loss 1.349686    Objective Loss 1.349686                                        LR 0.100000    Time 0.055684    
2022-01-29 03:39:49,339 - Epoch: [57][  300/  391]    Overall Loss 1.359716    Objective Loss 1.359716                                        LR 0.100000    Time 0.055288    
2022-01-29 03:39:54,250 - Epoch: [57][  391/  391]    Overall Loss 1.359985    Objective Loss 1.359985    Top1 59.615385    Top5 90.865385    LR 0.100000    Time 0.054979    
2022-01-29 03:39:54,309 - --- validate (epoch=57)-----------
2022-01-29 03:39:54,309 - 10000 samples (128 per mini-batch)
2022-01-29 03:39:56,145 - Epoch: [57][   79/   79]    Loss 2.017463    Top1 47.440000    Top5 77.600000    
2022-01-29 03:39:56,202 - ==> Top1: 47.440    Top5: 77.600    Loss: 2.017

2022-01-29 03:39:56,207 - ==> Best [Top1: 53.300   Top5: 82.080   Sparsity:0.00   Params: 1341960 on epoch: 47]
2022-01-29 03:39:56,207 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:39:56,255 - 

2022-01-29 03:39:56,255 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:40:01,940 - Epoch: [58][  100/  391]    Overall Loss 1.317921    Objective Loss 1.317921                                        LR 0.100000    Time 0.056824    
2022-01-29 03:40:07,323 - Epoch: [58][  200/  391]    Overall Loss 1.338724    Objective Loss 1.338724                                        LR 0.100000    Time 0.055325    
2022-01-29 03:40:12,715 - Epoch: [58][  300/  391]    Overall Loss 1.346731    Objective Loss 1.346731                                        LR 0.100000    Time 0.054854    
2022-01-29 03:40:17,609 - Epoch: [58][  391/  391]    Overall Loss 1.355724    Objective Loss 1.355724    Top1 62.500000    Top5 88.942308    LR 0.100000    Time 0.054601    
2022-01-29 03:40:17,674 - --- validate (epoch=58)-----------
2022-01-29 03:40:17,674 - 10000 samples (128 per mini-batch)
2022-01-29 03:40:19,540 - Epoch: [58][   79/   79]    Loss 1.896974    Top1 49.410000    Top5 80.360000    
2022-01-29 03:40:19,591 - ==> Top1: 49.410    Top5: 80.360    Loss: 1.897

2022-01-29 03:40:19,596 - ==> Best [Top1: 53.300   Top5: 82.080   Sparsity:0.00   Params: 1341960 on epoch: 47]
2022-01-29 03:40:19,596 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:40:19,644 - 

2022-01-29 03:40:19,644 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:40:25,250 - Epoch: [59][  100/  391]    Overall Loss 1.313533    Objective Loss 1.313533                                        LR 0.100000    Time 0.056030    
2022-01-29 03:40:30,533 - Epoch: [59][  200/  391]    Overall Loss 1.322610    Objective Loss 1.322610                                        LR 0.100000    Time 0.054428    
2022-01-29 03:40:35,822 - Epoch: [59][  300/  391]    Overall Loss 1.336500    Objective Loss 1.336500                                        LR 0.100000    Time 0.053909    
2022-01-29 03:40:40,635 - Epoch: [59][  391/  391]    Overall Loss 1.344632    Objective Loss 1.344632    Top1 65.384615    Top5 90.384615    LR 0.100000    Time 0.053669    
2022-01-29 03:40:40,691 - --- validate (epoch=59)-----------
2022-01-29 03:40:40,691 - 10000 samples (128 per mini-batch)
2022-01-29 03:40:42,518 - Epoch: [59][   79/   79]    Loss 1.688200    Top1 52.880000    Top5 82.800000    
2022-01-29 03:40:42,574 - ==> Top1: 52.880    Top5: 82.800    Loss: 1.688

2022-01-29 03:40:42,579 - ==> Best [Top1: 53.300   Top5: 82.080   Sparsity:0.00   Params: 1341960 on epoch: 47]
2022-01-29 03:40:42,580 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:40:42,626 - 

2022-01-29 03:40:42,626 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:40:48,260 - Epoch: [60][  100/  391]    Overall Loss 1.301216    Objective Loss 1.301216                                        LR 0.100000    Time 0.056311    
2022-01-29 03:40:53,646 - Epoch: [60][  200/  391]    Overall Loss 1.310656    Objective Loss 1.310656                                        LR 0.100000    Time 0.055084    
2022-01-29 03:40:59,036 - Epoch: [60][  300/  391]    Overall Loss 1.324370    Objective Loss 1.324370                                        LR 0.100000    Time 0.054687    
2022-01-29 03:41:03,932 - Epoch: [60][  391/  391]    Overall Loss 1.336115    Objective Loss 1.336115    Top1 57.211538    Top5 88.942308    LR 0.100000    Time 0.054479    
2022-01-29 03:41:03,991 - --- validate (epoch=60)-----------
2022-01-29 03:41:03,991 - 10000 samples (128 per mini-batch)
2022-01-29 03:41:05,863 - Epoch: [60][   79/   79]    Loss 1.742787    Top1 52.160000    Top5 82.950000    
2022-01-29 03:41:05,920 - ==> Top1: 52.160    Top5: 82.950    Loss: 1.743

2022-01-29 03:41:05,926 - ==> Best [Top1: 53.300   Top5: 82.080   Sparsity:0.00   Params: 1341960 on epoch: 47]
2022-01-29 03:41:05,926 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:41:05,969 - 

2022-01-29 03:41:05,969 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:41:11,680 - Epoch: [61][  100/  391]    Overall Loss 1.283003    Objective Loss 1.283003                                        LR 0.100000    Time 0.057083    
2022-01-29 03:41:17,072 - Epoch: [61][  200/  391]    Overall Loss 1.312291    Objective Loss 1.312291                                        LR 0.100000    Time 0.055494    
2022-01-29 03:41:22,455 - Epoch: [61][  300/  391]    Overall Loss 1.319220    Objective Loss 1.319220                                        LR 0.100000    Time 0.054939    
2022-01-29 03:41:27,352 - Epoch: [61][  391/  391]    Overall Loss 1.326900    Objective Loss 1.326900    Top1 55.769231    Top5 88.461538    LR 0.100000    Time 0.054673    
2022-01-29 03:41:27,411 - --- validate (epoch=61)-----------
2022-01-29 03:41:27,411 - 10000 samples (128 per mini-batch)
2022-01-29 03:41:29,271 - Epoch: [61][   79/   79]    Loss 1.771725    Top1 51.900000    Top5 81.610000    
2022-01-29 03:41:29,324 - ==> Top1: 51.900    Top5: 81.610    Loss: 1.772

2022-01-29 03:41:29,329 - ==> Best [Top1: 53.300   Top5: 82.080   Sparsity:0.00   Params: 1341960 on epoch: 47]
2022-01-29 03:41:29,329 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:41:29,372 - 

2022-01-29 03:41:29,372 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:41:35,012 - Epoch: [62][  100/  391]    Overall Loss 1.277557    Objective Loss 1.277557                                        LR 0.100000    Time 0.056372    
2022-01-29 03:41:40,408 - Epoch: [62][  200/  391]    Overall Loss 1.294061    Objective Loss 1.294061                                        LR 0.100000    Time 0.055164    
2022-01-29 03:41:45,791 - Epoch: [62][  300/  391]    Overall Loss 1.314075    Objective Loss 1.314075                                        LR 0.100000    Time 0.054714    
2022-01-29 03:41:50,739 - Epoch: [62][  391/  391]    Overall Loss 1.322285    Objective Loss 1.322285    Top1 56.730769    Top5 88.942308    LR 0.100000    Time 0.054634    
2022-01-29 03:41:50,796 - --- validate (epoch=62)-----------
2022-01-29 03:41:50,796 - 10000 samples (128 per mini-batch)
2022-01-29 03:41:52,645 - Epoch: [62][   79/   79]    Loss 1.714443    Top1 53.370000    Top5 82.960000    
2022-01-29 03:41:52,695 - ==> Top1: 53.370    Top5: 82.960    Loss: 1.714

2022-01-29 03:41:52,700 - ==> Best [Top1: 53.370   Top5: 82.960   Sparsity:0.00   Params: 1341960 on epoch: 62]
2022-01-29 03:41:52,700 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:41:52,757 - 

2022-01-29 03:41:52,757 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:41:58,502 - Epoch: [63][  100/  391]    Overall Loss 1.286135    Objective Loss 1.286135                                        LR 0.100000    Time 0.057419    
2022-01-29 03:42:03,876 - Epoch: [63][  200/  391]    Overall Loss 1.291376    Objective Loss 1.291376                                        LR 0.100000    Time 0.055575    
2022-01-29 03:42:09,292 - Epoch: [63][  300/  391]    Overall Loss 1.308459    Objective Loss 1.308459                                        LR 0.100000    Time 0.055101    
2022-01-29 03:42:14,217 - Epoch: [63][  391/  391]    Overall Loss 1.315207    Objective Loss 1.315207    Top1 62.019231    Top5 87.980769    LR 0.100000    Time 0.054873    
2022-01-29 03:42:14,282 - --- validate (epoch=63)-----------
2022-01-29 03:42:14,282 - 10000 samples (128 per mini-batch)
2022-01-29 03:42:16,164 - Epoch: [63][   79/   79]    Loss 1.752645    Top1 52.920000    Top5 82.640000    
2022-01-29 03:42:16,213 - ==> Top1: 52.920    Top5: 82.640    Loss: 1.753

2022-01-29 03:42:16,218 - ==> Best [Top1: 53.370   Top5: 82.960   Sparsity:0.00   Params: 1341960 on epoch: 62]
2022-01-29 03:42:16,218 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:42:16,267 - 

2022-01-29 03:42:16,267 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:42:21,988 - Epoch: [64][  100/  391]    Overall Loss 1.268503    Objective Loss 1.268503                                        LR 0.100000    Time 0.057186    
2022-01-29 03:42:27,378 - Epoch: [64][  200/  391]    Overall Loss 1.299844    Objective Loss 1.299844                                        LR 0.100000    Time 0.055537    
2022-01-29 03:42:32,765 - Epoch: [64][  300/  391]    Overall Loss 1.305411    Objective Loss 1.305411                                        LR 0.100000    Time 0.054978    
2022-01-29 03:42:37,621 - Epoch: [64][  391/  391]    Overall Loss 1.307014    Objective Loss 1.307014    Top1 64.903846    Top5 89.903846    LR 0.100000    Time 0.054601    
2022-01-29 03:42:37,685 - --- validate (epoch=64)-----------
2022-01-29 03:42:37,685 - 10000 samples (128 per mini-batch)
2022-01-29 03:42:39,509 - Epoch: [64][   79/   79]    Loss 1.859796    Top1 49.950000    Top5 81.670000    
2022-01-29 03:42:39,561 - ==> Top1: 49.950    Top5: 81.670    Loss: 1.860

2022-01-29 03:42:39,566 - ==> Best [Top1: 53.370   Top5: 82.960   Sparsity:0.00   Params: 1341960 on epoch: 62]
2022-01-29 03:42:39,566 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:42:39,612 - 

2022-01-29 03:42:39,613 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:42:45,144 - Epoch: [65][  100/  391]    Overall Loss 1.239482    Objective Loss 1.239482                                        LR 0.100000    Time 0.055288    
2022-01-29 03:42:50,480 - Epoch: [65][  200/  391]    Overall Loss 1.269407    Objective Loss 1.269407                                        LR 0.100000    Time 0.054318    
2022-01-29 03:42:55,866 - Epoch: [65][  300/  391]    Overall Loss 1.286615    Objective Loss 1.286615                                        LR 0.100000    Time 0.054165    
2022-01-29 03:43:00,750 - Epoch: [65][  391/  391]    Overall Loss 1.293689    Objective Loss 1.293689    Top1 59.134615    Top5 85.096154    LR 0.100000    Time 0.054046    
2022-01-29 03:43:00,806 - --- validate (epoch=65)-----------
2022-01-29 03:43:00,807 - 10000 samples (128 per mini-batch)
2022-01-29 03:43:02,694 - Epoch: [65][   79/   79]    Loss 1.787578    Top1 52.160000    Top5 82.440000    
2022-01-29 03:43:02,743 - ==> Top1: 52.160    Top5: 82.440    Loss: 1.788

2022-01-29 03:43:02,748 - ==> Best [Top1: 53.370   Top5: 82.960   Sparsity:0.00   Params: 1341960 on epoch: 62]
2022-01-29 03:43:02,748 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:43:02,792 - 

2022-01-29 03:43:02,793 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:43:08,489 - Epoch: [66][  100/  391]    Overall Loss 1.250943    Objective Loss 1.250943                                        LR 0.100000    Time 0.056933    
2022-01-29 03:43:13,878 - Epoch: [66][  200/  391]    Overall Loss 1.265175    Objective Loss 1.265175                                        LR 0.100000    Time 0.055407    
2022-01-29 03:43:19,265 - Epoch: [66][  300/  391]    Overall Loss 1.287388    Objective Loss 1.287388                                        LR 0.100000    Time 0.054893    
2022-01-29 03:43:24,171 - Epoch: [66][  391/  391]    Overall Loss 1.293813    Objective Loss 1.293813    Top1 53.365385    Top5 84.134615    LR 0.100000    Time 0.054662    
2022-01-29 03:43:24,229 - --- validate (epoch=66)-----------
2022-01-29 03:43:24,229 - 10000 samples (128 per mini-batch)
2022-01-29 03:43:26,083 - Epoch: [66][   79/   79]    Loss 1.863455    Top1 50.380000    Top5 81.120000    
2022-01-29 03:43:26,136 - ==> Top1: 50.380    Top5: 81.120    Loss: 1.863

2022-01-29 03:43:26,141 - ==> Best [Top1: 53.370   Top5: 82.960   Sparsity:0.00   Params: 1341960 on epoch: 62]
2022-01-29 03:43:26,141 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:43:26,185 - 

2022-01-29 03:43:26,185 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:43:31,877 - Epoch: [67][  100/  391]    Overall Loss 1.246695    Objective Loss 1.246695                                        LR 0.100000    Time 0.056892    
2022-01-29 03:43:37,276 - Epoch: [67][  200/  391]    Overall Loss 1.262013    Objective Loss 1.262013                                        LR 0.100000    Time 0.055437    
2022-01-29 03:43:42,675 - Epoch: [67][  300/  391]    Overall Loss 1.277215    Objective Loss 1.277215                                        LR 0.100000    Time 0.054954    
2022-01-29 03:43:47,583 - Epoch: [67][  391/  391]    Overall Loss 1.288378    Objective Loss 1.288378    Top1 68.750000    Top5 92.788462    LR 0.100000    Time 0.054713    
2022-01-29 03:43:47,640 - --- validate (epoch=67)-----------
2022-01-29 03:43:47,640 - 10000 samples (128 per mini-batch)
2022-01-29 03:43:49,468 - Epoch: [67][   79/   79]    Loss 1.716457    Top1 53.910000    Top5 82.830000    
2022-01-29 03:43:49,524 - ==> Top1: 53.910    Top5: 82.830    Loss: 1.716

2022-01-29 03:43:49,529 - ==> Best [Top1: 53.910   Top5: 82.830   Sparsity:0.00   Params: 1341960 on epoch: 67]
2022-01-29 03:43:49,530 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:43:49,586 - 

2022-01-29 03:43:49,586 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:43:55,155 - Epoch: [68][  100/  391]    Overall Loss 1.245066    Objective Loss 1.245066                                        LR 0.100000    Time 0.055655    
2022-01-29 03:44:00,474 - Epoch: [68][  200/  391]    Overall Loss 1.260736    Objective Loss 1.260736                                        LR 0.100000    Time 0.054419    
2022-01-29 03:44:05,796 - Epoch: [68][  300/  391]    Overall Loss 1.266907    Objective Loss 1.266907                                        LR 0.100000    Time 0.054018    
2022-01-29 03:44:10,652 - Epoch: [68][  391/  391]    Overall Loss 1.277976    Objective Loss 1.277976    Top1 62.980769    Top5 89.903846    LR 0.100000    Time 0.053862    
2022-01-29 03:44:10,711 - --- validate (epoch=68)-----------
2022-01-29 03:44:10,711 - 10000 samples (128 per mini-batch)
2022-01-29 03:44:12,531 - Epoch: [68][   79/   79]    Loss 1.712541    Top1 53.310000    Top5 83.560000    
2022-01-29 03:44:12,580 - ==> Top1: 53.310    Top5: 83.560    Loss: 1.713

2022-01-29 03:44:12,585 - ==> Best [Top1: 53.910   Top5: 82.830   Sparsity:0.00   Params: 1341960 on epoch: 67]
2022-01-29 03:44:12,586 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:44:12,632 - 

2022-01-29 03:44:12,632 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:44:18,328 - Epoch: [69][  100/  391]    Overall Loss 1.233091    Objective Loss 1.233091                                        LR 0.100000    Time 0.056928    
2022-01-29 03:44:23,722 - Epoch: [69][  200/  391]    Overall Loss 1.239742    Objective Loss 1.239742                                        LR 0.100000    Time 0.055431    
2022-01-29 03:44:29,115 - Epoch: [69][  300/  391]    Overall Loss 1.261200    Objective Loss 1.261200                                        LR 0.100000    Time 0.054928    
2022-01-29 03:44:33,954 - Epoch: [69][  391/  391]    Overall Loss 1.269222    Objective Loss 1.269222    Top1 61.057692    Top5 89.903846    LR 0.100000    Time 0.054518    
2022-01-29 03:44:34,020 - --- validate (epoch=69)-----------
2022-01-29 03:44:34,020 - 10000 samples (128 per mini-batch)
2022-01-29 03:44:35,834 - Epoch: [69][   79/   79]    Loss 1.729746    Top1 53.040000    Top5 82.890000    
2022-01-29 03:44:35,887 - ==> Top1: 53.040    Top5: 82.890    Loss: 1.730

2022-01-29 03:44:35,892 - ==> Best [Top1: 53.910   Top5: 82.830   Sparsity:0.00   Params: 1341960 on epoch: 67]
2022-01-29 03:44:35,892 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:44:35,938 - 

2022-01-29 03:44:35,938 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:44:41,565 - Epoch: [70][  100/  391]    Overall Loss 1.240052    Objective Loss 1.240052                                        LR 0.100000    Time 0.056241    
2022-01-29 03:44:46,964 - Epoch: [70][  200/  391]    Overall Loss 1.243195    Objective Loss 1.243195                                        LR 0.100000    Time 0.055109    
2022-01-29 03:44:52,353 - Epoch: [70][  300/  391]    Overall Loss 1.257380    Objective Loss 1.257380                                        LR 0.100000    Time 0.054700    
2022-01-29 03:44:57,246 - Epoch: [70][  391/  391]    Overall Loss 1.265334    Objective Loss 1.265334    Top1 62.980769    Top5 88.461538    LR 0.100000    Time 0.054483    
2022-01-29 03:44:57,304 - --- validate (epoch=70)-----------
2022-01-29 03:44:57,304 - 10000 samples (128 per mini-batch)
2022-01-29 03:44:59,113 - Epoch: [70][   79/   79]    Loss 1.807173    Top1 50.830000    Top5 82.070000    
2022-01-29 03:44:59,170 - ==> Top1: 50.830    Top5: 82.070    Loss: 1.807

2022-01-29 03:44:59,175 - ==> Best [Top1: 53.910   Top5: 82.830   Sparsity:0.00   Params: 1341960 on epoch: 67]
2022-01-29 03:44:59,175 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:44:59,221 - 

2022-01-29 03:44:59,222 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:45:04,873 - Epoch: [71][  100/  391]    Overall Loss 1.197284    Objective Loss 1.197284                                        LR 0.100000    Time 0.056489    
2022-01-29 03:45:10,321 - Epoch: [71][  200/  391]    Overall Loss 1.228871    Objective Loss 1.228871                                        LR 0.100000    Time 0.055478    
2022-01-29 03:45:15,762 - Epoch: [71][  300/  391]    Overall Loss 1.240210    Objective Loss 1.240210                                        LR 0.100000    Time 0.055121    
2022-01-29 03:45:20,646 - Epoch: [71][  391/  391]    Overall Loss 1.257438    Objective Loss 1.257438    Top1 64.423077    Top5 92.307692    LR 0.100000    Time 0.054780    
2022-01-29 03:45:20,709 - --- validate (epoch=71)-----------
2022-01-29 03:45:20,709 - 10000 samples (128 per mini-batch)
2022-01-29 03:45:22,522 - Epoch: [71][   79/   79]    Loss 1.808186    Top1 52.380000    Top5 81.580000    
2022-01-29 03:45:22,578 - ==> Top1: 52.380    Top5: 81.580    Loss: 1.808

2022-01-29 03:45:22,583 - ==> Best [Top1: 53.910   Top5: 82.830   Sparsity:0.00   Params: 1341960 on epoch: 67]
2022-01-29 03:45:22,583 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:45:22,630 - 

2022-01-29 03:45:22,630 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:45:28,317 - Epoch: [72][  100/  391]    Overall Loss 1.214041    Objective Loss 1.214041                                        LR 0.100000    Time 0.056848    
2022-01-29 03:45:33,701 - Epoch: [72][  200/  391]    Overall Loss 1.217961    Objective Loss 1.217961                                        LR 0.100000    Time 0.055338    
2022-01-29 03:45:39,094 - Epoch: [72][  300/  391]    Overall Loss 1.241550    Objective Loss 1.241550                                        LR 0.100000    Time 0.054865    
2022-01-29 03:45:43,992 - Epoch: [72][  391/  391]    Overall Loss 1.248180    Objective Loss 1.248180    Top1 60.576923    Top5 88.942308    LR 0.100000    Time 0.054621    
2022-01-29 03:45:44,050 - --- validate (epoch=72)-----------
2022-01-29 03:45:44,050 - 10000 samples (128 per mini-batch)
2022-01-29 03:45:45,875 - Epoch: [72][   79/   79]    Loss 1.736667    Top1 52.640000    Top5 82.850000    
2022-01-29 03:45:45,930 - ==> Top1: 52.640    Top5: 82.850    Loss: 1.737

2022-01-29 03:45:45,935 - ==> Best [Top1: 53.910   Top5: 82.830   Sparsity:0.00   Params: 1341960 on epoch: 67]
2022-01-29 03:45:45,936 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:45:45,984 - 

2022-01-29 03:45:45,984 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:45:51,675 - Epoch: [73][  100/  391]    Overall Loss 1.220820    Objective Loss 1.220820                                        LR 0.100000    Time 0.056881    
2022-01-29 03:45:57,056 - Epoch: [73][  200/  391]    Overall Loss 1.216651    Objective Loss 1.216651                                        LR 0.100000    Time 0.055341    
2022-01-29 03:46:02,442 - Epoch: [73][  300/  391]    Overall Loss 1.237319    Objective Loss 1.237319                                        LR 0.100000    Time 0.054846    
2022-01-29 03:46:07,336 - Epoch: [73][  391/  391]    Overall Loss 1.246038    Objective Loss 1.246038    Top1 63.942308    Top5 92.307692    LR 0.100000    Time 0.054597    
2022-01-29 03:46:07,393 - --- validate (epoch=73)-----------
2022-01-29 03:46:07,393 - 10000 samples (128 per mini-batch)
2022-01-29 03:46:09,236 - Epoch: [73][   79/   79]    Loss 1.617205    Top1 55.240000    Top5 84.310000    
2022-01-29 03:46:09,292 - ==> Top1: 55.240    Top5: 84.310    Loss: 1.617

2022-01-29 03:46:09,297 - ==> Best [Top1: 55.240   Top5: 84.310   Sparsity:0.00   Params: 1341960 on epoch: 73]
2022-01-29 03:46:09,297 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:46:09,355 - 

2022-01-29 03:46:09,355 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:46:14,985 - Epoch: [74][  100/  391]    Overall Loss 1.197167    Objective Loss 1.197167                                        LR 0.100000    Time 0.056273    
2022-01-29 03:46:20,372 - Epoch: [74][  200/  391]    Overall Loss 1.220702    Objective Loss 1.220702                                        LR 0.100000    Time 0.055066    
2022-01-29 03:46:25,682 - Epoch: [74][  300/  391]    Overall Loss 1.231140    Objective Loss 1.231140                                        LR 0.100000    Time 0.054407    
2022-01-29 03:46:30,514 - Epoch: [74][  391/  391]    Overall Loss 1.242521    Objective Loss 1.242521    Top1 64.903846    Top5 90.384615    LR 0.100000    Time 0.054101    
2022-01-29 03:46:30,572 - --- validate (epoch=74)-----------
2022-01-29 03:46:30,573 - 10000 samples (128 per mini-batch)
2022-01-29 03:46:32,384 - Epoch: [74][   79/   79]    Loss 1.792890    Top1 51.740000    Top5 81.800000    
2022-01-29 03:46:32,441 - ==> Top1: 51.740    Top5: 81.800    Loss: 1.793

2022-01-29 03:46:32,446 - ==> Best [Top1: 55.240   Top5: 84.310   Sparsity:0.00   Params: 1341960 on epoch: 73]
2022-01-29 03:46:32,446 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:46:32,493 - 

2022-01-29 03:46:32,493 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:46:38,120 - Epoch: [75][  100/  391]    Overall Loss 1.202626    Objective Loss 1.202626                                        LR 0.100000    Time 0.056244    
2022-01-29 03:46:43,516 - Epoch: [75][  200/  391]    Overall Loss 1.209797    Objective Loss 1.209797                                        LR 0.100000    Time 0.055097    
2022-01-29 03:46:48,867 - Epoch: [75][  300/  391]    Overall Loss 1.218218    Objective Loss 1.218218                                        LR 0.100000    Time 0.054566    
2022-01-29 03:46:53,733 - Epoch: [75][  391/  391]    Overall Loss 1.228578    Objective Loss 1.228578    Top1 60.576923    Top5 87.500000    LR 0.100000    Time 0.054310    
2022-01-29 03:46:53,791 - --- validate (epoch=75)-----------
2022-01-29 03:46:53,792 - 10000 samples (128 per mini-batch)
2022-01-29 03:46:55,606 - Epoch: [75][   79/   79]    Loss 1.839776    Top1 51.070000    Top5 81.150000    
2022-01-29 03:46:55,661 - ==> Top1: 51.070    Top5: 81.150    Loss: 1.840

2022-01-29 03:46:55,666 - ==> Best [Top1: 55.240   Top5: 84.310   Sparsity:0.00   Params: 1341960 on epoch: 73]
2022-01-29 03:46:55,666 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:46:55,713 - 

2022-01-29 03:46:55,713 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:47:01,355 - Epoch: [76][  100/  391]    Overall Loss 1.171509    Objective Loss 1.171509                                        LR 0.100000    Time 0.056388    
2022-01-29 03:47:06,696 - Epoch: [76][  200/  391]    Overall Loss 1.199631    Objective Loss 1.199631                                        LR 0.100000    Time 0.054899    
2022-01-29 03:47:12,043 - Epoch: [76][  300/  391]    Overall Loss 1.216106    Objective Loss 1.216106                                        LR 0.100000    Time 0.054419    
2022-01-29 03:47:16,918 - Epoch: [76][  391/  391]    Overall Loss 1.228519    Objective Loss 1.228519    Top1 61.538462    Top5 92.307692    LR 0.100000    Time 0.054220    
2022-01-29 03:47:16,983 - --- validate (epoch=76)-----------
2022-01-29 03:47:16,983 - 10000 samples (128 per mini-batch)
2022-01-29 03:47:18,805 - Epoch: [76][   79/   79]    Loss 1.613484    Top1 55.170000    Top5 84.770000    
2022-01-29 03:47:18,863 - ==> Top1: 55.170    Top5: 84.770    Loss: 1.613

2022-01-29 03:47:18,868 - ==> Best [Top1: 55.240   Top5: 84.310   Sparsity:0.00   Params: 1341960 on epoch: 73]
2022-01-29 03:47:18,868 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:47:18,915 - 

2022-01-29 03:47:18,916 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:47:24,513 - Epoch: [77][  100/  391]    Overall Loss 1.153096    Objective Loss 1.153096                                        LR 0.100000    Time 0.055945    
2022-01-29 03:47:29,904 - Epoch: [77][  200/  391]    Overall Loss 1.189112    Objective Loss 1.189112                                        LR 0.100000    Time 0.054923    
2022-01-29 03:47:35,295 - Epoch: [77][  300/  391]    Overall Loss 1.206068    Objective Loss 1.206068                                        LR 0.100000    Time 0.054584    
2022-01-29 03:47:40,193 - Epoch: [77][  391/  391]    Overall Loss 1.217630    Objective Loss 1.217630    Top1 68.269231    Top5 92.788462    LR 0.100000    Time 0.054405    
2022-01-29 03:47:40,256 - --- validate (epoch=77)-----------
2022-01-29 03:47:40,256 - 10000 samples (128 per mini-batch)
2022-01-29 03:47:42,092 - Epoch: [77][   79/   79]    Loss 1.752075    Top1 52.540000    Top5 82.620000    
2022-01-29 03:47:42,149 - ==> Top1: 52.540    Top5: 82.620    Loss: 1.752

2022-01-29 03:47:42,154 - ==> Best [Top1: 55.240   Top5: 84.310   Sparsity:0.00   Params: 1341960 on epoch: 73]
2022-01-29 03:47:42,154 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:47:42,202 - 

2022-01-29 03:47:42,202 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:47:47,884 - Epoch: [78][  100/  391]    Overall Loss 1.171542    Objective Loss 1.171542                                        LR 0.100000    Time 0.056794    
2022-01-29 03:47:53,273 - Epoch: [78][  200/  391]    Overall Loss 1.184599    Objective Loss 1.184599                                        LR 0.100000    Time 0.055340    
2022-01-29 03:47:58,658 - Epoch: [78][  300/  391]    Overall Loss 1.202780    Objective Loss 1.202780                                        LR 0.100000    Time 0.054838    
2022-01-29 03:48:03,558 - Epoch: [78][  391/  391]    Overall Loss 1.207245    Objective Loss 1.207245    Top1 64.903846    Top5 93.750000    LR 0.100000    Time 0.054606    
2022-01-29 03:48:03,617 - --- validate (epoch=78)-----------
2022-01-29 03:48:03,617 - 10000 samples (128 per mini-batch)
2022-01-29 03:48:05,461 - Epoch: [78][   79/   79]    Loss 1.812422    Top1 51.220000    Top5 82.180000    
2022-01-29 03:48:05,514 - ==> Top1: 51.220    Top5: 82.180    Loss: 1.812

2022-01-29 03:48:05,519 - ==> Best [Top1: 55.240   Top5: 84.310   Sparsity:0.00   Params: 1341960 on epoch: 73]
2022-01-29 03:48:05,519 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:48:05,566 - 

2022-01-29 03:48:05,566 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:48:11,147 - Epoch: [79][  100/  391]    Overall Loss 1.163751    Objective Loss 1.163751                                        LR 0.100000    Time 0.055788    
2022-01-29 03:48:16,445 - Epoch: [79][  200/  391]    Overall Loss 1.184642    Objective Loss 1.184642                                        LR 0.100000    Time 0.054376    
2022-01-29 03:48:21,828 - Epoch: [79][  300/  391]    Overall Loss 1.194949    Objective Loss 1.194949                                        LR 0.100000    Time 0.054190    
2022-01-29 03:48:26,727 - Epoch: [79][  391/  391]    Overall Loss 1.205829    Objective Loss 1.205829    Top1 62.019231    Top5 89.903846    LR 0.100000    Time 0.054106    
2022-01-29 03:48:26,786 - --- validate (epoch=79)-----------
2022-01-29 03:48:26,786 - 10000 samples (128 per mini-batch)
2022-01-29 03:48:28,745 - Epoch: [79][   79/   79]    Loss 1.647813    Top1 54.090000    Top5 84.100000    
2022-01-29 03:48:28,809 - ==> Top1: 54.090    Top5: 84.100    Loss: 1.648

2022-01-29 03:48:28,814 - ==> Best [Top1: 55.240   Top5: 84.310   Sparsity:0.00   Params: 1341960 on epoch: 73]
2022-01-29 03:48:28,814 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:48:28,853 - 

2022-01-29 03:48:28,853 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:48:35,592 - Epoch: [80][  100/  391]    Overall Loss 1.159187    Objective Loss 1.159187                                        LR 0.100000    Time 0.067362    
2022-01-29 03:48:40,965 - Epoch: [80][  200/  391]    Overall Loss 1.166283    Objective Loss 1.166283                                        LR 0.100000    Time 0.060540    
2022-01-29 03:48:46,335 - Epoch: [80][  300/  391]    Overall Loss 1.187376    Objective Loss 1.187376                                        LR 0.100000    Time 0.058256    
2022-01-29 03:48:51,216 - Epoch: [80][  391/  391]    Overall Loss 1.199120    Objective Loss 1.199120    Top1 63.942308    Top5 89.423077    LR 0.100000    Time 0.057180    
2022-01-29 03:48:51,271 - --- validate (epoch=80)-----------
2022-01-29 03:48:51,271 - 10000 samples (128 per mini-batch)
2022-01-29 03:48:53,221 - Epoch: [80][   79/   79]    Loss 1.768661    Top1 52.950000    Top5 81.850000    
2022-01-29 03:48:53,279 - ==> Top1: 52.950    Top5: 81.850    Loss: 1.769

2022-01-29 03:48:53,284 - ==> Best [Top1: 55.240   Top5: 84.310   Sparsity:0.00   Params: 1341960 on epoch: 73]
2022-01-29 03:48:53,285 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:48:53,333 - 

2022-01-29 03:48:53,333 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:48:59,031 - Epoch: [81][  100/  391]    Overall Loss 1.134106    Objective Loss 1.134106                                        LR 0.100000    Time 0.056953    
2022-01-29 03:49:04,646 - Epoch: [81][  200/  391]    Overall Loss 1.172355    Objective Loss 1.172355                                        LR 0.100000    Time 0.056545    
2022-01-29 03:49:10,851 - Epoch: [81][  300/  391]    Overall Loss 1.191971    Objective Loss 1.191971                                        LR 0.100000    Time 0.058377    
2022-01-29 03:49:15,814 - Epoch: [81][  391/  391]    Overall Loss 1.201492    Objective Loss 1.201492    Top1 57.692308    Top5 88.942308    LR 0.100000    Time 0.057480    
2022-01-29 03:49:15,872 - --- validate (epoch=81)-----------
2022-01-29 03:49:15,872 - 10000 samples (128 per mini-batch)
2022-01-29 03:49:17,797 - Epoch: [81][   79/   79]    Loss 1.847160    Top1 50.850000    Top5 81.390000    
2022-01-29 03:49:17,849 - ==> Top1: 50.850    Top5: 81.390    Loss: 1.847

2022-01-29 03:49:17,854 - ==> Best [Top1: 55.240   Top5: 84.310   Sparsity:0.00   Params: 1341960 on epoch: 73]
2022-01-29 03:49:17,854 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:49:17,902 - 

2022-01-29 03:49:17,902 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:49:23,676 - Epoch: [82][  100/  391]    Overall Loss 1.162847    Objective Loss 1.162847                                        LR 0.100000    Time 0.057708    
2022-01-29 03:49:29,131 - Epoch: [82][  200/  391]    Overall Loss 1.168901    Objective Loss 1.168901                                        LR 0.100000    Time 0.056126    
2022-01-29 03:49:34,589 - Epoch: [82][  300/  391]    Overall Loss 1.174387    Objective Loss 1.174387                                        LR 0.100000    Time 0.055608    
2022-01-29 03:49:39,550 - Epoch: [82][  391/  391]    Overall Loss 1.185409    Objective Loss 1.185409    Top1 68.269231    Top5 92.307692    LR 0.100000    Time 0.055351    
2022-01-29 03:49:39,607 - --- validate (epoch=82)-----------
2022-01-29 03:49:39,608 - 10000 samples (128 per mini-batch)
2022-01-29 03:49:41,493 - Epoch: [82][   79/   79]    Loss 1.711581    Top1 53.600000    Top5 83.500000    
2022-01-29 03:49:41,545 - ==> Top1: 53.600    Top5: 83.500    Loss: 1.712

2022-01-29 03:49:41,550 - ==> Best [Top1: 55.240   Top5: 84.310   Sparsity:0.00   Params: 1341960 on epoch: 73]
2022-01-29 03:49:41,551 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:49:41,599 - 

2022-01-29 03:49:41,599 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:49:47,300 - Epoch: [83][  100/  391]    Overall Loss 1.152241    Objective Loss 1.152241                                        LR 0.100000    Time 0.056989    
2022-01-29 03:49:52,745 - Epoch: [83][  200/  391]    Overall Loss 1.168978    Objective Loss 1.168978                                        LR 0.100000    Time 0.055714    
2022-01-29 03:49:58,151 - Epoch: [83][  300/  391]    Overall Loss 1.188727    Objective Loss 1.188727                                        LR 0.100000    Time 0.055159    
2022-01-29 03:50:03,072 - Epoch: [83][  391/  391]    Overall Loss 1.187881    Objective Loss 1.187881    Top1 70.673077    Top5 89.423077    LR 0.100000    Time 0.054905    
2022-01-29 03:50:03,131 - --- validate (epoch=83)-----------
2022-01-29 03:50:03,131 - 10000 samples (128 per mini-batch)
2022-01-29 03:50:04,972 - Epoch: [83][   79/   79]    Loss 1.906643    Top1 50.180000    Top5 80.780000    
2022-01-29 03:50:05,030 - ==> Top1: 50.180    Top5: 80.780    Loss: 1.907

2022-01-29 03:50:05,035 - ==> Best [Top1: 55.240   Top5: 84.310   Sparsity:0.00   Params: 1341960 on epoch: 73]
2022-01-29 03:50:05,035 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:50:05,083 - 

2022-01-29 03:50:05,083 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:50:10,789 - Epoch: [84][  100/  391]    Overall Loss 1.131378    Objective Loss 1.131378                                        LR 0.100000    Time 0.057023    
2022-01-29 03:50:16,223 - Epoch: [84][  200/  391]    Overall Loss 1.140534    Objective Loss 1.140534                                        LR 0.100000    Time 0.055678    
2022-01-29 03:50:21,663 - Epoch: [84][  300/  391]    Overall Loss 1.165715    Objective Loss 1.165715                                        LR 0.100000    Time 0.055251    
2022-01-29 03:50:26,583 - Epoch: [84][  391/  391]    Overall Loss 1.181573    Objective Loss 1.181573    Top1 62.980769    Top5 89.423077    LR 0.100000    Time 0.054973    
2022-01-29 03:50:26,643 - --- validate (epoch=84)-----------
2022-01-29 03:50:26,643 - 10000 samples (128 per mini-batch)
2022-01-29 03:50:28,503 - Epoch: [84][   79/   79]    Loss 1.633388    Top1 54.490000    Top5 84.000000    
2022-01-29 03:50:28,559 - ==> Top1: 54.490    Top5: 84.000    Loss: 1.633

2022-01-29 03:50:28,564 - ==> Best [Top1: 55.240   Top5: 84.310   Sparsity:0.00   Params: 1341960 on epoch: 73]
2022-01-29 03:50:28,564 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:50:28,612 - 

2022-01-29 03:50:28,612 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:50:34,371 - Epoch: [85][  100/  391]    Overall Loss 1.132629    Objective Loss 1.132629                                        LR 0.100000    Time 0.057562    
2022-01-29 03:50:39,710 - Epoch: [85][  200/  391]    Overall Loss 1.166703    Objective Loss 1.166703                                        LR 0.100000    Time 0.055470    
2022-01-29 03:50:45,045 - Epoch: [85][  300/  391]    Overall Loss 1.175016    Objective Loss 1.175016                                        LR 0.100000    Time 0.054760    
2022-01-29 03:50:49,941 - Epoch: [85][  391/  391]    Overall Loss 1.179928    Objective Loss 1.179928    Top1 67.788462    Top5 90.384615    LR 0.100000    Time 0.054537    
2022-01-29 03:50:50,000 - --- validate (epoch=85)-----------
2022-01-29 03:50:50,000 - 10000 samples (128 per mini-batch)
2022-01-29 03:50:51,848 - Epoch: [85][   79/   79]    Loss 1.769724    Top1 52.730000    Top5 82.370000    
2022-01-29 03:50:51,904 - ==> Top1: 52.730    Top5: 82.370    Loss: 1.770

2022-01-29 03:50:51,909 - ==> Best [Top1: 55.240   Top5: 84.310   Sparsity:0.00   Params: 1341960 on epoch: 73]
2022-01-29 03:50:51,909 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:50:51,957 - 

2022-01-29 03:50:51,957 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:50:57,554 - Epoch: [86][  100/  391]    Overall Loss 1.115645    Objective Loss 1.115645                                        LR 0.100000    Time 0.055943    
2022-01-29 03:51:02,890 - Epoch: [86][  200/  391]    Overall Loss 1.151949    Objective Loss 1.151949                                        LR 0.100000    Time 0.054645    
2022-01-29 03:51:08,227 - Epoch: [86][  300/  391]    Overall Loss 1.166735    Objective Loss 1.166735                                        LR 0.100000    Time 0.054216    
2022-01-29 03:51:13,071 - Epoch: [86][  391/  391]    Overall Loss 1.174420    Objective Loss 1.174420    Top1 60.576923    Top5 89.423077    LR 0.100000    Time 0.053985    
2022-01-29 03:51:13,129 - --- validate (epoch=86)-----------
2022-01-29 03:51:13,130 - 10000 samples (128 per mini-batch)
2022-01-29 03:51:14,950 - Epoch: [86][   79/   79]    Loss 1.602242    Top1 55.210000    Top5 85.010000    
2022-01-29 03:51:15,000 - ==> Top1: 55.210    Top5: 85.010    Loss: 1.602

2022-01-29 03:51:15,006 - ==> Best [Top1: 55.240   Top5: 84.310   Sparsity:0.00   Params: 1341960 on epoch: 73]
2022-01-29 03:51:15,006 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:51:15,053 - 

2022-01-29 03:51:15,053 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:51:20,743 - Epoch: [87][  100/  391]    Overall Loss 1.116351    Objective Loss 1.116351                                        LR 0.100000    Time 0.056875    
2022-01-29 03:51:26,084 - Epoch: [87][  200/  391]    Overall Loss 1.142694    Objective Loss 1.142694                                        LR 0.100000    Time 0.055139    
2022-01-29 03:51:31,478 - Epoch: [87][  300/  391]    Overall Loss 1.150810    Objective Loss 1.150810                                        LR 0.100000    Time 0.054735    
2022-01-29 03:51:36,370 - Epoch: [87][  391/  391]    Overall Loss 1.156715    Objective Loss 1.156715    Top1 58.173077    Top5 90.384615    LR 0.100000    Time 0.054506    
2022-01-29 03:51:36,435 - --- validate (epoch=87)-----------
2022-01-29 03:51:36,436 - 10000 samples (128 per mini-batch)
2022-01-29 03:51:38,275 - Epoch: [87][   79/   79]    Loss 1.693921    Top1 54.840000    Top5 83.950000    
2022-01-29 03:51:38,333 - ==> Top1: 54.840    Top5: 83.950    Loss: 1.694

2022-01-29 03:51:38,339 - ==> Best [Top1: 55.240   Top5: 84.310   Sparsity:0.00   Params: 1341960 on epoch: 73]
2022-01-29 03:51:38,339 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:51:38,386 - 

2022-01-29 03:51:38,387 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:51:44,092 - Epoch: [88][  100/  391]    Overall Loss 1.113654    Objective Loss 1.113654                                        LR 0.100000    Time 0.057024    
2022-01-29 03:51:49,476 - Epoch: [88][  200/  391]    Overall Loss 1.134176    Objective Loss 1.134176                                        LR 0.100000    Time 0.055428    
2022-01-29 03:51:54,872 - Epoch: [88][  300/  391]    Overall Loss 1.145622    Objective Loss 1.145622                                        LR 0.100000    Time 0.054936    
2022-01-29 03:51:59,781 - Epoch: [88][  391/  391]    Overall Loss 1.158782    Objective Loss 1.158782    Top1 67.788462    Top5 93.750000    LR 0.100000    Time 0.054704    
2022-01-29 03:51:59,839 - --- validate (epoch=88)-----------
2022-01-29 03:51:59,839 - 10000 samples (128 per mini-batch)
2022-01-29 03:52:01,681 - Epoch: [88][   79/   79]    Loss 1.804777    Top1 52.080000    Top5 82.040000    
2022-01-29 03:52:01,738 - ==> Top1: 52.080    Top5: 82.040    Loss: 1.805

2022-01-29 03:52:01,744 - ==> Best [Top1: 55.240   Top5: 84.310   Sparsity:0.00   Params: 1341960 on epoch: 73]
2022-01-29 03:52:01,744 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:52:01,792 - 

2022-01-29 03:52:01,792 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:52:07,373 - Epoch: [89][  100/  391]    Overall Loss 1.113418    Objective Loss 1.113418                                        LR 0.100000    Time 0.055781    
2022-01-29 03:52:12,725 - Epoch: [89][  200/  391]    Overall Loss 1.127412    Objective Loss 1.127412                                        LR 0.100000    Time 0.054646    
2022-01-29 03:52:18,111 - Epoch: [89][  300/  391]    Overall Loss 1.155258    Objective Loss 1.155258                                        LR 0.100000    Time 0.054382    
2022-01-29 03:52:23,004 - Epoch: [89][  391/  391]    Overall Loss 1.160576    Objective Loss 1.160576    Top1 68.750000    Top5 91.826923    LR 0.100000    Time 0.054238    
2022-01-29 03:52:23,062 - --- validate (epoch=89)-----------
2022-01-29 03:52:23,063 - 10000 samples (128 per mini-batch)
2022-01-29 03:52:24,923 - Epoch: [89][   79/   79]    Loss 1.884704    Top1 51.510000    Top5 80.280000    
2022-01-29 03:52:24,975 - ==> Top1: 51.510    Top5: 80.280    Loss: 1.885

2022-01-29 03:52:24,980 - ==> Best [Top1: 55.240   Top5: 84.310   Sparsity:0.00   Params: 1341960 on epoch: 73]
2022-01-29 03:52:24,981 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:52:25,026 - 

2022-01-29 03:52:25,026 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:52:30,704 - Epoch: [90][  100/  391]    Overall Loss 1.100026    Objective Loss 1.100026                                        LR 0.100000    Time 0.056752    
2022-01-29 03:52:36,090 - Epoch: [90][  200/  391]    Overall Loss 1.122170    Objective Loss 1.122170                                        LR 0.100000    Time 0.055301    
2022-01-29 03:52:41,493 - Epoch: [90][  300/  391]    Overall Loss 1.132471    Objective Loss 1.132471                                        LR 0.100000    Time 0.054873    
2022-01-29 03:52:46,399 - Epoch: [90][  391/  391]    Overall Loss 1.144854    Objective Loss 1.144854    Top1 63.461538    Top5 92.788462    LR 0.100000    Time 0.054648    
2022-01-29 03:52:46,457 - --- validate (epoch=90)-----------
2022-01-29 03:52:46,457 - 10000 samples (128 per mini-batch)
2022-01-29 03:52:48,300 - Epoch: [90][   79/   79]    Loss 1.625489    Top1 55.510000    Top5 84.620000    
2022-01-29 03:52:48,359 - ==> Top1: 55.510    Top5: 84.620    Loss: 1.625

2022-01-29 03:52:48,364 - ==> Best [Top1: 55.510   Top5: 84.620   Sparsity:0.00   Params: 1341960 on epoch: 90]
2022-01-29 03:52:48,364 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:52:48,422 - 

2022-01-29 03:52:48,422 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:52:54,165 - Epoch: [91][  100/  391]    Overall Loss 1.089959    Objective Loss 1.089959                                        LR 0.100000    Time 0.057405    
2022-01-29 03:52:59,617 - Epoch: [91][  200/  391]    Overall Loss 1.110684    Objective Loss 1.110684                                        LR 0.100000    Time 0.055960    
2022-01-29 03:53:05,054 - Epoch: [91][  300/  391]    Overall Loss 1.127415    Objective Loss 1.127415                                        LR 0.100000    Time 0.055426    
2022-01-29 03:53:09,938 - Epoch: [91][  391/  391]    Overall Loss 1.135144    Objective Loss 1.135144    Top1 64.423077    Top5 92.307692    LR 0.100000    Time 0.055016    
2022-01-29 03:53:09,996 - --- validate (epoch=91)-----------
2022-01-29 03:53:09,996 - 10000 samples (128 per mini-batch)
2022-01-29 03:53:11,814 - Epoch: [91][   79/   79]    Loss 1.678606    Top1 55.690000    Top5 83.670000    
2022-01-29 03:53:11,873 - ==> Top1: 55.690    Top5: 83.670    Loss: 1.679

2022-01-29 03:53:11,878 - ==> Best [Top1: 55.690   Top5: 83.670   Sparsity:0.00   Params: 1341960 on epoch: 91]
2022-01-29 03:53:11,878 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:53:11,934 - 

2022-01-29 03:53:11,934 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:53:17,551 - Epoch: [92][  100/  391]    Overall Loss 1.100726    Objective Loss 1.100726                                        LR 0.100000    Time 0.056139    
2022-01-29 03:53:22,945 - Epoch: [92][  200/  391]    Overall Loss 1.113654    Objective Loss 1.113654                                        LR 0.100000    Time 0.055036    
2022-01-29 03:53:28,346 - Epoch: [92][  300/  391]    Overall Loss 1.126764    Objective Loss 1.126764                                        LR 0.100000    Time 0.054692    
2022-01-29 03:53:33,250 - Epoch: [92][  391/  391]    Overall Loss 1.140859    Objective Loss 1.140859    Top1 59.615385    Top5 87.980769    LR 0.100000    Time 0.054505    
2022-01-29 03:53:33,309 - --- validate (epoch=92)-----------
2022-01-29 03:53:33,309 - 10000 samples (128 per mini-batch)
2022-01-29 03:53:35,170 - Epoch: [92][   79/   79]    Loss 1.696493    Top1 54.770000    Top5 83.730000    
2022-01-29 03:53:35,228 - ==> Top1: 54.770    Top5: 83.730    Loss: 1.696

2022-01-29 03:53:35,233 - ==> Best [Top1: 55.690   Top5: 83.670   Sparsity:0.00   Params: 1341960 on epoch: 91]
2022-01-29 03:53:35,233 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:53:35,280 - 

2022-01-29 03:53:35,280 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:53:40,989 - Epoch: [93][  100/  391]    Overall Loss 1.103916    Objective Loss 1.103916                                        LR 0.100000    Time 0.057067    
2022-01-29 03:53:46,389 - Epoch: [93][  200/  391]    Overall Loss 1.126084    Objective Loss 1.126084                                        LR 0.100000    Time 0.055526    
2022-01-29 03:53:51,788 - Epoch: [93][  300/  391]    Overall Loss 1.139510    Objective Loss 1.139510                                        LR 0.100000    Time 0.055012    
2022-01-29 03:53:56,698 - Epoch: [93][  391/  391]    Overall Loss 1.146549    Objective Loss 1.146549    Top1 63.461538    Top5 95.673077    LR 0.100000    Time 0.054766    
2022-01-29 03:53:56,763 - --- validate (epoch=93)-----------
2022-01-29 03:53:56,763 - 10000 samples (128 per mini-batch)
2022-01-29 03:53:58,603 - Epoch: [93][   79/   79]    Loss 1.893190    Top1 51.030000    Top5 80.390000    
2022-01-29 03:53:58,656 - ==> Top1: 51.030    Top5: 80.390    Loss: 1.893

2022-01-29 03:53:58,662 - ==> Best [Top1: 55.690   Top5: 83.670   Sparsity:0.00   Params: 1341960 on epoch: 91]
2022-01-29 03:53:58,662 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:53:58,709 - 

2022-01-29 03:53:58,710 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:54:04,390 - Epoch: [94][  100/  391]    Overall Loss 1.085723    Objective Loss 1.085723                                        LR 0.100000    Time 0.056776    
2022-01-29 03:54:09,787 - Epoch: [94][  200/  391]    Overall Loss 1.098741    Objective Loss 1.098741                                        LR 0.100000    Time 0.055368    
2022-01-29 03:54:15,177 - Epoch: [94][  300/  391]    Overall Loss 1.107510    Objective Loss 1.107510                                        LR 0.100000    Time 0.054878    
2022-01-29 03:54:20,042 - Epoch: [94][  391/  391]    Overall Loss 1.124548    Objective Loss 1.124548    Top1 65.384615    Top5 91.346154    LR 0.100000    Time 0.054547    
2022-01-29 03:54:20,098 - --- validate (epoch=94)-----------
2022-01-29 03:54:20,098 - 10000 samples (128 per mini-batch)
2022-01-29 03:54:21,939 - Epoch: [94][   79/   79]    Loss 1.638706    Top1 55.070000    Top5 84.030000    
2022-01-29 03:54:21,997 - ==> Top1: 55.070    Top5: 84.030    Loss: 1.639

2022-01-29 03:54:22,002 - ==> Best [Top1: 55.690   Top5: 83.670   Sparsity:0.00   Params: 1341960 on epoch: 91]
2022-01-29 03:54:22,003 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:54:22,049 - 

2022-01-29 03:54:22,050 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:54:27,627 - Epoch: [95][  100/  391]    Overall Loss 1.083299    Objective Loss 1.083299                                        LR 0.100000    Time 0.055751    
2022-01-29 03:54:33,012 - Epoch: [95][  200/  391]    Overall Loss 1.093350    Objective Loss 1.093350                                        LR 0.100000    Time 0.054792    
2022-01-29 03:54:38,972 - Epoch: [95][  300/  391]    Overall Loss 1.109065    Objective Loss 1.109065                                        LR 0.100000    Time 0.056392    
2022-01-29 03:54:45,933 - Epoch: [95][  391/  391]    Overall Loss 1.121201    Objective Loss 1.121201    Top1 66.346154    Top5 92.307692    LR 0.100000    Time 0.061070    
2022-01-29 03:54:45,991 - --- validate (epoch=95)-----------
2022-01-29 03:54:45,991 - 10000 samples (128 per mini-batch)
2022-01-29 03:54:47,843 - Epoch: [95][   79/   79]    Loss 1.868555    Top1 50.390000    Top5 81.220000    
2022-01-29 03:54:47,899 - ==> Top1: 50.390    Top5: 81.220    Loss: 1.869

2022-01-29 03:54:47,904 - ==> Best [Top1: 55.690   Top5: 83.670   Sparsity:0.00   Params: 1341960 on epoch: 91]
2022-01-29 03:54:47,904 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:54:47,952 - 

2022-01-29 03:54:47,952 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:54:53,646 - Epoch: [96][  100/  391]    Overall Loss 1.084347    Objective Loss 1.084347                                        LR 0.100000    Time 0.056910    
2022-01-29 03:54:59,033 - Epoch: [96][  200/  391]    Overall Loss 1.106899    Objective Loss 1.106899                                        LR 0.100000    Time 0.055387    
2022-01-29 03:55:04,437 - Epoch: [96][  300/  391]    Overall Loss 1.116686    Objective Loss 1.116686                                        LR 0.100000    Time 0.054936    
2022-01-29 03:55:09,339 - Epoch: [96][  391/  391]    Overall Loss 1.123672    Objective Loss 1.123672    Top1 63.942308    Top5 91.346154    LR 0.100000    Time 0.054686    
2022-01-29 03:55:09,404 - --- validate (epoch=96)-----------
2022-01-29 03:55:09,404 - 10000 samples (128 per mini-batch)
2022-01-29 03:55:11,231 - Epoch: [96][   79/   79]    Loss 1.706724    Top1 53.820000    Top5 83.030000    
2022-01-29 03:55:11,282 - ==> Top1: 53.820    Top5: 83.030    Loss: 1.707

2022-01-29 03:55:11,288 - ==> Best [Top1: 55.690   Top5: 83.670   Sparsity:0.00   Params: 1341960 on epoch: 91]
2022-01-29 03:55:11,288 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:55:11,334 - 

2022-01-29 03:55:11,335 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:55:17,061 - Epoch: [97][  100/  391]    Overall Loss 1.091013    Objective Loss 1.091013                                        LR 0.100000    Time 0.057237    
2022-01-29 03:55:22,502 - Epoch: [97][  200/  391]    Overall Loss 1.101718    Objective Loss 1.101718                                        LR 0.100000    Time 0.055818    
2022-01-29 03:55:27,945 - Epoch: [97][  300/  391]    Overall Loss 1.117115    Objective Loss 1.117115                                        LR 0.100000    Time 0.055353    
2022-01-29 03:55:32,874 - Epoch: [97][  391/  391]    Overall Loss 1.128591    Objective Loss 1.128591    Top1 62.019231    Top5 92.307692    LR 0.100000    Time 0.055076    
2022-01-29 03:55:32,932 - --- validate (epoch=97)-----------
2022-01-29 03:55:32,932 - 10000 samples (128 per mini-batch)
2022-01-29 03:55:34,871 - Epoch: [97][   79/   79]    Loss 1.776200    Top1 52.380000    Top5 82.650000    
2022-01-29 03:55:34,929 - ==> Top1: 52.380    Top5: 82.650    Loss: 1.776

2022-01-29 03:55:34,934 - ==> Best [Top1: 55.690   Top5: 83.670   Sparsity:0.00   Params: 1341960 on epoch: 91]
2022-01-29 03:55:34,934 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:55:34,982 - 

2022-01-29 03:55:34,983 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:55:40,614 - Epoch: [98][  100/  391]    Overall Loss 1.089494    Objective Loss 1.089494                                        LR 0.100000    Time 0.056287    
2022-01-29 03:55:45,996 - Epoch: [98][  200/  391]    Overall Loss 1.089211    Objective Loss 1.089211                                        LR 0.100000    Time 0.055051    
2022-01-29 03:55:51,350 - Epoch: [98][  300/  391]    Overall Loss 1.105548    Objective Loss 1.105548                                        LR 0.100000    Time 0.054544    
2022-01-29 03:55:56,231 - Epoch: [98][  391/  391]    Overall Loss 1.116014    Objective Loss 1.116014    Top1 65.384615    Top5 92.307692    LR 0.100000    Time 0.054330    
2022-01-29 03:55:56,289 - --- validate (epoch=98)-----------
2022-01-29 03:55:56,289 - 10000 samples (128 per mini-batch)
2022-01-29 03:55:58,136 - Epoch: [98][   79/   79]    Loss 1.757350    Top1 53.940000    Top5 83.250000    
2022-01-29 03:55:58,191 - ==> Top1: 53.940    Top5: 83.250    Loss: 1.757

2022-01-29 03:55:58,196 - ==> Best [Top1: 55.690   Top5: 83.670   Sparsity:0.00   Params: 1341960 on epoch: 91]
2022-01-29 03:55:58,197 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:55:58,243 - 

2022-01-29 03:55:58,243 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:56:03,873 - Epoch: [99][  100/  391]    Overall Loss 1.058860    Objective Loss 1.058860                                        LR 0.100000    Time 0.056265    
2022-01-29 03:56:09,246 - Epoch: [99][  200/  391]    Overall Loss 1.087224    Objective Loss 1.087224                                        LR 0.100000    Time 0.054998    
2022-01-29 03:56:14,626 - Epoch: [99][  300/  391]    Overall Loss 1.097891    Objective Loss 1.097891                                        LR 0.100000    Time 0.054595    
2022-01-29 03:56:19,515 - Epoch: [99][  391/  391]    Overall Loss 1.102318    Objective Loss 1.102318    Top1 71.634615    Top5 93.269231    LR 0.100000    Time 0.054389    
2022-01-29 03:56:19,573 - --- validate (epoch=99)-----------
2022-01-29 03:56:19,573 - 10000 samples (128 per mini-batch)
2022-01-29 03:56:21,427 - Epoch: [99][   79/   79]    Loss 1.694989    Top1 54.380000    Top5 83.230000    
2022-01-29 03:56:21,487 - ==> Top1: 54.380    Top5: 83.230    Loss: 1.695

2022-01-29 03:56:21,492 - ==> Best [Top1: 55.690   Top5: 83.670   Sparsity:0.00   Params: 1341960 on epoch: 91]
2022-01-29 03:56:21,492 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:56:21,540 - 

2022-01-29 03:56:21,540 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:56:27,216 - Epoch: [100][  100/  391]    Overall Loss 0.872207    Objective Loss 0.872207                                        LR 0.023500    Time 0.056735    
2022-01-29 03:56:32,604 - Epoch: [100][  200/  391]    Overall Loss 0.831011    Objective Loss 0.831011                                        LR 0.023500    Time 0.055301    
2022-01-29 03:56:38,000 - Epoch: [100][  300/  391]    Overall Loss 0.809585    Objective Loss 0.809585                                        LR 0.023500    Time 0.054851    
2022-01-29 03:56:42,908 - Epoch: [100][  391/  391]    Overall Loss 0.799930    Objective Loss 0.799930    Top1 78.365385    Top5 94.711538    LR 0.023500    Time 0.054637    
2022-01-29 03:56:42,967 - --- validate (epoch=100)-----------
2022-01-29 03:56:42,968 - 10000 samples (128 per mini-batch)
2022-01-29 03:56:44,825 - Epoch: [100][   79/   79]    Loss 1.212181    Top1 65.820000    Top5 90.620000    
2022-01-29 03:56:44,876 - ==> Top1: 65.820    Top5: 90.620    Loss: 1.212

2022-01-29 03:56:44,882 - ==> Best [Top1: 65.820   Top5: 90.620   Sparsity:0.00   Params: 1341960 on epoch: 100]
2022-01-29 03:56:44,882 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:56:44,937 - 

2022-01-29 03:56:44,937 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:56:50,496 - Epoch: [101][  100/  391]    Overall Loss 0.719557    Objective Loss 0.719557                                        LR 0.023500    Time 0.055558    
2022-01-29 03:56:55,856 - Epoch: [101][  200/  391]    Overall Loss 0.723752    Objective Loss 0.723752                                        LR 0.023500    Time 0.054573    
2022-01-29 03:57:01,226 - Epoch: [101][  300/  391]    Overall Loss 0.721355    Objective Loss 0.721355                                        LR 0.023500    Time 0.054282    
2022-01-29 03:57:06,058 - Epoch: [101][  391/  391]    Overall Loss 0.722468    Objective Loss 0.722468    Top1 79.326923    Top5 96.634615    LR 0.023500    Time 0.054003    
2022-01-29 03:57:06,116 - --- validate (epoch=101)-----------
2022-01-29 03:57:06,117 - 10000 samples (128 per mini-batch)
2022-01-29 03:57:07,955 - Epoch: [101][   79/   79]    Loss 1.195643    Top1 66.080000    Top5 90.380000    
2022-01-29 03:57:08,006 - ==> Top1: 66.080    Top5: 90.380    Loss: 1.196

2022-01-29 03:57:08,011 - ==> Best [Top1: 66.080   Top5: 90.380   Sparsity:0.00   Params: 1341960 on epoch: 101]
2022-01-29 03:57:08,011 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:57:08,067 - 

2022-01-29 03:57:08,067 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:57:13,691 - Epoch: [102][  100/  391]    Overall Loss 0.685718    Objective Loss 0.685718                                        LR 0.023500    Time 0.056206    
2022-01-29 03:57:19,009 - Epoch: [102][  200/  391]    Overall Loss 0.688689    Objective Loss 0.688689                                        LR 0.023500    Time 0.054688    
2022-01-29 03:57:24,326 - Epoch: [102][  300/  391]    Overall Loss 0.690649    Objective Loss 0.690649                                        LR 0.023500    Time 0.054182    
2022-01-29 03:57:29,162 - Epoch: [102][  391/  391]    Overall Loss 0.690487    Objective Loss 0.690487    Top1 78.846154    Top5 94.711538    LR 0.023500    Time 0.053937    
2022-01-29 03:57:29,218 - --- validate (epoch=102)-----------
2022-01-29 03:57:29,218 - 10000 samples (128 per mini-batch)
2022-01-29 03:57:31,032 - Epoch: [102][   79/   79]    Loss 1.216385    Top1 65.990000    Top5 90.330000    
2022-01-29 03:57:31,088 - ==> Top1: 65.990    Top5: 90.330    Loss: 1.216

2022-01-29 03:57:31,094 - ==> Best [Top1: 66.080   Top5: 90.380   Sparsity:0.00   Params: 1341960 on epoch: 101]
2022-01-29 03:57:31,094 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:57:31,141 - 

2022-01-29 03:57:31,141 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:57:36,765 - Epoch: [103][  100/  391]    Overall Loss 0.642530    Objective Loss 0.642530                                        LR 0.023500    Time 0.056213    
2022-01-29 03:57:42,069 - Epoch: [103][  200/  391]    Overall Loss 0.652764    Objective Loss 0.652764                                        LR 0.023500    Time 0.054624    
2022-01-29 03:57:47,372 - Epoch: [103][  300/  391]    Overall Loss 0.664835    Objective Loss 0.664835                                        LR 0.023500    Time 0.054090    
2022-01-29 03:57:52,195 - Epoch: [103][  391/  391]    Overall Loss 0.669280    Objective Loss 0.669280    Top1 82.211538    Top5 96.153846    LR 0.023500    Time 0.053834    
2022-01-29 03:57:52,254 - --- validate (epoch=103)-----------
2022-01-29 03:57:52,254 - 10000 samples (128 per mini-batch)
2022-01-29 03:57:54,079 - Epoch: [103][   79/   79]    Loss 1.213357    Top1 65.800000    Top5 90.200000    
2022-01-29 03:57:54,133 - ==> Top1: 65.800    Top5: 90.200    Loss: 1.213

2022-01-29 03:57:54,138 - ==> Best [Top1: 66.080   Top5: 90.380   Sparsity:0.00   Params: 1341960 on epoch: 101]
2022-01-29 03:57:54,138 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:57:54,185 - 

2022-01-29 03:57:54,185 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:57:59,729 - Epoch: [104][  100/  391]    Overall Loss 0.633228    Objective Loss 0.633228                                        LR 0.023500    Time 0.055417    
2022-01-29 03:58:05,043 - Epoch: [104][  200/  391]    Overall Loss 0.644387    Objective Loss 0.644387                                        LR 0.023500    Time 0.054269    
2022-01-29 03:58:10,356 - Epoch: [104][  300/  391]    Overall Loss 0.646803    Objective Loss 0.646803                                        LR 0.023500    Time 0.053888    
2022-01-29 03:58:15,183 - Epoch: [104][  391/  391]    Overall Loss 0.655468    Objective Loss 0.655468    Top1 77.884615    Top5 95.192308    LR 0.023500    Time 0.053690    
2022-01-29 03:58:15,242 - --- validate (epoch=104)-----------
2022-01-29 03:58:15,242 - 10000 samples (128 per mini-batch)
2022-01-29 03:58:17,084 - Epoch: [104][   79/   79]    Loss 1.208443    Top1 65.980000    Top5 90.520000    
2022-01-29 03:58:17,141 - ==> Top1: 65.980    Top5: 90.520    Loss: 1.208

2022-01-29 03:58:17,146 - ==> Best [Top1: 66.080   Top5: 90.380   Sparsity:0.00   Params: 1341960 on epoch: 101]
2022-01-29 03:58:17,146 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:58:17,193 - 

2022-01-29 03:58:17,193 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:58:22,785 - Epoch: [105][  100/  391]    Overall Loss 0.618649    Objective Loss 0.618649                                        LR 0.023500    Time 0.055889    
2022-01-29 03:58:28,071 - Epoch: [105][  200/  391]    Overall Loss 0.627982    Objective Loss 0.627982                                        LR 0.023500    Time 0.054372    
2022-01-29 03:58:33,371 - Epoch: [105][  300/  391]    Overall Loss 0.637072    Objective Loss 0.637072                                        LR 0.023500    Time 0.053910    
2022-01-29 03:58:38,227 - Epoch: [105][  391/  391]    Overall Loss 0.641280    Objective Loss 0.641280    Top1 78.365385    Top5 96.153846    LR 0.023500    Time 0.053781    
2022-01-29 03:58:38,286 - --- validate (epoch=105)-----------
2022-01-29 03:58:38,286 - 10000 samples (128 per mini-batch)
2022-01-29 03:58:40,093 - Epoch: [105][   79/   79]    Loss 1.205730    Top1 65.430000    Top5 90.430000    
2022-01-29 03:58:40,149 - ==> Top1: 65.430    Top5: 90.430    Loss: 1.206

2022-01-29 03:58:40,154 - ==> Best [Top1: 66.080   Top5: 90.380   Sparsity:0.00   Params: 1341960 on epoch: 101]
2022-01-29 03:58:40,154 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:58:40,201 - 

2022-01-29 03:58:40,201 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:58:45,907 - Epoch: [106][  100/  391]    Overall Loss 0.615876    Objective Loss 0.615876                                        LR 0.023500    Time 0.057032    
2022-01-29 03:58:51,308 - Epoch: [106][  200/  391]    Overall Loss 0.624618    Objective Loss 0.624618                                        LR 0.023500    Time 0.055518    
2022-01-29 03:58:56,705 - Epoch: [106][  300/  391]    Overall Loss 0.626339    Objective Loss 0.626339                                        LR 0.023500    Time 0.055001    
2022-01-29 03:59:01,606 - Epoch: [106][  391/  391]    Overall Loss 0.627312    Objective Loss 0.627312    Top1 86.057692    Top5 97.115385    LR 0.023500    Time 0.054731    
2022-01-29 03:59:01,668 - --- validate (epoch=106)-----------
2022-01-29 03:59:01,668 - 10000 samples (128 per mini-batch)
2022-01-29 03:59:03,480 - Epoch: [106][   79/   79]    Loss 1.205678    Top1 65.780000    Top5 90.560000    
2022-01-29 03:59:03,531 - ==> Top1: 65.780    Top5: 90.560    Loss: 1.206

2022-01-29 03:59:03,536 - ==> Best [Top1: 66.080   Top5: 90.380   Sparsity:0.00   Params: 1341960 on epoch: 101]
2022-01-29 03:59:03,536 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:59:03,582 - 

2022-01-29 03:59:03,583 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:59:09,135 - Epoch: [107][  100/  391]    Overall Loss 0.594707    Objective Loss 0.594707                                        LR 0.023500    Time 0.055497    
2022-01-29 03:59:14,511 - Epoch: [107][  200/  391]    Overall Loss 0.604584    Objective Loss 0.604584                                        LR 0.023500    Time 0.054624    
2022-01-29 03:59:19,888 - Epoch: [107][  300/  391]    Overall Loss 0.607206    Objective Loss 0.607206                                        LR 0.023500    Time 0.054336    
2022-01-29 03:59:24,772 - Epoch: [107][  391/  391]    Overall Loss 0.620541    Objective Loss 0.620541    Top1 80.288462    Top5 97.596154    LR 0.023500    Time 0.054180    
2022-01-29 03:59:24,830 - --- validate (epoch=107)-----------
2022-01-29 03:59:24,831 - 10000 samples (128 per mini-batch)
2022-01-29 03:59:26,770 - Epoch: [107][   79/   79]    Loss 1.228850    Top1 65.940000    Top5 90.250000    
2022-01-29 03:59:26,826 - ==> Top1: 65.940    Top5: 90.250    Loss: 1.229

2022-01-29 03:59:26,831 - ==> Best [Top1: 66.080   Top5: 90.380   Sparsity:0.00   Params: 1341960 on epoch: 101]
2022-01-29 03:59:26,831 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:59:26,879 - 

2022-01-29 03:59:26,879 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:59:32,582 - Epoch: [108][  100/  391]    Overall Loss 0.583449    Objective Loss 0.583449                                        LR 0.023500    Time 0.057003    
2022-01-29 03:59:37,968 - Epoch: [108][  200/  391]    Overall Loss 0.591985    Objective Loss 0.591985                                        LR 0.023500    Time 0.055426    
2022-01-29 03:59:43,373 - Epoch: [108][  300/  391]    Overall Loss 0.606259    Objective Loss 0.606259                                        LR 0.023500    Time 0.054964    
2022-01-29 03:59:48,268 - Epoch: [108][  391/  391]    Overall Loss 0.611182    Objective Loss 0.611182    Top1 81.730769    Top5 99.038462    LR 0.023500    Time 0.054688    
2022-01-29 03:59:48,325 - --- validate (epoch=108)-----------
2022-01-29 03:59:48,325 - 10000 samples (128 per mini-batch)
2022-01-29 03:59:50,178 - Epoch: [108][   79/   79]    Loss 1.218018    Top1 65.870000    Top5 90.220000    
2022-01-29 03:59:50,236 - ==> Top1: 65.870    Top5: 90.220    Loss: 1.218

2022-01-29 03:59:50,241 - ==> Best [Top1: 66.080   Top5: 90.380   Sparsity:0.00   Params: 1341960 on epoch: 101]
2022-01-29 03:59:50,241 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 03:59:50,289 - 

2022-01-29 03:59:50,289 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 03:59:55,988 - Epoch: [109][  100/  391]    Overall Loss 0.573295    Objective Loss 0.573295                                        LR 0.023500    Time 0.056964    
2022-01-29 04:00:01,367 - Epoch: [109][  200/  391]    Overall Loss 0.586122    Objective Loss 0.586122                                        LR 0.023500    Time 0.055373    
2022-01-29 04:00:06,761 - Epoch: [109][  300/  391]    Overall Loss 0.594888    Objective Loss 0.594888                                        LR 0.023500    Time 0.054893    
2022-01-29 04:00:11,691 - Epoch: [109][  391/  391]    Overall Loss 0.599860    Objective Loss 0.599860    Top1 81.730769    Top5 96.153846    LR 0.023500    Time 0.054723    
2022-01-29 04:00:11,747 - --- validate (epoch=109)-----------
2022-01-29 04:00:11,747 - 10000 samples (128 per mini-batch)
2022-01-29 04:00:13,571 - Epoch: [109][   79/   79]    Loss 1.218744    Top1 65.770000    Top5 90.370000    
2022-01-29 04:00:13,622 - ==> Top1: 65.770    Top5: 90.370    Loss: 1.219

2022-01-29 04:00:13,627 - ==> Best [Top1: 66.080   Top5: 90.380   Sparsity:0.00   Params: 1341960 on epoch: 101]
2022-01-29 04:00:13,627 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:00:13,675 - 

2022-01-29 04:00:13,675 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:00:19,324 - Epoch: [110][  100/  391]    Overall Loss 0.564156    Objective Loss 0.564156                                        LR 0.023500    Time 0.056459    
2022-01-29 04:00:24,762 - Epoch: [110][  200/  391]    Overall Loss 0.577880    Objective Loss 0.577880                                        LR 0.023500    Time 0.055417    
2022-01-29 04:00:30,204 - Epoch: [110][  300/  391]    Overall Loss 0.583970    Objective Loss 0.583970                                        LR 0.023500    Time 0.055082    
2022-01-29 04:00:35,151 - Epoch: [110][  391/  391]    Overall Loss 0.591135    Objective Loss 0.591135    Top1 76.923077    Top5 96.634615    LR 0.023500    Time 0.054912    
2022-01-29 04:00:35,210 - --- validate (epoch=110)-----------
2022-01-29 04:00:35,210 - 10000 samples (128 per mini-batch)
2022-01-29 04:00:37,046 - Epoch: [110][   79/   79]    Loss 1.215106    Top1 66.160000    Top5 90.230000    
2022-01-29 04:00:37,097 - ==> Top1: 66.160    Top5: 90.230    Loss: 1.215

2022-01-29 04:00:37,102 - ==> Best [Top1: 66.160   Top5: 90.230   Sparsity:0.00   Params: 1341960 on epoch: 110]
2022-01-29 04:00:37,102 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:00:37,158 - 

2022-01-29 04:00:37,158 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:00:42,858 - Epoch: [111][  100/  391]    Overall Loss 0.554345    Objective Loss 0.554345                                        LR 0.023500    Time 0.056974    
2022-01-29 04:00:48,273 - Epoch: [111][  200/  391]    Overall Loss 0.561438    Objective Loss 0.561438                                        LR 0.023500    Time 0.055557    
2022-01-29 04:00:53,685 - Epoch: [111][  300/  391]    Overall Loss 0.573653    Objective Loss 0.573653                                        LR 0.023500    Time 0.055075    
2022-01-29 04:00:58,604 - Epoch: [111][  391/  391]    Overall Loss 0.582799    Objective Loss 0.582799    Top1 80.288462    Top5 97.115385    LR 0.023500    Time 0.054834    
2022-01-29 04:00:58,662 - --- validate (epoch=111)-----------
2022-01-29 04:00:58,662 - 10000 samples (128 per mini-batch)
2022-01-29 04:01:00,536 - Epoch: [111][   79/   79]    Loss 1.235875    Top1 66.200000    Top5 90.110000    
2022-01-29 04:01:00,594 - ==> Top1: 66.200    Top5: 90.110    Loss: 1.236

2022-01-29 04:01:00,600 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:01:00,600 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:01:00,657 - 

2022-01-29 04:01:00,657 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:01:06,384 - Epoch: [112][  100/  391]    Overall Loss 0.558805    Objective Loss 0.558805                                        LR 0.023500    Time 0.057234    
2022-01-29 04:01:11,777 - Epoch: [112][  200/  391]    Overall Loss 0.567451    Objective Loss 0.567451                                        LR 0.023500    Time 0.055582    
2022-01-29 04:01:17,177 - Epoch: [112][  300/  391]    Overall Loss 0.572872    Objective Loss 0.572872                                        LR 0.023500    Time 0.055049    
2022-01-29 04:01:22,082 - Epoch: [112][  391/  391]    Overall Loss 0.581876    Objective Loss 0.581876    Top1 75.480769    Top5 95.192308    LR 0.023500    Time 0.054781    
2022-01-29 04:01:22,141 - --- validate (epoch=112)-----------
2022-01-29 04:01:22,141 - 10000 samples (128 per mini-batch)
2022-01-29 04:01:23,998 - Epoch: [112][   79/   79]    Loss 1.265800    Top1 65.280000    Top5 90.360000    
2022-01-29 04:01:24,056 - ==> Top1: 65.280    Top5: 90.360    Loss: 1.266

2022-01-29 04:01:24,062 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:01:24,062 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:01:24,110 - 

2022-01-29 04:01:24,110 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:01:29,757 - Epoch: [113][  100/  391]    Overall Loss 0.553693    Objective Loss 0.553693                                        LR 0.023500    Time 0.056437    
2022-01-29 04:01:35,155 - Epoch: [113][  200/  391]    Overall Loss 0.558538    Objective Loss 0.558538                                        LR 0.023500    Time 0.055203    
2022-01-29 04:01:40,582 - Epoch: [113][  300/  391]    Overall Loss 0.568101    Objective Loss 0.568101                                        LR 0.023500    Time 0.054889    
2022-01-29 04:01:45,526 - Epoch: [113][  391/  391]    Overall Loss 0.574473    Objective Loss 0.574473    Top1 80.769231    Top5 96.153846    LR 0.023500    Time 0.054757    
2022-01-29 04:01:45,584 - --- validate (epoch=113)-----------
2022-01-29 04:01:45,584 - 10000 samples (128 per mini-batch)
2022-01-29 04:01:47,450 - Epoch: [113][   79/   79]    Loss 1.287369    Top1 64.850000    Top5 89.710000    
2022-01-29 04:01:47,503 - ==> Top1: 64.850    Top5: 89.710    Loss: 1.287

2022-01-29 04:01:47,509 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:01:47,509 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:01:47,556 - 

2022-01-29 04:01:47,557 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:01:53,292 - Epoch: [114][  100/  391]    Overall Loss 0.544962    Objective Loss 0.544962                                        LR 0.023500    Time 0.057329    
2022-01-29 04:01:58,702 - Epoch: [114][  200/  391]    Overall Loss 0.550670    Objective Loss 0.550670                                        LR 0.023500    Time 0.055711    
2022-01-29 04:02:04,113 - Epoch: [114][  300/  391]    Overall Loss 0.560607    Objective Loss 0.560607                                        LR 0.023500    Time 0.055175    
2022-01-29 04:02:09,033 - Epoch: [114][  391/  391]    Overall Loss 0.570885    Objective Loss 0.570885    Top1 81.250000    Top5 97.115385    LR 0.023500    Time 0.054914    
2022-01-29 04:02:09,088 - --- validate (epoch=114)-----------
2022-01-29 04:02:09,088 - 10000 samples (128 per mini-batch)
2022-01-29 04:02:10,955 - Epoch: [114][   79/   79]    Loss 1.246264    Top1 65.770000    Top5 90.300000    
2022-01-29 04:02:11,007 - ==> Top1: 65.770    Top5: 90.300    Loss: 1.246

2022-01-29 04:02:11,012 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:02:11,012 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:02:11,058 - 

2022-01-29 04:02:11,058 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:02:16,744 - Epoch: [115][  100/  391]    Overall Loss 0.542570    Objective Loss 0.542570                                        LR 0.023500    Time 0.056835    
2022-01-29 04:02:22,148 - Epoch: [115][  200/  391]    Overall Loss 0.547842    Objective Loss 0.547842                                        LR 0.023500    Time 0.055433    
2022-01-29 04:02:27,545 - Epoch: [115][  300/  391]    Overall Loss 0.556804    Objective Loss 0.556804                                        LR 0.023500    Time 0.054943    
2022-01-29 04:02:32,451 - Epoch: [115][  391/  391]    Overall Loss 0.560896    Objective Loss 0.560896    Top1 78.846154    Top5 95.192308    LR 0.023500    Time 0.054701    
2022-01-29 04:02:32,514 - --- validate (epoch=115)-----------
2022-01-29 04:02:32,514 - 10000 samples (128 per mini-batch)
2022-01-29 04:02:34,438 - Epoch: [115][   79/   79]    Loss 1.316596    Top1 64.490000    Top5 89.560000    
2022-01-29 04:02:34,493 - ==> Top1: 64.490    Top5: 89.560    Loss: 1.317

2022-01-29 04:02:34,498 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:02:34,498 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:02:34,546 - 

2022-01-29 04:02:34,546 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:02:40,187 - Epoch: [116][  100/  391]    Overall Loss 0.542520    Objective Loss 0.542520                                        LR 0.023500    Time 0.056385    
2022-01-29 04:02:45,588 - Epoch: [116][  200/  391]    Overall Loss 0.548437    Objective Loss 0.548437                                        LR 0.023500    Time 0.055188    
2022-01-29 04:02:51,007 - Epoch: [116][  300/  391]    Overall Loss 0.555051    Objective Loss 0.555051                                        LR 0.023500    Time 0.054852    
2022-01-29 04:02:55,927 - Epoch: [116][  391/  391]    Overall Loss 0.563423    Objective Loss 0.563423    Top1 80.288462    Top5 98.076923    LR 0.023500    Time 0.054667    
2022-01-29 04:02:55,983 - --- validate (epoch=116)-----------
2022-01-29 04:02:55,983 - 10000 samples (128 per mini-batch)
2022-01-29 04:02:57,861 - Epoch: [116][   79/   79]    Loss 1.254939    Top1 64.930000    Top5 90.270000    
2022-01-29 04:02:57,918 - ==> Top1: 64.930    Top5: 90.270    Loss: 1.255

2022-01-29 04:02:57,923 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:02:57,923 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:02:57,970 - 

2022-01-29 04:02:57,971 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:03:03,691 - Epoch: [117][  100/  391]    Overall Loss 0.538675    Objective Loss 0.538675                                        LR 0.023500    Time 0.057174    
2022-01-29 04:03:09,129 - Epoch: [117][  200/  391]    Overall Loss 0.540497    Objective Loss 0.540497                                        LR 0.023500    Time 0.055772    
2022-01-29 04:03:14,573 - Epoch: [117][  300/  391]    Overall Loss 0.549675    Objective Loss 0.549675                                        LR 0.023500    Time 0.055326    
2022-01-29 04:03:19,530 - Epoch: [117][  391/  391]    Overall Loss 0.557689    Objective Loss 0.557689    Top1 80.769231    Top5 97.115385    LR 0.023500    Time 0.055126    
2022-01-29 04:03:19,589 - --- validate (epoch=117)-----------
2022-01-29 04:03:19,589 - 10000 samples (128 per mini-batch)
2022-01-29 04:03:21,431 - Epoch: [117][   79/   79]    Loss 1.271502    Top1 65.060000    Top5 90.160000    
2022-01-29 04:03:21,488 - ==> Top1: 65.060    Top5: 90.160    Loss: 1.272

2022-01-29 04:03:21,493 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:03:21,493 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:03:21,540 - 

2022-01-29 04:03:21,540 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:03:27,229 - Epoch: [118][  100/  391]    Overall Loss 0.541336    Objective Loss 0.541336                                        LR 0.023500    Time 0.056861    
2022-01-29 04:03:32,630 - Epoch: [118][  200/  391]    Overall Loss 0.541085    Objective Loss 0.541085                                        LR 0.023500    Time 0.055432    
2022-01-29 04:03:38,031 - Epoch: [118][  300/  391]    Overall Loss 0.548991    Objective Loss 0.548991                                        LR 0.023500    Time 0.054955    
2022-01-29 04:03:42,957 - Epoch: [118][  391/  391]    Overall Loss 0.559688    Objective Loss 0.559688    Top1 81.730769    Top5 95.673077    LR 0.023500    Time 0.054762    
2022-01-29 04:03:43,016 - --- validate (epoch=118)-----------
2022-01-29 04:03:43,016 - 10000 samples (128 per mini-batch)
2022-01-29 04:03:44,826 - Epoch: [118][   79/   79]    Loss 1.260857    Top1 65.580000    Top5 90.000000    
2022-01-29 04:03:44,883 - ==> Top1: 65.580    Top5: 90.000    Loss: 1.261

2022-01-29 04:03:44,889 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:03:44,889 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:03:44,936 - 

2022-01-29 04:03:44,937 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:03:50,545 - Epoch: [119][  100/  391]    Overall Loss 0.511771    Objective Loss 0.511771                                        LR 0.023500    Time 0.056053    
2022-01-29 04:03:55,941 - Epoch: [119][  200/  391]    Overall Loss 0.525749    Objective Loss 0.525749                                        LR 0.023500    Time 0.055007    
2022-01-29 04:04:01,335 - Epoch: [119][  300/  391]    Overall Loss 0.538403    Objective Loss 0.538403                                        LR 0.023500    Time 0.054646    
2022-01-29 04:04:06,254 - Epoch: [119][  391/  391]    Overall Loss 0.551797    Objective Loss 0.551797    Top1 80.769231    Top5 97.115385    LR 0.023500    Time 0.054508    
2022-01-29 04:04:06,313 - --- validate (epoch=119)-----------
2022-01-29 04:04:06,313 - 10000 samples (128 per mini-batch)
2022-01-29 04:04:08,175 - Epoch: [119][   79/   79]    Loss 1.315643    Top1 64.610000    Top5 89.340000    
2022-01-29 04:04:08,226 - ==> Top1: 64.610    Top5: 89.340    Loss: 1.316

2022-01-29 04:04:08,231 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:04:08,231 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:04:08,279 - 

2022-01-29 04:04:08,280 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:04:13,984 - Epoch: [120][  100/  391]    Overall Loss 0.514836    Objective Loss 0.514836                                        LR 0.023500    Time 0.057021    
2022-01-29 04:04:19,376 - Epoch: [120][  200/  391]    Overall Loss 0.527181    Objective Loss 0.527181                                        LR 0.023500    Time 0.055465    
2022-01-29 04:04:24,769 - Epoch: [120][  300/  391]    Overall Loss 0.537386    Objective Loss 0.537386                                        LR 0.023500    Time 0.054950    
2022-01-29 04:04:29,672 - Epoch: [120][  391/  391]    Overall Loss 0.548961    Objective Loss 0.548961    Top1 78.846154    Top5 96.634615    LR 0.023500    Time 0.054699    
2022-01-29 04:04:29,730 - --- validate (epoch=120)-----------
2022-01-29 04:04:29,730 - 10000 samples (128 per mini-batch)
2022-01-29 04:04:31,593 - Epoch: [120][   79/   79]    Loss 1.297015    Top1 64.800000    Top5 89.950000    
2022-01-29 04:04:31,643 - ==> Top1: 64.800    Top5: 89.950    Loss: 1.297

2022-01-29 04:04:31,648 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:04:31,649 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:04:31,695 - 

2022-01-29 04:04:31,695 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:04:37,392 - Epoch: [121][  100/  391]    Overall Loss 0.511561    Objective Loss 0.511561                                        LR 0.023500    Time 0.056938    
2022-01-29 04:04:42,729 - Epoch: [121][  200/  391]    Overall Loss 0.528573    Objective Loss 0.528573                                        LR 0.023500    Time 0.055150    
2022-01-29 04:04:48,072 - Epoch: [121][  300/  391]    Overall Loss 0.538226    Objective Loss 0.538226                                        LR 0.023500    Time 0.054575    
2022-01-29 04:04:52,921 - Epoch: [121][  391/  391]    Overall Loss 0.546968    Objective Loss 0.546968    Top1 87.980769    Top5 99.519231    LR 0.023500    Time 0.054273    
2022-01-29 04:04:52,978 - --- validate (epoch=121)-----------
2022-01-29 04:04:52,978 - 10000 samples (128 per mini-batch)
2022-01-29 04:04:54,808 - Epoch: [121][   79/   79]    Loss 1.291577    Top1 65.280000    Top5 89.710000    
2022-01-29 04:04:54,863 - ==> Top1: 65.280    Top5: 89.710    Loss: 1.292

2022-01-29 04:04:54,868 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:04:54,868 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:04:54,915 - 

2022-01-29 04:04:54,915 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:05:00,491 - Epoch: [122][  100/  391]    Overall Loss 0.513471    Objective Loss 0.513471                                        LR 0.023500    Time 0.055733    
2022-01-29 04:05:05,870 - Epoch: [122][  200/  391]    Overall Loss 0.521886    Objective Loss 0.521886                                        LR 0.023500    Time 0.054756    
2022-01-29 04:05:11,270 - Epoch: [122][  300/  391]    Overall Loss 0.531071    Objective Loss 0.531071                                        LR 0.023500    Time 0.054502    
2022-01-29 04:05:16,145 - Epoch: [122][  391/  391]    Overall Loss 0.540163    Objective Loss 0.540163    Top1 78.846154    Top5 98.076923    LR 0.023500    Time 0.054282    
2022-01-29 04:05:16,197 - --- validate (epoch=122)-----------
2022-01-29 04:05:16,197 - 10000 samples (128 per mini-batch)
2022-01-29 04:05:18,035 - Epoch: [122][   79/   79]    Loss 1.332760    Top1 64.690000    Top5 89.680000    
2022-01-29 04:05:18,092 - ==> Top1: 64.690    Top5: 89.680    Loss: 1.333

2022-01-29 04:05:18,098 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:05:18,098 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:05:18,146 - 

2022-01-29 04:05:18,146 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:05:23,824 - Epoch: [123][  100/  391]    Overall Loss 0.499271    Objective Loss 0.499271                                        LR 0.023500    Time 0.056749    
2022-01-29 04:05:29,204 - Epoch: [123][  200/  391]    Overall Loss 0.515690    Objective Loss 0.515690                                        LR 0.023500    Time 0.055274    
2022-01-29 04:05:34,595 - Epoch: [123][  300/  391]    Overall Loss 0.529120    Objective Loss 0.529120                                        LR 0.023500    Time 0.054817    
2022-01-29 04:05:39,493 - Epoch: [123][  391/  391]    Overall Loss 0.540324    Objective Loss 0.540324    Top1 79.326923    Top5 97.115385    LR 0.023500    Time 0.054583    
2022-01-29 04:05:39,551 - --- validate (epoch=123)-----------
2022-01-29 04:05:39,551 - 10000 samples (128 per mini-batch)
2022-01-29 04:05:41,377 - Epoch: [123][   79/   79]    Loss 1.315419    Top1 64.430000    Top5 89.680000    
2022-01-29 04:05:41,433 - ==> Top1: 64.430    Top5: 89.680    Loss: 1.315

2022-01-29 04:05:41,439 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:05:41,439 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:05:41,485 - 

2022-01-29 04:05:41,485 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:05:47,113 - Epoch: [124][  100/  391]    Overall Loss 0.519050    Objective Loss 0.519050                                        LR 0.023500    Time 0.056253    
2022-01-29 04:05:52,421 - Epoch: [124][  200/  391]    Overall Loss 0.526647    Objective Loss 0.526647                                        LR 0.023500    Time 0.054660    
2022-01-29 04:05:57,756 - Epoch: [124][  300/  391]    Overall Loss 0.534229    Objective Loss 0.534229                                        LR 0.023500    Time 0.054221    
2022-01-29 04:06:02,593 - Epoch: [124][  391/  391]    Overall Loss 0.540004    Objective Loss 0.540004    Top1 83.173077    Top5 98.557692    LR 0.023500    Time 0.053971    
2022-01-29 04:06:02,654 - --- validate (epoch=124)-----------
2022-01-29 04:06:02,654 - 10000 samples (128 per mini-batch)
2022-01-29 04:06:04,495 - Epoch: [124][   79/   79]    Loss 1.263687    Top1 65.520000    Top5 89.840000    
2022-01-29 04:06:04,545 - ==> Top1: 65.520    Top5: 89.840    Loss: 1.264

2022-01-29 04:06:04,551 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:06:04,551 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:06:04,598 - 

2022-01-29 04:06:04,598 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:06:10,219 - Epoch: [125][  100/  391]    Overall Loss 0.515250    Objective Loss 0.515250                                        LR 0.023500    Time 0.056175    
2022-01-29 04:06:15,618 - Epoch: [125][  200/  391]    Overall Loss 0.523279    Objective Loss 0.523279                                        LR 0.023500    Time 0.055082    
2022-01-29 04:06:21,027 - Epoch: [125][  300/  391]    Overall Loss 0.526819    Objective Loss 0.526819                                        LR 0.023500    Time 0.054747    
2022-01-29 04:06:25,912 - Epoch: [125][  391/  391]    Overall Loss 0.534512    Objective Loss 0.534512    Top1 77.884615    Top5 98.557692    LR 0.023500    Time 0.054497    
2022-01-29 04:06:25,964 - --- validate (epoch=125)-----------
2022-01-29 04:06:25,964 - 10000 samples (128 per mini-batch)
2022-01-29 04:06:27,811 - Epoch: [125][   79/   79]    Loss 1.314440    Top1 64.670000    Top5 89.040000    
2022-01-29 04:06:27,863 - ==> Top1: 64.670    Top5: 89.040    Loss: 1.314

2022-01-29 04:06:27,869 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:06:27,869 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:06:27,914 - 

2022-01-29 04:06:27,914 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:06:33,607 - Epoch: [126][  100/  391]    Overall Loss 0.497655    Objective Loss 0.497655                                        LR 0.023500    Time 0.056899    
2022-01-29 04:06:38,913 - Epoch: [126][  200/  391]    Overall Loss 0.519738    Objective Loss 0.519738                                        LR 0.023500    Time 0.054975    
2022-01-29 04:06:44,218 - Epoch: [126][  300/  391]    Overall Loss 0.528132    Objective Loss 0.528132                                        LR 0.023500    Time 0.054330    
2022-01-29 04:06:49,040 - Epoch: [126][  391/  391]    Overall Loss 0.531791    Objective Loss 0.531791    Top1 80.288462    Top5 97.115385    LR 0.023500    Time 0.054016    
2022-01-29 04:06:49,098 - --- validate (epoch=126)-----------
2022-01-29 04:06:49,098 - 10000 samples (128 per mini-batch)
2022-01-29 04:06:51,000 - Epoch: [126][   79/   79]    Loss 1.279899    Top1 65.730000    Top5 89.990000    
2022-01-29 04:06:51,052 - ==> Top1: 65.730    Top5: 89.990    Loss: 1.280

2022-01-29 04:06:51,058 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:06:51,058 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:06:51,105 - 

2022-01-29 04:06:51,105 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:06:56,713 - Epoch: [127][  100/  391]    Overall Loss 0.511030    Objective Loss 0.511030                                        LR 0.023500    Time 0.056054    
2022-01-29 04:07:02,055 - Epoch: [127][  200/  391]    Overall Loss 0.518912    Objective Loss 0.518912                                        LR 0.023500    Time 0.054731    
2022-01-29 04:07:07,393 - Epoch: [127][  300/  391]    Overall Loss 0.526355    Objective Loss 0.526355                                        LR 0.023500    Time 0.054281    
2022-01-29 04:07:12,241 - Epoch: [127][  391/  391]    Overall Loss 0.536226    Objective Loss 0.536226    Top1 82.692308    Top5 97.596154    LR 0.023500    Time 0.054045    
2022-01-29 04:07:12,301 - --- validate (epoch=127)-----------
2022-01-29 04:07:12,301 - 10000 samples (128 per mini-batch)
2022-01-29 04:07:14,202 - Epoch: [127][   79/   79]    Loss 1.329854    Top1 63.720000    Top5 89.490000    
2022-01-29 04:07:14,259 - ==> Top1: 63.720    Top5: 89.490    Loss: 1.330

2022-01-29 04:07:14,264 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:07:14,264 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:07:14,310 - 

2022-01-29 04:07:14,311 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:07:19,861 - Epoch: [128][  100/  391]    Overall Loss 0.498332    Objective Loss 0.498332                                        LR 0.023500    Time 0.055479    
2022-01-29 04:07:25,220 - Epoch: [128][  200/  391]    Overall Loss 0.512157    Objective Loss 0.512157                                        LR 0.023500    Time 0.054530    
2022-01-29 04:07:30,629 - Epoch: [128][  300/  391]    Overall Loss 0.524571    Objective Loss 0.524571                                        LR 0.023500    Time 0.054380    
2022-01-29 04:07:35,549 - Epoch: [128][  391/  391]    Overall Loss 0.531929    Objective Loss 0.531929    Top1 83.653846    Top5 99.519231    LR 0.023500    Time 0.054306    
2022-01-29 04:07:35,612 - --- validate (epoch=128)-----------
2022-01-29 04:07:35,613 - 10000 samples (128 per mini-batch)
2022-01-29 04:07:37,475 - Epoch: [128][   79/   79]    Loss 1.309037    Top1 65.060000    Top5 89.570000    
2022-01-29 04:07:37,533 - ==> Top1: 65.060    Top5: 89.570    Loss: 1.309

2022-01-29 04:07:37,539 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:07:37,539 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:07:37,587 - 

2022-01-29 04:07:37,587 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:07:43,286 - Epoch: [129][  100/  391]    Overall Loss 0.507736    Objective Loss 0.507736                                        LR 0.023500    Time 0.056969    
2022-01-29 04:07:48,662 - Epoch: [129][  200/  391]    Overall Loss 0.507450    Objective Loss 0.507450                                        LR 0.023500    Time 0.055358    
2022-01-29 04:07:54,045 - Epoch: [129][  300/  391]    Overall Loss 0.515270    Objective Loss 0.515270                                        LR 0.023500    Time 0.054847    
2022-01-29 04:07:58,938 - Epoch: [129][  391/  391]    Overall Loss 0.521466    Objective Loss 0.521466    Top1 84.615385    Top5 98.557692    LR 0.023500    Time 0.054595    
2022-01-29 04:07:58,999 - --- validate (epoch=129)-----------
2022-01-29 04:07:59,000 - 10000 samples (128 per mini-batch)
2022-01-29 04:08:00,838 - Epoch: [129][   79/   79]    Loss 1.318647    Top1 64.270000    Top5 89.740000    
2022-01-29 04:08:00,894 - ==> Top1: 64.270    Top5: 89.740    Loss: 1.319

2022-01-29 04:08:00,899 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:08:00,899 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:08:00,946 - 

2022-01-29 04:08:00,946 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:08:06,548 - Epoch: [130][  100/  391]    Overall Loss 0.485960    Objective Loss 0.485960                                        LR 0.023500    Time 0.055993    
2022-01-29 04:08:11,940 - Epoch: [130][  200/  391]    Overall Loss 0.494335    Objective Loss 0.494335                                        LR 0.023500    Time 0.054952    
2022-01-29 04:08:17,388 - Epoch: [130][  300/  391]    Overall Loss 0.507268    Objective Loss 0.507268                                        LR 0.023500    Time 0.054791    
2022-01-29 04:08:22,301 - Epoch: [130][  391/  391]    Overall Loss 0.516429    Objective Loss 0.516429    Top1 81.250000    Top5 97.596154    LR 0.023500    Time 0.054604    
2022-01-29 04:08:22,359 - --- validate (epoch=130)-----------
2022-01-29 04:08:22,359 - 10000 samples (128 per mini-batch)
2022-01-29 04:08:24,228 - Epoch: [130][   79/   79]    Loss 1.318944    Top1 64.850000    Top5 89.400000    
2022-01-29 04:08:24,282 - ==> Top1: 64.850    Top5: 89.400    Loss: 1.319

2022-01-29 04:08:24,287 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:08:24,287 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:08:24,330 - 

2022-01-29 04:08:24,330 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:08:29,994 - Epoch: [131][  100/  391]    Overall Loss 0.482329    Objective Loss 0.482329                                        LR 0.023500    Time 0.056613    
2022-01-29 04:08:35,426 - Epoch: [131][  200/  391]    Overall Loss 0.497023    Objective Loss 0.497023                                        LR 0.023500    Time 0.055463    
2022-01-29 04:08:40,860 - Epoch: [131][  300/  391]    Overall Loss 0.512119    Objective Loss 0.512119                                        LR 0.023500    Time 0.055086    
2022-01-29 04:08:45,774 - Epoch: [131][  391/  391]    Overall Loss 0.519368    Objective Loss 0.519368    Top1 78.365385    Top5 98.076923    LR 0.023500    Time 0.054830    
2022-01-29 04:08:45,830 - --- validate (epoch=131)-----------
2022-01-29 04:08:45,830 - 10000 samples (128 per mini-batch)
2022-01-29 04:08:47,764 - Epoch: [131][   79/   79]    Loss 1.341639    Top1 64.500000    Top5 89.400000    
2022-01-29 04:08:47,821 - ==> Top1: 64.500    Top5: 89.400    Loss: 1.342

2022-01-29 04:08:47,827 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:08:47,827 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:08:47,875 - 

2022-01-29 04:08:47,875 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:08:53,585 - Epoch: [132][  100/  391]    Overall Loss 0.496431    Objective Loss 0.496431                                        LR 0.023500    Time 0.057069    
2022-01-29 04:08:58,996 - Epoch: [132][  200/  391]    Overall Loss 0.503147    Objective Loss 0.503147                                        LR 0.023500    Time 0.055587    
2022-01-29 04:09:04,410 - Epoch: [132][  300/  391]    Overall Loss 0.511064    Objective Loss 0.511064                                        LR 0.023500    Time 0.055104    
2022-01-29 04:09:09,338 - Epoch: [132][  391/  391]    Overall Loss 0.519694    Objective Loss 0.519694    Top1 83.653846    Top5 97.596154    LR 0.023500    Time 0.054879    
2022-01-29 04:09:09,394 - --- validate (epoch=132)-----------
2022-01-29 04:09:09,394 - 10000 samples (128 per mini-batch)
2022-01-29 04:09:11,263 - Epoch: [132][   79/   79]    Loss 1.383119    Top1 63.330000    Top5 88.770000    
2022-01-29 04:09:11,312 - ==> Top1: 63.330    Top5: 88.770    Loss: 1.383

2022-01-29 04:09:11,317 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:09:11,317 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:09:11,365 - 

2022-01-29 04:09:11,365 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:09:17,114 - Epoch: [133][  100/  391]    Overall Loss 0.488647    Objective Loss 0.488647                                        LR 0.023500    Time 0.057459    
2022-01-29 04:09:22,543 - Epoch: [133][  200/  391]    Overall Loss 0.499831    Objective Loss 0.499831                                        LR 0.023500    Time 0.055871    
2022-01-29 04:09:27,980 - Epoch: [133][  300/  391]    Overall Loss 0.511691    Objective Loss 0.511691                                        LR 0.023500    Time 0.055369    
2022-01-29 04:09:32,932 - Epoch: [133][  391/  391]    Overall Loss 0.519442    Objective Loss 0.519442    Top1 80.769231    Top5 98.076923    LR 0.023500    Time 0.055145    
2022-01-29 04:09:32,991 - --- validate (epoch=133)-----------
2022-01-29 04:09:32,991 - 10000 samples (128 per mini-batch)
2022-01-29 04:09:34,845 - Epoch: [133][   79/   79]    Loss 1.377591    Top1 63.370000    Top5 88.920000    
2022-01-29 04:09:34,900 - ==> Top1: 63.370    Top5: 88.920    Loss: 1.378

2022-01-29 04:09:34,906 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:09:34,906 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:09:34,949 - 

2022-01-29 04:09:34,949 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:09:40,615 - Epoch: [134][  100/  391]    Overall Loss 0.489998    Objective Loss 0.489998                                        LR 0.023500    Time 0.056632    
2022-01-29 04:09:46,010 - Epoch: [134][  200/  391]    Overall Loss 0.497666    Objective Loss 0.497666                                        LR 0.023500    Time 0.055288    
2022-01-29 04:09:51,408 - Epoch: [134][  300/  391]    Overall Loss 0.505601    Objective Loss 0.505601                                        LR 0.023500    Time 0.054848    
2022-01-29 04:09:56,311 - Epoch: [134][  391/  391]    Overall Loss 0.515954    Objective Loss 0.515954    Top1 80.288462    Top5 99.038462    LR 0.023500    Time 0.054621    
2022-01-29 04:09:56,370 - --- validate (epoch=134)-----------
2022-01-29 04:09:56,370 - 10000 samples (128 per mini-batch)
2022-01-29 04:09:58,217 - Epoch: [134][   79/   79]    Loss 1.400818    Top1 63.210000    Top5 88.800000    
2022-01-29 04:09:58,278 - ==> Top1: 63.210    Top5: 88.800    Loss: 1.401

2022-01-29 04:09:58,284 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:09:58,284 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:09:58,327 - 

2022-01-29 04:09:58,327 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:10:04,046 - Epoch: [135][  100/  391]    Overall Loss 0.488586    Objective Loss 0.488586                                        LR 0.023500    Time 0.057158    
2022-01-29 04:10:09,436 - Epoch: [135][  200/  391]    Overall Loss 0.497928    Objective Loss 0.497928                                        LR 0.023500    Time 0.055527    
2022-01-29 04:10:14,869 - Epoch: [135][  300/  391]    Overall Loss 0.506831    Objective Loss 0.506831                                        LR 0.023500    Time 0.055124    
2022-01-29 04:10:19,786 - Epoch: [135][  391/  391]    Overall Loss 0.512649    Objective Loss 0.512649    Top1 86.538462    Top5 98.076923    LR 0.023500    Time 0.054867    
2022-01-29 04:10:19,845 - --- validate (epoch=135)-----------
2022-01-29 04:10:19,845 - 10000 samples (128 per mini-batch)
2022-01-29 04:10:21,685 - Epoch: [135][   79/   79]    Loss 1.332011    Top1 64.360000    Top5 89.540000    
2022-01-29 04:10:21,739 - ==> Top1: 64.360    Top5: 89.540    Loss: 1.332

2022-01-29 04:10:21,744 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:10:21,744 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:10:21,792 - 

2022-01-29 04:10:21,792 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:10:27,521 - Epoch: [136][  100/  391]    Overall Loss 0.482399    Objective Loss 0.482399                                        LR 0.023500    Time 0.057262    
2022-01-29 04:10:32,933 - Epoch: [136][  200/  391]    Overall Loss 0.484557    Objective Loss 0.484557                                        LR 0.023500    Time 0.055681    
2022-01-29 04:10:38,366 - Epoch: [136][  300/  391]    Overall Loss 0.496575    Objective Loss 0.496575                                        LR 0.023500    Time 0.055230    
2022-01-29 04:10:43,251 - Epoch: [136][  391/  391]    Overall Loss 0.510651    Objective Loss 0.510651    Top1 81.730769    Top5 99.038462    LR 0.023500    Time 0.054867    
2022-01-29 04:10:43,306 - --- validate (epoch=136)-----------
2022-01-29 04:10:43,307 - 10000 samples (128 per mini-batch)
2022-01-29 04:10:45,152 - Epoch: [136][   79/   79]    Loss 1.342679    Top1 63.930000    Top5 89.530000    
2022-01-29 04:10:45,208 - ==> Top1: 63.930    Top5: 89.530    Loss: 1.343

2022-01-29 04:10:45,214 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:10:45,214 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:10:45,262 - 

2022-01-29 04:10:45,262 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:10:50,907 - Epoch: [137][  100/  391]    Overall Loss 0.490400    Objective Loss 0.490400                                        LR 0.023500    Time 0.056428    
2022-01-29 04:10:56,311 - Epoch: [137][  200/  391]    Overall Loss 0.491323    Objective Loss 0.491323                                        LR 0.023500    Time 0.055228    
2022-01-29 04:11:01,742 - Epoch: [137][  300/  391]    Overall Loss 0.500841    Objective Loss 0.500841                                        LR 0.023500    Time 0.054918    
2022-01-29 04:11:06,671 - Epoch: [137][  391/  391]    Overall Loss 0.509783    Objective Loss 0.509783    Top1 80.769231    Top5 98.557692    LR 0.023500    Time 0.054740    
2022-01-29 04:11:06,728 - --- validate (epoch=137)-----------
2022-01-29 04:11:06,728 - 10000 samples (128 per mini-batch)
2022-01-29 04:11:08,566 - Epoch: [137][   79/   79]    Loss 1.381093    Top1 63.770000    Top5 89.160000    
2022-01-29 04:11:08,626 - ==> Top1: 63.770    Top5: 89.160    Loss: 1.381

2022-01-29 04:11:08,631 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:11:08,631 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:11:08,679 - 

2022-01-29 04:11:08,679 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:11:14,407 - Epoch: [138][  100/  391]    Overall Loss 0.463670    Objective Loss 0.463670                                        LR 0.023500    Time 0.057249    
2022-01-29 04:11:19,815 - Epoch: [138][  200/  391]    Overall Loss 0.479087    Objective Loss 0.479087                                        LR 0.023500    Time 0.055658    
2022-01-29 04:11:25,218 - Epoch: [138][  300/  391]    Overall Loss 0.494064    Objective Loss 0.494064                                        LR 0.023500    Time 0.055115    
2022-01-29 04:11:30,129 - Epoch: [138][  391/  391]    Overall Loss 0.503842    Objective Loss 0.503842    Top1 83.173077    Top5 97.115385    LR 0.023500    Time 0.054843    
2022-01-29 04:11:30,194 - --- validate (epoch=138)-----------
2022-01-29 04:11:30,194 - 10000 samples (128 per mini-batch)
2022-01-29 04:11:32,028 - Epoch: [138][   79/   79]    Loss 1.382399    Top1 63.880000    Top5 89.240000    
2022-01-29 04:11:32,079 - ==> Top1: 63.880    Top5: 89.240    Loss: 1.382

2022-01-29 04:11:32,084 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:11:32,084 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:11:32,131 - 

2022-01-29 04:11:32,131 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:11:37,846 - Epoch: [139][  100/  391]    Overall Loss 0.480216    Objective Loss 0.480216                                        LR 0.023500    Time 0.057123    
2022-01-29 04:11:43,271 - Epoch: [139][  200/  391]    Overall Loss 0.492920    Objective Loss 0.492920                                        LR 0.023500    Time 0.055683    
2022-01-29 04:11:48,711 - Epoch: [139][  300/  391]    Overall Loss 0.498596    Objective Loss 0.498596                                        LR 0.023500    Time 0.055252    
2022-01-29 04:11:53,653 - Epoch: [139][  391/  391]    Overall Loss 0.504251    Objective Loss 0.504251    Top1 84.134615    Top5 99.038462    LR 0.023500    Time 0.055031    
2022-01-29 04:11:53,719 - --- validate (epoch=139)-----------
2022-01-29 04:11:53,719 - 10000 samples (128 per mini-batch)
2022-01-29 04:11:55,572 - Epoch: [139][   79/   79]    Loss 1.356668    Top1 64.000000    Top5 89.170000    
2022-01-29 04:11:55,624 - ==> Top1: 64.000    Top5: 89.170    Loss: 1.357

2022-01-29 04:11:55,630 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:11:55,630 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:11:55,678 - 

2022-01-29 04:11:55,678 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:12:01,347 - Epoch: [140][  100/  391]    Overall Loss 0.468969    Objective Loss 0.468969                                        LR 0.023500    Time 0.056663    
2022-01-29 04:12:06,777 - Epoch: [140][  200/  391]    Overall Loss 0.481307    Objective Loss 0.481307                                        LR 0.023500    Time 0.055474    
2022-01-29 04:12:12,180 - Epoch: [140][  300/  391]    Overall Loss 0.489416    Objective Loss 0.489416                                        LR 0.023500    Time 0.054990    
2022-01-29 04:12:17,078 - Epoch: [140][  391/  391]    Overall Loss 0.498472    Objective Loss 0.498472    Top1 80.769231    Top5 97.596154    LR 0.023500    Time 0.054719    
2022-01-29 04:12:17,135 - --- validate (epoch=140)-----------
2022-01-29 04:12:17,135 - 10000 samples (128 per mini-batch)
2022-01-29 04:12:18,991 - Epoch: [140][   79/   79]    Loss 1.352003    Top1 64.290000    Top5 89.170000    
2022-01-29 04:12:19,043 - ==> Top1: 64.290    Top5: 89.170    Loss: 1.352

2022-01-29 04:12:19,049 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:12:19,049 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:12:19,095 - 

2022-01-29 04:12:19,095 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:12:24,818 - Epoch: [141][  100/  391]    Overall Loss 0.471944    Objective Loss 0.471944                                        LR 0.023500    Time 0.057194    
2022-01-29 04:12:30,233 - Epoch: [141][  200/  391]    Overall Loss 0.481005    Objective Loss 0.481005                                        LR 0.023500    Time 0.055672    
2022-01-29 04:12:35,641 - Epoch: [141][  300/  391]    Overall Loss 0.488903    Objective Loss 0.488903                                        LR 0.023500    Time 0.055136    
2022-01-29 04:12:40,558 - Epoch: [141][  391/  391]    Overall Loss 0.494755    Objective Loss 0.494755    Top1 87.019231    Top5 99.519231    LR 0.023500    Time 0.054879    
2022-01-29 04:12:40,617 - --- validate (epoch=141)-----------
2022-01-29 04:12:40,617 - 10000 samples (128 per mini-batch)
2022-01-29 04:12:42,562 - Epoch: [141][   79/   79]    Loss 1.385930    Top1 63.520000    Top5 88.740000    
2022-01-29 04:12:42,618 - ==> Top1: 63.520    Top5: 88.740    Loss: 1.386

2022-01-29 04:12:42,624 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:12:42,624 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:12:42,672 - 

2022-01-29 04:12:42,672 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:12:48,410 - Epoch: [142][  100/  391]    Overall Loss 0.472848    Objective Loss 0.472848                                        LR 0.023500    Time 0.057354    
2022-01-29 04:12:53,820 - Epoch: [142][  200/  391]    Overall Loss 0.473635    Objective Loss 0.473635                                        LR 0.023500    Time 0.055720    
2022-01-29 04:12:59,228 - Epoch: [142][  300/  391]    Overall Loss 0.487454    Objective Loss 0.487454                                        LR 0.023500    Time 0.055172    
2022-01-29 04:13:04,146 - Epoch: [142][  391/  391]    Overall Loss 0.494434    Objective Loss 0.494434    Top1 85.576923    Top5 98.557692    LR 0.023500    Time 0.054906    
2022-01-29 04:13:04,211 - --- validate (epoch=142)-----------
2022-01-29 04:13:04,211 - 10000 samples (128 per mini-batch)
2022-01-29 04:13:06,086 - Epoch: [142][   79/   79]    Loss 1.338070    Top1 64.790000    Top5 89.400000    
2022-01-29 04:13:06,144 - ==> Top1: 64.790    Top5: 89.400    Loss: 1.338

2022-01-29 04:13:06,149 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:13:06,150 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:13:06,197 - 

2022-01-29 04:13:06,198 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:13:11,835 - Epoch: [143][  100/  391]    Overall Loss 0.463151    Objective Loss 0.463151                                        LR 0.023500    Time 0.056344    
2022-01-29 04:13:17,231 - Epoch: [143][  200/  391]    Overall Loss 0.476576    Objective Loss 0.476576                                        LR 0.023500    Time 0.055148    
2022-01-29 04:13:22,663 - Epoch: [143][  300/  391]    Overall Loss 0.485436    Objective Loss 0.485436                                        LR 0.023500    Time 0.054871    
2022-01-29 04:13:27,581 - Epoch: [143][  391/  391]    Overall Loss 0.494424    Objective Loss 0.494424    Top1 88.942308    Top5 99.519231    LR 0.023500    Time 0.054675    
2022-01-29 04:13:27,645 - --- validate (epoch=143)-----------
2022-01-29 04:13:27,645 - 10000 samples (128 per mini-batch)
2022-01-29 04:13:29,507 - Epoch: [143][   79/   79]    Loss 1.366911    Top1 63.930000    Top5 89.160000    
2022-01-29 04:13:29,562 - ==> Top1: 63.930    Top5: 89.160    Loss: 1.367

2022-01-29 04:13:29,568 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:13:29,568 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:13:29,616 - 

2022-01-29 04:13:29,616 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:13:35,298 - Epoch: [144][  100/  391]    Overall Loss 0.470493    Objective Loss 0.470493                                        LR 0.023500    Time 0.056796    
2022-01-29 04:13:40,635 - Epoch: [144][  200/  391]    Overall Loss 0.477155    Objective Loss 0.477155                                        LR 0.023500    Time 0.055079    
2022-01-29 04:13:45,972 - Epoch: [144][  300/  391]    Overall Loss 0.485269    Objective Loss 0.485269                                        LR 0.023500    Time 0.054507    
2022-01-29 04:13:50,817 - Epoch: [144][  391/  391]    Overall Loss 0.495030    Objective Loss 0.495030    Top1 83.173077    Top5 99.519231    LR 0.023500    Time 0.054210    
2022-01-29 04:13:50,876 - --- validate (epoch=144)-----------
2022-01-29 04:13:50,876 - 10000 samples (128 per mini-batch)
2022-01-29 04:13:52,711 - Epoch: [144][   79/   79]    Loss 1.414786    Top1 63.450000    Top5 88.570000    
2022-01-29 04:13:52,763 - ==> Top1: 63.450    Top5: 88.570    Loss: 1.415

2022-01-29 04:13:52,769 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:13:52,769 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:13:52,816 - 

2022-01-29 04:13:52,816 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:13:58,477 - Epoch: [145][  100/  391]    Overall Loss 0.452087    Objective Loss 0.452087                                        LR 0.023500    Time 0.056587    
2022-01-29 04:14:03,804 - Epoch: [145][  200/  391]    Overall Loss 0.458675    Objective Loss 0.458675                                        LR 0.023500    Time 0.054924    
2022-01-29 04:14:09,114 - Epoch: [145][  300/  391]    Overall Loss 0.472546    Objective Loss 0.472546                                        LR 0.023500    Time 0.054311    
2022-01-29 04:14:13,939 - Epoch: [145][  391/  391]    Overall Loss 0.484618    Objective Loss 0.484618    Top1 86.538462    Top5 99.519231    LR 0.023500    Time 0.054009    
2022-01-29 04:14:13,998 - --- validate (epoch=145)-----------
2022-01-29 04:14:13,999 - 10000 samples (128 per mini-batch)
2022-01-29 04:14:15,823 - Epoch: [145][   79/   79]    Loss 1.406960    Top1 63.370000    Top5 88.880000    
2022-01-29 04:14:15,882 - ==> Top1: 63.370    Top5: 88.880    Loss: 1.407

2022-01-29 04:14:15,888 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:14:15,888 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:14:15,933 - 

2022-01-29 04:14:15,933 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:14:21,501 - Epoch: [146][  100/  391]    Overall Loss 0.468211    Objective Loss 0.468211                                        LR 0.023500    Time 0.055650    
2022-01-29 04:14:26,898 - Epoch: [146][  200/  391]    Overall Loss 0.475201    Objective Loss 0.475201                                        LR 0.023500    Time 0.054806    
2022-01-29 04:14:32,298 - Epoch: [146][  300/  391]    Overall Loss 0.479353    Objective Loss 0.479353                                        LR 0.023500    Time 0.054533    
2022-01-29 04:14:37,214 - Epoch: [146][  391/  391]    Overall Loss 0.491449    Objective Loss 0.491449    Top1 83.653846    Top5 97.596154    LR 0.023500    Time 0.054412    
2022-01-29 04:14:37,264 - --- validate (epoch=146)-----------
2022-01-29 04:14:37,264 - 10000 samples (128 per mini-batch)
2022-01-29 04:14:39,102 - Epoch: [146][   79/   79]    Loss 1.360984    Top1 64.480000    Top5 89.070000    
2022-01-29 04:14:39,159 - ==> Top1: 64.480    Top5: 89.070    Loss: 1.361

2022-01-29 04:14:39,164 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:14:39,164 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:14:39,212 - 

2022-01-29 04:14:39,212 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:14:44,907 - Epoch: [147][  100/  391]    Overall Loss 0.474032    Objective Loss 0.474032                                        LR 0.023500    Time 0.056921    
2022-01-29 04:14:50,299 - Epoch: [147][  200/  391]    Overall Loss 0.487632    Objective Loss 0.487632                                        LR 0.023500    Time 0.055419    
2022-01-29 04:14:55,701 - Epoch: [147][  300/  391]    Overall Loss 0.492236    Objective Loss 0.492236                                        LR 0.023500    Time 0.054949    
2022-01-29 04:15:00,613 - Epoch: [147][  391/  391]    Overall Loss 0.497262    Objective Loss 0.497262    Top1 87.980769    Top5 99.519231    LR 0.023500    Time 0.054720    
2022-01-29 04:15:00,669 - --- validate (epoch=147)-----------
2022-01-29 04:15:00,669 - 10000 samples (128 per mini-batch)
2022-01-29 04:15:02,542 - Epoch: [147][   79/   79]    Loss 1.453026    Top1 62.100000    Top5 87.930000    
2022-01-29 04:15:02,593 - ==> Top1: 62.100    Top5: 87.930    Loss: 1.453

2022-01-29 04:15:02,598 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:15:02,598 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:15:02,645 - 

2022-01-29 04:15:02,646 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:15:08,366 - Epoch: [148][  100/  391]    Overall Loss 0.454192    Objective Loss 0.454192                                        LR 0.023500    Time 0.057179    
2022-01-29 04:15:13,709 - Epoch: [148][  200/  391]    Overall Loss 0.465419    Objective Loss 0.465419                                        LR 0.023500    Time 0.055301    
2022-01-29 04:15:19,047 - Epoch: [148][  300/  391]    Overall Loss 0.478229    Objective Loss 0.478229                                        LR 0.023500    Time 0.054658    
2022-01-29 04:15:23,944 - Epoch: [148][  391/  391]    Overall Loss 0.488479    Objective Loss 0.488479    Top1 86.538462    Top5 97.596154    LR 0.023500    Time 0.054460    
2022-01-29 04:15:24,004 - --- validate (epoch=148)-----------
2022-01-29 04:15:24,004 - 10000 samples (128 per mini-batch)
2022-01-29 04:15:25,847 - Epoch: [148][   79/   79]    Loss 1.378904    Top1 63.560000    Top5 89.230000    
2022-01-29 04:15:25,899 - ==> Top1: 63.560    Top5: 89.230    Loss: 1.379

2022-01-29 04:15:25,905 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:15:25,905 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:15:25,953 - 

2022-01-29 04:15:25,953 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:15:31,579 - Epoch: [149][  100/  391]    Overall Loss 0.452717    Objective Loss 0.452717                                        LR 0.023500    Time 0.056237    
2022-01-29 04:15:36,920 - Epoch: [149][  200/  391]    Overall Loss 0.462897    Objective Loss 0.462897                                        LR 0.023500    Time 0.054818    
2022-01-29 04:15:42,225 - Epoch: [149][  300/  391]    Overall Loss 0.476666    Objective Loss 0.476666                                        LR 0.023500    Time 0.054223    
2022-01-29 04:15:47,088 - Epoch: [149][  391/  391]    Overall Loss 0.489020    Objective Loss 0.489020    Top1 78.846154    Top5 98.557692    LR 0.023500    Time 0.054039    
2022-01-29 04:15:47,151 - --- validate (epoch=149)-----------
2022-01-29 04:15:47,151 - 10000 samples (128 per mini-batch)
2022-01-29 04:15:49,029 - Epoch: [149][   79/   79]    Loss 1.356304    Top1 64.410000    Top5 89.640000    
2022-01-29 04:15:49,085 - ==> Top1: 64.410    Top5: 89.640    Loss: 1.356

2022-01-29 04:15:49,091 - ==> Best [Top1: 66.200   Top5: 90.110   Sparsity:0.00   Params: 1341960 on epoch: 111]
2022-01-29 04:15:49,091 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:15:49,139 - 

2022-01-29 04:15:49,139 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:15:54,836 - Epoch: [150][  100/  391]    Overall Loss 0.374029    Objective Loss 0.374029                                        LR 0.005522    Time 0.056940    
2022-01-29 04:16:00,215 - Epoch: [150][  200/  391]    Overall Loss 0.360981    Objective Loss 0.360981                                        LR 0.005522    Time 0.055362    
2022-01-29 04:16:05,622 - Epoch: [150][  300/  391]    Overall Loss 0.347797    Objective Loss 0.347797                                        LR 0.005522    Time 0.054931    
2022-01-29 04:16:10,536 - Epoch: [150][  391/  391]    Overall Loss 0.342768    Objective Loss 0.342768    Top1 93.269231    Top5 99.519231    LR 0.005522    Time 0.054710    
2022-01-29 04:16:10,601 - --- validate (epoch=150)-----------
2022-01-29 04:16:10,601 - 10000 samples (128 per mini-batch)
2022-01-29 04:16:12,440 - Epoch: [150][   79/   79]    Loss 1.242412    Top1 67.350000    Top5 90.620000    
2022-01-29 04:16:12,491 - ==> Top1: 67.350    Top5: 90.620    Loss: 1.242

2022-01-29 04:16:12,496 - ==> Best [Top1: 67.350   Top5: 90.620   Sparsity:0.00   Params: 1341960 on epoch: 150]
2022-01-29 04:16:12,496 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:16:12,553 - 

2022-01-29 04:16:12,553 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:16:18,254 - Epoch: [151][  100/  391]    Overall Loss 0.304107    Objective Loss 0.304107                                        LR 0.005522    Time 0.056975    
2022-01-29 04:16:23,632 - Epoch: [151][  200/  391]    Overall Loss 0.300706    Objective Loss 0.300706                                        LR 0.005522    Time 0.055377    
2022-01-29 04:16:29,022 - Epoch: [151][  300/  391]    Overall Loss 0.298130    Objective Loss 0.298130                                        LR 0.005522    Time 0.054881    
2022-01-29 04:16:33,919 - Epoch: [151][  391/  391]    Overall Loss 0.299524    Objective Loss 0.299524    Top1 91.346154    Top5 99.519231    LR 0.005522    Time 0.054631    
2022-01-29 04:16:33,979 - --- validate (epoch=151)-----------
2022-01-29 04:16:33,979 - 10000 samples (128 per mini-batch)
2022-01-29 04:16:35,925 - Epoch: [151][   79/   79]    Loss 1.226603    Top1 67.600000    Top5 90.690000    
2022-01-29 04:16:35,983 - ==> Top1: 67.600    Top5: 90.690    Loss: 1.227

2022-01-29 04:16:35,988 - ==> Best [Top1: 67.600   Top5: 90.690   Sparsity:0.00   Params: 1341960 on epoch: 151]
2022-01-29 04:16:35,989 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:16:36,044 - 

2022-01-29 04:16:36,044 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:16:41,668 - Epoch: [152][  100/  391]    Overall Loss 0.281695    Objective Loss 0.281695                                        LR 0.005522    Time 0.056209    
2022-01-29 04:16:47,052 - Epoch: [152][  200/  391]    Overall Loss 0.278763    Objective Loss 0.278763                                        LR 0.005522    Time 0.055021    
2022-01-29 04:16:52,435 - Epoch: [152][  300/  391]    Overall Loss 0.280277    Objective Loss 0.280277                                        LR 0.005522    Time 0.054623    
2022-01-29 04:16:57,281 - Epoch: [152][  391/  391]    Overall Loss 0.282540    Objective Loss 0.282540    Top1 91.346154    Top5 100.000000    LR 0.005522    Time 0.054302    
2022-01-29 04:16:57,338 - --- validate (epoch=152)-----------
2022-01-29 04:16:57,338 - 10000 samples (128 per mini-batch)
2022-01-29 04:16:59,194 - Epoch: [152][   79/   79]    Loss 1.232809    Top1 67.670000    Top5 90.750000    
2022-01-29 04:16:59,246 - ==> Top1: 67.670    Top5: 90.750    Loss: 1.233

2022-01-29 04:16:59,252 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:16:59,252 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:16:59,309 - 

2022-01-29 04:16:59,309 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:17:05,011 - Epoch: [153][  100/  391]    Overall Loss 0.270074    Objective Loss 0.270074                                        LR 0.005522    Time 0.056992    
2022-01-29 04:17:10,367 - Epoch: [153][  200/  391]    Overall Loss 0.275691    Objective Loss 0.275691                                        LR 0.005522    Time 0.055272    
2022-01-29 04:17:15,696 - Epoch: [153][  300/  391]    Overall Loss 0.273645    Objective Loss 0.273645                                        LR 0.005522    Time 0.054608    
2022-01-29 04:17:20,545 - Epoch: [153][  391/  391]    Overall Loss 0.273441    Objective Loss 0.273441    Top1 93.750000    Top5 100.000000    LR 0.005522    Time 0.054298    
2022-01-29 04:17:20,604 - --- validate (epoch=153)-----------
2022-01-29 04:17:20,604 - 10000 samples (128 per mini-batch)
2022-01-29 04:17:22,511 - Epoch: [153][   79/   79]    Loss 1.232132    Top1 67.300000    Top5 90.800000    
2022-01-29 04:17:22,562 - ==> Top1: 67.300    Top5: 90.800    Loss: 1.232

2022-01-29 04:17:22,567 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:17:22,567 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:17:22,606 - 

2022-01-29 04:17:22,606 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:17:28,268 - Epoch: [154][  100/  391]    Overall Loss 0.255599    Objective Loss 0.255599                                        LR 0.005522    Time 0.056597    
2022-01-29 04:17:33,594 - Epoch: [154][  200/  391]    Overall Loss 0.262705    Objective Loss 0.262705                                        LR 0.005522    Time 0.054923    
2022-01-29 04:17:38,924 - Epoch: [154][  300/  391]    Overall Loss 0.263971    Objective Loss 0.263971                                        LR 0.005522    Time 0.054377    
2022-01-29 04:17:43,763 - Epoch: [154][  391/  391]    Overall Loss 0.264830    Objective Loss 0.264830    Top1 89.423077    Top5 99.519231    LR 0.005522    Time 0.054096    
2022-01-29 04:17:43,822 - --- validate (epoch=154)-----------
2022-01-29 04:17:43,822 - 10000 samples (128 per mini-batch)
2022-01-29 04:17:45,733 - Epoch: [154][   79/   79]    Loss 1.253347    Top1 67.040000    Top5 90.760000    
2022-01-29 04:17:45,785 - ==> Top1: 67.040    Top5: 90.760    Loss: 1.253

2022-01-29 04:17:45,790 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:17:45,790 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:17:45,836 - 

2022-01-29 04:17:45,836 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:17:51,396 - Epoch: [155][  100/  391]    Overall Loss 0.248557    Objective Loss 0.248557                                        LR 0.005522    Time 0.055565    
2022-01-29 04:17:56,730 - Epoch: [155][  200/  391]    Overall Loss 0.255113    Objective Loss 0.255113                                        LR 0.005522    Time 0.054449    
2022-01-29 04:18:02,064 - Epoch: [155][  300/  391]    Overall Loss 0.255601    Objective Loss 0.255601                                        LR 0.005522    Time 0.054076    
2022-01-29 04:18:06,904 - Epoch: [155][  391/  391]    Overall Loss 0.257273    Objective Loss 0.257273    Top1 94.230769    Top5 100.000000    LR 0.005522    Time 0.053867    
2022-01-29 04:18:06,960 - --- validate (epoch=155)-----------
2022-01-29 04:18:06,960 - 10000 samples (128 per mini-batch)
2022-01-29 04:18:08,771 - Epoch: [155][   79/   79]    Loss 1.249353    Top1 67.510000    Top5 90.740000    
2022-01-29 04:18:08,825 - ==> Top1: 67.510    Top5: 90.740    Loss: 1.249

2022-01-29 04:18:08,830 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:18:08,830 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:18:08,877 - 

2022-01-29 04:18:08,877 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:18:14,509 - Epoch: [156][  100/  391]    Overall Loss 0.235472    Objective Loss 0.235472                                        LR 0.005522    Time 0.056289    
2022-01-29 04:18:19,814 - Epoch: [156][  200/  391]    Overall Loss 0.245406    Objective Loss 0.245406                                        LR 0.005522    Time 0.054667    
2022-01-29 04:18:25,123 - Epoch: [156][  300/  391]    Overall Loss 0.249060    Objective Loss 0.249060                                        LR 0.005522    Time 0.054137    
2022-01-29 04:18:29,943 - Epoch: [156][  391/  391]    Overall Loss 0.251443    Objective Loss 0.251443    Top1 96.153846    Top5 100.000000    LR 0.005522    Time 0.053863    
2022-01-29 04:18:30,006 - --- validate (epoch=156)-----------
2022-01-29 04:18:30,006 - 10000 samples (128 per mini-batch)
2022-01-29 04:18:31,935 - Epoch: [156][   79/   79]    Loss 1.253055    Top1 67.480000    Top5 90.760000    
2022-01-29 04:18:31,988 - ==> Top1: 67.480    Top5: 90.760    Loss: 1.253

2022-01-29 04:18:31,993 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:18:31,994 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:18:32,040 - 

2022-01-29 04:18:32,040 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:18:37,741 - Epoch: [157][  100/  391]    Overall Loss 0.238940    Objective Loss 0.238940                                        LR 0.005522    Time 0.056976    
2022-01-29 04:18:43,138 - Epoch: [157][  200/  391]    Overall Loss 0.241437    Objective Loss 0.241437                                        LR 0.005522    Time 0.055469    
2022-01-29 04:18:48,561 - Epoch: [157][  300/  391]    Overall Loss 0.243098    Objective Loss 0.243098                                        LR 0.005522    Time 0.055056    
2022-01-29 04:18:53,497 - Epoch: [157][  391/  391]    Overall Loss 0.246585    Objective Loss 0.246585    Top1 93.269231    Top5 100.000000    LR 0.005522    Time 0.054863    
2022-01-29 04:18:53,562 - --- validate (epoch=157)-----------
2022-01-29 04:18:53,562 - 10000 samples (128 per mini-batch)
2022-01-29 04:18:55,398 - Epoch: [157][   79/   79]    Loss 1.276318    Top1 67.570000    Top5 90.450000    
2022-01-29 04:18:55,454 - ==> Top1: 67.570    Top5: 90.450    Loss: 1.276

2022-01-29 04:18:55,459 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:18:55,459 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:18:55,501 - 

2022-01-29 04:18:55,501 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:19:01,135 - Epoch: [158][  100/  391]    Overall Loss 0.228480    Objective Loss 0.228480                                        LR 0.005522    Time 0.056310    
2022-01-29 04:19:06,541 - Epoch: [158][  200/  391]    Overall Loss 0.234899    Objective Loss 0.234899                                        LR 0.005522    Time 0.055184    
2022-01-29 04:19:11,921 - Epoch: [158][  300/  391]    Overall Loss 0.237792    Objective Loss 0.237792                                        LR 0.005522    Time 0.054718    
2022-01-29 04:19:16,849 - Epoch: [158][  391/  391]    Overall Loss 0.239499    Objective Loss 0.239499    Top1 92.788462    Top5 100.000000    LR 0.005522    Time 0.054585    
2022-01-29 04:19:16,907 - --- validate (epoch=158)-----------
2022-01-29 04:19:16,907 - 10000 samples (128 per mini-batch)
2022-01-29 04:19:18,733 - Epoch: [158][   79/   79]    Loss 1.271465    Top1 67.130000    Top5 90.660000    
2022-01-29 04:19:18,784 - ==> Top1: 67.130    Top5: 90.660    Loss: 1.271

2022-01-29 04:19:18,789 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:19:18,789 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:19:18,836 - 

2022-01-29 04:19:18,836 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:19:24,541 - Epoch: [159][  100/  391]    Overall Loss 0.226808    Objective Loss 0.226808                                        LR 0.005522    Time 0.057022    
2022-01-29 04:19:29,923 - Epoch: [159][  200/  391]    Overall Loss 0.230258    Objective Loss 0.230258                                        LR 0.005522    Time 0.055419    
2022-01-29 04:19:35,318 - Epoch: [159][  300/  391]    Overall Loss 0.236006    Objective Loss 0.236006                                        LR 0.005522    Time 0.054926    
2022-01-29 04:19:40,217 - Epoch: [159][  391/  391]    Overall Loss 0.234754    Objective Loss 0.234754    Top1 94.230769    Top5 99.519231    LR 0.005522    Time 0.054670    
2022-01-29 04:19:40,275 - --- validate (epoch=159)-----------
2022-01-29 04:19:40,275 - 10000 samples (128 per mini-batch)
2022-01-29 04:19:42,100 - Epoch: [159][   79/   79]    Loss 1.276065    Top1 67.530000    Top5 90.580000    
2022-01-29 04:19:42,156 - ==> Top1: 67.530    Top5: 90.580    Loss: 1.276

2022-01-29 04:19:42,161 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:19:42,161 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:19:42,209 - 

2022-01-29 04:19:42,209 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:19:47,945 - Epoch: [160][  100/  391]    Overall Loss 0.219584    Objective Loss 0.219584                                        LR 0.005522    Time 0.057326    
2022-01-29 04:19:53,371 - Epoch: [160][  200/  391]    Overall Loss 0.226756    Objective Loss 0.226756                                        LR 0.005522    Time 0.055790    
2022-01-29 04:19:58,774 - Epoch: [160][  300/  391]    Overall Loss 0.228328    Objective Loss 0.228328                                        LR 0.005522    Time 0.055200    
2022-01-29 04:20:03,677 - Epoch: [160][  391/  391]    Overall Loss 0.231364    Objective Loss 0.231364    Top1 97.596154    Top5 100.000000    LR 0.005522    Time 0.054890    
2022-01-29 04:20:03,733 - --- validate (epoch=160)-----------
2022-01-29 04:20:03,734 - 10000 samples (128 per mini-batch)
2022-01-29 04:20:05,591 - Epoch: [160][   79/   79]    Loss 1.269828    Top1 67.450000    Top5 90.750000    
2022-01-29 04:20:05,645 - ==> Top1: 67.450    Top5: 90.750    Loss: 1.270

2022-01-29 04:20:05,650 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:20:05,650 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:20:05,698 - 

2022-01-29 04:20:05,698 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:20:11,348 - Epoch: [161][  100/  391]    Overall Loss 0.220306    Objective Loss 0.220306                                        LR 0.005522    Time 0.056470    
2022-01-29 04:20:16,739 - Epoch: [161][  200/  391]    Overall Loss 0.219896    Objective Loss 0.219896                                        LR 0.005522    Time 0.055183    
2022-01-29 04:20:22,136 - Epoch: [161][  300/  391]    Overall Loss 0.223580    Objective Loss 0.223580                                        LR 0.005522    Time 0.054777    
2022-01-29 04:20:27,039 - Epoch: [161][  391/  391]    Overall Loss 0.224134    Objective Loss 0.224134    Top1 94.230769    Top5 99.519231    LR 0.005522    Time 0.054565    
2022-01-29 04:20:27,102 - --- validate (epoch=161)-----------
2022-01-29 04:20:27,102 - 10000 samples (128 per mini-batch)
2022-01-29 04:20:28,983 - Epoch: [161][   79/   79]    Loss 1.282980    Top1 67.560000    Top5 90.640000    
2022-01-29 04:20:29,038 - ==> Top1: 67.560    Top5: 90.640    Loss: 1.283

2022-01-29 04:20:29,043 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:20:29,043 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:20:29,090 - 

2022-01-29 04:20:29,090 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:20:34,754 - Epoch: [162][  100/  391]    Overall Loss 0.213510    Objective Loss 0.213510                                        LR 0.005522    Time 0.056607    
2022-01-29 04:20:40,120 - Epoch: [162][  200/  391]    Overall Loss 0.219097    Objective Loss 0.219097                                        LR 0.005522    Time 0.055130    
2022-01-29 04:20:45,539 - Epoch: [162][  300/  391]    Overall Loss 0.220086    Objective Loss 0.220086                                        LR 0.005522    Time 0.054814    
2022-01-29 04:20:50,435 - Epoch: [162][  391/  391]    Overall Loss 0.220975    Objective Loss 0.220975    Top1 94.230769    Top5 98.557692    LR 0.005522    Time 0.054578    
2022-01-29 04:20:50,500 - --- validate (epoch=162)-----------
2022-01-29 04:20:50,500 - 10000 samples (128 per mini-batch)
2022-01-29 04:20:52,338 - Epoch: [162][   79/   79]    Loss 1.261671    Top1 67.300000    Top5 90.810000    
2022-01-29 04:20:52,397 - ==> Top1: 67.300    Top5: 90.810    Loss: 1.262

2022-01-29 04:20:52,403 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:20:52,403 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:20:52,451 - 

2022-01-29 04:20:52,451 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:20:58,132 - Epoch: [163][  100/  391]    Overall Loss 0.209536    Objective Loss 0.209536                                        LR 0.005522    Time 0.056782    
2022-01-29 04:21:03,507 - Epoch: [163][  200/  391]    Overall Loss 0.211392    Objective Loss 0.211392                                        LR 0.005522    Time 0.055264    
2022-01-29 04:21:08,886 - Epoch: [163][  300/  391]    Overall Loss 0.215120    Objective Loss 0.215120                                        LR 0.005522    Time 0.054771    
2022-01-29 04:21:13,754 - Epoch: [163][  391/  391]    Overall Loss 0.217501    Objective Loss 0.217501    Top1 92.307692    Top5 100.000000    LR 0.005522    Time 0.054472    
2022-01-29 04:21:13,819 - --- validate (epoch=163)-----------
2022-01-29 04:21:13,819 - 10000 samples (128 per mini-batch)
2022-01-29 04:21:15,663 - Epoch: [163][   79/   79]    Loss 1.303896    Top1 67.470000    Top5 90.420000    
2022-01-29 04:21:15,720 - ==> Top1: 67.470    Top5: 90.420    Loss: 1.304

2022-01-29 04:21:15,726 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:21:15,726 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:21:15,774 - 

2022-01-29 04:21:15,774 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:21:21,401 - Epoch: [164][  100/  391]    Overall Loss 0.211501    Objective Loss 0.211501                                        LR 0.005522    Time 0.056245    
2022-01-29 04:21:26,789 - Epoch: [164][  200/  391]    Overall Loss 0.213621    Objective Loss 0.213621                                        LR 0.005522    Time 0.055060    
2022-01-29 04:21:32,195 - Epoch: [164][  300/  391]    Overall Loss 0.214198    Objective Loss 0.214198                                        LR 0.005522    Time 0.054724    
2022-01-29 04:21:37,101 - Epoch: [164][  391/  391]    Overall Loss 0.215784    Objective Loss 0.215784    Top1 97.115385    Top5 100.000000    LR 0.005522    Time 0.054532    
2022-01-29 04:21:37,160 - --- validate (epoch=164)-----------
2022-01-29 04:21:37,160 - 10000 samples (128 per mini-batch)
2022-01-29 04:21:39,012 - Epoch: [164][   79/   79]    Loss 1.296906    Top1 67.130000    Top5 90.580000    
2022-01-29 04:21:39,070 - ==> Top1: 67.130    Top5: 90.580    Loss: 1.297

2022-01-29 04:21:39,075 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:21:39,076 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:21:39,119 - 

2022-01-29 04:21:39,119 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:21:44,838 - Epoch: [165][  100/  391]    Overall Loss 0.204733    Objective Loss 0.204733                                        LR 0.005522    Time 0.057166    
2022-01-29 04:21:50,155 - Epoch: [165][  200/  391]    Overall Loss 0.209199    Objective Loss 0.209199                                        LR 0.005522    Time 0.055133    
2022-01-29 04:21:55,546 - Epoch: [165][  300/  391]    Overall Loss 0.211770    Objective Loss 0.211770                                        LR 0.005522    Time 0.054723    
2022-01-29 04:22:00,447 - Epoch: [165][  391/  391]    Overall Loss 0.213091    Objective Loss 0.213091    Top1 95.192308    Top5 100.000000    LR 0.005522    Time 0.054520    
2022-01-29 04:22:00,507 - --- validate (epoch=165)-----------
2022-01-29 04:22:00,507 - 10000 samples (128 per mini-batch)
2022-01-29 04:22:02,434 - Epoch: [165][   79/   79]    Loss 1.296392    Top1 67.420000    Top5 90.380000    
2022-01-29 04:22:02,485 - ==> Top1: 67.420    Top5: 90.380    Loss: 1.296

2022-01-29 04:22:02,490 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:22:02,490 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:22:02,538 - 

2022-01-29 04:22:02,538 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:22:08,260 - Epoch: [166][  100/  391]    Overall Loss 0.210812    Objective Loss 0.210812                                        LR 0.005522    Time 0.057189    
2022-01-29 04:22:13,673 - Epoch: [166][  200/  391]    Overall Loss 0.211329    Objective Loss 0.211329                                        LR 0.005522    Time 0.055654    
2022-01-29 04:22:19,078 - Epoch: [166][  300/  391]    Overall Loss 0.210450    Objective Loss 0.210450                                        LR 0.005522    Time 0.055117    
2022-01-29 04:22:23,989 - Epoch: [166][  391/  391]    Overall Loss 0.210788    Objective Loss 0.210788    Top1 93.750000    Top5 99.038462    LR 0.005522    Time 0.054848    
2022-01-29 04:22:24,047 - --- validate (epoch=166)-----------
2022-01-29 04:22:24,047 - 10000 samples (128 per mini-batch)
2022-01-29 04:22:25,920 - Epoch: [166][   79/   79]    Loss 1.304545    Top1 67.420000    Top5 90.560000    
2022-01-29 04:22:25,973 - ==> Top1: 67.420    Top5: 90.560    Loss: 1.305

2022-01-29 04:22:25,978 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:22:25,978 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:22:26,026 - 

2022-01-29 04:22:26,026 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:22:31,669 - Epoch: [167][  100/  391]    Overall Loss 0.192986    Objective Loss 0.192986                                        LR 0.005522    Time 0.056403    
2022-01-29 04:22:37,052 - Epoch: [167][  200/  391]    Overall Loss 0.198402    Objective Loss 0.198402                                        LR 0.005522    Time 0.055110    
2022-01-29 04:22:42,377 - Epoch: [167][  300/  391]    Overall Loss 0.200035    Objective Loss 0.200035                                        LR 0.005522    Time 0.054485    
2022-01-29 04:22:47,213 - Epoch: [167][  391/  391]    Overall Loss 0.203643    Objective Loss 0.203643    Top1 94.711538    Top5 100.000000    LR 0.005522    Time 0.054173    
2022-01-29 04:22:47,279 - --- validate (epoch=167)-----------
2022-01-29 04:22:47,279 - 10000 samples (128 per mini-batch)
2022-01-29 04:22:49,106 - Epoch: [167][   79/   79]    Loss 1.314753    Top1 66.820000    Top5 90.570000    
2022-01-29 04:22:49,156 - ==> Top1: 66.820    Top5: 90.570    Loss: 1.315

2022-01-29 04:22:49,162 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:22:49,162 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:22:49,208 - 

2022-01-29 04:22:49,209 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:22:54,852 - Epoch: [168][  100/  391]    Overall Loss 0.194604    Objective Loss 0.194604                                        LR 0.005522    Time 0.056405    
2022-01-29 04:23:00,185 - Epoch: [168][  200/  391]    Overall Loss 0.200458    Objective Loss 0.200458                                        LR 0.005522    Time 0.054865    
2022-01-29 04:23:05,571 - Epoch: [168][  300/  391]    Overall Loss 0.205864    Objective Loss 0.205864                                        LR 0.005522    Time 0.054529    
2022-01-29 04:23:10,468 - Epoch: [168][  391/  391]    Overall Loss 0.206235    Objective Loss 0.206235    Top1 94.230769    Top5 100.000000    LR 0.005522    Time 0.054360    
2022-01-29 04:23:10,534 - --- validate (epoch=168)-----------
2022-01-29 04:23:10,534 - 10000 samples (128 per mini-batch)
2022-01-29 04:23:12,390 - Epoch: [168][   79/   79]    Loss 1.299959    Top1 67.180000    Top5 90.710000    
2022-01-29 04:23:12,442 - ==> Top1: 67.180    Top5: 90.710    Loss: 1.300

2022-01-29 04:23:12,448 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:23:12,448 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:23:12,496 - 

2022-01-29 04:23:12,496 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:23:18,146 - Epoch: [169][  100/  391]    Overall Loss 0.195698    Objective Loss 0.195698                                        LR 0.005522    Time 0.056475    
2022-01-29 04:23:23,516 - Epoch: [169][  200/  391]    Overall Loss 0.200423    Objective Loss 0.200423                                        LR 0.005522    Time 0.055084    
2022-01-29 04:23:28,940 - Epoch: [169][  300/  391]    Overall Loss 0.201397    Objective Loss 0.201397                                        LR 0.005522    Time 0.054800    
2022-01-29 04:23:33,855 - Epoch: [169][  391/  391]    Overall Loss 0.202590    Objective Loss 0.202590    Top1 95.192308    Top5 100.000000    LR 0.005522    Time 0.054613    
2022-01-29 04:23:33,918 - --- validate (epoch=169)-----------
2022-01-29 04:23:33,918 - 10000 samples (128 per mini-batch)
2022-01-29 04:23:35,912 - Epoch: [169][   79/   79]    Loss 1.319009    Top1 67.360000    Top5 90.400000    
2022-01-29 04:23:35,965 - ==> Top1: 67.360    Top5: 90.400    Loss: 1.319

2022-01-29 04:23:35,970 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:23:35,970 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:23:36,018 - 

2022-01-29 04:23:36,019 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:23:41,650 - Epoch: [170][  100/  391]    Overall Loss 0.189396    Objective Loss 0.189396                                        LR 0.005522    Time 0.056284    
2022-01-29 04:23:47,031 - Epoch: [170][  200/  391]    Overall Loss 0.193329    Objective Loss 0.193329                                        LR 0.005522    Time 0.055045    
2022-01-29 04:23:52,408 - Epoch: [170][  300/  391]    Overall Loss 0.195440    Objective Loss 0.195440                                        LR 0.005522    Time 0.054617    
2022-01-29 04:23:57,286 - Epoch: [170][  391/  391]    Overall Loss 0.199007    Objective Loss 0.199007    Top1 93.269231    Top5 100.000000    LR 0.005522    Time 0.054379    
2022-01-29 04:23:57,344 - --- validate (epoch=170)-----------
2022-01-29 04:23:57,345 - 10000 samples (128 per mini-batch)
2022-01-29 04:23:59,195 - Epoch: [170][   79/   79]    Loss 1.309249    Top1 67.490000    Top5 90.610000    
2022-01-29 04:23:59,250 - ==> Top1: 67.490    Top5: 90.610    Loss: 1.309

2022-01-29 04:23:59,256 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:23:59,256 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:23:59,299 - 

2022-01-29 04:23:59,299 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:24:05,012 - Epoch: [171][  100/  391]    Overall Loss 0.191769    Objective Loss 0.191769                                        LR 0.005522    Time 0.057095    
2022-01-29 04:24:10,389 - Epoch: [171][  200/  391]    Overall Loss 0.193317    Objective Loss 0.193317                                        LR 0.005522    Time 0.055428    
2022-01-29 04:24:15,761 - Epoch: [171][  300/  391]    Overall Loss 0.195310    Objective Loss 0.195310                                        LR 0.005522    Time 0.054857    
2022-01-29 04:24:20,639 - Epoch: [171][  391/  391]    Overall Loss 0.195081    Objective Loss 0.195081    Top1 95.192308    Top5 100.000000    LR 0.005522    Time 0.054563    
2022-01-29 04:24:20,697 - --- validate (epoch=171)-----------
2022-01-29 04:24:20,698 - 10000 samples (128 per mini-batch)
2022-01-29 04:24:22,517 - Epoch: [171][   79/   79]    Loss 1.305461    Top1 67.270000    Top5 90.690000    
2022-01-29 04:24:22,574 - ==> Top1: 67.270    Top5: 90.690    Loss: 1.305

2022-01-29 04:24:22,580 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:24:22,580 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:24:22,628 - 

2022-01-29 04:24:22,628 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:24:28,338 - Epoch: [172][  100/  391]    Overall Loss 0.186173    Objective Loss 0.186173                                        LR 0.005522    Time 0.057075    
2022-01-29 04:24:33,731 - Epoch: [172][  200/  391]    Overall Loss 0.191541    Objective Loss 0.191541                                        LR 0.005522    Time 0.055497    
2022-01-29 04:24:39,131 - Epoch: [172][  300/  391]    Overall Loss 0.195302    Objective Loss 0.195302                                        LR 0.005522    Time 0.054994    
2022-01-29 04:24:44,042 - Epoch: [172][  391/  391]    Overall Loss 0.197953    Objective Loss 0.197953    Top1 94.230769    Top5 100.000000    LR 0.005522    Time 0.054753    
2022-01-29 04:24:44,098 - --- validate (epoch=172)-----------
2022-01-29 04:24:44,098 - 10000 samples (128 per mini-batch)
2022-01-29 04:24:46,093 - Epoch: [172][   79/   79]    Loss 1.315649    Top1 67.480000    Top5 90.520000    
2022-01-29 04:24:46,147 - ==> Top1: 67.480    Top5: 90.520    Loss: 1.316

2022-01-29 04:24:46,152 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:24:46,152 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:24:46,194 - 

2022-01-29 04:24:46,194 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:24:51,824 - Epoch: [173][  100/  391]    Overall Loss 0.185293    Objective Loss 0.185293                                        LR 0.005522    Time 0.056265    
2022-01-29 04:24:57,226 - Epoch: [173][  200/  391]    Overall Loss 0.190673    Objective Loss 0.190673                                        LR 0.005522    Time 0.055142    
2022-01-29 04:25:02,631 - Epoch: [173][  300/  391]    Overall Loss 0.193513    Objective Loss 0.193513                                        LR 0.005522    Time 0.054772    
2022-01-29 04:25:07,540 - Epoch: [173][  391/  391]    Overall Loss 0.193638    Objective Loss 0.193638    Top1 95.192308    Top5 100.000000    LR 0.005522    Time 0.054578    
2022-01-29 04:25:07,596 - --- validate (epoch=173)-----------
2022-01-29 04:25:07,597 - 10000 samples (128 per mini-batch)
2022-01-29 04:25:09,548 - Epoch: [173][   79/   79]    Loss 1.330997    Top1 67.130000    Top5 90.100000    
2022-01-29 04:25:09,602 - ==> Top1: 67.130    Top5: 90.100    Loss: 1.331

2022-01-29 04:25:09,608 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:25:09,608 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:25:09,652 - 

2022-01-29 04:25:09,652 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:25:15,356 - Epoch: [174][  100/  391]    Overall Loss 0.179839    Objective Loss 0.179839                                        LR 0.005522    Time 0.057013    
2022-01-29 04:25:20,765 - Epoch: [174][  200/  391]    Overall Loss 0.185719    Objective Loss 0.185719                                        LR 0.005522    Time 0.055546    
2022-01-29 04:25:26,152 - Epoch: [174][  300/  391]    Overall Loss 0.187039    Objective Loss 0.187039                                        LR 0.005522    Time 0.054984    
2022-01-29 04:25:31,037 - Epoch: [174][  391/  391]    Overall Loss 0.190105    Objective Loss 0.190105    Top1 96.153846    Top5 99.519231    LR 0.005522    Time 0.054678    
2022-01-29 04:25:31,096 - --- validate (epoch=174)-----------
2022-01-29 04:25:31,096 - 10000 samples (128 per mini-batch)
2022-01-29 04:25:32,945 - Epoch: [174][   79/   79]    Loss 1.327823    Top1 67.020000    Top5 90.360000    
2022-01-29 04:25:33,004 - ==> Top1: 67.020    Top5: 90.360    Loss: 1.328

2022-01-29 04:25:33,009 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:25:33,009 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:25:33,052 - 

2022-01-29 04:25:33,052 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:25:38,741 - Epoch: [175][  100/  391]    Overall Loss 0.169684    Objective Loss 0.169684                                        LR 0.001298    Time 0.056856    
2022-01-29 04:25:44,113 - Epoch: [175][  200/  391]    Overall Loss 0.173294    Objective Loss 0.173294                                        LR 0.001298    Time 0.055286    
2022-01-29 04:25:49,490 - Epoch: [175][  300/  391]    Overall Loss 0.170795    Objective Loss 0.170795                                        LR 0.001298    Time 0.054776    
2022-01-29 04:25:54,378 - Epoch: [175][  391/  391]    Overall Loss 0.170301    Objective Loss 0.170301    Top1 96.634615    Top5 100.000000    LR 0.001298    Time 0.054527    
2022-01-29 04:25:54,437 - --- validate (epoch=175)-----------
2022-01-29 04:25:54,437 - 10000 samples (128 per mini-batch)
2022-01-29 04:25:56,276 - Epoch: [175][   79/   79]    Loss 1.300342    Top1 67.440000    Top5 90.720000    
2022-01-29 04:25:56,334 - ==> Top1: 67.440    Top5: 90.720    Loss: 1.300

2022-01-29 04:25:56,340 - ==> Best [Top1: 67.670   Top5: 90.750   Sparsity:0.00   Params: 1341960 on epoch: 152]
2022-01-29 04:25:56,340 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:25:56,388 - 

2022-01-29 04:25:56,388 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:26:02,007 - Epoch: [176][  100/  391]    Overall Loss 0.165044    Objective Loss 0.165044                                        LR 0.001298    Time 0.056166    
2022-01-29 04:26:07,385 - Epoch: [176][  200/  391]    Overall Loss 0.162607    Objective Loss 0.162607                                        LR 0.001298    Time 0.054968    
2022-01-29 04:26:12,764 - Epoch: [176][  300/  391]    Overall Loss 0.163057    Objective Loss 0.163057                                        LR 0.001298    Time 0.054573    
2022-01-29 04:26:17,661 - Epoch: [176][  391/  391]    Overall Loss 0.165106    Objective Loss 0.165106    Top1 96.153846    Top5 99.038462    LR 0.001298    Time 0.054393    
2022-01-29 04:26:17,719 - --- validate (epoch=176)-----------
2022-01-29 04:26:17,719 - 10000 samples (128 per mini-batch)
2022-01-29 04:26:19,572 - Epoch: [176][   79/   79]    Loss 1.304588    Top1 67.760000    Top5 90.570000    
2022-01-29 04:26:19,626 - ==> Top1: 67.760    Top5: 90.570    Loss: 1.305

2022-01-29 04:26:19,632 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:26:19,632 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:26:19,689 - 

2022-01-29 04:26:19,689 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:26:25,387 - Epoch: [177][  100/  391]    Overall Loss 0.152538    Objective Loss 0.152538                                        LR 0.001298    Time 0.056953    
2022-01-29 04:26:30,774 - Epoch: [177][  200/  391]    Overall Loss 0.159156    Objective Loss 0.159156                                        LR 0.001298    Time 0.055407    
2022-01-29 04:26:36,168 - Epoch: [177][  300/  391]    Overall Loss 0.159855    Objective Loss 0.159855                                        LR 0.001298    Time 0.054913    
2022-01-29 04:26:41,058 - Epoch: [177][  391/  391]    Overall Loss 0.159679    Objective Loss 0.159679    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.054638    
2022-01-29 04:26:41,117 - --- validate (epoch=177)-----------
2022-01-29 04:26:41,117 - 10000 samples (128 per mini-batch)
2022-01-29 04:26:42,951 - Epoch: [177][   79/   79]    Loss 1.303606    Top1 67.640000    Top5 90.560000    
2022-01-29 04:26:43,008 - ==> Top1: 67.640    Top5: 90.560    Loss: 1.304

2022-01-29 04:26:43,013 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:26:43,013 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:26:43,060 - 

2022-01-29 04:26:43,060 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:26:48,697 - Epoch: [178][  100/  391]    Overall Loss 0.150261    Objective Loss 0.150261                                        LR 0.001298    Time 0.056350    
2022-01-29 04:26:54,086 - Epoch: [178][  200/  391]    Overall Loss 0.154205    Objective Loss 0.154205                                        LR 0.001298    Time 0.055116    
2022-01-29 04:26:59,440 - Epoch: [178][  300/  391]    Overall Loss 0.155237    Objective Loss 0.155237                                        LR 0.001298    Time 0.054588    
2022-01-29 04:27:04,264 - Epoch: [178][  391/  391]    Overall Loss 0.155451    Objective Loss 0.155451    Top1 99.038462    Top5 100.000000    LR 0.001298    Time 0.054218    
2022-01-29 04:27:04,328 - --- validate (epoch=178)-----------
2022-01-29 04:27:04,328 - 10000 samples (128 per mini-batch)
2022-01-29 04:27:06,172 - Epoch: [178][   79/   79]    Loss 1.306481    Top1 67.510000    Top5 90.600000    
2022-01-29 04:27:06,231 - ==> Top1: 67.510    Top5: 90.600    Loss: 1.306

2022-01-29 04:27:06,237 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:27:06,237 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:27:06,283 - 

2022-01-29 04:27:06,283 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:27:11,831 - Epoch: [179][  100/  391]    Overall Loss 0.154345    Objective Loss 0.154345                                        LR 0.001298    Time 0.055449    
2022-01-29 04:27:17,217 - Epoch: [179][  200/  391]    Overall Loss 0.154114    Objective Loss 0.154114                                        LR 0.001298    Time 0.054649    
2022-01-29 04:27:22,595 - Epoch: [179][  300/  391]    Overall Loss 0.153552    Objective Loss 0.153552                                        LR 0.001298    Time 0.054358    
2022-01-29 04:27:27,512 - Epoch: [179][  391/  391]    Overall Loss 0.154469    Objective Loss 0.154469    Top1 96.634615    Top5 100.000000    LR 0.001298    Time 0.054279    
2022-01-29 04:27:27,575 - --- validate (epoch=179)-----------
2022-01-29 04:27:27,575 - 10000 samples (128 per mini-batch)
2022-01-29 04:27:29,443 - Epoch: [179][   79/   79]    Loss 1.307811    Top1 67.520000    Top5 90.580000    
2022-01-29 04:27:29,497 - ==> Top1: 67.520    Top5: 90.580    Loss: 1.308

2022-01-29 04:27:29,502 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:27:29,502 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:27:29,550 - 

2022-01-29 04:27:29,551 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:27:35,262 - Epoch: [180][  100/  391]    Overall Loss 0.153544    Objective Loss 0.153544                                        LR 0.001298    Time 0.057084    
2022-01-29 04:27:40,668 - Epoch: [180][  200/  391]    Overall Loss 0.154873    Objective Loss 0.154873                                        LR 0.001298    Time 0.055569    
2022-01-29 04:27:46,047 - Epoch: [180][  300/  391]    Overall Loss 0.155186    Objective Loss 0.155186                                        LR 0.001298    Time 0.054974    
2022-01-29 04:27:50,969 - Epoch: [180][  391/  391]    Overall Loss 0.155723    Objective Loss 0.155723    Top1 94.230769    Top5 100.000000    LR 0.001298    Time 0.054766    
2022-01-29 04:27:51,029 - --- validate (epoch=180)-----------
2022-01-29 04:27:51,029 - 10000 samples (128 per mini-batch)
2022-01-29 04:27:52,885 - Epoch: [180][   79/   79]    Loss 1.306263    Top1 67.460000    Top5 90.640000    
2022-01-29 04:27:52,939 - ==> Top1: 67.460    Top5: 90.640    Loss: 1.306

2022-01-29 04:27:52,944 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:27:52,944 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:27:52,992 - 

2022-01-29 04:27:52,992 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:27:58,697 - Epoch: [181][  100/  391]    Overall Loss 0.147519    Objective Loss 0.147519                                        LR 0.001298    Time 0.057025    
2022-01-29 04:28:04,063 - Epoch: [181][  200/  391]    Overall Loss 0.152392    Objective Loss 0.152392                                        LR 0.001298    Time 0.055334    
2022-01-29 04:28:09,377 - Epoch: [181][  300/  391]    Overall Loss 0.152691    Objective Loss 0.152691                                        LR 0.001298    Time 0.054600    
2022-01-29 04:28:14,207 - Epoch: [181][  391/  391]    Overall Loss 0.152324    Objective Loss 0.152324    Top1 99.519231    Top5 100.000000    LR 0.001298    Time 0.054243    
2022-01-29 04:28:14,263 - --- validate (epoch=181)-----------
2022-01-29 04:28:14,263 - 10000 samples (128 per mini-batch)
2022-01-29 04:28:16,179 - Epoch: [181][   79/   79]    Loss 1.308674    Top1 67.530000    Top5 90.660000    
2022-01-29 04:28:16,236 - ==> Top1: 67.530    Top5: 90.660    Loss: 1.309

2022-01-29 04:28:16,241 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:28:16,241 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:28:16,284 - 

2022-01-29 04:28:16,284 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:28:21,914 - Epoch: [182][  100/  391]    Overall Loss 0.147627    Objective Loss 0.147627                                        LR 0.001298    Time 0.056272    
2022-01-29 04:28:27,297 - Epoch: [182][  200/  391]    Overall Loss 0.148014    Objective Loss 0.148014                                        LR 0.001298    Time 0.055047    
2022-01-29 04:28:32,685 - Epoch: [182][  300/  391]    Overall Loss 0.149875    Objective Loss 0.149875                                        LR 0.001298    Time 0.054655    
2022-01-29 04:28:37,586 - Epoch: [182][  391/  391]    Overall Loss 0.151808    Objective Loss 0.151808    Top1 96.153846    Top5 100.000000    LR 0.001298    Time 0.054469    
2022-01-29 04:28:37,645 - --- validate (epoch=182)-----------
2022-01-29 04:28:37,646 - 10000 samples (128 per mini-batch)
2022-01-29 04:28:39,516 - Epoch: [182][   79/   79]    Loss 1.312808    Top1 67.420000    Top5 90.700000    
2022-01-29 04:28:39,576 - ==> Top1: 67.420    Top5: 90.700    Loss: 1.313

2022-01-29 04:28:39,581 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:28:39,581 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:28:39,629 - 

2022-01-29 04:28:39,630 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:28:45,362 - Epoch: [183][  100/  391]    Overall Loss 0.146670    Objective Loss 0.146670                                        LR 0.001298    Time 0.057300    
2022-01-29 04:28:50,783 - Epoch: [183][  200/  391]    Overall Loss 0.148489    Objective Loss 0.148489                                        LR 0.001298    Time 0.055751    
2022-01-29 04:28:56,213 - Epoch: [183][  300/  391]    Overall Loss 0.149384    Objective Loss 0.149384                                        LR 0.001298    Time 0.055263    
2022-01-29 04:29:01,139 - Epoch: [183][  391/  391]    Overall Loss 0.149853    Objective Loss 0.149853    Top1 98.076923    Top5 100.000000    LR 0.001298    Time 0.054996    
2022-01-29 04:29:01,202 - --- validate (epoch=183)-----------
2022-01-29 04:29:01,202 - 10000 samples (128 per mini-batch)
2022-01-29 04:29:03,064 - Epoch: [183][   79/   79]    Loss 1.323613    Top1 67.440000    Top5 90.580000    
2022-01-29 04:29:03,123 - ==> Top1: 67.440    Top5: 90.580    Loss: 1.324

2022-01-29 04:29:03,129 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:29:03,129 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:29:03,177 - 

2022-01-29 04:29:03,177 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:29:08,877 - Epoch: [184][  100/  391]    Overall Loss 0.144205    Objective Loss 0.144205                                        LR 0.001298    Time 0.056971    
2022-01-29 04:29:14,269 - Epoch: [184][  200/  391]    Overall Loss 0.147373    Objective Loss 0.147373                                        LR 0.001298    Time 0.055441    
2022-01-29 04:29:19,643 - Epoch: [184][  300/  391]    Overall Loss 0.149186    Objective Loss 0.149186                                        LR 0.001298    Time 0.054872    
2022-01-29 04:29:24,528 - Epoch: [184][  391/  391]    Overall Loss 0.149750    Objective Loss 0.149750    Top1 97.115385    Top5 100.000000    LR 0.001298    Time 0.054593    
2022-01-29 04:29:24,582 - --- validate (epoch=184)-----------
2022-01-29 04:29:24,582 - 10000 samples (128 per mini-batch)
2022-01-29 04:29:26,511 - Epoch: [184][   79/   79]    Loss 1.313039    Top1 67.560000    Top5 90.650000    
2022-01-29 04:29:26,561 - ==> Top1: 67.560    Top5: 90.650    Loss: 1.313

2022-01-29 04:29:26,566 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:29:26,566 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:29:26,614 - 

2022-01-29 04:29:26,615 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:29:32,311 - Epoch: [185][  100/  391]    Overall Loss 0.146376    Objective Loss 0.146376                                        LR 0.001298    Time 0.056937    
2022-01-29 04:29:37,764 - Epoch: [185][  200/  391]    Overall Loss 0.146801    Objective Loss 0.146801                                        LR 0.001298    Time 0.055727    
2022-01-29 04:29:43,221 - Epoch: [185][  300/  391]    Overall Loss 0.147363    Objective Loss 0.147363                                        LR 0.001298    Time 0.055341    
2022-01-29 04:29:48,183 - Epoch: [185][  391/  391]    Overall Loss 0.148448    Objective Loss 0.148448    Top1 98.557692    Top5 100.000000    LR 0.001298    Time 0.055149    
2022-01-29 04:29:48,239 - --- validate (epoch=185)-----------
2022-01-29 04:29:48,239 - 10000 samples (128 per mini-batch)
2022-01-29 04:29:50,229 - Epoch: [185][   79/   79]    Loss 1.320282    Top1 67.470000    Top5 90.480000    
2022-01-29 04:29:50,279 - ==> Top1: 67.470    Top5: 90.480    Loss: 1.320

2022-01-29 04:29:50,284 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:29:50,284 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:29:50,332 - 

2022-01-29 04:29:50,332 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:29:56,065 - Epoch: [186][  100/  391]    Overall Loss 0.142105    Objective Loss 0.142105                                        LR 0.001298    Time 0.057300    
2022-01-29 04:30:01,451 - Epoch: [186][  200/  391]    Overall Loss 0.144549    Objective Loss 0.144549                                        LR 0.001298    Time 0.055578    
2022-01-29 04:30:06,832 - Epoch: [186][  300/  391]    Overall Loss 0.144686    Objective Loss 0.144686                                        LR 0.001298    Time 0.054986    
2022-01-29 04:30:11,728 - Epoch: [186][  391/  391]    Overall Loss 0.146785    Objective Loss 0.146785    Top1 96.153846    Top5 100.000000    LR 0.001298    Time 0.054709    
2022-01-29 04:30:11,786 - --- validate (epoch=186)-----------
2022-01-29 04:30:11,786 - 10000 samples (128 per mini-batch)
2022-01-29 04:30:13,614 - Epoch: [186][   79/   79]    Loss 1.335391    Top1 67.490000    Top5 90.370000    
2022-01-29 04:30:13,668 - ==> Top1: 67.490    Top5: 90.370    Loss: 1.335

2022-01-29 04:30:13,673 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:30:13,673 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:30:13,719 - 

2022-01-29 04:30:13,720 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:30:19,403 - Epoch: [187][  100/  391]    Overall Loss 0.145494    Objective Loss 0.145494                                        LR 0.001298    Time 0.056810    
2022-01-29 04:30:24,788 - Epoch: [187][  200/  391]    Overall Loss 0.144155    Objective Loss 0.144155                                        LR 0.001298    Time 0.055327    
2022-01-29 04:30:30,177 - Epoch: [187][  300/  391]    Overall Loss 0.145891    Objective Loss 0.145891                                        LR 0.001298    Time 0.054843    
2022-01-29 04:30:35,071 - Epoch: [187][  391/  391]    Overall Loss 0.147022    Objective Loss 0.147022    Top1 92.307692    Top5 100.000000    LR 0.001298    Time 0.054595    
2022-01-29 04:30:35,127 - --- validate (epoch=187)-----------
2022-01-29 04:30:35,127 - 10000 samples (128 per mini-batch)
2022-01-29 04:30:37,014 - Epoch: [187][   79/   79]    Loss 1.315433    Top1 67.240000    Top5 90.510000    
2022-01-29 04:30:37,069 - ==> Top1: 67.240    Top5: 90.510    Loss: 1.315

2022-01-29 04:30:37,074 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:30:37,074 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:30:37,121 - 

2022-01-29 04:30:37,121 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:30:42,746 - Epoch: [188][  100/  391]    Overall Loss 0.142145    Objective Loss 0.142145                                        LR 0.001298    Time 0.056225    
2022-01-29 04:30:48,107 - Epoch: [188][  200/  391]    Overall Loss 0.141933    Objective Loss 0.141933                                        LR 0.001298    Time 0.054913    
2022-01-29 04:30:53,500 - Epoch: [188][  300/  391]    Overall Loss 0.143627    Objective Loss 0.143627                                        LR 0.001298    Time 0.054583    
2022-01-29 04:30:58,401 - Epoch: [188][  391/  391]    Overall Loss 0.143816    Objective Loss 0.143816    Top1 97.115385    Top5 100.000000    LR 0.001298    Time 0.054412    
2022-01-29 04:30:58,465 - --- validate (epoch=188)-----------
2022-01-29 04:30:58,466 - 10000 samples (128 per mini-batch)
2022-01-29 04:31:00,302 - Epoch: [188][   79/   79]    Loss 1.318932    Top1 67.390000    Top5 90.590000    
2022-01-29 04:31:00,360 - ==> Top1: 67.390    Top5: 90.590    Loss: 1.319

2022-01-29 04:31:00,366 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:31:00,366 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:31:00,414 - 

2022-01-29 04:31:00,414 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:31:06,148 - Epoch: [189][  100/  391]    Overall Loss 0.144479    Objective Loss 0.144479                                        LR 0.001298    Time 0.057316    
2022-01-29 04:31:11,589 - Epoch: [189][  200/  391]    Overall Loss 0.141890    Objective Loss 0.141890                                        LR 0.001298    Time 0.055858    
2022-01-29 04:31:17,032 - Epoch: [189][  300/  391]    Overall Loss 0.143411    Objective Loss 0.143411                                        LR 0.001298    Time 0.055380    
2022-01-29 04:31:21,980 - Epoch: [189][  391/  391]    Overall Loss 0.144935    Objective Loss 0.144935    Top1 97.596154    Top5 100.000000    LR 0.001298    Time 0.055144    
2022-01-29 04:31:22,039 - --- validate (epoch=189)-----------
2022-01-29 04:31:22,039 - 10000 samples (128 per mini-batch)
2022-01-29 04:31:23,888 - Epoch: [189][   79/   79]    Loss 1.319417    Top1 67.220000    Top5 90.680000    
2022-01-29 04:31:23,938 - ==> Top1: 67.220    Top5: 90.680    Loss: 1.319

2022-01-29 04:31:23,944 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:31:23,944 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:31:23,992 - 

2022-01-29 04:31:23,992 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:31:29,749 - Epoch: [190][  100/  391]    Overall Loss 0.140558    Objective Loss 0.140558                                        LR 0.001298    Time 0.057549    
2022-01-29 04:31:35,155 - Epoch: [190][  200/  391]    Overall Loss 0.140149    Objective Loss 0.140149                                        LR 0.001298    Time 0.055796    
2022-01-29 04:31:40,534 - Epoch: [190][  300/  391]    Overall Loss 0.140610    Objective Loss 0.140610                                        LR 0.001298    Time 0.055128    
2022-01-29 04:31:45,449 - Epoch: [190][  391/  391]    Overall Loss 0.141761    Objective Loss 0.141761    Top1 96.634615    Top5 100.000000    LR 0.001298    Time 0.054864    
2022-01-29 04:31:45,508 - --- validate (epoch=190)-----------
2022-01-29 04:31:45,508 - 10000 samples (128 per mini-batch)
2022-01-29 04:31:47,383 - Epoch: [190][   79/   79]    Loss 1.316981    Top1 67.600000    Top5 90.540000    
2022-01-29 04:31:47,440 - ==> Top1: 67.600    Top5: 90.540    Loss: 1.317

2022-01-29 04:31:47,445 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:31:47,445 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:31:47,488 - 

2022-01-29 04:31:47,488 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:31:53,139 - Epoch: [191][  100/  391]    Overall Loss 0.142163    Objective Loss 0.142163                                        LR 0.001298    Time 0.056479    
2022-01-29 04:31:58,561 - Epoch: [191][  200/  391]    Overall Loss 0.140328    Objective Loss 0.140328                                        LR 0.001298    Time 0.055349    
2022-01-29 04:32:03,986 - Epoch: [191][  300/  391]    Overall Loss 0.142723    Objective Loss 0.142723                                        LR 0.001298    Time 0.054980    
2022-01-29 04:32:08,904 - Epoch: [191][  391/  391]    Overall Loss 0.143031    Objective Loss 0.143031    Top1 96.634615    Top5 100.000000    LR 0.001298    Time 0.054758    
2022-01-29 04:32:08,960 - --- validate (epoch=191)-----------
2022-01-29 04:32:08,961 - 10000 samples (128 per mini-batch)
2022-01-29 04:32:10,888 - Epoch: [191][   79/   79]    Loss 1.338941    Top1 67.430000    Top5 90.470000    
2022-01-29 04:32:10,940 - ==> Top1: 67.430    Top5: 90.470    Loss: 1.339

2022-01-29 04:32:10,945 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:32:10,945 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:32:10,992 - 

2022-01-29 04:32:10,993 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:32:16,693 - Epoch: [192][  100/  391]    Overall Loss 0.144208    Objective Loss 0.144208                                        LR 0.001298    Time 0.056979    
2022-01-29 04:32:22,085 - Epoch: [192][  200/  391]    Overall Loss 0.142052    Objective Loss 0.142052                                        LR 0.001298    Time 0.055444    
2022-01-29 04:32:27,477 - Epoch: [192][  300/  391]    Overall Loss 0.140551    Objective Loss 0.140551                                        LR 0.001298    Time 0.054935    
2022-01-29 04:32:32,375 - Epoch: [192][  391/  391]    Overall Loss 0.140944    Objective Loss 0.140944    Top1 96.153846    Top5 100.000000    LR 0.001298    Time 0.054674    
2022-01-29 04:32:32,440 - --- validate (epoch=192)-----------
2022-01-29 04:32:32,440 - 10000 samples (128 per mini-batch)
2022-01-29 04:32:34,258 - Epoch: [192][   79/   79]    Loss 1.324457    Top1 67.580000    Top5 90.460000    
2022-01-29 04:32:34,311 - ==> Top1: 67.580    Top5: 90.460    Loss: 1.324

2022-01-29 04:32:34,316 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:32:34,317 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:32:34,363 - 

2022-01-29 04:32:34,364 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:32:40,069 - Epoch: [193][  100/  391]    Overall Loss 0.139681    Objective Loss 0.139681                                        LR 0.001298    Time 0.057022    
2022-01-29 04:32:45,451 - Epoch: [193][  200/  391]    Overall Loss 0.140443    Objective Loss 0.140443                                        LR 0.001298    Time 0.055420    
2022-01-29 04:32:50,832 - Epoch: [193][  300/  391]    Overall Loss 0.140998    Objective Loss 0.140998                                        LR 0.001298    Time 0.054881    
2022-01-29 04:32:55,687 - Epoch: [193][  391/  391]    Overall Loss 0.142343    Objective Loss 0.142343    Top1 98.076923    Top5 100.000000    LR 0.001298    Time 0.054523    
2022-01-29 04:32:55,750 - --- validate (epoch=193)-----------
2022-01-29 04:32:55,750 - 10000 samples (128 per mini-batch)
2022-01-29 04:32:57,579 - Epoch: [193][   79/   79]    Loss 1.331367    Top1 67.480000    Top5 90.500000    
2022-01-29 04:32:57,632 - ==> Top1: 67.480    Top5: 90.500    Loss: 1.331

2022-01-29 04:32:57,637 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:32:57,637 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:32:57,683 - 

2022-01-29 04:32:57,684 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:33:03,259 - Epoch: [194][  100/  391]    Overall Loss 0.137349    Objective Loss 0.137349                                        LR 0.001298    Time 0.055726    
2022-01-29 04:33:08,656 - Epoch: [194][  200/  391]    Overall Loss 0.140480    Objective Loss 0.140480                                        LR 0.001298    Time 0.054843    
2022-01-29 04:33:14,058 - Epoch: [194][  300/  391]    Overall Loss 0.142368    Objective Loss 0.142368                                        LR 0.001298    Time 0.054566    
2022-01-29 04:33:18,968 - Epoch: [194][  391/  391]    Overall Loss 0.142324    Objective Loss 0.142324    Top1 97.596154    Top5 100.000000    LR 0.001298    Time 0.054422    
2022-01-29 04:33:19,024 - --- validate (epoch=194)-----------
2022-01-29 04:33:19,024 - 10000 samples (128 per mini-batch)
2022-01-29 04:33:20,975 - Epoch: [194][   79/   79]    Loss 1.340121    Top1 67.310000    Top5 90.270000    
2022-01-29 04:33:21,033 - ==> Top1: 67.310    Top5: 90.270    Loss: 1.340

2022-01-29 04:33:21,038 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:33:21,038 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:33:21,087 - 

2022-01-29 04:33:21,087 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:33:26,817 - Epoch: [195][  100/  391]    Overall Loss 0.141496    Objective Loss 0.141496                                        LR 0.001298    Time 0.057272    
2022-01-29 04:33:32,169 - Epoch: [195][  200/  391]    Overall Loss 0.140494    Objective Loss 0.140494                                        LR 0.001298    Time 0.055392    
2022-01-29 04:33:37,506 - Epoch: [195][  300/  391]    Overall Loss 0.141397    Objective Loss 0.141397                                        LR 0.001298    Time 0.054715    
2022-01-29 04:33:42,347 - Epoch: [195][  391/  391]    Overall Loss 0.141264    Objective Loss 0.141264    Top1 96.634615    Top5 100.000000    LR 0.001298    Time 0.054360    
2022-01-29 04:33:42,407 - --- validate (epoch=195)-----------
2022-01-29 04:33:42,407 - 10000 samples (128 per mini-batch)
2022-01-29 04:33:44,232 - Epoch: [195][   79/   79]    Loss 1.333249    Top1 67.300000    Top5 90.310000    
2022-01-29 04:33:44,286 - ==> Top1: 67.300    Top5: 90.310    Loss: 1.333

2022-01-29 04:33:44,291 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:33:44,291 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:33:44,338 - 

2022-01-29 04:33:44,338 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:33:50,055 - Epoch: [196][  100/  391]    Overall Loss 0.138149    Objective Loss 0.138149                                        LR 0.001298    Time 0.057140    
2022-01-29 04:33:55,459 - Epoch: [196][  200/  391]    Overall Loss 0.137549    Objective Loss 0.137549                                        LR 0.001298    Time 0.055587    
2022-01-29 04:34:00,789 - Epoch: [196][  300/  391]    Overall Loss 0.137432    Objective Loss 0.137432                                        LR 0.001298    Time 0.054821    
2022-01-29 04:34:05,608 - Epoch: [196][  391/  391]    Overall Loss 0.138992    Objective Loss 0.138992    Top1 98.076923    Top5 100.000000    LR 0.001298    Time 0.054384    
2022-01-29 04:34:05,666 - --- validate (epoch=196)-----------
2022-01-29 04:34:05,666 - 10000 samples (128 per mini-batch)
2022-01-29 04:34:07,484 - Epoch: [196][   79/   79]    Loss 1.345447    Top1 67.360000    Top5 90.430000    
2022-01-29 04:34:07,535 - ==> Top1: 67.360    Top5: 90.430    Loss: 1.345

2022-01-29 04:34:07,540 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:34:07,540 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:34:07,587 - 

2022-01-29 04:34:07,587 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:34:13,139 - Epoch: [197][  100/  391]    Overall Loss 0.137685    Objective Loss 0.137685                                        LR 0.001298    Time 0.055490    
2022-01-29 04:34:18,471 - Epoch: [197][  200/  391]    Overall Loss 0.136930    Objective Loss 0.136930                                        LR 0.001298    Time 0.054400    
2022-01-29 04:34:23,795 - Epoch: [197][  300/  391]    Overall Loss 0.137013    Objective Loss 0.137013                                        LR 0.001298    Time 0.054012    
2022-01-29 04:34:28,666 - Epoch: [197][  391/  391]    Overall Loss 0.138425    Objective Loss 0.138425    Top1 95.192308    Top5 99.519231    LR 0.001298    Time 0.053896    
2022-01-29 04:34:28,724 - --- validate (epoch=197)-----------
2022-01-29 04:34:28,724 - 10000 samples (128 per mini-batch)
2022-01-29 04:34:30,601 - Epoch: [197][   79/   79]    Loss 1.342346    Top1 67.610000    Top5 90.460000    
2022-01-29 04:34:30,656 - ==> Top1: 67.610    Top5: 90.460    Loss: 1.342

2022-01-29 04:34:30,661 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:34:30,661 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:34:30,708 - 

2022-01-29 04:34:30,708 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:34:36,463 - Epoch: [198][  100/  391]    Overall Loss 0.136654    Objective Loss 0.136654                                        LR 0.001298    Time 0.057516    
2022-01-29 04:34:41,918 - Epoch: [198][  200/  391]    Overall Loss 0.134529    Objective Loss 0.134529                                        LR 0.001298    Time 0.056030    
2022-01-29 04:34:47,360 - Epoch: [198][  300/  391]    Overall Loss 0.137303    Objective Loss 0.137303                                        LR 0.001298    Time 0.055490    
2022-01-29 04:34:52,283 - Epoch: [198][  391/  391]    Overall Loss 0.139177    Objective Loss 0.139177    Top1 96.153846    Top5 100.000000    LR 0.001298    Time 0.055165    
2022-01-29 04:34:52,343 - --- validate (epoch=198)-----------
2022-01-29 04:34:52,343 - 10000 samples (128 per mini-batch)
2022-01-29 04:34:54,177 - Epoch: [198][   79/   79]    Loss 1.340450    Top1 67.200000    Top5 90.380000    
2022-01-29 04:34:54,228 - ==> Top1: 67.200    Top5: 90.380    Loss: 1.340

2022-01-29 04:34:54,233 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:34:54,233 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:34:54,280 - 

2022-01-29 04:34:54,280 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:34:59,979 - Epoch: [199][  100/  391]    Overall Loss 0.134510    Objective Loss 0.134510                                        LR 0.001298    Time 0.056969    
2022-01-29 04:35:05,395 - Epoch: [199][  200/  391]    Overall Loss 0.137260    Objective Loss 0.137260                                        LR 0.001298    Time 0.055560    
2022-01-29 04:35:10,815 - Epoch: [199][  300/  391]    Overall Loss 0.138025    Objective Loss 0.138025                                        LR 0.001298    Time 0.055102    
2022-01-29 04:35:15,739 - Epoch: [199][  391/  391]    Overall Loss 0.137913    Objective Loss 0.137913    Top1 98.076923    Top5 100.000000    LR 0.001298    Time 0.054869    
2022-01-29 04:35:15,804 - --- validate (epoch=199)-----------
2022-01-29 04:35:15,804 - 10000 samples (128 per mini-batch)
2022-01-29 04:35:17,671 - Epoch: [199][   79/   79]    Loss 1.345109    Top1 67.270000    Top5 90.300000    
2022-01-29 04:35:17,722 - ==> Top1: 67.270    Top5: 90.300    Loss: 1.345

2022-01-29 04:35:17,727 - ==> Best [Top1: 67.760   Top5: 90.570   Sparsity:0.00   Params: 1341960 on epoch: 176]
2022-01-29 04:35:17,727 - Saving checkpoint to: logs/2022.01.29-031718/checkpoint.pth.tar
2022-01-29 04:35:17,851 - 

2022-01-29 04:35:17,851 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:35:25,723 - Epoch: [200][  100/  391]    Overall Loss 0.564544    Objective Loss 0.564544                                        LR 0.001298    Time 0.078699    
2022-01-29 04:35:33,351 - Epoch: [200][  200/  391]    Overall Loss 0.541910    Objective Loss 0.541910                                        LR 0.001298    Time 0.077481    
2022-01-29 04:35:40,980 - Epoch: [200][  300/  391]    Overall Loss 0.532206    Objective Loss 0.532206                                        LR 0.001298    Time 0.077081    
2022-01-29 04:35:47,918 - Epoch: [200][  391/  391]    Overall Loss 0.529153    Objective Loss 0.529153    Top1 90.384615    Top5 99.519231    LR 0.001298    Time 0.076883    
2022-01-29 04:35:47,982 - --- validate (epoch=200)-----------
2022-01-29 04:35:47,982 - 10000 samples (128 per mini-batch)
2022-01-29 04:35:51,582 - Epoch: [200][   79/   79]    Loss 1.242293    Top1 65.350000    Top5 89.500000    
2022-01-29 04:35:51,641 - ==> Top1: 65.350    Top5: 89.500    Loss: 1.242

2022-01-29 04:35:51,646 - ==> Best [Top1: 65.350   Top5: 89.500   Sparsity:0.00   Params: 1341960 on epoch: 200]
2022-01-29 04:35:51,646 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:35:51,681 - 

2022-01-29 04:35:51,681 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:35:59,593 - Epoch: [201][  100/  391]    Overall Loss 0.477505    Objective Loss 0.477505                                        LR 0.001298    Time 0.079085    
2022-01-29 04:36:07,240 - Epoch: [201][  200/  391]    Overall Loss 0.488976    Objective Loss 0.488976                                        LR 0.001298    Time 0.077775    
2022-01-29 04:36:14,890 - Epoch: [201][  300/  391]    Overall Loss 0.491739    Objective Loss 0.491739                                        LR 0.001298    Time 0.077345    
2022-01-29 04:36:21,817 - Epoch: [201][  391/  391]    Overall Loss 0.492166    Objective Loss 0.492166    Top1 84.134615    Top5 99.519231    LR 0.001298    Time 0.077060    
2022-01-29 04:36:21,875 - --- validate (epoch=201)-----------
2022-01-29 04:36:21,875 - 10000 samples (128 per mini-batch)
2022-01-29 04:36:25,443 - Epoch: [201][   79/   79]    Loss 1.238783    Top1 65.220000    Top5 89.540000    
2022-01-29 04:36:25,492 - ==> Top1: 65.220    Top5: 89.540    Loss: 1.239

2022-01-29 04:36:25,497 - ==> Best [Top1: 65.350   Top5: 89.500   Sparsity:0.00   Params: 1341960 on epoch: 200]
2022-01-29 04:36:25,497 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:36:25,539 - 

2022-01-29 04:36:25,539 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:36:33,507 - Epoch: [202][  100/  391]    Overall Loss 0.467493    Objective Loss 0.467493                                        LR 0.001298    Time 0.079657    
2022-01-29 04:36:41,076 - Epoch: [202][  200/  391]    Overall Loss 0.468936    Objective Loss 0.468936                                        LR 0.001298    Time 0.077667    
2022-01-29 04:36:48,660 - Epoch: [202][  300/  391]    Overall Loss 0.476444    Objective Loss 0.476444                                        LR 0.001298    Time 0.077055    
2022-01-29 04:36:55,615 - Epoch: [202][  391/  391]    Overall Loss 0.481353    Objective Loss 0.481353    Top1 88.461538    Top5 99.519231    LR 0.001298    Time 0.076906    
2022-01-29 04:36:55,671 - --- validate (epoch=202)-----------
2022-01-29 04:36:55,671 - 10000 samples (128 per mini-batch)
2022-01-29 04:36:59,304 - Epoch: [202][   79/   79]    Loss 1.220384    Top1 65.510000    Top5 89.980000    
2022-01-29 04:36:59,355 - ==> Top1: 65.510    Top5: 89.980    Loss: 1.220

2022-01-29 04:36:59,360 - ==> Best [Top1: 65.510   Top5: 89.980   Sparsity:0.00   Params: 1341960 on epoch: 202]
2022-01-29 04:36:59,360 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:36:59,411 - 

2022-01-29 04:36:59,412 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:37:07,278 - Epoch: [203][  100/  391]    Overall Loss 0.452322    Objective Loss 0.452322                                        LR 0.001298    Time 0.078637    
2022-01-29 04:37:14,922 - Epoch: [203][  200/  391]    Overall Loss 0.451996    Objective Loss 0.451996                                        LR 0.001298    Time 0.077531    
2022-01-29 04:37:22,553 - Epoch: [203][  300/  391]    Overall Loss 0.463818    Objective Loss 0.463818                                        LR 0.001298    Time 0.077121    
2022-01-29 04:37:29,550 - Epoch: [203][  391/  391]    Overall Loss 0.463097    Objective Loss 0.463097    Top1 90.865385    Top5 99.519231    LR 0.001298    Time 0.077066    
2022-01-29 04:37:29,608 - --- validate (epoch=203)-----------
2022-01-29 04:37:29,609 - 10000 samples (128 per mini-batch)
2022-01-29 04:37:33,312 - Epoch: [203][   79/   79]    Loss 1.230475    Top1 65.510000    Top5 89.530000    
2022-01-29 04:37:33,362 - ==> Top1: 65.510    Top5: 89.530    Loss: 1.230

2022-01-29 04:37:33,367 - ==> Best [Top1: 65.510   Top5: 89.980   Sparsity:0.00   Params: 1341960 on epoch: 202]
2022-01-29 04:37:33,367 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:37:33,409 - 

2022-01-29 04:37:33,409 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:37:41,310 - Epoch: [204][  100/  391]    Overall Loss 0.441603    Objective Loss 0.441603                                        LR 0.001298    Time 0.078979    
2022-01-29 04:37:48,943 - Epoch: [204][  200/  391]    Overall Loss 0.451002    Objective Loss 0.451002                                        LR 0.001298    Time 0.077646    
2022-01-29 04:37:56,580 - Epoch: [204][  300/  391]    Overall Loss 0.455847    Objective Loss 0.455847                                        LR 0.001298    Time 0.077217    
2022-01-29 04:38:03,552 - Epoch: [204][  391/  391]    Overall Loss 0.461106    Objective Loss 0.461106    Top1 85.576923    Top5 100.000000    LR 0.001298    Time 0.077075    
2022-01-29 04:38:03,609 - --- validate (epoch=204)-----------
2022-01-29 04:38:03,609 - 10000 samples (128 per mini-batch)
2022-01-29 04:38:07,232 - Epoch: [204][   79/   79]    Loss 1.229495    Top1 65.800000    Top5 89.390000    
2022-01-29 04:38:07,289 - ==> Top1: 65.800    Top5: 89.390    Loss: 1.229

2022-01-29 04:38:07,294 - ==> Best [Top1: 65.800   Top5: 89.390   Sparsity:0.00   Params: 1341960 on epoch: 204]
2022-01-29 04:38:07,294 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:38:07,343 - 

2022-01-29 04:38:07,344 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:38:15,296 - Epoch: [205][  100/  391]    Overall Loss 0.451595    Objective Loss 0.451595                                        LR 0.001298    Time 0.079494    
2022-01-29 04:38:22,997 - Epoch: [205][  200/  391]    Overall Loss 0.442079    Objective Loss 0.442079                                        LR 0.001298    Time 0.078248    
2022-01-29 04:38:30,705 - Epoch: [205][  300/  391]    Overall Loss 0.447859    Objective Loss 0.447859                                        LR 0.001298    Time 0.077856    
2022-01-29 04:38:37,665 - Epoch: [205][  391/  391]    Overall Loss 0.448705    Objective Loss 0.448705    Top1 88.942308    Top5 99.519231    LR 0.001298    Time 0.077537    
2022-01-29 04:38:37,724 - --- validate (epoch=205)-----------
2022-01-29 04:38:37,724 - 10000 samples (128 per mini-batch)
2022-01-29 04:38:41,366 - Epoch: [205][   79/   79]    Loss 1.261894    Top1 64.630000    Top5 89.010000    
2022-01-29 04:38:41,424 - ==> Top1: 64.630    Top5: 89.010    Loss: 1.262

2022-01-29 04:38:41,429 - ==> Best [Top1: 65.800   Top5: 89.390   Sparsity:0.00   Params: 1341960 on epoch: 204]
2022-01-29 04:38:41,430 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:38:41,471 - 

2022-01-29 04:38:41,471 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:38:49,343 - Epoch: [206][  100/  391]    Overall Loss 0.440286    Objective Loss 0.440286                                        LR 0.001298    Time 0.078697    
2022-01-29 04:38:56,999 - Epoch: [206][  200/  391]    Overall Loss 0.429458    Objective Loss 0.429458                                        LR 0.001298    Time 0.077625    
2022-01-29 04:39:04,628 - Epoch: [206][  300/  391]    Overall Loss 0.439153    Objective Loss 0.439153                                        LR 0.001298    Time 0.077176    
2022-01-29 04:39:11,513 - Epoch: [206][  391/  391]    Overall Loss 0.442759    Objective Loss 0.442759    Top1 92.307692    Top5 99.519231    LR 0.001298    Time 0.076820    
2022-01-29 04:39:11,571 - --- validate (epoch=206)-----------
2022-01-29 04:39:11,571 - 10000 samples (128 per mini-batch)
2022-01-29 04:39:15,180 - Epoch: [206][   79/   79]    Loss 1.257453    Top1 64.980000    Top5 89.050000    
2022-01-29 04:39:15,231 - ==> Top1: 64.980    Top5: 89.050    Loss: 1.257

2022-01-29 04:39:15,236 - ==> Best [Top1: 65.800   Top5: 89.390   Sparsity:0.00   Params: 1341960 on epoch: 204]
2022-01-29 04:39:15,236 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:39:15,278 - 

2022-01-29 04:39:15,278 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:39:23,025 - Epoch: [207][  100/  391]    Overall Loss 0.423512    Objective Loss 0.423512                                        LR 0.001298    Time 0.077444    
2022-01-29 04:39:30,520 - Epoch: [207][  200/  391]    Overall Loss 0.422174    Objective Loss 0.422174                                        LR 0.001298    Time 0.076192    
2022-01-29 04:39:38,013 - Epoch: [207][  300/  391]    Overall Loss 0.429900    Objective Loss 0.429900                                        LR 0.001298    Time 0.075768    
2022-01-29 04:39:44,854 - Epoch: [207][  391/  391]    Overall Loss 0.432794    Objective Loss 0.432794    Top1 91.346154    Top5 98.557692    LR 0.001298    Time 0.075628    
2022-01-29 04:39:44,913 - --- validate (epoch=207)-----------
2022-01-29 04:39:44,913 - 10000 samples (128 per mini-batch)
2022-01-29 04:39:48,446 - Epoch: [207][   79/   79]    Loss 1.200621    Top1 66.090000    Top5 89.890000    
2022-01-29 04:39:48,505 - ==> Top1: 66.090    Top5: 89.890    Loss: 1.201

2022-01-29 04:39:48,510 - ==> Best [Top1: 66.090   Top5: 89.890   Sparsity:0.00   Params: 1341960 on epoch: 207]
2022-01-29 04:39:48,510 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:39:48,560 - 

2022-01-29 04:39:48,561 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:39:56,417 - Epoch: [208][  100/  391]    Overall Loss 0.416381    Objective Loss 0.416381                                        LR 0.001298    Time 0.078537    
2022-01-29 04:40:03,978 - Epoch: [208][  200/  391]    Overall Loss 0.415021    Objective Loss 0.415021                                        LR 0.001298    Time 0.077069    
2022-01-29 04:40:11,548 - Epoch: [208][  300/  391]    Overall Loss 0.418199    Objective Loss 0.418199                                        LR 0.001298    Time 0.076610    
2022-01-29 04:40:18,436 - Epoch: [208][  391/  391]    Overall Loss 0.423260    Objective Loss 0.423260    Top1 89.903846    Top5 99.038462    LR 0.001298    Time 0.076394    
2022-01-29 04:40:18,493 - --- validate (epoch=208)-----------
2022-01-29 04:40:18,493 - 10000 samples (128 per mini-batch)
2022-01-29 04:40:22,056 - Epoch: [208][   79/   79]    Loss 1.244399    Top1 65.440000    Top5 89.380000    
2022-01-29 04:40:22,107 - ==> Top1: 65.440    Top5: 89.380    Loss: 1.244

2022-01-29 04:40:22,112 - ==> Best [Top1: 66.090   Top5: 89.890   Sparsity:0.00   Params: 1341960 on epoch: 207]
2022-01-29 04:40:22,112 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:40:22,153 - 

2022-01-29 04:40:22,153 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:40:30,010 - Epoch: [209][  100/  391]    Overall Loss 0.416427    Objective Loss 0.416427                                        LR 0.001298    Time 0.078542    
2022-01-29 04:40:37,555 - Epoch: [209][  200/  391]    Overall Loss 0.410400    Objective Loss 0.410400                                        LR 0.001298    Time 0.076990    
2022-01-29 04:40:45,047 - Epoch: [209][  300/  391]    Overall Loss 0.415071    Objective Loss 0.415071                                        LR 0.001298    Time 0.076299    
2022-01-29 04:40:51,858 - Epoch: [209][  391/  391]    Overall Loss 0.418631    Objective Loss 0.418631    Top1 91.346154    Top5 100.000000    LR 0.001298    Time 0.075959    
2022-01-29 04:40:51,917 - --- validate (epoch=209)-----------
2022-01-29 04:40:51,917 - 10000 samples (128 per mini-batch)
2022-01-29 04:40:55,459 - Epoch: [209][   79/   79]    Loss 1.224802    Top1 65.980000    Top5 89.400000    
2022-01-29 04:40:55,516 - ==> Top1: 65.980    Top5: 89.400    Loss: 1.225

2022-01-29 04:40:55,521 - ==> Best [Top1: 66.090   Top5: 89.890   Sparsity:0.00   Params: 1341960 on epoch: 207]
2022-01-29 04:40:55,521 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:40:55,562 - 

2022-01-29 04:40:55,563 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:41:03,370 - Epoch: [210][  100/  391]    Overall Loss 0.403826    Objective Loss 0.403826                                        LR 0.001298    Time 0.078051    
2022-01-29 04:41:10,964 - Epoch: [210][  200/  391]    Overall Loss 0.402022    Objective Loss 0.402022                                        LR 0.001298    Time 0.076990    
2022-01-29 04:41:18,569 - Epoch: [210][  300/  391]    Overall Loss 0.409001    Objective Loss 0.409001                                        LR 0.001298    Time 0.076675    
2022-01-29 04:41:25,486 - Epoch: [210][  391/  391]    Overall Loss 0.412168    Objective Loss 0.412168    Top1 92.307692    Top5 99.519231    LR 0.001298    Time 0.076518    
2022-01-29 04:41:25,551 - --- validate (epoch=210)-----------
2022-01-29 04:41:25,551 - 10000 samples (128 per mini-batch)
2022-01-29 04:41:29,156 - Epoch: [210][   79/   79]    Loss 1.254814    Top1 65.170000    Top5 89.540000    
2022-01-29 04:41:29,214 - ==> Top1: 65.170    Top5: 89.540    Loss: 1.255

2022-01-29 04:41:29,219 - ==> Best [Top1: 66.090   Top5: 89.890   Sparsity:0.00   Params: 1341960 on epoch: 207]
2022-01-29 04:41:29,219 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:41:29,261 - 

2022-01-29 04:41:29,261 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:41:37,174 - Epoch: [211][  100/  391]    Overall Loss 0.406787    Objective Loss 0.406787                                        LR 0.001298    Time 0.079102    
2022-01-29 04:41:44,769 - Epoch: [211][  200/  391]    Overall Loss 0.413068    Objective Loss 0.413068                                        LR 0.001298    Time 0.077521    
2022-01-29 04:41:52,366 - Epoch: [211][  300/  391]    Overall Loss 0.413079    Objective Loss 0.413079                                        LR 0.001298    Time 0.077001    
2022-01-29 04:41:59,254 - Epoch: [211][  391/  391]    Overall Loss 0.414102    Objective Loss 0.414102    Top1 89.903846    Top5 100.000000    LR 0.001298    Time 0.076696    
2022-01-29 04:41:59,313 - --- validate (epoch=211)-----------
2022-01-29 04:41:59,313 - 10000 samples (128 per mini-batch)
2022-01-29 04:42:02,953 - Epoch: [211][   79/   79]    Loss 1.239854    Top1 65.500000    Top5 89.680000    
2022-01-29 04:42:03,011 - ==> Top1: 65.500    Top5: 89.680    Loss: 1.240

2022-01-29 04:42:03,016 - ==> Best [Top1: 66.090   Top5: 89.890   Sparsity:0.00   Params: 1341960 on epoch: 207]
2022-01-29 04:42:03,016 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:42:03,053 - 

2022-01-29 04:42:03,053 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:42:10,972 - Epoch: [212][  100/  391]    Overall Loss 0.389097    Objective Loss 0.389097                                        LR 0.001298    Time 0.079160    
2022-01-29 04:42:18,544 - Epoch: [212][  200/  391]    Overall Loss 0.392755    Objective Loss 0.392755                                        LR 0.001298    Time 0.077437    
2022-01-29 04:42:26,133 - Epoch: [212][  300/  391]    Overall Loss 0.394547    Objective Loss 0.394547                                        LR 0.001298    Time 0.076919    
2022-01-29 04:42:33,027 - Epoch: [212][  391/  391]    Overall Loss 0.398771    Objective Loss 0.398771    Top1 90.384615    Top5 100.000000    LR 0.001298    Time 0.076646    
2022-01-29 04:42:33,086 - --- validate (epoch=212)-----------
2022-01-29 04:42:33,086 - 10000 samples (128 per mini-batch)
2022-01-29 04:42:36,747 - Epoch: [212][   79/   79]    Loss 1.270649    Top1 65.150000    Top5 89.270000    
2022-01-29 04:42:36,802 - ==> Top1: 65.150    Top5: 89.270    Loss: 1.271

2022-01-29 04:42:36,807 - ==> Best [Top1: 66.090   Top5: 89.890   Sparsity:0.00   Params: 1341960 on epoch: 207]
2022-01-29 04:42:36,808 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:42:36,845 - 

2022-01-29 04:42:36,845 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:42:44,669 - Epoch: [213][  100/  391]    Overall Loss 0.391879    Objective Loss 0.391879                                        LR 0.001298    Time 0.078210    
2022-01-29 04:42:52,163 - Epoch: [213][  200/  391]    Overall Loss 0.399573    Objective Loss 0.399573                                        LR 0.001298    Time 0.076573    
2022-01-29 04:42:59,669 - Epoch: [213][  300/  391]    Overall Loss 0.401302    Objective Loss 0.401302                                        LR 0.001298    Time 0.076065    
2022-01-29 04:43:06,492 - Epoch: [213][  391/  391]    Overall Loss 0.402862    Objective Loss 0.402862    Top1 90.384615    Top5 99.519231    LR 0.001298    Time 0.075810    
2022-01-29 04:43:06,549 - --- validate (epoch=213)-----------
2022-01-29 04:43:06,549 - 10000 samples (128 per mini-batch)
2022-01-29 04:43:10,161 - Epoch: [213][   79/   79]    Loss 1.222740    Top1 65.620000    Top5 89.820000    
2022-01-29 04:43:10,216 - ==> Top1: 65.620    Top5: 89.820    Loss: 1.223

2022-01-29 04:43:10,221 - ==> Best [Top1: 66.090   Top5: 89.890   Sparsity:0.00   Params: 1341960 on epoch: 207]
2022-01-29 04:43:10,221 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:43:10,259 - 

2022-01-29 04:43:10,259 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:43:17,977 - Epoch: [214][  100/  391]    Overall Loss 0.381794    Objective Loss 0.381794                                        LR 0.001298    Time 0.077151    
2022-01-29 04:43:25,577 - Epoch: [214][  200/  391]    Overall Loss 0.391987    Objective Loss 0.391987                                        LR 0.001298    Time 0.076574    
2022-01-29 04:43:33,136 - Epoch: [214][  300/  391]    Overall Loss 0.396037    Objective Loss 0.396037                                        LR 0.001298    Time 0.076244    
2022-01-29 04:43:39,948 - Epoch: [214][  391/  391]    Overall Loss 0.402516    Objective Loss 0.402516    Top1 87.980769    Top5 99.519231    LR 0.001298    Time 0.075918    
2022-01-29 04:43:40,007 - --- validate (epoch=214)-----------
2022-01-29 04:43:40,007 - 10000 samples (128 per mini-batch)
2022-01-29 04:43:43,537 - Epoch: [214][   79/   79]    Loss 1.235648    Top1 65.560000    Top5 89.590000    
2022-01-29 04:43:43,594 - ==> Top1: 65.560    Top5: 89.590    Loss: 1.236

2022-01-29 04:43:43,599 - ==> Best [Top1: 66.090   Top5: 89.890   Sparsity:0.00   Params: 1341960 on epoch: 207]
2022-01-29 04:43:43,599 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:43:43,635 - 

2022-01-29 04:43:43,635 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:43:51,378 - Epoch: [215][  100/  391]    Overall Loss 0.377588    Objective Loss 0.377588                                        LR 0.001298    Time 0.077395    
2022-01-29 04:43:58,827 - Epoch: [215][  200/  391]    Overall Loss 0.387739    Objective Loss 0.387739                                        LR 0.001298    Time 0.075940    
2022-01-29 04:44:06,312 - Epoch: [215][  300/  391]    Overall Loss 0.391127    Objective Loss 0.391127                                        LR 0.001298    Time 0.075572    
2022-01-29 04:44:13,124 - Epoch: [215][  391/  391]    Overall Loss 0.392690    Objective Loss 0.392690    Top1 88.461538    Top5 99.519231    LR 0.001298    Time 0.075403    
2022-01-29 04:44:13,182 - --- validate (epoch=215)-----------
2022-01-29 04:44:13,182 - 10000 samples (128 per mini-batch)
2022-01-29 04:44:16,732 - Epoch: [215][   79/   79]    Loss 1.236073    Top1 65.920000    Top5 89.630000    
2022-01-29 04:44:16,787 - ==> Top1: 65.920    Top5: 89.630    Loss: 1.236

2022-01-29 04:44:16,792 - ==> Best [Top1: 66.090   Top5: 89.890   Sparsity:0.00   Params: 1341960 on epoch: 207]
2022-01-29 04:44:16,792 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:44:16,829 - 

2022-01-29 04:44:16,830 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:44:24,641 - Epoch: [216][  100/  391]    Overall Loss 0.375010    Objective Loss 0.375010                                        LR 0.001298    Time 0.078084    
2022-01-29 04:44:32,237 - Epoch: [216][  200/  391]    Overall Loss 0.384751    Objective Loss 0.384751                                        LR 0.001298    Time 0.077018    
2022-01-29 04:44:39,871 - Epoch: [216][  300/  391]    Overall Loss 0.387239    Objective Loss 0.387239                                        LR 0.001298    Time 0.076789    
2022-01-29 04:44:46,822 - Epoch: [216][  391/  391]    Overall Loss 0.395284    Objective Loss 0.395284    Top1 91.826923    Top5 99.519231    LR 0.001298    Time 0.076693    
2022-01-29 04:44:46,879 - --- validate (epoch=216)-----------
2022-01-29 04:44:46,879 - 10000 samples (128 per mini-batch)
2022-01-29 04:44:50,528 - Epoch: [216][   79/   79]    Loss 1.254379    Top1 65.240000    Top5 89.770000    
2022-01-29 04:44:50,579 - ==> Top1: 65.240    Top5: 89.770    Loss: 1.254

2022-01-29 04:44:50,584 - ==> Best [Top1: 66.090   Top5: 89.890   Sparsity:0.00   Params: 1341960 on epoch: 207]
2022-01-29 04:44:50,584 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:44:50,621 - 

2022-01-29 04:44:50,621 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:44:58,435 - Epoch: [217][  100/  391]    Overall Loss 0.388293    Objective Loss 0.388293                                        LR 0.001298    Time 0.078114    
2022-01-29 04:45:05,931 - Epoch: [217][  200/  391]    Overall Loss 0.387479    Objective Loss 0.387479                                        LR 0.001298    Time 0.076532    
2022-01-29 04:45:13,425 - Epoch: [217][  300/  391]    Overall Loss 0.392899    Objective Loss 0.392899                                        LR 0.001298    Time 0.075998    
2022-01-29 04:45:20,233 - Epoch: [217][  391/  391]    Overall Loss 0.391272    Objective Loss 0.391272    Top1 94.711538    Top5 99.519231    LR 0.001298    Time 0.075718    
2022-01-29 04:45:20,288 - --- validate (epoch=217)-----------
2022-01-29 04:45:20,288 - 10000 samples (128 per mini-batch)
2022-01-29 04:45:23,840 - Epoch: [217][   79/   79]    Loss 1.271702    Top1 65.010000    Top5 89.120000    
2022-01-29 04:45:23,893 - ==> Top1: 65.010    Top5: 89.120    Loss: 1.272

2022-01-29 04:45:23,898 - ==> Best [Top1: 66.090   Top5: 89.890   Sparsity:0.00   Params: 1341960 on epoch: 207]
2022-01-29 04:45:23,898 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:45:23,934 - 

2022-01-29 04:45:23,935 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:45:31,746 - Epoch: [218][  100/  391]    Overall Loss 0.382248    Objective Loss 0.382248                                        LR 0.001298    Time 0.078081    
2022-01-29 04:45:39,246 - Epoch: [218][  200/  391]    Overall Loss 0.381122    Objective Loss 0.381122                                        LR 0.001298    Time 0.076539    
2022-01-29 04:45:46,812 - Epoch: [218][  300/  391]    Overall Loss 0.383295    Objective Loss 0.383295                                        LR 0.001298    Time 0.076244    
2022-01-29 04:45:53,747 - Epoch: [218][  391/  391]    Overall Loss 0.386147    Objective Loss 0.386147    Top1 93.269231    Top5 98.557692    LR 0.001298    Time 0.076232    
2022-01-29 04:45:53,806 - --- validate (epoch=218)-----------
2022-01-29 04:45:53,806 - 10000 samples (128 per mini-batch)
2022-01-29 04:45:57,456 - Epoch: [218][   79/   79]    Loss 1.258003    Top1 65.070000    Top5 89.070000    
2022-01-29 04:45:57,511 - ==> Top1: 65.070    Top5: 89.070    Loss: 1.258

2022-01-29 04:45:57,517 - ==> Best [Top1: 66.090   Top5: 89.890   Sparsity:0.00   Params: 1341960 on epoch: 207]
2022-01-29 04:45:57,517 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:45:57,555 - 

2022-01-29 04:45:57,555 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:46:05,498 - Epoch: [219][  100/  391]    Overall Loss 0.376176    Objective Loss 0.376176                                        LR 0.001298    Time 0.079404    
2022-01-29 04:46:13,112 - Epoch: [219][  200/  391]    Overall Loss 0.377846    Objective Loss 0.377846                                        LR 0.001298    Time 0.077768    
2022-01-29 04:46:20,722 - Epoch: [219][  300/  391]    Overall Loss 0.381194    Objective Loss 0.381194                                        LR 0.001298    Time 0.077210    
2022-01-29 04:46:27,558 - Epoch: [219][  391/  391]    Overall Loss 0.382614    Objective Loss 0.382614    Top1 90.865385    Top5 99.038462    LR 0.001298    Time 0.076720    
2022-01-29 04:46:27,614 - --- validate (epoch=219)-----------
2022-01-29 04:46:27,614 - 10000 samples (128 per mini-batch)
2022-01-29 04:46:31,197 - Epoch: [219][   79/   79]    Loss 1.231495    Top1 66.200000    Top5 89.520000    
2022-01-29 04:46:31,250 - ==> Top1: 66.200    Top5: 89.520    Loss: 1.231

2022-01-29 04:46:31,255 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:46:31,255 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:46:31,305 - 

2022-01-29 04:46:31,306 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:46:39,105 - Epoch: [220][  100/  391]    Overall Loss 0.384920    Objective Loss 0.384920                                        LR 0.001298    Time 0.077963    
2022-01-29 04:46:46,666 - Epoch: [220][  200/  391]    Overall Loss 0.380177    Objective Loss 0.380177                                        LR 0.001298    Time 0.076783    
2022-01-29 04:46:54,164 - Epoch: [220][  300/  391]    Overall Loss 0.379703    Objective Loss 0.379703                                        LR 0.001298    Time 0.076181    
2022-01-29 04:47:00,978 - Epoch: [220][  391/  391]    Overall Loss 0.382791    Objective Loss 0.382791    Top1 89.423077    Top5 100.000000    LR 0.001298    Time 0.075875    
2022-01-29 04:47:01,036 - --- validate (epoch=220)-----------
2022-01-29 04:47:01,036 - 10000 samples (128 per mini-batch)
2022-01-29 04:47:04,577 - Epoch: [220][   79/   79]    Loss 1.240764    Top1 65.540000    Top5 89.860000    
2022-01-29 04:47:04,627 - ==> Top1: 65.540    Top5: 89.860    Loss: 1.241

2022-01-29 04:47:04,632 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:47:04,632 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:47:04,673 - 

2022-01-29 04:47:04,673 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:47:12,474 - Epoch: [221][  100/  391]    Overall Loss 0.350795    Objective Loss 0.350795                                        LR 0.001298    Time 0.077985    
2022-01-29 04:47:20,083 - Epoch: [221][  200/  391]    Overall Loss 0.362398    Objective Loss 0.362398                                        LR 0.001298    Time 0.077030    
2022-01-29 04:47:27,583 - Epoch: [221][  300/  391]    Overall Loss 0.365089    Objective Loss 0.365089                                        LR 0.001298    Time 0.076350    
2022-01-29 04:47:34,362 - Epoch: [221][  391/  391]    Overall Loss 0.372071    Objective Loss 0.372071    Top1 90.384615    Top5 99.038462    LR 0.001298    Time 0.075918    
2022-01-29 04:47:34,418 - --- validate (epoch=221)-----------
2022-01-29 04:47:34,418 - 10000 samples (128 per mini-batch)
2022-01-29 04:47:37,971 - Epoch: [221][   79/   79]    Loss 1.265111    Top1 65.380000    Top5 89.770000    
2022-01-29 04:47:38,029 - ==> Top1: 65.380    Top5: 89.770    Loss: 1.265

2022-01-29 04:47:38,034 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:47:38,034 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:47:38,075 - 

2022-01-29 04:47:38,075 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:47:45,852 - Epoch: [222][  100/  391]    Overall Loss 0.364663    Objective Loss 0.364663                                        LR 0.001298    Time 0.077739    
2022-01-29 04:47:53,340 - Epoch: [222][  200/  391]    Overall Loss 0.365288    Objective Loss 0.365288                                        LR 0.001298    Time 0.076305    
2022-01-29 04:48:00,828 - Epoch: [222][  300/  391]    Overall Loss 0.368579    Objective Loss 0.368579                                        LR 0.001298    Time 0.075827    
2022-01-29 04:48:07,638 - Epoch: [222][  391/  391]    Overall Loss 0.374575    Objective Loss 0.374575    Top1 91.826923    Top5 99.519231    LR 0.001298    Time 0.075594    
2022-01-29 04:48:07,697 - --- validate (epoch=222)-----------
2022-01-29 04:48:07,697 - 10000 samples (128 per mini-batch)
2022-01-29 04:48:11,278 - Epoch: [222][   79/   79]    Loss 1.271168    Top1 65.230000    Top5 89.220000    
2022-01-29 04:48:11,331 - ==> Top1: 65.230    Top5: 89.220    Loss: 1.271

2022-01-29 04:48:11,336 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:48:11,336 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:48:11,374 - 

2022-01-29 04:48:11,374 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:48:19,210 - Epoch: [223][  100/  391]    Overall Loss 0.354310    Objective Loss 0.354310                                        LR 0.001298    Time 0.078332    
2022-01-29 04:48:26,830 - Epoch: [223][  200/  391]    Overall Loss 0.354789    Objective Loss 0.354789                                        LR 0.001298    Time 0.077262    
2022-01-29 04:48:34,422 - Epoch: [223][  300/  391]    Overall Loss 0.365625    Objective Loss 0.365625                                        LR 0.001298    Time 0.076812    
2022-01-29 04:48:41,232 - Epoch: [223][  391/  391]    Overall Loss 0.367682    Objective Loss 0.367682    Top1 92.307692    Top5 99.038462    LR 0.001298    Time 0.076351    
2022-01-29 04:48:41,290 - --- validate (epoch=223)-----------
2022-01-29 04:48:41,291 - 10000 samples (128 per mini-batch)
2022-01-29 04:48:44,882 - Epoch: [223][   79/   79]    Loss 1.251012    Top1 65.600000    Top5 89.600000    
2022-01-29 04:48:44,939 - ==> Top1: 65.600    Top5: 89.600    Loss: 1.251

2022-01-29 04:48:44,944 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:48:44,944 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:48:44,977 - 

2022-01-29 04:48:44,977 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:48:52,698 - Epoch: [224][  100/  391]    Overall Loss 0.350557    Objective Loss 0.350557                                        LR 0.001298    Time 0.077182    
2022-01-29 04:49:00,198 - Epoch: [224][  200/  391]    Overall Loss 0.355654    Objective Loss 0.355654                                        LR 0.001298    Time 0.076085    
2022-01-29 04:49:07,694 - Epoch: [224][  300/  391]    Overall Loss 0.361202    Objective Loss 0.361202                                        LR 0.001298    Time 0.075708    
2022-01-29 04:49:14,510 - Epoch: [224][  391/  391]    Overall Loss 0.365052    Objective Loss 0.365052    Top1 89.903846    Top5 99.038462    LR 0.001298    Time 0.075519    
2022-01-29 04:49:14,568 - --- validate (epoch=224)-----------
2022-01-29 04:49:14,569 - 10000 samples (128 per mini-batch)
2022-01-29 04:49:18,082 - Epoch: [224][   79/   79]    Loss 1.284469    Top1 64.800000    Top5 89.610000    
2022-01-29 04:49:18,135 - ==> Top1: 64.800    Top5: 89.610    Loss: 1.284

2022-01-29 04:49:18,140 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:49:18,140 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:49:18,181 - 

2022-01-29 04:49:18,181 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:49:26,057 - Epoch: [225][  100/  391]    Overall Loss 0.343444    Objective Loss 0.343444                                        LR 0.001298    Time 0.078727    
2022-01-29 04:49:33,693 - Epoch: [225][  200/  391]    Overall Loss 0.347527    Objective Loss 0.347527                                        LR 0.001298    Time 0.077540    
2022-01-29 04:49:41,287 - Epoch: [225][  300/  391]    Overall Loss 0.356218    Objective Loss 0.356218                                        LR 0.001298    Time 0.077003    
2022-01-29 04:49:48,100 - Epoch: [225][  391/  391]    Overall Loss 0.361536    Objective Loss 0.361536    Top1 91.346154    Top5 99.519231    LR 0.001298    Time 0.076506    
2022-01-29 04:49:48,159 - --- validate (epoch=225)-----------
2022-01-29 04:49:48,159 - 10000 samples (128 per mini-batch)
2022-01-29 04:49:51,680 - Epoch: [225][   79/   79]    Loss 1.251603    Top1 65.450000    Top5 89.710000    
2022-01-29 04:49:51,739 - ==> Top1: 65.450    Top5: 89.710    Loss: 1.252

2022-01-29 04:49:51,744 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:49:51,744 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:49:51,785 - 

2022-01-29 04:49:51,785 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:49:59,584 - Epoch: [226][  100/  391]    Overall Loss 0.350498    Objective Loss 0.350498                                        LR 0.001298    Time 0.077956    
2022-01-29 04:50:07,084 - Epoch: [226][  200/  391]    Overall Loss 0.353037    Objective Loss 0.353037                                        LR 0.001298    Time 0.076475    
2022-01-29 04:50:14,573 - Epoch: [226][  300/  391]    Overall Loss 0.355133    Objective Loss 0.355133                                        LR 0.001298    Time 0.075943    
2022-01-29 04:50:21,379 - Epoch: [226][  391/  391]    Overall Loss 0.360320    Objective Loss 0.360320    Top1 88.942308    Top5 100.000000    LR 0.001298    Time 0.075673    
2022-01-29 04:50:21,437 - --- validate (epoch=226)-----------
2022-01-29 04:50:21,438 - 10000 samples (128 per mini-batch)
2022-01-29 04:50:24,998 - Epoch: [226][   79/   79]    Loss 1.259169    Top1 65.440000    Top5 89.370000    
2022-01-29 04:50:25,049 - ==> Top1: 65.440    Top5: 89.370    Loss: 1.259

2022-01-29 04:50:25,054 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:50:25,054 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:50:25,090 - 

2022-01-29 04:50:25,090 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:50:32,795 - Epoch: [227][  100/  391]    Overall Loss 0.350155    Objective Loss 0.350155                                        LR 0.001298    Time 0.077019    
2022-01-29 04:50:40,303 - Epoch: [227][  200/  391]    Overall Loss 0.347918    Objective Loss 0.347918                                        LR 0.001298    Time 0.076046    
2022-01-29 04:50:47,808 - Epoch: [227][  300/  391]    Overall Loss 0.353301    Objective Loss 0.353301                                        LR 0.001298    Time 0.075711    
2022-01-29 04:50:54,629 - Epoch: [227][  391/  391]    Overall Loss 0.356005    Objective Loss 0.356005    Top1 90.865385    Top5 99.038462    LR 0.001298    Time 0.075534    
2022-01-29 04:50:54,686 - --- validate (epoch=227)-----------
2022-01-29 04:50:54,686 - 10000 samples (128 per mini-batch)
2022-01-29 04:50:58,256 - Epoch: [227][   79/   79]    Loss 1.281284    Top1 65.120000    Top5 89.310000    
2022-01-29 04:50:58,315 - ==> Top1: 65.120    Top5: 89.310    Loss: 1.281

2022-01-29 04:50:58,320 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:50:58,320 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:50:58,362 - 

2022-01-29 04:50:58,362 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:51:06,264 - Epoch: [228][  100/  391]    Overall Loss 0.360143    Objective Loss 0.360143                                        LR 0.001298    Time 0.078991    
2022-01-29 04:51:13,862 - Epoch: [228][  200/  391]    Overall Loss 0.348793    Objective Loss 0.348793                                        LR 0.001298    Time 0.077481    
2022-01-29 04:51:21,461 - Epoch: [228][  300/  391]    Overall Loss 0.353279    Objective Loss 0.353279                                        LR 0.001298    Time 0.076982    
2022-01-29 04:51:28,371 - Epoch: [228][  391/  391]    Overall Loss 0.353525    Objective Loss 0.353525    Top1 90.865385    Top5 99.038462    LR 0.001298    Time 0.076737    
2022-01-29 04:51:28,431 - --- validate (epoch=228)-----------
2022-01-29 04:51:28,431 - 10000 samples (128 per mini-batch)
2022-01-29 04:51:32,044 - Epoch: [228][   79/   79]    Loss 1.287513    Top1 65.380000    Top5 89.560000    
2022-01-29 04:51:32,103 - ==> Top1: 65.380    Top5: 89.560    Loss: 1.288

2022-01-29 04:51:32,108 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:51:32,108 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:51:32,145 - 

2022-01-29 04:51:32,145 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:51:40,089 - Epoch: [229][  100/  391]    Overall Loss 0.347094    Objective Loss 0.347094                                        LR 0.001298    Time 0.079414    
2022-01-29 04:51:47,733 - Epoch: [229][  200/  391]    Overall Loss 0.347462    Objective Loss 0.347462                                        LR 0.001298    Time 0.077920    
2022-01-29 04:51:55,337 - Epoch: [229][  300/  391]    Overall Loss 0.351588    Objective Loss 0.351588                                        LR 0.001298    Time 0.077290    
2022-01-29 04:52:02,156 - Epoch: [229][  391/  391]    Overall Loss 0.355736    Objective Loss 0.355736    Top1 94.230769    Top5 100.000000    LR 0.001298    Time 0.076741    
2022-01-29 04:52:02,216 - --- validate (epoch=229)-----------
2022-01-29 04:52:02,216 - 10000 samples (128 per mini-batch)
2022-01-29 04:52:05,749 - Epoch: [229][   79/   79]    Loss 1.265833    Top1 65.820000    Top5 89.310000    
2022-01-29 04:52:05,806 - ==> Top1: 65.820    Top5: 89.310    Loss: 1.266

2022-01-29 04:52:05,812 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:52:05,812 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:52:05,853 - 

2022-01-29 04:52:05,853 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:52:13,548 - Epoch: [230][  100/  391]    Overall Loss 0.327647    Objective Loss 0.327647                                        LR 0.001298    Time 0.076927    
2022-01-29 04:52:21,020 - Epoch: [230][  200/  391]    Overall Loss 0.342917    Objective Loss 0.342917                                        LR 0.001298    Time 0.075817    
2022-01-29 04:52:28,511 - Epoch: [230][  300/  391]    Overall Loss 0.346688    Objective Loss 0.346688                                        LR 0.001298    Time 0.075513    
2022-01-29 04:52:35,327 - Epoch: [230][  391/  391]    Overall Loss 0.349736    Objective Loss 0.349736    Top1 92.788462    Top5 99.038462    LR 0.001298    Time 0.075367    
2022-01-29 04:52:35,386 - --- validate (epoch=230)-----------
2022-01-29 04:52:35,386 - 10000 samples (128 per mini-batch)
2022-01-29 04:52:38,975 - Epoch: [230][   79/   79]    Loss 1.282815    Top1 65.500000    Top5 89.370000    
2022-01-29 04:52:39,030 - ==> Top1: 65.500    Top5: 89.370    Loss: 1.283

2022-01-29 04:52:39,035 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:52:39,035 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:52:39,077 - 

2022-01-29 04:52:39,077 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:52:46,932 - Epoch: [231][  100/  391]    Overall Loss 0.330655    Objective Loss 0.330655                                        LR 0.001298    Time 0.078519    
2022-01-29 04:52:54,403 - Epoch: [231][  200/  391]    Overall Loss 0.345521    Objective Loss 0.345521                                        LR 0.001298    Time 0.076608    
2022-01-29 04:53:01,945 - Epoch: [231][  300/  391]    Overall Loss 0.347738    Objective Loss 0.347738                                        LR 0.001298    Time 0.076209    
2022-01-29 04:53:08,873 - Epoch: [231][  391/  391]    Overall Loss 0.351416    Objective Loss 0.351416    Top1 91.826923    Top5 100.000000    LR 0.001298    Time 0.076190    
2022-01-29 04:53:08,932 - --- validate (epoch=231)-----------
2022-01-29 04:53:08,932 - 10000 samples (128 per mini-batch)
2022-01-29 04:53:12,540 - Epoch: [231][   79/   79]    Loss 1.294630    Top1 65.320000    Top5 89.170000    
2022-01-29 04:53:12,598 - ==> Top1: 65.320    Top5: 89.170    Loss: 1.295

2022-01-29 04:53:12,603 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:53:12,603 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:53:12,644 - 

2022-01-29 04:53:12,644 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:53:20,516 - Epoch: [232][  100/  391]    Overall Loss 0.347687    Objective Loss 0.347687                                        LR 0.001298    Time 0.078695    
2022-01-29 04:53:28,158 - Epoch: [232][  200/  391]    Overall Loss 0.351329    Objective Loss 0.351329                                        LR 0.001298    Time 0.077553    
2022-01-29 04:53:35,786 - Epoch: [232][  300/  391]    Overall Loss 0.353595    Objective Loss 0.353595                                        LR 0.001298    Time 0.077125    
2022-01-29 04:53:42,719 - Epoch: [232][  391/  391]    Overall Loss 0.354537    Objective Loss 0.354537    Top1 94.711538    Top5 99.519231    LR 0.001298    Time 0.076906    
2022-01-29 04:53:42,774 - --- validate (epoch=232)-----------
2022-01-29 04:53:42,774 - 10000 samples (128 per mini-batch)
2022-01-29 04:53:46,413 - Epoch: [232][   79/   79]    Loss 1.258247    Top1 65.860000    Top5 89.720000    
2022-01-29 04:53:46,469 - ==> Top1: 65.860    Top5: 89.720    Loss: 1.258

2022-01-29 04:53:46,474 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:53:46,474 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:53:46,516 - 

2022-01-29 04:53:46,517 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:53:54,355 - Epoch: [233][  100/  391]    Overall Loss 0.340849    Objective Loss 0.340849                                        LR 0.001298    Time 0.078354    
2022-01-29 04:54:01,951 - Epoch: [233][  200/  391]    Overall Loss 0.335737    Objective Loss 0.335737                                        LR 0.001298    Time 0.077156    
2022-01-29 04:54:09,555 - Epoch: [233][  300/  391]    Overall Loss 0.335444    Objective Loss 0.335444                                        LR 0.001298    Time 0.076779    
2022-01-29 04:54:16,427 - Epoch: [233][  391/  391]    Overall Loss 0.341001    Objective Loss 0.341001    Top1 91.826923    Top5 100.000000    LR 0.001298    Time 0.076485    
2022-01-29 04:54:16,484 - --- validate (epoch=233)-----------
2022-01-29 04:54:16,484 - 10000 samples (128 per mini-batch)
2022-01-29 04:54:20,075 - Epoch: [233][   79/   79]    Loss 1.271756    Top1 65.470000    Top5 89.640000    
2022-01-29 04:54:20,131 - ==> Top1: 65.470    Top5: 89.640    Loss: 1.272

2022-01-29 04:54:20,136 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:54:20,137 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:54:20,179 - 

2022-01-29 04:54:20,179 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:54:28,151 - Epoch: [234][  100/  391]    Overall Loss 0.341549    Objective Loss 0.341549                                        LR 0.001298    Time 0.079694    
2022-01-29 04:54:35,833 - Epoch: [234][  200/  391]    Overall Loss 0.350788    Objective Loss 0.350788                                        LR 0.001298    Time 0.078250    
2022-01-29 04:54:43,517 - Epoch: [234][  300/  391]    Overall Loss 0.349411    Objective Loss 0.349411                                        LR 0.001298    Time 0.077780    
2022-01-29 04:54:50,506 - Epoch: [234][  391/  391]    Overall Loss 0.351227    Objective Loss 0.351227    Top1 91.826923    Top5 100.000000    LR 0.001298    Time 0.077550    
2022-01-29 04:54:50,566 - --- validate (epoch=234)-----------
2022-01-29 04:54:50,567 - 10000 samples (128 per mini-batch)
2022-01-29 04:54:54,178 - Epoch: [234][   79/   79]    Loss 1.282961    Top1 65.770000    Top5 89.150000    
2022-01-29 04:54:54,232 - ==> Top1: 65.770    Top5: 89.150    Loss: 1.283

2022-01-29 04:54:54,237 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:54:54,237 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:54:54,276 - 

2022-01-29 04:54:54,276 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:55:02,214 - Epoch: [235][  100/  391]    Overall Loss 0.318814    Objective Loss 0.318814                                        LR 0.001298    Time 0.079351    
2022-01-29 04:55:09,797 - Epoch: [235][  200/  391]    Overall Loss 0.330288    Objective Loss 0.330288                                        LR 0.001298    Time 0.077588    
2022-01-29 04:55:17,366 - Epoch: [235][  300/  391]    Overall Loss 0.335815    Objective Loss 0.335815                                        LR 0.001298    Time 0.076952    
2022-01-29 04:55:24,245 - Epoch: [235][  391/  391]    Overall Loss 0.339932    Objective Loss 0.339932    Top1 91.826923    Top5 99.519231    LR 0.001298    Time 0.076635    
2022-01-29 04:55:24,304 - --- validate (epoch=235)-----------
2022-01-29 04:55:24,304 - 10000 samples (128 per mini-batch)
2022-01-29 04:55:27,887 - Epoch: [235][   79/   79]    Loss 1.288692    Top1 65.600000    Top5 89.420000    
2022-01-29 04:55:27,946 - ==> Top1: 65.600    Top5: 89.420    Loss: 1.289

2022-01-29 04:55:27,951 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:55:27,951 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:55:27,987 - 

2022-01-29 04:55:27,988 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:55:35,790 - Epoch: [236][  100/  391]    Overall Loss 0.344591    Objective Loss 0.344591                                        LR 0.001298    Time 0.077998    
2022-01-29 04:55:43,370 - Epoch: [236][  200/  391]    Overall Loss 0.344917    Objective Loss 0.344917                                        LR 0.001298    Time 0.076896    
2022-01-29 04:55:50,947 - Epoch: [236][  300/  391]    Overall Loss 0.345442    Objective Loss 0.345442                                        LR 0.001298    Time 0.076515    
2022-01-29 04:55:57,841 - Epoch: [236][  391/  391]    Overall Loss 0.345253    Objective Loss 0.345253    Top1 91.346154    Top5 100.000000    LR 0.001298    Time 0.076337    
2022-01-29 04:55:57,899 - --- validate (epoch=236)-----------
2022-01-29 04:55:57,900 - 10000 samples (128 per mini-batch)
2022-01-29 04:56:01,450 - Epoch: [236][   79/   79]    Loss 1.265670    Top1 65.480000    Top5 89.390000    
2022-01-29 04:56:01,502 - ==> Top1: 65.480    Top5: 89.390    Loss: 1.266

2022-01-29 04:56:01,507 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:56:01,507 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:56:01,543 - 

2022-01-29 04:56:01,543 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:56:09,388 - Epoch: [237][  100/  391]    Overall Loss 0.328810    Objective Loss 0.328810                                        LR 0.001298    Time 0.078425    
2022-01-29 04:56:16,965 - Epoch: [237][  200/  391]    Overall Loss 0.336025    Objective Loss 0.336025                                        LR 0.001298    Time 0.077089    
2022-01-29 04:56:24,599 - Epoch: [237][  300/  391]    Overall Loss 0.341024    Objective Loss 0.341024                                        LR 0.001298    Time 0.076838    
2022-01-29 04:56:31,544 - Epoch: [237][  391/  391]    Overall Loss 0.345792    Objective Loss 0.345792    Top1 93.750000    Top5 99.519231    LR 0.001298    Time 0.076715    
2022-01-29 04:56:31,603 - --- validate (epoch=237)-----------
2022-01-29 04:56:31,603 - 10000 samples (128 per mini-batch)
2022-01-29 04:56:35,218 - Epoch: [237][   79/   79]    Loss 1.294928    Top1 65.260000    Top5 89.140000    
2022-01-29 04:56:35,270 - ==> Top1: 65.260    Top5: 89.140    Loss: 1.295

2022-01-29 04:56:35,276 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:56:35,276 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:56:35,313 - 

2022-01-29 04:56:35,313 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:56:43,318 - Epoch: [238][  100/  391]    Overall Loss 0.328621    Objective Loss 0.328621                                        LR 0.001298    Time 0.080024    
2022-01-29 04:56:50,932 - Epoch: [238][  200/  391]    Overall Loss 0.330896    Objective Loss 0.330896                                        LR 0.001298    Time 0.078075    
2022-01-29 04:56:58,562 - Epoch: [238][  300/  391]    Overall Loss 0.334632    Objective Loss 0.334632                                        LR 0.001298    Time 0.077480    
2022-01-29 04:57:05,502 - Epoch: [238][  391/  391]    Overall Loss 0.338901    Objective Loss 0.338901    Top1 90.384615    Top5 100.000000    LR 0.001298    Time 0.077194    
2022-01-29 04:57:05,552 - --- validate (epoch=238)-----------
2022-01-29 04:57:05,552 - 10000 samples (128 per mini-batch)
2022-01-29 04:57:09,175 - Epoch: [238][   79/   79]    Loss 1.314623    Top1 64.590000    Top5 88.970000    
2022-01-29 04:57:09,231 - ==> Top1: 64.590    Top5: 88.970    Loss: 1.315

2022-01-29 04:57:09,236 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:57:09,236 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:57:09,278 - 

2022-01-29 04:57:09,278 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:57:17,077 - Epoch: [239][  100/  391]    Overall Loss 0.330515    Objective Loss 0.330515                                        LR 0.001298    Time 0.077964    
2022-01-29 04:57:24,638 - Epoch: [239][  200/  391]    Overall Loss 0.334309    Objective Loss 0.334309                                        LR 0.001298    Time 0.076783    
2022-01-29 04:57:32,196 - Epoch: [239][  300/  391]    Overall Loss 0.336919    Objective Loss 0.336919                                        LR 0.001298    Time 0.076378    
2022-01-29 04:57:39,107 - Epoch: [239][  391/  391]    Overall Loss 0.337817    Objective Loss 0.337817    Top1 87.019231    Top5 99.038462    LR 0.001298    Time 0.076276    
2022-01-29 04:57:39,163 - --- validate (epoch=239)-----------
2022-01-29 04:57:39,163 - 10000 samples (128 per mini-batch)
2022-01-29 04:57:42,702 - Epoch: [239][   79/   79]    Loss 1.318493    Top1 65.010000    Top5 89.390000    
2022-01-29 04:57:42,761 - ==> Top1: 65.010    Top5: 89.390    Loss: 1.318

2022-01-29 04:57:42,766 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:57:42,766 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:57:42,807 - 

2022-01-29 04:57:42,808 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:57:50,679 - Epoch: [240][  100/  391]    Overall Loss 0.326464    Objective Loss 0.326464                                        LR 0.001298    Time 0.078688    
2022-01-29 04:57:58,197 - Epoch: [240][  200/  391]    Overall Loss 0.328965    Objective Loss 0.328965                                        LR 0.001298    Time 0.076928    
2022-01-29 04:58:05,677 - Epoch: [240][  300/  391]    Overall Loss 0.331947    Objective Loss 0.331947                                        LR 0.001298    Time 0.076218    
2022-01-29 04:58:12,476 - Epoch: [240][  391/  391]    Overall Loss 0.333221    Objective Loss 0.333221    Top1 91.826923    Top5 99.519231    LR 0.001298    Time 0.075865    
2022-01-29 04:58:12,542 - --- validate (epoch=240)-----------
2022-01-29 04:58:12,542 - 10000 samples (128 per mini-batch)
2022-01-29 04:58:16,097 - Epoch: [240][   79/   79]    Loss 1.300326    Top1 65.120000    Top5 89.040000    
2022-01-29 04:58:16,153 - ==> Top1: 65.120    Top5: 89.040    Loss: 1.300

2022-01-29 04:58:16,159 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:58:16,159 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:58:16,200 - 

2022-01-29 04:58:16,200 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:58:23,989 - Epoch: [241][  100/  391]    Overall Loss 0.330930    Objective Loss 0.330930                                        LR 0.001298    Time 0.077863    
2022-01-29 04:58:31,584 - Epoch: [241][  200/  391]    Overall Loss 0.337362    Objective Loss 0.337362                                        LR 0.001298    Time 0.076902    
2022-01-29 04:58:39,115 - Epoch: [241][  300/  391]    Overall Loss 0.340132    Objective Loss 0.340132                                        LR 0.001298    Time 0.076370    
2022-01-29 04:58:45,953 - Epoch: [241][  391/  391]    Overall Loss 0.338205    Objective Loss 0.338205    Top1 93.750000    Top5 99.038462    LR 0.001298    Time 0.076082    
2022-01-29 04:58:46,011 - --- validate (epoch=241)-----------
2022-01-29 04:58:46,011 - 10000 samples (128 per mini-batch)
2022-01-29 04:58:49,662 - Epoch: [241][   79/   79]    Loss 1.275283    Top1 65.840000    Top5 89.270000    
2022-01-29 04:58:49,719 - ==> Top1: 65.840    Top5: 89.270    Loss: 1.275

2022-01-29 04:58:49,724 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:58:49,724 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:58:49,765 - 

2022-01-29 04:58:49,765 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:58:57,580 - Epoch: [242][  100/  391]    Overall Loss 0.312315    Objective Loss 0.312315                                        LR 0.001298    Time 0.078120    
2022-01-29 04:59:05,217 - Epoch: [242][  200/  391]    Overall Loss 0.323341    Objective Loss 0.323341                                        LR 0.001298    Time 0.077243    
2022-01-29 04:59:12,856 - Epoch: [242][  300/  391]    Overall Loss 0.329531    Objective Loss 0.329531                                        LR 0.001298    Time 0.076955    
2022-01-29 04:59:19,796 - Epoch: [242][  391/  391]    Overall Loss 0.336907    Objective Loss 0.336907    Top1 92.788462    Top5 99.519231    LR 0.001298    Time 0.076793    
2022-01-29 04:59:19,855 - --- validate (epoch=242)-----------
2022-01-29 04:59:19,855 - 10000 samples (128 per mini-batch)
2022-01-29 04:59:23,513 - Epoch: [242][   79/   79]    Loss 1.301435    Top1 65.040000    Top5 89.330000    
2022-01-29 04:59:23,566 - ==> Top1: 65.040    Top5: 89.330    Loss: 1.301

2022-01-29 04:59:23,571 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:59:23,571 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:59:23,607 - 

2022-01-29 04:59:23,607 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 04:59:31,521 - Epoch: [243][  100/  391]    Overall Loss 0.320219    Objective Loss 0.320219                                        LR 0.001298    Time 0.079121    
2022-01-29 04:59:39,123 - Epoch: [243][  200/  391]    Overall Loss 0.328310    Objective Loss 0.328310                                        LR 0.001298    Time 0.077563    
2022-01-29 04:59:46,733 - Epoch: [243][  300/  391]    Overall Loss 0.331591    Objective Loss 0.331591                                        LR 0.001298    Time 0.077073    
2022-01-29 04:59:53,611 - Epoch: [243][  391/  391]    Overall Loss 0.335645    Objective Loss 0.335645    Top1 91.826923    Top5 100.000000    LR 0.001298    Time 0.076724    
2022-01-29 04:59:53,666 - --- validate (epoch=243)-----------
2022-01-29 04:59:53,667 - 10000 samples (128 per mini-batch)
2022-01-29 04:59:57,273 - Epoch: [243][   79/   79]    Loss 1.292350    Top1 65.220000    Top5 89.510000    
2022-01-29 04:59:57,326 - ==> Top1: 65.220    Top5: 89.510    Loss: 1.292

2022-01-29 04:59:57,331 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 04:59:57,331 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 04:59:57,367 - 

2022-01-29 04:59:57,367 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:00:05,180 - Epoch: [244][  100/  391]    Overall Loss 0.327268    Objective Loss 0.327268                                        LR 0.001298    Time 0.078109    
2022-01-29 05:00:12,691 - Epoch: [244][  200/  391]    Overall Loss 0.330054    Objective Loss 0.330054                                        LR 0.001298    Time 0.076603    
2022-01-29 05:00:20,199 - Epoch: [244][  300/  391]    Overall Loss 0.324659    Objective Loss 0.324659                                        LR 0.001298    Time 0.076093    
2022-01-29 05:00:27,025 - Epoch: [244][  391/  391]    Overall Loss 0.325730    Objective Loss 0.325730    Top1 91.826923    Top5 98.557692    LR 0.001298    Time 0.075839    
2022-01-29 05:00:27,084 - --- validate (epoch=244)-----------
2022-01-29 05:00:27,084 - 10000 samples (128 per mini-batch)
2022-01-29 05:00:30,677 - Epoch: [244][   79/   79]    Loss 1.291287    Top1 65.250000    Top5 89.600000    
2022-01-29 05:00:30,728 - ==> Top1: 65.250    Top5: 89.600    Loss: 1.291

2022-01-29 05:00:30,733 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 05:00:30,733 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:00:30,770 - 

2022-01-29 05:00:30,770 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:00:38,606 - Epoch: [245][  100/  391]    Overall Loss 0.318656    Objective Loss 0.318656                                        LR 0.001298    Time 0.078332    
2022-01-29 05:00:46,128 - Epoch: [245][  200/  391]    Overall Loss 0.322261    Objective Loss 0.322261                                        LR 0.001298    Time 0.076770    
2022-01-29 05:00:53,615 - Epoch: [245][  300/  391]    Overall Loss 0.323319    Objective Loss 0.323319                                        LR 0.001298    Time 0.076131    
2022-01-29 05:01:00,418 - Epoch: [245][  391/  391]    Overall Loss 0.324999    Objective Loss 0.324999    Top1 92.788462    Top5 100.000000    LR 0.001298    Time 0.075810    
2022-01-29 05:01:00,475 - --- validate (epoch=245)-----------
2022-01-29 05:01:00,476 - 10000 samples (128 per mini-batch)
2022-01-29 05:01:04,056 - Epoch: [245][   79/   79]    Loss 1.276068    Top1 65.220000    Top5 89.870000    
2022-01-29 05:01:04,113 - ==> Top1: 65.220    Top5: 89.870    Loss: 1.276

2022-01-29 05:01:04,118 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 05:01:04,118 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:01:04,155 - 

2022-01-29 05:01:04,155 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:01:12,094 - Epoch: [246][  100/  391]    Overall Loss 0.299667    Objective Loss 0.299667                                        LR 0.001298    Time 0.079363    
2022-01-29 05:01:19,717 - Epoch: [246][  200/  391]    Overall Loss 0.310042    Objective Loss 0.310042                                        LR 0.001298    Time 0.077794    
2022-01-29 05:01:27,331 - Epoch: [246][  300/  391]    Overall Loss 0.319522    Objective Loss 0.319522                                        LR 0.001298    Time 0.077238    
2022-01-29 05:01:34,220 - Epoch: [246][  391/  391]    Overall Loss 0.325241    Objective Loss 0.325241    Top1 93.269231    Top5 100.000000    LR 0.001298    Time 0.076879    
2022-01-29 05:01:34,278 - --- validate (epoch=246)-----------
2022-01-29 05:01:34,278 - 10000 samples (128 per mini-batch)
2022-01-29 05:01:37,899 - Epoch: [246][   79/   79]    Loss 1.295396    Top1 65.140000    Top5 89.020000    
2022-01-29 05:01:37,957 - ==> Top1: 65.140    Top5: 89.020    Loss: 1.295

2022-01-29 05:01:37,962 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 05:01:37,962 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:01:37,996 - 

2022-01-29 05:01:37,996 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:01:45,797 - Epoch: [247][  100/  391]    Overall Loss 0.300430    Objective Loss 0.300430                                        LR 0.001298    Time 0.077976    
2022-01-29 05:01:53,287 - Epoch: [247][  200/  391]    Overall Loss 0.313982    Objective Loss 0.313982                                        LR 0.001298    Time 0.076433    
2022-01-29 05:02:00,783 - Epoch: [247][  300/  391]    Overall Loss 0.318171    Objective Loss 0.318171                                        LR 0.001298    Time 0.075935    
2022-01-29 05:02:07,685 - Epoch: [247][  391/  391]    Overall Loss 0.322204    Objective Loss 0.322204    Top1 94.230769    Top5 100.000000    LR 0.001298    Time 0.075911    
2022-01-29 05:02:07,743 - --- validate (epoch=247)-----------
2022-01-29 05:02:07,744 - 10000 samples (128 per mini-batch)
2022-01-29 05:02:11,347 - Epoch: [247][   79/   79]    Loss 1.324783    Top1 64.380000    Top5 89.060000    
2022-01-29 05:02:11,405 - ==> Top1: 64.380    Top5: 89.060    Loss: 1.325

2022-01-29 05:02:11,410 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 05:02:11,410 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:02:11,452 - 

2022-01-29 05:02:11,452 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:02:19,295 - Epoch: [248][  100/  391]    Overall Loss 0.296178    Objective Loss 0.296178                                        LR 0.001298    Time 0.078396    
2022-01-29 05:02:26,920 - Epoch: [248][  200/  391]    Overall Loss 0.310689    Objective Loss 0.310689                                        LR 0.001298    Time 0.077321    
2022-01-29 05:02:34,520 - Epoch: [248][  300/  391]    Overall Loss 0.317466    Objective Loss 0.317466                                        LR 0.001298    Time 0.076876    
2022-01-29 05:02:41,442 - Epoch: [248][  391/  391]    Overall Loss 0.322231    Objective Loss 0.322231    Top1 88.942308    Top5 100.000000    LR 0.001298    Time 0.076686    
2022-01-29 05:02:41,497 - --- validate (epoch=248)-----------
2022-01-29 05:02:41,497 - 10000 samples (128 per mini-batch)
2022-01-29 05:02:45,071 - Epoch: [248][   79/   79]    Loss 1.328392    Top1 64.330000    Top5 89.060000    
2022-01-29 05:02:45,128 - ==> Top1: 64.330    Top5: 89.060    Loss: 1.328

2022-01-29 05:02:45,133 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 05:02:45,133 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:02:45,174 - 

2022-01-29 05:02:45,174 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:02:53,090 - Epoch: [249][  100/  391]    Overall Loss 0.322577    Objective Loss 0.322577                                        LR 0.001298    Time 0.079141    
2022-01-29 05:03:00,716 - Epoch: [249][  200/  391]    Overall Loss 0.319987    Objective Loss 0.319987                                        LR 0.001298    Time 0.077693    
2022-01-29 05:03:08,336 - Epoch: [249][  300/  391]    Overall Loss 0.317930    Objective Loss 0.317930                                        LR 0.001298    Time 0.077193    
2022-01-29 05:03:15,270 - Epoch: [249][  391/  391]    Overall Loss 0.322378    Objective Loss 0.322378    Top1 95.673077    Top5 100.000000    LR 0.001298    Time 0.076960    
2022-01-29 05:03:15,322 - --- validate (epoch=249)-----------
2022-01-29 05:03:15,322 - 10000 samples (128 per mini-batch)
2022-01-29 05:03:18,957 - Epoch: [249][   79/   79]    Loss 1.300765    Top1 65.210000    Top5 89.480000    
2022-01-29 05:03:19,010 - ==> Top1: 65.210    Top5: 89.480    Loss: 1.301

2022-01-29 05:03:19,016 - ==> Best [Top1: 66.200   Top5: 89.520   Sparsity:0.00   Params: 1341960 on epoch: 219]
2022-01-29 05:03:19,016 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:03:19,058 - 

2022-01-29 05:03:19,058 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:03:26,955 - Epoch: [250][  100/  391]    Overall Loss 0.251066    Objective Loss 0.251066                                        LR 0.000305    Time 0.078940    
2022-01-29 05:03:34,507 - Epoch: [250][  200/  391]    Overall Loss 0.247342    Objective Loss 0.247342                                        LR 0.000305    Time 0.077229    
2022-01-29 05:03:42,053 - Epoch: [250][  300/  391]    Overall Loss 0.246481    Objective Loss 0.246481                                        LR 0.000305    Time 0.076638    
2022-01-29 05:03:48,923 - Epoch: [250][  391/  391]    Overall Loss 0.244635    Objective Loss 0.244635    Top1 94.711538    Top5 100.000000    LR 0.000305    Time 0.076367    
2022-01-29 05:03:48,982 - --- validate (epoch=250)-----------
2022-01-29 05:03:48,982 - 10000 samples (128 per mini-batch)
2022-01-29 05:03:52,555 - Epoch: [250][   79/   79]    Loss 1.234250    Top1 66.870000    Top5 89.900000    
2022-01-29 05:03:52,610 - ==> Top1: 66.870    Top5: 89.900    Loss: 1.234

2022-01-29 05:03:52,615 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:03:52,615 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:03:52,666 - 

2022-01-29 05:03:52,666 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:04:00,526 - Epoch: [251][  100/  391]    Overall Loss 0.235774    Objective Loss 0.235774                                        LR 0.000305    Time 0.078562    
2022-01-29 05:04:08,125 - Epoch: [251][  200/  391]    Overall Loss 0.234989    Objective Loss 0.234989                                        LR 0.000305    Time 0.077275    
2022-01-29 05:04:15,692 - Epoch: [251][  300/  391]    Overall Loss 0.234818    Objective Loss 0.234818                                        LR 0.000305    Time 0.076737    
2022-01-29 05:04:22,569 - Epoch: [251][  391/  391]    Overall Loss 0.234586    Objective Loss 0.234586    Top1 97.115385    Top5 100.000000    LR 0.000305    Time 0.076462    
2022-01-29 05:04:22,625 - --- validate (epoch=251)-----------
2022-01-29 05:04:22,625 - 10000 samples (128 per mini-batch)
2022-01-29 05:04:26,151 - Epoch: [251][   79/   79]    Loss 1.250663    Top1 66.330000    Top5 89.900000    
2022-01-29 05:04:26,204 - ==> Top1: 66.330    Top5: 89.900    Loss: 1.251

2022-01-29 05:04:26,208 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:04:26,208 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:04:26,249 - 

2022-01-29 05:04:26,249 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:04:34,111 - Epoch: [252][  100/  391]    Overall Loss 0.223949    Objective Loss 0.223949                                        LR 0.000305    Time 0.078592    
2022-01-29 05:04:41,673 - Epoch: [252][  200/  391]    Overall Loss 0.228364    Objective Loss 0.228364                                        LR 0.000305    Time 0.077100    
2022-01-29 05:04:49,239 - Epoch: [252][  300/  391]    Overall Loss 0.230902    Objective Loss 0.230902                                        LR 0.000305    Time 0.076619    
2022-01-29 05:04:56,116 - Epoch: [252][  391/  391]    Overall Loss 0.230948    Objective Loss 0.230948    Top1 96.634615    Top5 100.000000    LR 0.000305    Time 0.076374    
2022-01-29 05:04:56,174 - --- validate (epoch=252)-----------
2022-01-29 05:04:56,174 - 10000 samples (128 per mini-batch)
2022-01-29 05:04:59,801 - Epoch: [252][   79/   79]    Loss 1.254526    Top1 66.380000    Top5 90.020000    
2022-01-29 05:04:59,860 - ==> Top1: 66.380    Top5: 90.020    Loss: 1.255

2022-01-29 05:04:59,865 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:04:59,865 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:04:59,907 - 

2022-01-29 05:04:59,907 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:05:07,839 - Epoch: [253][  100/  391]    Overall Loss 0.222719    Objective Loss 0.222719                                        LR 0.000305    Time 0.079289    
2022-01-29 05:05:15,471 - Epoch: [253][  200/  391]    Overall Loss 0.226550    Objective Loss 0.226550                                        LR 0.000305    Time 0.077800    
2022-01-29 05:05:23,103 - Epoch: [253][  300/  391]    Overall Loss 0.226277    Objective Loss 0.226277                                        LR 0.000305    Time 0.077302    
2022-01-29 05:05:30,044 - Epoch: [253][  391/  391]    Overall Loss 0.226973    Objective Loss 0.226973    Top1 92.788462    Top5 99.519231    LR 0.000305    Time 0.077062    
2022-01-29 05:05:30,103 - --- validate (epoch=253)-----------
2022-01-29 05:05:30,103 - 10000 samples (128 per mini-batch)
2022-01-29 05:05:33,891 - Epoch: [253][   79/   79]    Loss 1.261085    Top1 66.520000    Top5 89.650000    
2022-01-29 05:05:33,946 - ==> Top1: 66.520    Top5: 89.650    Loss: 1.261

2022-01-29 05:05:33,952 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:05:33,952 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:05:33,994 - 

2022-01-29 05:05:33,994 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:05:41,845 - Epoch: [254][  100/  391]    Overall Loss 0.221848    Objective Loss 0.221848                                        LR 0.000305    Time 0.078479    
2022-01-29 05:05:49,415 - Epoch: [254][  200/  391]    Overall Loss 0.223075    Objective Loss 0.223075                                        LR 0.000305    Time 0.077086    
2022-01-29 05:05:56,926 - Epoch: [254][  300/  391]    Overall Loss 0.224686    Objective Loss 0.224686                                        LR 0.000305    Time 0.076426    
2022-01-29 05:06:03,739 - Epoch: [254][  391/  391]    Overall Loss 0.224175    Objective Loss 0.224175    Top1 97.596154    Top5 100.000000    LR 0.000305    Time 0.076061    
2022-01-29 05:06:03,797 - --- validate (epoch=254)-----------
2022-01-29 05:06:03,798 - 10000 samples (128 per mini-batch)
2022-01-29 05:06:07,439 - Epoch: [254][   79/   79]    Loss 1.257445    Top1 66.800000    Top5 89.800000    
2022-01-29 05:06:07,495 - ==> Top1: 66.800    Top5: 89.800    Loss: 1.257

2022-01-29 05:06:07,500 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:06:07,500 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:06:07,537 - 

2022-01-29 05:06:07,537 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:06:15,320 - Epoch: [255][  100/  391]    Overall Loss 0.223035    Objective Loss 0.223035                                        LR 0.000305    Time 0.077803    
2022-01-29 05:06:22,791 - Epoch: [255][  200/  391]    Overall Loss 0.223827    Objective Loss 0.223827                                        LR 0.000305    Time 0.076252    
2022-01-29 05:06:30,269 - Epoch: [255][  300/  391]    Overall Loss 0.223115    Objective Loss 0.223115                                        LR 0.000305    Time 0.075756    
2022-01-29 05:06:37,058 - Epoch: [255][  391/  391]    Overall Loss 0.222369    Objective Loss 0.222369    Top1 96.153846    Top5 100.000000    LR 0.000305    Time 0.075486    
2022-01-29 05:06:37,114 - --- validate (epoch=255)-----------
2022-01-29 05:06:37,114 - 10000 samples (128 per mini-batch)
2022-01-29 05:06:40,628 - Epoch: [255][   79/   79]    Loss 1.255881    Top1 66.440000    Top5 89.810000    
2022-01-29 05:06:40,686 - ==> Top1: 66.440    Top5: 89.810    Loss: 1.256

2022-01-29 05:06:40,691 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:06:40,691 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:06:40,729 - 

2022-01-29 05:06:40,729 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:06:48,493 - Epoch: [256][  100/  391]    Overall Loss 0.210587    Objective Loss 0.210587                                        LR 0.000305    Time 0.077609    
2022-01-29 05:06:56,078 - Epoch: [256][  200/  391]    Overall Loss 0.214362    Objective Loss 0.214362                                        LR 0.000305    Time 0.076723    
2022-01-29 05:07:03,688 - Epoch: [256][  300/  391]    Overall Loss 0.216546    Objective Loss 0.216546                                        LR 0.000305    Time 0.076512    
2022-01-29 05:07:10,620 - Epoch: [256][  391/  391]    Overall Loss 0.218374    Objective Loss 0.218374    Top1 96.634615    Top5 100.000000    LR 0.000305    Time 0.076432    
2022-01-29 05:07:10,674 - --- validate (epoch=256)-----------
2022-01-29 05:07:10,674 - 10000 samples (128 per mini-batch)
2022-01-29 05:07:14,261 - Epoch: [256][   79/   79]    Loss 1.268229    Top1 66.540000    Top5 90.040000    
2022-01-29 05:07:14,318 - ==> Top1: 66.540    Top5: 90.040    Loss: 1.268

2022-01-29 05:07:14,323 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:07:14,323 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:07:14,360 - 

2022-01-29 05:07:14,360 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:07:22,178 - Epoch: [257][  100/  391]    Overall Loss 0.216236    Objective Loss 0.216236                                        LR 0.000305    Time 0.078153    
2022-01-29 05:07:29,764 - Epoch: [257][  200/  391]    Overall Loss 0.215639    Objective Loss 0.215639                                        LR 0.000305    Time 0.077000    
2022-01-29 05:07:37,413 - Epoch: [257][  300/  391]    Overall Loss 0.215063    Objective Loss 0.215063                                        LR 0.000305    Time 0.076827    
2022-01-29 05:07:44,373 - Epoch: [257][  391/  391]    Overall Loss 0.216371    Objective Loss 0.216371    Top1 97.115385    Top5 100.000000    LR 0.000305    Time 0.076747    
2022-01-29 05:07:44,430 - --- validate (epoch=257)-----------
2022-01-29 05:07:44,430 - 10000 samples (128 per mini-batch)
2022-01-29 05:07:48,116 - Epoch: [257][   79/   79]    Loss 1.247657    Top1 66.500000    Top5 90.210000    
2022-01-29 05:07:48,173 - ==> Top1: 66.500    Top5: 90.210    Loss: 1.248

2022-01-29 05:07:48,178 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:07:48,178 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:07:48,220 - 

2022-01-29 05:07:48,220 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:07:56,190 - Epoch: [258][  100/  391]    Overall Loss 0.216728    Objective Loss 0.216728                                        LR 0.000305    Time 0.079671    
2022-01-29 05:08:03,845 - Epoch: [258][  200/  391]    Overall Loss 0.218633    Objective Loss 0.218633                                        LR 0.000305    Time 0.078106    
2022-01-29 05:08:11,498 - Epoch: [258][  300/  391]    Overall Loss 0.216840    Objective Loss 0.216840                                        LR 0.000305    Time 0.077580    
2022-01-29 05:08:18,463 - Epoch: [258][  391/  391]    Overall Loss 0.217975    Objective Loss 0.217975    Top1 94.711538    Top5 100.000000    LR 0.000305    Time 0.077334    
2022-01-29 05:08:18,522 - --- validate (epoch=258)-----------
2022-01-29 05:08:18,522 - 10000 samples (128 per mini-batch)
2022-01-29 05:08:22,135 - Epoch: [258][   79/   79]    Loss 1.272640    Top1 65.930000    Top5 89.670000    
2022-01-29 05:08:22,185 - ==> Top1: 65.930    Top5: 89.670    Loss: 1.273

2022-01-29 05:08:22,190 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:08:22,190 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:08:22,232 - 

2022-01-29 05:08:22,232 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:08:30,196 - Epoch: [259][  100/  391]    Overall Loss 0.214403    Objective Loss 0.214403                                        LR 0.000305    Time 0.079611    
2022-01-29 05:08:37,833 - Epoch: [259][  200/  391]    Overall Loss 0.216343    Objective Loss 0.216343                                        LR 0.000305    Time 0.077985    
2022-01-29 05:08:45,472 - Epoch: [259][  300/  391]    Overall Loss 0.215789    Objective Loss 0.215789                                        LR 0.000305    Time 0.077449    
2022-01-29 05:08:52,429 - Epoch: [259][  391/  391]    Overall Loss 0.216639    Objective Loss 0.216639    Top1 97.115385    Top5 100.000000    LR 0.000305    Time 0.077215    
2022-01-29 05:08:52,489 - --- validate (epoch=259)-----------
2022-01-29 05:08:52,489 - 10000 samples (128 per mini-batch)
2022-01-29 05:08:56,167 - Epoch: [259][   79/   79]    Loss 1.270172    Top1 66.560000    Top5 89.940000    
2022-01-29 05:08:56,217 - ==> Top1: 66.560    Top5: 89.940    Loss: 1.270

2022-01-29 05:08:56,222 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:08:56,222 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:08:56,264 - 

2022-01-29 05:08:56,265 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:09:04,131 - Epoch: [260][  100/  391]    Overall Loss 0.216378    Objective Loss 0.216378                                        LR 0.000305    Time 0.078633    
2022-01-29 05:09:11,751 - Epoch: [260][  200/  391]    Overall Loss 0.214302    Objective Loss 0.214302                                        LR 0.000305    Time 0.077410    
2022-01-29 05:09:19,371 - Epoch: [260][  300/  391]    Overall Loss 0.213819    Objective Loss 0.213819                                        LR 0.000305    Time 0.077005    
2022-01-29 05:09:26,310 - Epoch: [260][  391/  391]    Overall Loss 0.214830    Objective Loss 0.214830    Top1 93.750000    Top5 100.000000    LR 0.000305    Time 0.076827    
2022-01-29 05:09:26,365 - --- validate (epoch=260)-----------
2022-01-29 05:09:26,365 - 10000 samples (128 per mini-batch)
2022-01-29 05:09:30,012 - Epoch: [260][   79/   79]    Loss 1.259017    Top1 66.550000    Top5 90.140000    
2022-01-29 05:09:30,063 - ==> Top1: 66.550    Top5: 90.140    Loss: 1.259

2022-01-29 05:09:30,068 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:09:30,068 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:09:30,107 - 

2022-01-29 05:09:30,107 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:09:37,918 - Epoch: [261][  100/  391]    Overall Loss 0.212642    Objective Loss 0.212642                                        LR 0.000305    Time 0.078077    
2022-01-29 05:09:45,504 - Epoch: [261][  200/  391]    Overall Loss 0.213681    Objective Loss 0.213681                                        LR 0.000305    Time 0.076963    
2022-01-29 05:09:53,089 - Epoch: [261][  300/  391]    Overall Loss 0.215574    Objective Loss 0.215574                                        LR 0.000305    Time 0.076591    
2022-01-29 05:09:59,993 - Epoch: [261][  391/  391]    Overall Loss 0.216956    Objective Loss 0.216956    Top1 93.750000    Top5 100.000000    LR 0.000305    Time 0.076421    
2022-01-29 05:10:00,052 - --- validate (epoch=261)-----------
2022-01-29 05:10:00,053 - 10000 samples (128 per mini-batch)
2022-01-29 05:10:03,609 - Epoch: [261][   79/   79]    Loss 1.268971    Top1 66.230000    Top5 89.790000    
2022-01-29 05:10:03,663 - ==> Top1: 66.230    Top5: 89.790    Loss: 1.269

2022-01-29 05:10:03,668 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:10:03,669 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:10:03,710 - 

2022-01-29 05:10:03,711 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:10:11,726 - Epoch: [262][  100/  391]    Overall Loss 0.206055    Objective Loss 0.206055                                        LR 0.000305    Time 0.080125    
2022-01-29 05:10:19,426 - Epoch: [262][  200/  391]    Overall Loss 0.212635    Objective Loss 0.212635                                        LR 0.000305    Time 0.078558    
2022-01-29 05:10:27,123 - Epoch: [262][  300/  391]    Overall Loss 0.213643    Objective Loss 0.213643                                        LR 0.000305    Time 0.078027    
2022-01-29 05:10:34,105 - Epoch: [262][  391/  391]    Overall Loss 0.213281    Objective Loss 0.213281    Top1 96.153846    Top5 100.000000    LR 0.000305    Time 0.077721    
2022-01-29 05:10:34,164 - --- validate (epoch=262)-----------
2022-01-29 05:10:34,164 - 10000 samples (128 per mini-batch)
2022-01-29 05:10:37,785 - Epoch: [262][   79/   79]    Loss 1.259441    Top1 66.300000    Top5 89.920000    
2022-01-29 05:10:37,838 - ==> Top1: 66.300    Top5: 89.920    Loss: 1.259

2022-01-29 05:10:37,843 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:10:37,843 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:10:37,884 - 

2022-01-29 05:10:37,885 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:10:45,795 - Epoch: [263][  100/  391]    Overall Loss 0.209119    Objective Loss 0.209119                                        LR 0.000305    Time 0.079071    
2022-01-29 05:10:53,396 - Epoch: [263][  200/  391]    Overall Loss 0.212849    Objective Loss 0.212849                                        LR 0.000305    Time 0.077536    
2022-01-29 05:11:00,999 - Epoch: [263][  300/  391]    Overall Loss 0.213776    Objective Loss 0.213776                                        LR 0.000305    Time 0.077030    
2022-01-29 05:11:07,919 - Epoch: [263][  391/  391]    Overall Loss 0.213689    Objective Loss 0.213689    Top1 97.596154    Top5 100.000000    LR 0.000305    Time 0.076800    
2022-01-29 05:11:07,978 - --- validate (epoch=263)-----------
2022-01-29 05:11:07,978 - 10000 samples (128 per mini-batch)
2022-01-29 05:11:11,673 - Epoch: [263][   79/   79]    Loss 1.256881    Top1 66.680000    Top5 89.980000    
2022-01-29 05:11:11,731 - ==> Top1: 66.680    Top5: 89.980    Loss: 1.257

2022-01-29 05:11:11,736 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:11:11,736 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:11:11,778 - 

2022-01-29 05:11:11,778 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:11:19,617 - Epoch: [264][  100/  391]    Overall Loss 0.206990    Objective Loss 0.206990                                        LR 0.000305    Time 0.078358    
2022-01-29 05:11:27,248 - Epoch: [264][  200/  391]    Overall Loss 0.206312    Objective Loss 0.206312                                        LR 0.000305    Time 0.077328    
2022-01-29 05:11:34,915 - Epoch: [264][  300/  391]    Overall Loss 0.207929    Objective Loss 0.207929                                        LR 0.000305    Time 0.077105    
2022-01-29 05:11:41,836 - Epoch: [264][  391/  391]    Overall Loss 0.208627    Objective Loss 0.208627    Top1 96.153846    Top5 99.519231    LR 0.000305    Time 0.076859    
2022-01-29 05:11:41,893 - --- validate (epoch=264)-----------
2022-01-29 05:11:41,893 - 10000 samples (128 per mini-batch)
2022-01-29 05:11:45,439 - Epoch: [264][   79/   79]    Loss 1.275001    Top1 66.270000    Top5 89.500000    
2022-01-29 05:11:45,496 - ==> Top1: 66.270    Top5: 89.500    Loss: 1.275

2022-01-29 05:11:45,502 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:11:45,502 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:11:45,544 - 

2022-01-29 05:11:45,544 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:11:53,507 - Epoch: [265][  100/  391]    Overall Loss 0.208174    Objective Loss 0.208174                                        LR 0.000305    Time 0.079609    
2022-01-29 05:12:01,111 - Epoch: [265][  200/  391]    Overall Loss 0.210173    Objective Loss 0.210173                                        LR 0.000305    Time 0.077817    
2022-01-29 05:12:08,769 - Epoch: [265][  300/  391]    Overall Loss 0.211102    Objective Loss 0.211102                                        LR 0.000305    Time 0.077402    
2022-01-29 05:12:15,692 - Epoch: [265][  391/  391]    Overall Loss 0.212143    Objective Loss 0.212143    Top1 95.192308    Top5 100.000000    LR 0.000305    Time 0.077091    
2022-01-29 05:12:15,751 - --- validate (epoch=265)-----------
2022-01-29 05:12:15,751 - 10000 samples (128 per mini-batch)
2022-01-29 05:12:19,384 - Epoch: [265][   79/   79]    Loss 1.276447    Top1 66.480000    Top5 89.830000    
2022-01-29 05:12:19,439 - ==> Top1: 66.480    Top5: 89.830    Loss: 1.276

2022-01-29 05:12:19,444 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:12:19,444 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:12:19,481 - 

2022-01-29 05:12:19,481 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:12:27,349 - Epoch: [266][  100/  391]    Overall Loss 0.204935    Objective Loss 0.204935                                        LR 0.000305    Time 0.078647    
2022-01-29 05:12:34,921 - Epoch: [266][  200/  391]    Overall Loss 0.204614    Objective Loss 0.204614                                        LR 0.000305    Time 0.077181    
2022-01-29 05:12:42,543 - Epoch: [266][  300/  391]    Overall Loss 0.207685    Objective Loss 0.207685                                        LR 0.000305    Time 0.076856    
2022-01-29 05:12:49,465 - Epoch: [266][  391/  391]    Overall Loss 0.208982    Objective Loss 0.208982    Top1 95.192308    Top5 100.000000    LR 0.000305    Time 0.076672    
2022-01-29 05:12:49,517 - --- validate (epoch=266)-----------
2022-01-29 05:12:49,517 - 10000 samples (128 per mini-batch)
2022-01-29 05:12:53,128 - Epoch: [266][   79/   79]    Loss 1.254028    Top1 66.830000    Top5 90.170000    
2022-01-29 05:12:53,184 - ==> Top1: 66.830    Top5: 90.170    Loss: 1.254

2022-01-29 05:12:53,190 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:12:53,190 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:12:53,232 - 

2022-01-29 05:12:53,232 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:13:01,060 - Epoch: [267][  100/  391]    Overall Loss 0.204066    Objective Loss 0.204066                                        LR 0.000305    Time 0.078258    
2022-01-29 05:13:08,660 - Epoch: [267][  200/  391]    Overall Loss 0.207094    Objective Loss 0.207094                                        LR 0.000305    Time 0.077122    
2022-01-29 05:13:16,260 - Epoch: [267][  300/  391]    Overall Loss 0.207498    Objective Loss 0.207498                                        LR 0.000305    Time 0.076747    
2022-01-29 05:13:23,173 - Epoch: [267][  391/  391]    Overall Loss 0.208600    Objective Loss 0.208600    Top1 94.230769    Top5 100.000000    LR 0.000305    Time 0.076562    
2022-01-29 05:13:23,231 - --- validate (epoch=267)-----------
2022-01-29 05:13:23,231 - 10000 samples (128 per mini-batch)
2022-01-29 05:13:26,858 - Epoch: [267][   79/   79]    Loss 1.272214    Top1 66.330000    Top5 89.960000    
2022-01-29 05:13:26,916 - ==> Top1: 66.330    Top5: 89.960    Loss: 1.272

2022-01-29 05:13:26,921 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:13:26,921 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:13:26,964 - 

2022-01-29 05:13:26,964 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:13:34,903 - Epoch: [268][  100/  391]    Overall Loss 0.201464    Objective Loss 0.201464                                        LR 0.000305    Time 0.079367    
2022-01-29 05:13:42,556 - Epoch: [268][  200/  391]    Overall Loss 0.203882    Objective Loss 0.203882                                        LR 0.000305    Time 0.077943    
2022-01-29 05:13:50,207 - Epoch: [268][  300/  391]    Overall Loss 0.206893    Objective Loss 0.206893                                        LR 0.000305    Time 0.077462    
2022-01-29 05:13:57,200 - Epoch: [268][  391/  391]    Overall Loss 0.208166    Objective Loss 0.208166    Top1 96.153846    Top5 100.000000    LR 0.000305    Time 0.077317    
2022-01-29 05:13:57,259 - --- validate (epoch=268)-----------
2022-01-29 05:13:57,260 - 10000 samples (128 per mini-batch)
2022-01-29 05:14:00,912 - Epoch: [268][   79/   79]    Loss 1.270620    Top1 66.270000    Top5 89.860000    
2022-01-29 05:14:00,965 - ==> Top1: 66.270    Top5: 89.860    Loss: 1.271

2022-01-29 05:14:00,970 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:14:00,970 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:14:01,012 - 

2022-01-29 05:14:01,012 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:14:08,989 - Epoch: [269][  100/  391]    Overall Loss 0.203812    Objective Loss 0.203812                                        LR 0.000305    Time 0.079750    
2022-01-29 05:14:16,646 - Epoch: [269][  200/  391]    Overall Loss 0.204727    Objective Loss 0.204727                                        LR 0.000305    Time 0.078154    
2022-01-29 05:14:24,280 - Epoch: [269][  300/  391]    Overall Loss 0.205342    Objective Loss 0.205342                                        LR 0.000305    Time 0.077547    
2022-01-29 05:14:31,243 - Epoch: [269][  391/  391]    Overall Loss 0.205433    Objective Loss 0.205433    Top1 94.230769    Top5 100.000000    LR 0.000305    Time 0.077305    
2022-01-29 05:14:31,303 - --- validate (epoch=269)-----------
2022-01-29 05:14:31,303 - 10000 samples (128 per mini-batch)
2022-01-29 05:14:34,879 - Epoch: [269][   79/   79]    Loss 1.259577    Top1 66.330000    Top5 90.060000    
2022-01-29 05:14:34,927 - ==> Top1: 66.330    Top5: 90.060    Loss: 1.260

2022-01-29 05:14:34,932 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:14:34,932 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:14:34,968 - 

2022-01-29 05:14:34,969 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:14:42,704 - Epoch: [270][  100/  391]    Overall Loss 0.203643    Objective Loss 0.203643                                        LR 0.000305    Time 0.077329    
2022-01-29 05:14:50,261 - Epoch: [270][  200/  391]    Overall Loss 0.206714    Objective Loss 0.206714                                        LR 0.000305    Time 0.076446    
2022-01-29 05:14:57,824 - Epoch: [270][  300/  391]    Overall Loss 0.206337    Objective Loss 0.206337                                        LR 0.000305    Time 0.076171    
2022-01-29 05:15:04,706 - Epoch: [270][  391/  391]    Overall Loss 0.205280    Objective Loss 0.205280    Top1 97.596154    Top5 100.000000    LR 0.000305    Time 0.076043    
2022-01-29 05:15:04,771 - --- validate (epoch=270)-----------
2022-01-29 05:15:04,771 - 10000 samples (128 per mini-batch)
2022-01-29 05:15:08,367 - Epoch: [270][   79/   79]    Loss 1.279440    Top1 66.350000    Top5 89.810000    
2022-01-29 05:15:08,419 - ==> Top1: 66.350    Top5: 89.810    Loss: 1.279

2022-01-29 05:15:08,424 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:15:08,424 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:15:08,460 - 

2022-01-29 05:15:08,460 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:15:16,234 - Epoch: [271][  100/  391]    Overall Loss 0.202313    Objective Loss 0.202313                                        LR 0.000305    Time 0.077712    
2022-01-29 05:15:23,794 - Epoch: [271][  200/  391]    Overall Loss 0.202152    Objective Loss 0.202152                                        LR 0.000305    Time 0.076651    
2022-01-29 05:15:31,356 - Epoch: [271][  300/  391]    Overall Loss 0.202255    Objective Loss 0.202255                                        LR 0.000305    Time 0.076305    
2022-01-29 05:15:38,263 - Epoch: [271][  391/  391]    Overall Loss 0.204566    Objective Loss 0.204566    Top1 95.192308    Top5 99.519231    LR 0.000305    Time 0.076210    
2022-01-29 05:15:38,321 - --- validate (epoch=271)-----------
2022-01-29 05:15:38,321 - 10000 samples (128 per mini-batch)
2022-01-29 05:15:42,016 - Epoch: [271][   79/   79]    Loss 1.271402    Top1 66.750000    Top5 89.980000    
2022-01-29 05:15:42,067 - ==> Top1: 66.750    Top5: 89.980    Loss: 1.271

2022-01-29 05:15:42,073 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:15:42,073 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:15:42,109 - 

2022-01-29 05:15:42,110 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:15:49,997 - Epoch: [272][  100/  391]    Overall Loss 0.201948    Objective Loss 0.201948                                        LR 0.000305    Time 0.078842    
2022-01-29 05:15:57,501 - Epoch: [272][  200/  391]    Overall Loss 0.201682    Objective Loss 0.201682                                        LR 0.000305    Time 0.076939    
2022-01-29 05:16:04,996 - Epoch: [272][  300/  391]    Overall Loss 0.204323    Objective Loss 0.204323                                        LR 0.000305    Time 0.076273    
2022-01-29 05:16:11,807 - Epoch: [272][  391/  391]    Overall Loss 0.203644    Objective Loss 0.203644    Top1 95.673077    Top5 99.519231    LR 0.000305    Time 0.075938    
2022-01-29 05:16:11,867 - --- validate (epoch=272)-----------
2022-01-29 05:16:11,867 - 10000 samples (128 per mini-batch)
2022-01-29 05:16:15,478 - Epoch: [272][   79/   79]    Loss 1.263621    Top1 66.410000    Top5 89.890000    
2022-01-29 05:16:15,530 - ==> Top1: 66.410    Top5: 89.890    Loss: 1.264

2022-01-29 05:16:15,535 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:16:15,535 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:16:15,575 - 

2022-01-29 05:16:15,575 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:16:23,521 - Epoch: [273][  100/  391]    Overall Loss 0.198630    Objective Loss 0.198630                                        LR 0.000305    Time 0.079432    
2022-01-29 05:16:31,144 - Epoch: [273][  200/  391]    Overall Loss 0.201095    Objective Loss 0.201095                                        LR 0.000305    Time 0.077823    
2022-01-29 05:16:38,764 - Epoch: [273][  300/  391]    Overall Loss 0.203352    Objective Loss 0.203352                                        LR 0.000305    Time 0.077282    
2022-01-29 05:16:45,706 - Epoch: [273][  391/  391]    Overall Loss 0.204190    Objective Loss 0.204190    Top1 97.115385    Top5 100.000000    LR 0.000305    Time 0.077047    
2022-01-29 05:16:45,765 - --- validate (epoch=273)-----------
2022-01-29 05:16:45,766 - 10000 samples (128 per mini-batch)
2022-01-29 05:16:49,425 - Epoch: [273][   79/   79]    Loss 1.268482    Top1 66.230000    Top5 89.660000    
2022-01-29 05:16:49,482 - ==> Top1: 66.230    Top5: 89.660    Loss: 1.268

2022-01-29 05:16:49,487 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:16:49,487 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:16:49,529 - 

2022-01-29 05:16:49,529 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:16:57,263 - Epoch: [274][  100/  391]    Overall Loss 0.199682    Objective Loss 0.199682                                        LR 0.000305    Time 0.077308    
2022-01-29 05:17:04,773 - Epoch: [274][  200/  391]    Overall Loss 0.201457    Objective Loss 0.201457                                        LR 0.000305    Time 0.076200    
2022-01-29 05:17:12,279 - Epoch: [274][  300/  391]    Overall Loss 0.201353    Objective Loss 0.201353                                        LR 0.000305    Time 0.075818    
2022-01-29 05:17:19,109 - Epoch: [274][  391/  391]    Overall Loss 0.202101    Objective Loss 0.202101    Top1 97.115385    Top5 100.000000    LR 0.000305    Time 0.075640    
2022-01-29 05:17:19,166 - --- validate (epoch=274)-----------
2022-01-29 05:17:19,166 - 10000 samples (128 per mini-batch)
2022-01-29 05:17:22,767 - Epoch: [274][   79/   79]    Loss 1.266226    Top1 66.410000    Top5 89.900000    
2022-01-29 05:17:22,819 - ==> Top1: 66.410    Top5: 89.900    Loss: 1.266

2022-01-29 05:17:22,824 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:17:22,824 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:17:22,867 - 

2022-01-29 05:17:22,867 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:17:30,717 - Epoch: [275][  100/  391]    Overall Loss 0.193066    Objective Loss 0.193066                                        LR 0.000305    Time 0.078478    
2022-01-29 05:17:38,210 - Epoch: [275][  200/  391]    Overall Loss 0.194167    Objective Loss 0.194167                                        LR 0.000305    Time 0.076700    
2022-01-29 05:17:45,705 - Epoch: [275][  300/  391]    Overall Loss 0.196788    Objective Loss 0.196788                                        LR 0.000305    Time 0.076113    
2022-01-29 05:17:52,529 - Epoch: [275][  391/  391]    Overall Loss 0.201121    Objective Loss 0.201121    Top1 95.192308    Top5 99.519231    LR 0.000305    Time 0.075851    
2022-01-29 05:17:52,588 - --- validate (epoch=275)-----------
2022-01-29 05:17:52,588 - 10000 samples (128 per mini-batch)
2022-01-29 05:17:56,137 - Epoch: [275][   79/   79]    Loss 1.284506    Top1 66.270000    Top5 89.760000    
2022-01-29 05:17:56,196 - ==> Top1: 66.270    Top5: 89.760    Loss: 1.285

2022-01-29 05:17:56,201 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:17:56,201 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:17:56,241 - 

2022-01-29 05:17:56,242 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:18:04,005 - Epoch: [276][  100/  391]    Overall Loss 0.206185    Objective Loss 0.206185                                        LR 0.000305    Time 0.077606    
2022-01-29 05:18:11,452 - Epoch: [276][  200/  391]    Overall Loss 0.203170    Objective Loss 0.203170                                        LR 0.000305    Time 0.076032    
2022-01-29 05:18:18,893 - Epoch: [276][  300/  391]    Overall Loss 0.202224    Objective Loss 0.202224                                        LR 0.000305    Time 0.075488    
2022-01-29 05:18:25,735 - Epoch: [276][  391/  391]    Overall Loss 0.202235    Objective Loss 0.202235    Top1 97.596154    Top5 100.000000    LR 0.000305    Time 0.075415    
2022-01-29 05:18:25,794 - --- validate (epoch=276)-----------
2022-01-29 05:18:25,794 - 10000 samples (128 per mini-batch)
2022-01-29 05:18:29,427 - Epoch: [276][   79/   79]    Loss 1.275318    Top1 66.490000    Top5 89.730000    
2022-01-29 05:18:29,482 - ==> Top1: 66.490    Top5: 89.730    Loss: 1.275

2022-01-29 05:18:29,487 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:18:29,487 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:18:29,525 - 

2022-01-29 05:18:29,526 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:18:37,381 - Epoch: [277][  100/  391]    Overall Loss 0.202920    Objective Loss 0.202920                                        LR 0.000305    Time 0.078529    
2022-01-29 05:18:44,884 - Epoch: [277][  200/  391]    Overall Loss 0.203490    Objective Loss 0.203490                                        LR 0.000305    Time 0.076774    
2022-01-29 05:18:52,367 - Epoch: [277][  300/  391]    Overall Loss 0.200353    Objective Loss 0.200353                                        LR 0.000305    Time 0.076123    
2022-01-29 05:18:59,169 - Epoch: [277][  391/  391]    Overall Loss 0.201115    Objective Loss 0.201115    Top1 95.673077    Top5 100.000000    LR 0.000305    Time 0.075801    
2022-01-29 05:18:59,226 - --- validate (epoch=277)-----------
2022-01-29 05:18:59,226 - 10000 samples (128 per mini-batch)
2022-01-29 05:19:02,795 - Epoch: [277][   79/   79]    Loss 1.280709    Top1 66.340000    Top5 89.540000    
2022-01-29 05:19:02,848 - ==> Top1: 66.340    Top5: 89.540    Loss: 1.281

2022-01-29 05:19:02,853 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:19:02,853 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:19:02,895 - 

2022-01-29 05:19:02,895 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:19:10,697 - Epoch: [278][  100/  391]    Overall Loss 0.198897    Objective Loss 0.198897                                        LR 0.000305    Time 0.077993    
2022-01-29 05:19:18,191 - Epoch: [278][  200/  391]    Overall Loss 0.200967    Objective Loss 0.200967                                        LR 0.000305    Time 0.076460    
2022-01-29 05:19:25,681 - Epoch: [278][  300/  391]    Overall Loss 0.200755    Objective Loss 0.200755                                        LR 0.000305    Time 0.075939    
2022-01-29 05:19:32,558 - Epoch: [278][  391/  391]    Overall Loss 0.202294    Objective Loss 0.202294    Top1 92.788462    Top5 100.000000    LR 0.000305    Time 0.075850    
2022-01-29 05:19:32,616 - --- validate (epoch=278)-----------
2022-01-29 05:19:32,616 - 10000 samples (128 per mini-batch)
2022-01-29 05:19:36,216 - Epoch: [278][   79/   79]    Loss 1.281711    Top1 66.280000    Top5 89.540000    
2022-01-29 05:19:36,275 - ==> Top1: 66.280    Top5: 89.540    Loss: 1.282

2022-01-29 05:19:36,281 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:19:36,281 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:19:36,323 - 

2022-01-29 05:19:36,323 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:19:44,209 - Epoch: [279][  100/  391]    Overall Loss 0.200142    Objective Loss 0.200142                                        LR 0.000305    Time 0.078832    
2022-01-29 05:19:51,658 - Epoch: [279][  200/  391]    Overall Loss 0.199996    Objective Loss 0.199996                                        LR 0.000305    Time 0.076660    
2022-01-29 05:19:59,099 - Epoch: [279][  300/  391]    Overall Loss 0.199917    Objective Loss 0.199917                                        LR 0.000305    Time 0.075906    
2022-01-29 05:20:05,867 - Epoch: [279][  391/  391]    Overall Loss 0.199958    Objective Loss 0.199958    Top1 98.557692    Top5 100.000000    LR 0.000305    Time 0.075546    
2022-01-29 05:20:05,925 - --- validate (epoch=279)-----------
2022-01-29 05:20:05,925 - 10000 samples (128 per mini-batch)
2022-01-29 05:20:09,480 - Epoch: [279][   79/   79]    Loss 1.283255    Top1 66.460000    Top5 89.750000    
2022-01-29 05:20:09,532 - ==> Top1: 66.460    Top5: 89.750    Loss: 1.283

2022-01-29 05:20:09,537 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:20:09,537 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:20:09,577 - 

2022-01-29 05:20:09,577 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:20:17,302 - Epoch: [280][  100/  391]    Overall Loss 0.195042    Objective Loss 0.195042                                        LR 0.000305    Time 0.077225    
2022-01-29 05:20:24,832 - Epoch: [280][  200/  391]    Overall Loss 0.198301    Objective Loss 0.198301                                        LR 0.000305    Time 0.076256    
2022-01-29 05:20:32,450 - Epoch: [280][  300/  391]    Overall Loss 0.198532    Objective Loss 0.198532                                        LR 0.000305    Time 0.076229    
2022-01-29 05:20:39,389 - Epoch: [280][  391/  391]    Overall Loss 0.200219    Objective Loss 0.200219    Top1 93.750000    Top5 100.000000    LR 0.000305    Time 0.076231    
2022-01-29 05:20:39,448 - --- validate (epoch=280)-----------
2022-01-29 05:20:39,448 - 10000 samples (128 per mini-batch)
2022-01-29 05:20:43,101 - Epoch: [280][   79/   79]    Loss 1.262656    Top1 66.520000    Top5 89.920000    
2022-01-29 05:20:43,152 - ==> Top1: 66.520    Top5: 89.920    Loss: 1.263

2022-01-29 05:20:43,158 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:20:43,158 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:20:43,200 - 

2022-01-29 05:20:43,200 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:20:51,146 - Epoch: [281][  100/  391]    Overall Loss 0.196396    Objective Loss 0.196396                                        LR 0.000305    Time 0.079433    
2022-01-29 05:20:58,792 - Epoch: [281][  200/  391]    Overall Loss 0.196977    Objective Loss 0.196977                                        LR 0.000305    Time 0.077942    
2022-01-29 05:21:06,429 - Epoch: [281][  300/  391]    Overall Loss 0.199602    Objective Loss 0.199602                                        LR 0.000305    Time 0.077416    
2022-01-29 05:21:13,381 - Epoch: [281][  391/  391]    Overall Loss 0.201523    Objective Loss 0.201523    Top1 95.192308    Top5 100.000000    LR 0.000305    Time 0.077175    
2022-01-29 05:21:13,439 - --- validate (epoch=281)-----------
2022-01-29 05:21:13,439 - 10000 samples (128 per mini-batch)
2022-01-29 05:21:17,042 - Epoch: [281][   79/   79]    Loss 1.284879    Top1 66.370000    Top5 89.620000    
2022-01-29 05:21:17,097 - ==> Top1: 66.370    Top5: 89.620    Loss: 1.285

2022-01-29 05:21:17,102 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:21:17,102 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:21:17,143 - 

2022-01-29 05:21:17,144 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:21:24,947 - Epoch: [282][  100/  391]    Overall Loss 0.198758    Objective Loss 0.198758                                        LR 0.000305    Time 0.078007    
2022-01-29 05:21:32,449 - Epoch: [282][  200/  391]    Overall Loss 0.196047    Objective Loss 0.196047                                        LR 0.000305    Time 0.076507    
2022-01-29 05:21:39,939 - Epoch: [282][  300/  391]    Overall Loss 0.199290    Objective Loss 0.199290                                        LR 0.000305    Time 0.075969    
2022-01-29 05:21:46,811 - Epoch: [282][  391/  391]    Overall Loss 0.199538    Objective Loss 0.199538    Top1 97.596154    Top5 100.000000    LR 0.000305    Time 0.075861    
2022-01-29 05:21:46,869 - --- validate (epoch=282)-----------
2022-01-29 05:21:46,869 - 10000 samples (128 per mini-batch)
2022-01-29 05:21:50,607 - Epoch: [282][   79/   79]    Loss 1.273589    Top1 66.690000    Top5 89.770000    
2022-01-29 05:21:50,657 - ==> Top1: 66.690    Top5: 89.770    Loss: 1.274

2022-01-29 05:21:50,662 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:21:50,662 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:21:50,704 - 

2022-01-29 05:21:50,704 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:21:58,561 - Epoch: [283][  100/  391]    Overall Loss 0.198529    Objective Loss 0.198529                                        LR 0.000305    Time 0.078538    
2022-01-29 05:22:06,190 - Epoch: [283][  200/  391]    Overall Loss 0.195653    Objective Loss 0.195653                                        LR 0.000305    Time 0.077409    
2022-01-29 05:22:13,809 - Epoch: [283][  300/  391]    Overall Loss 0.197957    Objective Loss 0.197957                                        LR 0.000305    Time 0.077001    
2022-01-29 05:22:20,742 - Epoch: [283][  391/  391]    Overall Loss 0.199181    Objective Loss 0.199181    Top1 97.596154    Top5 100.000000    LR 0.000305    Time 0.076810    
2022-01-29 05:22:20,804 - --- validate (epoch=283)-----------
2022-01-29 05:22:20,804 - 10000 samples (128 per mini-batch)
2022-01-29 05:22:24,423 - Epoch: [283][   79/   79]    Loss 1.272645    Top1 66.230000    Top5 89.820000    
2022-01-29 05:22:24,479 - ==> Top1: 66.230    Top5: 89.820    Loss: 1.273

2022-01-29 05:22:24,484 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:22:24,484 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:22:24,525 - 

2022-01-29 05:22:24,525 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:22:32,435 - Epoch: [284][  100/  391]    Overall Loss 0.187497    Objective Loss 0.187497                                        LR 0.000305    Time 0.079077    
2022-01-29 05:22:40,076 - Epoch: [284][  200/  391]    Overall Loss 0.194675    Objective Loss 0.194675                                        LR 0.000305    Time 0.077740    
2022-01-29 05:22:47,704 - Epoch: [284][  300/  391]    Overall Loss 0.194714    Objective Loss 0.194714                                        LR 0.000305    Time 0.077251    
2022-01-29 05:22:54,550 - Epoch: [284][  391/  391]    Overall Loss 0.195424    Objective Loss 0.195424    Top1 95.192308    Top5 99.519231    LR 0.000305    Time 0.076779    
2022-01-29 05:22:54,606 - --- validate (epoch=284)-----------
2022-01-29 05:22:54,606 - 10000 samples (128 per mini-batch)
2022-01-29 05:22:58,123 - Epoch: [284][   79/   79]    Loss 1.299337    Top1 66.320000    Top5 89.780000    
2022-01-29 05:22:58,180 - ==> Top1: 66.320    Top5: 89.780    Loss: 1.299

2022-01-29 05:22:58,186 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:22:58,186 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:22:58,221 - 

2022-01-29 05:22:58,221 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:23:06,058 - Epoch: [285][  100/  391]    Overall Loss 0.196861    Objective Loss 0.196861                                        LR 0.000305    Time 0.078345    
2022-01-29 05:23:13,616 - Epoch: [285][  200/  391]    Overall Loss 0.193787    Objective Loss 0.193787                                        LR 0.000305    Time 0.076958    
2022-01-29 05:23:21,296 - Epoch: [285][  300/  391]    Overall Loss 0.194617    Objective Loss 0.194617                                        LR 0.000305    Time 0.076902    
2022-01-29 05:23:28,295 - Epoch: [285][  391/  391]    Overall Loss 0.196208    Objective Loss 0.196208    Top1 94.230769    Top5 100.000000    LR 0.000305    Time 0.076902    
2022-01-29 05:23:28,353 - --- validate (epoch=285)-----------
2022-01-29 05:23:28,353 - 10000 samples (128 per mini-batch)
2022-01-29 05:23:31,970 - Epoch: [285][   79/   79]    Loss 1.287707    Top1 66.070000    Top5 89.670000    
2022-01-29 05:23:32,023 - ==> Top1: 66.070    Top5: 89.670    Loss: 1.288

2022-01-29 05:23:32,028 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:23:32,028 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:23:32,064 - 

2022-01-29 05:23:32,064 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:23:39,893 - Epoch: [286][  100/  391]    Overall Loss 0.196167    Objective Loss 0.196167                                        LR 0.000305    Time 0.078265    
2022-01-29 05:23:47,473 - Epoch: [286][  200/  391]    Overall Loss 0.195265    Objective Loss 0.195265                                        LR 0.000305    Time 0.077027    
2022-01-29 05:23:55,088 - Epoch: [286][  300/  391]    Overall Loss 0.193852    Objective Loss 0.193852                                        LR 0.000305    Time 0.076732    
2022-01-29 05:24:01,981 - Epoch: [286][  391/  391]    Overall Loss 0.194252    Objective Loss 0.194252    Top1 97.596154    Top5 100.000000    LR 0.000305    Time 0.076501    
2022-01-29 05:24:02,039 - --- validate (epoch=286)-----------
2022-01-29 05:24:02,039 - 10000 samples (128 per mini-batch)
2022-01-29 05:24:05,586 - Epoch: [286][   79/   79]    Loss 1.276548    Top1 66.300000    Top5 89.780000    
2022-01-29 05:24:05,642 - ==> Top1: 66.300    Top5: 89.780    Loss: 1.277

2022-01-29 05:24:05,647 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:24:05,647 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:24:05,688 - 

2022-01-29 05:24:05,688 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:24:13,582 - Epoch: [287][  100/  391]    Overall Loss 0.190770    Objective Loss 0.190770                                        LR 0.000305    Time 0.078909    
2022-01-29 05:24:21,263 - Epoch: [287][  200/  391]    Overall Loss 0.190149    Objective Loss 0.190149                                        LR 0.000305    Time 0.077856    
2022-01-29 05:24:28,880 - Epoch: [287][  300/  391]    Overall Loss 0.192242    Objective Loss 0.192242                                        LR 0.000305    Time 0.077291    
2022-01-29 05:24:35,809 - Epoch: [287][  391/  391]    Overall Loss 0.193437    Objective Loss 0.193437    Top1 97.115385    Top5 100.000000    LR 0.000305    Time 0.077023    
2022-01-29 05:24:35,869 - --- validate (epoch=287)-----------
2022-01-29 05:24:35,869 - 10000 samples (128 per mini-batch)
2022-01-29 05:24:39,492 - Epoch: [287][   79/   79]    Loss 1.285699    Top1 66.340000    Top5 89.710000    
2022-01-29 05:24:39,547 - ==> Top1: 66.340    Top5: 89.710    Loss: 1.286

2022-01-29 05:24:39,552 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:24:39,552 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:24:39,594 - 

2022-01-29 05:24:39,594 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:24:47,399 - Epoch: [288][  100/  391]    Overall Loss 0.189813    Objective Loss 0.189813                                        LR 0.000305    Time 0.078023    
2022-01-29 05:24:54,982 - Epoch: [288][  200/  391]    Overall Loss 0.191764    Objective Loss 0.191764                                        LR 0.000305    Time 0.076923    
2022-01-29 05:25:02,561 - Epoch: [288][  300/  391]    Overall Loss 0.194530    Objective Loss 0.194530                                        LR 0.000305    Time 0.076542    
2022-01-29 05:25:09,480 - Epoch: [288][  391/  391]    Overall Loss 0.195352    Objective Loss 0.195352    Top1 95.192308    Top5 100.000000    LR 0.000305    Time 0.076421    
2022-01-29 05:25:09,539 - --- validate (epoch=288)-----------
2022-01-29 05:25:09,539 - 10000 samples (128 per mini-batch)
2022-01-29 05:25:13,222 - Epoch: [288][   79/   79]    Loss 1.287318    Top1 66.090000    Top5 89.780000    
2022-01-29 05:25:13,277 - ==> Top1: 66.090    Top5: 89.780    Loss: 1.287

2022-01-29 05:25:13,282 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:25:13,283 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:25:13,319 - 

2022-01-29 05:25:13,320 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:25:21,130 - Epoch: [289][  100/  391]    Overall Loss 0.191205    Objective Loss 0.191205                                        LR 0.000305    Time 0.078081    
2022-01-29 05:25:28,752 - Epoch: [289][  200/  391]    Overall Loss 0.194537    Objective Loss 0.194537                                        LR 0.000305    Time 0.077144    
2022-01-29 05:25:36,403 - Epoch: [289][  300/  391]    Overall Loss 0.195169    Objective Loss 0.195169                                        LR 0.000305    Time 0.076931    
2022-01-29 05:25:43,334 - Epoch: [289][  391/  391]    Overall Loss 0.196166    Objective Loss 0.196166    Top1 99.038462    Top5 100.000000    LR 0.000305    Time 0.076749    
2022-01-29 05:25:43,396 - --- validate (epoch=289)-----------
2022-01-29 05:25:43,396 - 10000 samples (128 per mini-batch)
2022-01-29 05:25:46,999 - Epoch: [289][   79/   79]    Loss 1.279133    Top1 66.270000    Top5 89.840000    
2022-01-29 05:25:47,058 - ==> Top1: 66.270    Top5: 89.840    Loss: 1.279

2022-01-29 05:25:47,063 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:25:47,063 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:25:47,105 - 

2022-01-29 05:25:47,105 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:25:55,017 - Epoch: [290][  100/  391]    Overall Loss 0.191176    Objective Loss 0.191176                                        LR 0.000305    Time 0.079092    
2022-01-29 05:26:02,615 - Epoch: [290][  200/  391]    Overall Loss 0.193028    Objective Loss 0.193028                                        LR 0.000305    Time 0.077534    
2022-01-29 05:26:10,213 - Epoch: [290][  300/  391]    Overall Loss 0.195738    Objective Loss 0.195738                                        LR 0.000305    Time 0.077013    
2022-01-29 05:26:17,122 - Epoch: [290][  391/  391]    Overall Loss 0.195914    Objective Loss 0.195914    Top1 95.673077    Top5 100.000000    LR 0.000305    Time 0.076757    
2022-01-29 05:26:17,181 - --- validate (epoch=290)-----------
2022-01-29 05:26:17,181 - 10000 samples (128 per mini-batch)
2022-01-29 05:26:20,828 - Epoch: [290][   79/   79]    Loss 1.289955    Top1 66.030000    Top5 89.590000    
2022-01-29 05:26:20,887 - ==> Top1: 66.030    Top5: 89.590    Loss: 1.290

2022-01-29 05:26:20,892 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:26:20,892 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:26:20,934 - 

2022-01-29 05:26:20,934 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:26:28,895 - Epoch: [291][  100/  391]    Overall Loss 0.194826    Objective Loss 0.194826                                        LR 0.000305    Time 0.079584    
2022-01-29 05:26:36,613 - Epoch: [291][  200/  391]    Overall Loss 0.193327    Objective Loss 0.193327                                        LR 0.000305    Time 0.078378    
2022-01-29 05:26:44,262 - Epoch: [291][  300/  391]    Overall Loss 0.194787    Objective Loss 0.194787                                        LR 0.000305    Time 0.077746    
2022-01-29 05:26:51,199 - Epoch: [291][  391/  391]    Overall Loss 0.196428    Objective Loss 0.196428    Top1 95.192308    Top5 100.000000    LR 0.000305    Time 0.077390    
2022-01-29 05:26:51,255 - --- validate (epoch=291)-----------
2022-01-29 05:26:51,255 - 10000 samples (128 per mini-batch)
2022-01-29 05:26:54,889 - Epoch: [291][   79/   79]    Loss 1.293558    Top1 66.110000    Top5 89.810000    
2022-01-29 05:26:54,941 - ==> Top1: 66.110    Top5: 89.810    Loss: 1.294

2022-01-29 05:26:54,946 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:26:54,946 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:26:54,983 - 

2022-01-29 05:26:54,983 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:27:02,856 - Epoch: [292][  100/  391]    Overall Loss 0.189553    Objective Loss 0.189553                                        LR 0.000305    Time 0.078701    
2022-01-29 05:27:10,499 - Epoch: [292][  200/  391]    Overall Loss 0.190318    Objective Loss 0.190318                                        LR 0.000305    Time 0.077564    
2022-01-29 05:27:18,140 - Epoch: [292][  300/  391]    Overall Loss 0.193224    Objective Loss 0.193224                                        LR 0.000305    Time 0.077175    
2022-01-29 05:27:25,094 - Epoch: [292][  391/  391]    Overall Loss 0.194227    Objective Loss 0.194227    Top1 97.596154    Top5 100.000000    LR 0.000305    Time 0.076996    
2022-01-29 05:27:25,145 - --- validate (epoch=292)-----------
2022-01-29 05:27:25,145 - 10000 samples (128 per mini-batch)
2022-01-29 05:27:28,785 - Epoch: [292][   79/   79]    Loss 1.291129    Top1 66.440000    Top5 89.500000    
2022-01-29 05:27:28,842 - ==> Top1: 66.440    Top5: 89.500    Loss: 1.291

2022-01-29 05:27:28,847 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:27:28,847 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:27:28,889 - 

2022-01-29 05:27:28,890 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:27:36,798 - Epoch: [293][  100/  391]    Overall Loss 0.187104    Objective Loss 0.187104                                        LR 0.000305    Time 0.079059    
2022-01-29 05:27:44,396 - Epoch: [293][  200/  391]    Overall Loss 0.192599    Objective Loss 0.192599                                        LR 0.000305    Time 0.077515    
2022-01-29 05:27:52,003 - Epoch: [293][  300/  391]    Overall Loss 0.194743    Objective Loss 0.194743                                        LR 0.000305    Time 0.077029    
2022-01-29 05:27:58,922 - Epoch: [293][  391/  391]    Overall Loss 0.194554    Objective Loss 0.194554    Top1 95.673077    Top5 100.000000    LR 0.000305    Time 0.076797    
2022-01-29 05:27:58,981 - --- validate (epoch=293)-----------
2022-01-29 05:27:58,982 - 10000 samples (128 per mini-batch)
2022-01-29 05:28:02,674 - Epoch: [293][   79/   79]    Loss 1.299512    Top1 66.230000    Top5 89.510000    
2022-01-29 05:28:02,732 - ==> Top1: 66.230    Top5: 89.510    Loss: 1.300

2022-01-29 05:28:02,737 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:28:02,737 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:28:02,779 - 

2022-01-29 05:28:02,779 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:28:10,695 - Epoch: [294][  100/  391]    Overall Loss 0.196536    Objective Loss 0.196536                                        LR 0.000305    Time 0.079135    
2022-01-29 05:28:18,282 - Epoch: [294][  200/  391]    Overall Loss 0.191661    Objective Loss 0.191661                                        LR 0.000305    Time 0.077499    
2022-01-29 05:28:25,910 - Epoch: [294][  300/  391]    Overall Loss 0.192319    Objective Loss 0.192319                                        LR 0.000305    Time 0.077088    
2022-01-29 05:28:32,867 - Epoch: [294][  391/  391]    Overall Loss 0.193865    Objective Loss 0.193865    Top1 97.115385    Top5 99.519231    LR 0.000305    Time 0.076939    
2022-01-29 05:28:32,925 - --- validate (epoch=294)-----------
2022-01-29 05:28:32,925 - 10000 samples (128 per mini-batch)
2022-01-29 05:28:36,534 - Epoch: [294][   79/   79]    Loss 1.273198    Top1 66.550000    Top5 89.910000    
2022-01-29 05:28:36,593 - ==> Top1: 66.550    Top5: 89.910    Loss: 1.273

2022-01-29 05:28:36,598 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:28:36,598 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:28:36,639 - 

2022-01-29 05:28:36,639 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:28:44,425 - Epoch: [295][  100/  391]    Overall Loss 0.187847    Objective Loss 0.187847                                        LR 0.000305    Time 0.077830    
2022-01-29 05:28:51,979 - Epoch: [295][  200/  391]    Overall Loss 0.191144    Objective Loss 0.191144                                        LR 0.000305    Time 0.076680    
2022-01-29 05:28:59,534 - Epoch: [295][  300/  391]    Overall Loss 0.190974    Objective Loss 0.190974                                        LR 0.000305    Time 0.076302    
2022-01-29 05:29:06,405 - Epoch: [295][  391/  391]    Overall Loss 0.192721    Objective Loss 0.192721    Top1 94.230769    Top5 100.000000    LR 0.000305    Time 0.076115    
2022-01-29 05:29:06,467 - --- validate (epoch=295)-----------
2022-01-29 05:29:06,467 - 10000 samples (128 per mini-batch)
2022-01-29 05:29:10,137 - Epoch: [295][   79/   79]    Loss 1.302393    Top1 66.020000    Top5 89.500000    
2022-01-29 05:29:10,190 - ==> Top1: 66.020    Top5: 89.500    Loss: 1.302

2022-01-29 05:29:10,195 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:29:10,195 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:29:10,237 - 

2022-01-29 05:29:10,237 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:29:18,171 - Epoch: [296][  100/  391]    Overall Loss 0.188486    Objective Loss 0.188486                                        LR 0.000305    Time 0.079310    
2022-01-29 05:29:25,766 - Epoch: [296][  200/  391]    Overall Loss 0.191563    Objective Loss 0.191563                                        LR 0.000305    Time 0.077625    
2022-01-29 05:29:33,308 - Epoch: [296][  300/  391]    Overall Loss 0.192785    Objective Loss 0.192785                                        LR 0.000305    Time 0.076886    
2022-01-29 05:29:40,165 - Epoch: [296][  391/  391]    Overall Loss 0.193185    Objective Loss 0.193185    Top1 96.634615    Top5 100.000000    LR 0.000305    Time 0.076529    
2022-01-29 05:29:40,224 - --- validate (epoch=296)-----------
2022-01-29 05:29:40,225 - 10000 samples (128 per mini-batch)
2022-01-29 05:29:43,754 - Epoch: [296][   79/   79]    Loss 1.297603    Top1 66.280000    Top5 89.620000    
2022-01-29 05:29:43,808 - ==> Top1: 66.280    Top5: 89.620    Loss: 1.298

2022-01-29 05:29:43,813 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:29:43,813 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:29:43,849 - 

2022-01-29 05:29:43,849 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:29:51,706 - Epoch: [297][  100/  391]    Overall Loss 0.193147    Objective Loss 0.193147                                        LR 0.000305    Time 0.078542    
2022-01-29 05:29:59,253 - Epoch: [297][  200/  391]    Overall Loss 0.191241    Objective Loss 0.191241                                        LR 0.000305    Time 0.077001    
2022-01-29 05:30:06,804 - Epoch: [297][  300/  391]    Overall Loss 0.193113    Objective Loss 0.193113                                        LR 0.000305    Time 0.076503    
2022-01-29 05:30:13,670 - Epoch: [297][  391/  391]    Overall Loss 0.194289    Objective Loss 0.194289    Top1 97.115385    Top5 100.000000    LR 0.000305    Time 0.076256    
2022-01-29 05:30:13,729 - --- validate (epoch=297)-----------
2022-01-29 05:30:13,730 - 10000 samples (128 per mini-batch)
2022-01-29 05:30:17,312 - Epoch: [297][   79/   79]    Loss 1.314337    Top1 65.710000    Top5 89.340000    
2022-01-29 05:30:17,364 - ==> Top1: 65.710    Top5: 89.340    Loss: 1.314

2022-01-29 05:30:17,370 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:30:17,370 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:30:17,406 - 

2022-01-29 05:30:17,407 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:30:25,299 - Epoch: [298][  100/  391]    Overall Loss 0.189155    Objective Loss 0.189155                                        LR 0.000305    Time 0.078898    
2022-01-29 05:30:32,909 - Epoch: [298][  200/  391]    Overall Loss 0.189852    Objective Loss 0.189852                                        LR 0.000305    Time 0.077496    
2022-01-29 05:30:40,506 - Epoch: [298][  300/  391]    Overall Loss 0.191286    Objective Loss 0.191286                                        LR 0.000305    Time 0.076984    
2022-01-29 05:30:47,422 - Epoch: [298][  391/  391]    Overall Loss 0.193158    Objective Loss 0.193158    Top1 97.596154    Top5 100.000000    LR 0.000305    Time 0.076753    
2022-01-29 05:30:47,481 - --- validate (epoch=298)-----------
2022-01-29 05:30:47,482 - 10000 samples (128 per mini-batch)
2022-01-29 05:30:51,027 - Epoch: [298][   79/   79]    Loss 1.285251    Top1 66.460000    Top5 89.760000    
2022-01-29 05:30:51,083 - ==> Top1: 66.460    Top5: 89.760    Loss: 1.285

2022-01-29 05:30:51,088 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:30:51,088 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:30:51,126 - 

2022-01-29 05:30:51,126 - Training epoch: 50000 samples (128 per mini-batch)
2022-01-29 05:30:58,933 - Epoch: [299][  100/  391]    Overall Loss 0.187897    Objective Loss 0.187897                                        LR 0.000305    Time 0.078039    
2022-01-29 05:31:06,510 - Epoch: [299][  200/  391]    Overall Loss 0.190292    Objective Loss 0.190292                                        LR 0.000305    Time 0.076903    
2022-01-29 05:31:14,087 - Epoch: [299][  300/  391]    Overall Loss 0.191831    Objective Loss 0.191831                                        LR 0.000305    Time 0.076522    
2022-01-29 05:31:20,978 - Epoch: [299][  391/  391]    Overall Loss 0.191806    Objective Loss 0.191806    Top1 97.596154    Top5 99.519231    LR 0.000305    Time 0.076333    
2022-01-29 05:31:21,038 - --- validate (epoch=299)-----------
2022-01-29 05:31:21,038 - 10000 samples (128 per mini-batch)
2022-01-29 05:31:24,579 - Epoch: [299][   79/   79]    Loss 1.292445    Top1 66.450000    Top5 89.740000    
2022-01-29 05:31:24,631 - ==> Top1: 66.450    Top5: 89.740    Loss: 1.292

2022-01-29 05:31:24,637 - ==> Best [Top1: 66.870   Top5: 89.900   Sparsity:0.00   Params: 1341960 on epoch: 250]
2022-01-29 05:31:24,637 - Saving checkpoint to: logs/2022.01.29-031718/qat_checkpoint.pth.tar
2022-01-29 05:31:24,673 - --- test ---------------------
2022-01-29 05:31:24,673 - 10000 samples (128 per mini-batch)
2022-01-29 05:31:28,225 - Test: [   79/   79]    Loss 1.295391    Top1 66.450000    Top5 89.740000    
2022-01-29 05:31:28,283 - ==> Top1: 66.450    Top5: 89.740    Loss: 1.295

2022-01-29 05:31:28,286 - 
2022-01-29 05:31:28,287 - Log file for this run: /home/ermanokman/repos/github/ai8x-training/logs/2022.01.29-031718/2022.01.29-031718.log
