2022-01-10 12:27:10,973 - Log file for this run: /home/gorkemulkar/Workspace/Python/AI8X_GitHub/ai8x-training/logs/2022.01.10-122710/2022.01.10-122710.log
2022-01-10 12:27:10,973 - Number of CPUs: 24
2022-01-10 12:27:10,990 - Number of GPUs: 1
2022-01-10 12:27:10,990 - CUDA version: 11.1
2022-01-10 12:27:10,990 - CUDNN version: 8005
2022-01-10 12:27:10,990 - Kernel: 5.4.0-90-generic
2022-01-10 12:27:10,991 - Python: 3.8.11 (default, Aug  3 2021, 15:09:35) 
[GCC 7.5.0]
2022-01-10 12:27:10,991 - pip freeze: {'absl-py': '0.13.0', 'aiohttp': '3.7.4', 'appdirs': '1.4.4', 'argon2-cffi': '20.1.0', 'astroid': '2.5', 'async-generator': '1.10', 'async-timeout': '3.0.1', 'atomicwrites': '1.4.0', 'attrs': '21.2.0', 'audioread': '2.1.9', 'backcall': '0.2.0', 'bleach': '3.3.0', 'blinker': '1.4', 'bottleneck': '1.3.2', 'bqplot': '0.11.5', 'brotlipy': '0.7.0', 'cachetools': '4.2.2', 'certifi': '2021.5.30', 'cffi': '1.14.6', 'chardet': '4.0.0', 'click': '8.0.1', 'cloudpickle': '1.6.0', 'colorama': '0.4.4', 'coverage': '5.5', 'cryptography': '3.4.7', 'cycler': '0.10.0', 'cython': '0.29.24', 'cytoolz': '0.11.0', 'dask': '2021.7.2', 'decorator': '5.0.9', 'defusedxml': '0.7.1', 'deprecated': '1.2.12', 'distiller': '0.4.0rc0', 'entrypoints': '0.3', 'fsspec': '2021.7.0', 'gitdb': '4.0.7', 'gitpython': '3.1.0', 'google-auth': '1.33.0', 'google-auth-oauthlib': '0.4.4', 'graphviz': '0.10.1', 'grpcio': '1.36.1', 'gym': '0.12.5', 'idna': '2.10', 'imageio': '2.9.0', 'importlib-metadata': '3.10.0', 'ipykernel': '5.5.3', 'ipython': '7.22.0', 'ipython-genutils': '0.2.0', 'ipywidgets': '7.4.2', 'isort': '5.9.2', 'jedi': '0.17.0', 'jinja2': '2.11.3', 'joblib': '1.0.1', 'jsonpatch': '1.32', 'jsonpointer': '2.1', 'jsonschema': '3.2.0', 'jupyter': '1.0.0', 'jupyter-client': '6.1.12', 'jupyter-console': '6.4.0', 'jupyter-core': '4.7.1', 'jupyterlab-pygments': '0.1.2', 'kiwisolver': '1.3.1', 'lazy-object-proxy': '1.6.0', 'librosa': '0.8.1', 'llvmlite': '0.36.0', 'locket': '0.2.1', 'markdown': '3.3.4', 'markupsafe': '1.1.1', 'matplotlib': '3.3.4', 'mccabe': '0.6.1', 'mistune': '0.8.4', 'mkl-fft': '1.3.0', 'mkl-random': '1.1.1', 'mkl-service': '2.3.0', 'more-itertools': '8.7.0', 'multidict': '5.1.0', 'munch': '2.5.0', 'nbclient': '0.5.3', 'nbconvert': '6.0.7', 'nbformat': '5.1.3', 'nest-asyncio': '1.5.1', 'networkx': '2.6.2', 'notebook': '6.3.0', 'numba': '0.53.1', 'numexpr': '2.7.3', 'numpy': '1.20.2', 'oauthlib': '3.1.1', 'olefile': '0.46', 'onnx': '1.10.1', 'opencv-python': '4.5.2.54', 'packaging': '21.0', 'pafy': '0.5.5', 'pandas': '1.3.1', 'pandocfilters': '1.4.3', 'parso': '0.8.2', 'partd': '1.2.0', 'pathspec': '0.8.1', 'pexpect': '4.8.0', 'pickleshare': '0.7.5', 'pillow': '8.3.1', 'pip': '21.2.2', 'pluggy': '0.13.1', 'pooch': '1.4.0', 'pretrainedmodels': '0.7.4', 'prometheus-client': '0.10.0', 'prompt-toolkit': '3.0.17', 'protobuf': '3.16.0', 'ptyprocess': '0.7.0', 'py': '1.10.0', 'pyasn1': '0.4.8', 'pyasn1-modules': '0.2.8', 'pycparser': '2.20', 'pydot': '1.4.1', 'pygithub': '1.55', 'pyglet': '1.5.15', 'pygments': '2.9.0', 'pyjwt': '2.1.0', 'pylint': '2.6.0', 'pynacl': '1.4.0', 'pyopenssl': '20.0.1', 'pyparsing': '2.4.7', 'pyqt5': '5.12.3', 'pyqt5-sip': '4.19.18', 'pyqtchart': '5.12', 'pyqtwebengine': '5.12.1', 'pyrsistent': '0.17.3', 'pysocks': '1.7.1', 'pytest': '4.6.11', 'python-dateutil': '2.8.2', 'pytsmod': '0.3.3', 'pytz': '2021.1', 'pywavelets': '1.1.1', 'pyyaml': '5.4.1', 'pyzmq': '22.0.3', 'qgrid': '1.1.1', 'qtconsole': '5.0.3', 'qtpy': '1.9.0', 'requests': '2.25.1', 'requests-oauthlib': '1.3.0', 'resampy': '0.2.2', 'rsa': '4.7.2', 'scikit-image': '0.18.1', 'scikit-learn': '0.23.2', 'scipy': '1.6.2', 'send2trash': '1.5.0', 'setuptools': '52.0.0.post20210125', 'shap': '0.37.0', 'six': '1.16.0', 'slicer': '0.0.7', 'smmap': '4.0.0', 'soundfile': '0.10.3.post1', 'tabulate': '0.8.3', 'tensorboard': '2.4.0', 'tensorboard-plugin-wit': '1.6.0', 'terminado': '0.9.4', 'testpath': '0.4.4', 'threadpoolctl': '2.2.0', 'tifffile': '2020.10.1', 'toml': '0.10.2', 'toolz': '0.11.1', 'torch': '1.8.1+cu111', 'torchaudio': '0.8.0a0+e4e171a', 'torchfile': '0.1.0', 'torchnet': '0.0.4', 'torchvision': '0.9.1+cu111', 'tornado': '6.1', 'tqdm': '4.33.0', 'traitlets': '5.0.5', 'traittypes': '0.2.1', 'typing-extensions': '3.10.0.0', 'urllib3': '1.26.6', 'visdom': '0.1.8.9', 'wcwidth': '0.2.5', 'webencodings': '0.5.1', 'websocket-client': '0.58.0', 'werkzeug': '1.0.1', 'wheel': '0.36.2', 'widgetsnbextension': '3.4.2', 'wrapt': '1.12.1', 'xlsxwriter': '1.3.8', 'yamllint': '1.26.1', 'yarl': '1.6.3', 'youtube-dl': '2021.6.6', 'zipp': '3.5.0'}
2022-01-10 12:27:10,991 - Command line: train.py --epochs 300 --optimizer Adam --lr 0.001 --compress schedule-cifar100.yaml --model ai85simplenet --dataset CIFAR100 --device MAX78000 --batch-size 100 --print-freq 250 --validation-split 0 --qat-policy qat_policy_cifar100.yaml --use-bias
2022-01-10 12:27:10,991 - Distiller: 0.4.0rc0
2022-01-10 12:27:12,763 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2022-01-10 12:27:12,763 - Optimizer Args: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}
2022-01-10 12:27:13,802 - Dataset sizes:
	training=50000
	validation=10000
	test=10000
2022-01-10 12:27:13,803 - Reading compression schedule from: schedule-cifar100.yaml
2022-01-10 12:27:13,804 - Schedule contents:
{
  "lr_schedulers": {
    "training_lr": {
      "class": "MultiStepLR",
      "milestones": [
        100,
        150,
        200
      ],
      "gamma": 0.25
    }
  },
  "policies": [
    {
      "lr_scheduler": {
        "instance_name": "training_lr"
      },
      "starting_epoch": 0,
      "ending_epoch": 250,
      "frequency": 1
    }
  ]
}
2022-01-10 12:27:13,806 - 

2022-01-10 12:27:13,806 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:27:17,574 - Epoch: [0][  250/  500]    Overall Loss 4.261852    Objective Loss 4.261852                                        LR 0.001000    Time 0.015063    
2022-01-10 12:27:20,754 - Epoch: [0][  500/  500]    Overall Loss 4.030863    Objective Loss 4.030863    Top1 16.000000    Top5 43.500000    LR 0.001000    Time 0.013888    
2022-01-10 12:27:20,809 - --- validate (epoch=0)-----------
2022-01-10 12:27:20,809 - 10000 samples (100 per mini-batch)
2022-01-10 12:27:21,515 - Epoch: [0][  100/  100]    Loss 3.632610    Top1 13.530000    Top5 38.080000    
2022-01-10 12:27:21,564 - ==> Top1: 13.530    Top5: 38.080    Loss: 3.633

2022-01-10 12:27:21,566 - ==> Best [Top1: 13.530   Top5: 38.080   Sparsity:0.00   Params: 381792 on epoch: 0]
2022-01-10 12:27:21,566 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:27:21,582 - 

2022-01-10 12:27:21,582 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:27:25,013 - Epoch: [1][  250/  500]    Overall Loss 3.489986    Objective Loss 3.489986                                        LR 0.001000    Time 0.013714    
2022-01-10 12:27:28,197 - Epoch: [1][  500/  500]    Overall Loss 3.374856    Objective Loss 3.374856    Top1 24.000000    Top5 57.000000    LR 0.001000    Time 0.013222    
2022-01-10 12:27:28,252 - --- validate (epoch=1)-----------
2022-01-10 12:27:28,252 - 10000 samples (100 per mini-batch)
2022-01-10 12:27:28,963 - Epoch: [1][  100/  100]    Loss 3.188602    Top1 20.980000    Top5 50.370000    
2022-01-10 12:27:29,015 - ==> Top1: 20.980    Top5: 50.370    Loss: 3.189

2022-01-10 12:27:29,017 - ==> Best [Top1: 20.980   Top5: 50.370   Sparsity:0.00   Params: 381792 on epoch: 1]
2022-01-10 12:27:29,017 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:27:29,043 - 

2022-01-10 12:27:29,043 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:27:32,700 - Epoch: [2][  250/  500]    Overall Loss 3.035343    Objective Loss 3.035343                                        LR 0.001000    Time 0.014620    
2022-01-10 12:27:36,008 - Epoch: [2][  500/  500]    Overall Loss 2.964811    Objective Loss 2.964811    Top1 23.000000    Top5 59.000000    LR 0.001000    Time 0.013921    
2022-01-10 12:27:36,064 - --- validate (epoch=2)-----------
2022-01-10 12:27:36,064 - 10000 samples (100 per mini-batch)
2022-01-10 12:27:36,821 - Epoch: [2][  100/  100]    Loss 2.890647    Top1 26.210000    Top5 58.110000    
2022-01-10 12:27:36,882 - ==> Top1: 26.210    Top5: 58.110    Loss: 2.891

2022-01-10 12:27:36,884 - ==> Best [Top1: 26.210   Top5: 58.110   Sparsity:0.00   Params: 381792 on epoch: 2]
2022-01-10 12:27:36,884 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:27:36,904 - 

2022-01-10 12:27:36,904 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:27:40,500 - Epoch: [3][  250/  500]    Overall Loss 2.735256    Objective Loss 2.735256                                        LR 0.001000    Time 0.014377    
2022-01-10 12:27:43,908 - Epoch: [3][  500/  500]    Overall Loss 2.693472    Objective Loss 2.693472    Top1 27.500000    Top5 62.000000    LR 0.001000    Time 0.014000    
2022-01-10 12:27:43,964 - --- validate (epoch=3)-----------
2022-01-10 12:27:43,965 - 10000 samples (100 per mini-batch)
2022-01-10 12:27:44,811 - Epoch: [3][  100/  100]    Loss 2.782680    Top1 27.820000    Top5 61.520000    
2022-01-10 12:27:44,869 - ==> Top1: 27.820    Top5: 61.520    Loss: 2.783

2022-01-10 12:27:44,871 - ==> Best [Top1: 27.820   Top5: 61.520   Sparsity:0.00   Params: 381792 on epoch: 3]
2022-01-10 12:27:44,871 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:27:44,890 - 

2022-01-10 12:27:44,891 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:27:48,477 - Epoch: [4][  250/  500]    Overall Loss 2.553336    Objective Loss 2.553336                                        LR 0.001000    Time 0.014334    
2022-01-10 12:27:52,048 - Epoch: [4][  500/  500]    Overall Loss 2.508883    Objective Loss 2.508883    Top1 36.000000    Top5 66.000000    LR 0.001000    Time 0.014305    
2022-01-10 12:27:52,096 - --- validate (epoch=4)-----------
2022-01-10 12:27:52,097 - 10000 samples (100 per mini-batch)
2022-01-10 12:27:52,846 - Epoch: [4][  100/  100]    Loss 2.540614    Top1 33.840000    Top5 66.030000    
2022-01-10 12:27:52,897 - ==> Top1: 33.840    Top5: 66.030    Loss: 2.541

2022-01-10 12:27:52,899 - ==> Best [Top1: 33.840   Top5: 66.030   Sparsity:0.00   Params: 381792 on epoch: 4]
2022-01-10 12:27:52,899 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:27:52,919 - 

2022-01-10 12:27:52,919 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:27:56,446 - Epoch: [5][  250/  500]    Overall Loss 2.386741    Objective Loss 2.386741                                        LR 0.001000    Time 0.014101    
2022-01-10 12:27:59,740 - Epoch: [5][  500/  500]    Overall Loss 2.369377    Objective Loss 2.369377    Top1 36.000000    Top5 66.500000    LR 0.001000    Time 0.013634    
2022-01-10 12:27:59,796 - --- validate (epoch=5)-----------
2022-01-10 12:27:59,796 - 10000 samples (100 per mini-batch)
2022-01-10 12:28:00,526 - Epoch: [5][  100/  100]    Loss 2.654760    Top1 30.910000    Top5 63.020000    
2022-01-10 12:28:00,577 - ==> Top1: 30.910    Top5: 63.020    Loss: 2.655

2022-01-10 12:28:00,578 - ==> Best [Top1: 33.840   Top5: 66.030   Sparsity:0.00   Params: 381792 on epoch: 4]
2022-01-10 12:28:00,578 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:28:00,600 - 

2022-01-10 12:28:00,600 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:28:04,161 - Epoch: [6][  250/  500]    Overall Loss 2.263997    Objective Loss 2.263997                                        LR 0.001000    Time 0.014234    
2022-01-10 12:28:07,811 - Epoch: [6][  500/  500]    Overall Loss 2.255507    Objective Loss 2.255507    Top1 43.000000    Top5 77.500000    LR 0.001000    Time 0.014411    
2022-01-10 12:28:07,860 - --- validate (epoch=6)-----------
2022-01-10 12:28:07,860 - 10000 samples (100 per mini-batch)
2022-01-10 12:28:08,633 - Epoch: [6][  100/  100]    Loss 2.378653    Top1 37.300000    Top5 69.820000    
2022-01-10 12:28:08,683 - ==> Top1: 37.300    Top5: 69.820    Loss: 2.379

2022-01-10 12:28:08,684 - ==> Best [Top1: 37.300   Top5: 69.820   Sparsity:0.00   Params: 381792 on epoch: 6]
2022-01-10 12:28:08,684 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:28:08,710 - 

2022-01-10 12:28:08,710 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:28:12,546 - Epoch: [7][  250/  500]    Overall Loss 2.186661    Objective Loss 2.186661                                        LR 0.001000    Time 0.015332    
2022-01-10 12:28:16,059 - Epoch: [7][  500/  500]    Overall Loss 2.164993    Objective Loss 2.164993    Top1 35.000000    Top5 69.500000    LR 0.001000    Time 0.014689    
2022-01-10 12:28:16,118 - --- validate (epoch=7)-----------
2022-01-10 12:28:16,118 - 10000 samples (100 per mini-batch)
2022-01-10 12:28:16,818 - Epoch: [7][  100/  100]    Loss 2.238720    Top1 40.260000    Top5 72.610000    
2022-01-10 12:28:16,871 - ==> Top1: 40.260    Top5: 72.610    Loss: 2.239

2022-01-10 12:28:16,872 - ==> Best [Top1: 40.260   Top5: 72.610   Sparsity:0.00   Params: 381792 on epoch: 7]
2022-01-10 12:28:16,872 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:28:16,898 - 

2022-01-10 12:28:16,898 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:28:20,487 - Epoch: [8][  250/  500]    Overall Loss 2.106600    Objective Loss 2.106600                                        LR 0.001000    Time 0.014345    
2022-01-10 12:28:23,803 - Epoch: [8][  500/  500]    Overall Loss 2.085876    Objective Loss 2.085876    Top1 47.000000    Top5 76.500000    LR 0.001000    Time 0.013800    
2022-01-10 12:28:23,857 - --- validate (epoch=8)-----------
2022-01-10 12:28:23,857 - 10000 samples (100 per mini-batch)
2022-01-10 12:28:24,603 - Epoch: [8][  100/  100]    Loss 2.160375    Top1 41.610000    Top5 74.610000    
2022-01-10 12:28:24,655 - ==> Top1: 41.610    Top5: 74.610    Loss: 2.160

2022-01-10 12:28:24,657 - ==> Best [Top1: 41.610   Top5: 74.610   Sparsity:0.00   Params: 381792 on epoch: 8]
2022-01-10 12:28:24,657 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:28:24,683 - 

2022-01-10 12:28:24,683 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:28:28,215 - Epoch: [9][  250/  500]    Overall Loss 2.019691    Objective Loss 2.019691                                        LR 0.001000    Time 0.014118    
2022-01-10 12:28:31,842 - Epoch: [9][  500/  500]    Overall Loss 2.014062    Objective Loss 2.014062    Top1 43.500000    Top5 78.500000    LR 0.001000    Time 0.014309    
2022-01-10 12:28:31,898 - --- validate (epoch=9)-----------
2022-01-10 12:28:31,898 - 10000 samples (100 per mini-batch)
2022-01-10 12:28:32,691 - Epoch: [9][  100/  100]    Loss 2.095855    Top1 43.160000    Top5 75.400000    
2022-01-10 12:28:32,742 - ==> Top1: 43.160    Top5: 75.400    Loss: 2.096

2022-01-10 12:28:32,743 - ==> Best [Top1: 43.160   Top5: 75.400   Sparsity:0.00   Params: 381792 on epoch: 9]
2022-01-10 12:28:32,743 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:28:32,763 - 

2022-01-10 12:28:32,763 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:28:36,386 - Epoch: [10][  250/  500]    Overall Loss 1.975275    Objective Loss 1.975275                                        LR 0.001000    Time 0.014481    
2022-01-10 12:28:39,799 - Epoch: [10][  500/  500]    Overall Loss 1.952652    Objective Loss 1.952652    Top1 44.500000    Top5 77.500000    LR 0.001000    Time 0.014062    
2022-01-10 12:28:39,852 - --- validate (epoch=10)-----------
2022-01-10 12:28:39,852 - 10000 samples (100 per mini-batch)
2022-01-10 12:28:40,558 - Epoch: [10][  100/  100]    Loss 2.056178    Top1 44.520000    Top5 76.390000    
2022-01-10 12:28:40,609 - ==> Top1: 44.520    Top5: 76.390    Loss: 2.056

2022-01-10 12:28:40,611 - ==> Best [Top1: 44.520   Top5: 76.390   Sparsity:0.00   Params: 381792 on epoch: 10]
2022-01-10 12:28:40,611 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:28:40,637 - 

2022-01-10 12:28:40,637 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:28:44,340 - Epoch: [11][  250/  500]    Overall Loss 1.895805    Objective Loss 1.895805                                        LR 0.001000    Time 0.014805    
2022-01-10 12:28:47,753 - Epoch: [11][  500/  500]    Overall Loss 1.901633    Objective Loss 1.901633    Top1 52.500000    Top5 85.000000    LR 0.001000    Time 0.014224    
2022-01-10 12:28:47,809 - --- validate (epoch=11)-----------
2022-01-10 12:28:47,810 - 10000 samples (100 per mini-batch)
2022-01-10 12:28:48,562 - Epoch: [11][  100/  100]    Loss 1.965756    Top1 46.240000    Top5 77.880000    
2022-01-10 12:28:48,619 - ==> Top1: 46.240    Top5: 77.880    Loss: 1.966

2022-01-10 12:28:48,621 - ==> Best [Top1: 46.240   Top5: 77.880   Sparsity:0.00   Params: 381792 on epoch: 11]
2022-01-10 12:28:48,621 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:28:48,646 - 

2022-01-10 12:28:48,647 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:28:52,183 - Epoch: [12][  250/  500]    Overall Loss 1.839222    Objective Loss 1.839222                                        LR 0.001000    Time 0.014138    
2022-01-10 12:28:55,470 - Epoch: [12][  500/  500]    Overall Loss 1.849780    Objective Loss 1.849780    Top1 43.500000    Top5 74.000000    LR 0.001000    Time 0.013638    
2022-01-10 12:28:55,518 - --- validate (epoch=12)-----------
2022-01-10 12:28:55,519 - 10000 samples (100 per mini-batch)
2022-01-10 12:28:56,382 - Epoch: [12][  100/  100]    Loss 1.957232    Top1 46.560000    Top5 77.690000    
2022-01-10 12:28:56,445 - ==> Top1: 46.560    Top5: 77.690    Loss: 1.957

2022-01-10 12:28:56,447 - ==> Best [Top1: 46.560   Top5: 77.690   Sparsity:0.00   Params: 381792 on epoch: 12]
2022-01-10 12:28:56,447 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:28:56,473 - 

2022-01-10 12:28:56,473 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:29:00,043 - Epoch: [13][  250/  500]    Overall Loss 1.788981    Objective Loss 1.788981                                        LR 0.001000    Time 0.014270    
2022-01-10 12:29:03,345 - Epoch: [13][  500/  500]    Overall Loss 1.802490    Objective Loss 1.802490    Top1 51.000000    Top5 84.000000    LR 0.001000    Time 0.013737    
2022-01-10 12:29:03,400 - --- validate (epoch=13)-----------
2022-01-10 12:29:03,401 - 10000 samples (100 per mini-batch)
2022-01-10 12:29:04,113 - Epoch: [13][  100/  100]    Loss 1.881998    Top1 48.490000    Top5 79.220000    
2022-01-10 12:29:04,166 - ==> Top1: 48.490    Top5: 79.220    Loss: 1.882

2022-01-10 12:29:04,168 - ==> Best [Top1: 48.490   Top5: 79.220   Sparsity:0.00   Params: 381792 on epoch: 13]
2022-01-10 12:29:04,168 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:29:04,193 - 

2022-01-10 12:29:04,193 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:29:07,793 - Epoch: [14][  250/  500]    Overall Loss 1.750956    Objective Loss 1.750956                                        LR 0.001000    Time 0.014387    
2022-01-10 12:29:11,166 - Epoch: [14][  500/  500]    Overall Loss 1.762750    Objective Loss 1.762750    Top1 55.000000    Top5 82.500000    LR 0.001000    Time 0.013937    
2022-01-10 12:29:11,215 - --- validate (epoch=14)-----------
2022-01-10 12:29:11,215 - 10000 samples (100 per mini-batch)
2022-01-10 12:29:11,928 - Epoch: [14][  100/  100]    Loss 1.918927    Top1 47.850000    Top5 79.160000    
2022-01-10 12:29:11,979 - ==> Top1: 47.850    Top5: 79.160    Loss: 1.919

2022-01-10 12:29:11,981 - ==> Best [Top1: 48.490   Top5: 79.220   Sparsity:0.00   Params: 381792 on epoch: 13]
2022-01-10 12:29:11,981 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:29:12,003 - 

2022-01-10 12:29:12,003 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:29:15,400 - Epoch: [15][  250/  500]    Overall Loss 1.715984    Objective Loss 1.715984                                        LR 0.001000    Time 0.013577    
2022-01-10 12:29:18,945 - Epoch: [15][  500/  500]    Overall Loss 1.724108    Objective Loss 1.724108    Top1 51.000000    Top5 80.500000    LR 0.001000    Time 0.013875    
2022-01-10 12:29:19,001 - --- validate (epoch=15)-----------
2022-01-10 12:29:19,001 - 10000 samples (100 per mini-batch)
2022-01-10 12:29:19,838 - Epoch: [15][  100/  100]    Loss 1.874610    Top1 49.300000    Top5 79.790000    
2022-01-10 12:29:19,888 - ==> Top1: 49.300    Top5: 79.790    Loss: 1.875

2022-01-10 12:29:19,890 - ==> Best [Top1: 49.300   Top5: 79.790   Sparsity:0.00   Params: 381792 on epoch: 15]
2022-01-10 12:29:19,890 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:29:19,915 - 

2022-01-10 12:29:19,916 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:29:23,448 - Epoch: [16][  250/  500]    Overall Loss 1.682188    Objective Loss 1.682188                                        LR 0.001000    Time 0.014118    
2022-01-10 12:29:26,675 - Epoch: [16][  500/  500]    Overall Loss 1.701433    Objective Loss 1.701433    Top1 50.000000    Top5 81.500000    LR 0.001000    Time 0.013510    
2022-01-10 12:29:26,725 - --- validate (epoch=16)-----------
2022-01-10 12:29:26,725 - 10000 samples (100 per mini-batch)
2022-01-10 12:29:27,460 - Epoch: [16][  100/  100]    Loss 1.802734    Top1 50.450000    Top5 80.960000    
2022-01-10 12:29:27,512 - ==> Top1: 50.450    Top5: 80.960    Loss: 1.803

2022-01-10 12:29:27,514 - ==> Best [Top1: 50.450   Top5: 80.960   Sparsity:0.00   Params: 381792 on epoch: 16]
2022-01-10 12:29:27,514 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:29:27,539 - 

2022-01-10 12:29:27,540 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:29:31,006 - Epoch: [17][  250/  500]    Overall Loss 1.653800    Objective Loss 1.653800                                        LR 0.001000    Time 0.013856    
2022-01-10 12:29:34,255 - Epoch: [17][  500/  500]    Overall Loss 1.656289    Objective Loss 1.656289    Top1 45.500000    Top5 78.500000    LR 0.001000    Time 0.013422    
2022-01-10 12:29:34,315 - --- validate (epoch=17)-----------
2022-01-10 12:29:34,315 - 10000 samples (100 per mini-batch)
2022-01-10 12:29:35,024 - Epoch: [17][  100/  100]    Loss 1.832567    Top1 49.810000    Top5 80.190000    
2022-01-10 12:29:35,075 - ==> Top1: 49.810    Top5: 80.190    Loss: 1.833

2022-01-10 12:29:35,077 - ==> Best [Top1: 50.450   Top5: 80.960   Sparsity:0.00   Params: 381792 on epoch: 16]
2022-01-10 12:29:35,077 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:29:35,099 - 

2022-01-10 12:29:35,099 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:29:38,558 - Epoch: [18][  250/  500]    Overall Loss 1.619390    Objective Loss 1.619390                                        LR 0.001000    Time 0.013829    
2022-01-10 12:29:41,821 - Epoch: [18][  500/  500]    Overall Loss 1.630090    Objective Loss 1.630090    Top1 57.000000    Top5 86.500000    LR 0.001000    Time 0.013437    
2022-01-10 12:29:41,876 - --- validate (epoch=18)-----------
2022-01-10 12:29:41,876 - 10000 samples (100 per mini-batch)
2022-01-10 12:29:42,630 - Epoch: [18][  100/  100]    Loss 1.946673    Top1 47.890000    Top5 77.810000    
2022-01-10 12:29:42,684 - ==> Top1: 47.890    Top5: 77.810    Loss: 1.947

2022-01-10 12:29:42,686 - ==> Best [Top1: 50.450   Top5: 80.960   Sparsity:0.00   Params: 381792 on epoch: 16]
2022-01-10 12:29:42,686 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:29:42,708 - 

2022-01-10 12:29:42,708 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:29:46,265 - Epoch: [19][  250/  500]    Overall Loss 1.597282    Objective Loss 1.597282                                        LR 0.001000    Time 0.014220    
2022-01-10 12:29:49,584 - Epoch: [19][  500/  500]    Overall Loss 1.605110    Objective Loss 1.605110    Top1 49.500000    Top5 87.000000    LR 0.001000    Time 0.013744    
2022-01-10 12:29:49,641 - --- validate (epoch=19)-----------
2022-01-10 12:29:49,642 - 10000 samples (100 per mini-batch)
2022-01-10 12:29:50,354 - Epoch: [19][  100/  100]    Loss 1.809205    Top1 50.710000    Top5 80.180000    
2022-01-10 12:29:50,403 - ==> Top1: 50.710    Top5: 80.180    Loss: 1.809

2022-01-10 12:29:50,405 - ==> Best [Top1: 50.710   Top5: 80.180   Sparsity:0.00   Params: 381792 on epoch: 19]
2022-01-10 12:29:50,405 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:29:50,431 - 

2022-01-10 12:29:50,431 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:29:54,023 - Epoch: [20][  250/  500]    Overall Loss 1.572536    Objective Loss 1.572536                                        LR 0.001000    Time 0.014357    
2022-01-10 12:29:57,337 - Epoch: [20][  500/  500]    Overall Loss 1.581244    Objective Loss 1.581244    Top1 54.000000    Top5 85.000000    LR 0.001000    Time 0.013803    
2022-01-10 12:29:57,386 - --- validate (epoch=20)-----------
2022-01-10 12:29:57,386 - 10000 samples (100 per mini-batch)
2022-01-10 12:29:58,104 - Epoch: [20][  100/  100]    Loss 1.766924    Top1 51.470000    Top5 81.310000    
2022-01-10 12:29:58,152 - ==> Top1: 51.470    Top5: 81.310    Loss: 1.767

2022-01-10 12:29:58,154 - ==> Best [Top1: 51.470   Top5: 81.310   Sparsity:0.00   Params: 381792 on epoch: 20]
2022-01-10 12:29:58,154 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:29:58,180 - 

2022-01-10 12:29:58,180 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:30:01,652 - Epoch: [21][  250/  500]    Overall Loss 1.544199    Objective Loss 1.544199                                        LR 0.001000    Time 0.013879    
2022-01-10 12:30:04,925 - Epoch: [21][  500/  500]    Overall Loss 1.557250    Objective Loss 1.557250    Top1 57.000000    Top5 88.000000    LR 0.001000    Time 0.013481    
2022-01-10 12:30:04,982 - --- validate (epoch=21)-----------
2022-01-10 12:30:04,982 - 10000 samples (100 per mini-batch)
2022-01-10 12:30:05,686 - Epoch: [21][  100/  100]    Loss 1.809397    Top1 50.530000    Top5 80.690000    
2022-01-10 12:30:05,738 - ==> Top1: 50.530    Top5: 80.690    Loss: 1.809

2022-01-10 12:30:05,740 - ==> Best [Top1: 51.470   Top5: 81.310   Sparsity:0.00   Params: 381792 on epoch: 20]
2022-01-10 12:30:05,740 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:30:05,755 - 

2022-01-10 12:30:05,755 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:30:09,235 - Epoch: [22][  250/  500]    Overall Loss 1.508936    Objective Loss 1.508936                                        LR 0.001000    Time 0.013912    
2022-01-10 12:30:12,528 - Epoch: [22][  500/  500]    Overall Loss 1.525920    Objective Loss 1.525920    Top1 54.000000    Top5 85.000000    LR 0.001000    Time 0.013538    
2022-01-10 12:30:12,583 - --- validate (epoch=22)-----------
2022-01-10 12:30:12,583 - 10000 samples (100 per mini-batch)
2022-01-10 12:30:13,291 - Epoch: [22][  100/  100]    Loss 1.764909    Top1 51.740000    Top5 81.570000    
2022-01-10 12:30:13,349 - ==> Top1: 51.740    Top5: 81.570    Loss: 1.765

2022-01-10 12:30:13,351 - ==> Best [Top1: 51.740   Top5: 81.570   Sparsity:0.00   Params: 381792 on epoch: 22]
2022-01-10 12:30:13,351 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:30:13,377 - 

2022-01-10 12:30:13,377 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:30:16,890 - Epoch: [23][  250/  500]    Overall Loss 1.493870    Objective Loss 1.493870                                        LR 0.001000    Time 0.014040    
2022-01-10 12:30:20,086 - Epoch: [23][  500/  500]    Overall Loss 1.508461    Objective Loss 1.508461    Top1 53.500000    Top5 84.500000    LR 0.001000    Time 0.013408    
2022-01-10 12:30:20,137 - --- validate (epoch=23)-----------
2022-01-10 12:30:20,137 - 10000 samples (100 per mini-batch)
2022-01-10 12:30:20,857 - Epoch: [23][  100/  100]    Loss 1.764471    Top1 51.390000    Top5 82.040000    
2022-01-10 12:30:20,915 - ==> Top1: 51.390    Top5: 82.040    Loss: 1.764

2022-01-10 12:30:20,916 - ==> Best [Top1: 51.740   Top5: 81.570   Sparsity:0.00   Params: 381792 on epoch: 22]
2022-01-10 12:30:20,916 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:30:20,938 - 

2022-01-10 12:30:20,939 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:30:24,304 - Epoch: [24][  250/  500]    Overall Loss 1.468167    Objective Loss 1.468167                                        LR 0.001000    Time 0.013451    
2022-01-10 12:30:27,506 - Epoch: [24][  500/  500]    Overall Loss 1.483082    Objective Loss 1.483082    Top1 57.000000    Top5 86.500000    LR 0.001000    Time 0.013126    
2022-01-10 12:30:27,553 - --- validate (epoch=24)-----------
2022-01-10 12:30:27,554 - 10000 samples (100 per mini-batch)
2022-01-10 12:30:28,330 - Epoch: [24][  100/  100]    Loss 1.736341    Top1 52.510000    Top5 81.860000    
2022-01-10 12:30:28,385 - ==> Top1: 52.510    Top5: 81.860    Loss: 1.736

2022-01-10 12:30:28,386 - ==> Best [Top1: 52.510   Top5: 81.860   Sparsity:0.00   Params: 381792 on epoch: 24]
2022-01-10 12:30:28,386 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:30:28,406 - 

2022-01-10 12:30:28,406 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:30:31,938 - Epoch: [25][  250/  500]    Overall Loss 1.450341    Objective Loss 1.450341                                        LR 0.001000    Time 0.014118    
2022-01-10 12:30:35,141 - Epoch: [25][  500/  500]    Overall Loss 1.467362    Objective Loss 1.467362    Top1 60.500000    Top5 84.500000    LR 0.001000    Time 0.013463    
2022-01-10 12:30:35,198 - --- validate (epoch=25)-----------
2022-01-10 12:30:35,198 - 10000 samples (100 per mini-batch)
2022-01-10 12:30:35,942 - Epoch: [25][  100/  100]    Loss 1.764837    Top1 51.840000    Top5 81.530000    
2022-01-10 12:30:35,992 - ==> Top1: 51.840    Top5: 81.530    Loss: 1.765

2022-01-10 12:30:35,994 - ==> Best [Top1: 52.510   Top5: 81.860   Sparsity:0.00   Params: 381792 on epoch: 24]
2022-01-10 12:30:35,994 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:30:36,008 - 

2022-01-10 12:30:36,008 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:30:39,694 - Epoch: [26][  250/  500]    Overall Loss 1.439090    Objective Loss 1.439090                                        LR 0.001000    Time 0.014734    
2022-01-10 12:30:43,068 - Epoch: [26][  500/  500]    Overall Loss 1.447820    Objective Loss 1.447820    Top1 61.500000    Top5 88.500000    LR 0.001000    Time 0.014111    
2022-01-10 12:30:43,116 - --- validate (epoch=26)-----------
2022-01-10 12:30:43,116 - 10000 samples (100 per mini-batch)
2022-01-10 12:30:43,852 - Epoch: [26][  100/  100]    Loss 1.705522    Top1 53.320000    Top5 82.630000    
2022-01-10 12:30:43,906 - ==> Top1: 53.320    Top5: 82.630    Loss: 1.706

2022-01-10 12:30:43,908 - ==> Best [Top1: 53.320   Top5: 82.630   Sparsity:0.00   Params: 381792 on epoch: 26]
2022-01-10 12:30:43,908 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:30:43,934 - 

2022-01-10 12:30:43,934 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:30:47,440 - Epoch: [27][  250/  500]    Overall Loss 1.401646    Objective Loss 1.401646                                        LR 0.001000    Time 0.014014    
2022-01-10 12:30:50,821 - Epoch: [27][  500/  500]    Overall Loss 1.422009    Objective Loss 1.422009    Top1 57.000000    Top5 90.500000    LR 0.001000    Time 0.013764    
2022-01-10 12:30:50,868 - --- validate (epoch=27)-----------
2022-01-10 12:30:50,869 - 10000 samples (100 per mini-batch)
2022-01-10 12:30:51,601 - Epoch: [27][  100/  100]    Loss 1.781225    Top1 51.250000    Top5 81.270000    
2022-01-10 12:30:51,661 - ==> Top1: 51.250    Top5: 81.270    Loss: 1.781

2022-01-10 12:30:51,663 - ==> Best [Top1: 53.320   Top5: 82.630   Sparsity:0.00   Params: 381792 on epoch: 26]
2022-01-10 12:30:51,663 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:30:51,679 - 

2022-01-10 12:30:51,679 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:30:55,198 - Epoch: [28][  250/  500]    Overall Loss 1.400906    Objective Loss 1.400906                                        LR 0.001000    Time 0.014065    
2022-01-10 12:30:58,564 - Epoch: [28][  500/  500]    Overall Loss 1.409872    Objective Loss 1.409872    Top1 66.000000    Top5 89.500000    LR 0.001000    Time 0.013762    
2022-01-10 12:30:58,613 - --- validate (epoch=28)-----------
2022-01-10 12:30:58,613 - 10000 samples (100 per mini-batch)
2022-01-10 12:30:59,364 - Epoch: [28][  100/  100]    Loss 1.707479    Top1 53.870000    Top5 82.580000    
2022-01-10 12:30:59,416 - ==> Top1: 53.870    Top5: 82.580    Loss: 1.707

2022-01-10 12:30:59,418 - ==> Best [Top1: 53.870   Top5: 82.580   Sparsity:0.00   Params: 381792 on epoch: 28]
2022-01-10 12:30:59,418 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:30:59,444 - 

2022-01-10 12:30:59,444 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:31:03,031 - Epoch: [29][  250/  500]    Overall Loss 1.383313    Objective Loss 1.383313                                        LR 0.001000    Time 0.014336    
2022-01-10 12:31:06,480 - Epoch: [29][  500/  500]    Overall Loss 1.396644    Objective Loss 1.396644    Top1 58.000000    Top5 88.000000    LR 0.001000    Time 0.014062    
2022-01-10 12:31:06,530 - --- validate (epoch=29)-----------
2022-01-10 12:31:06,530 - 10000 samples (100 per mini-batch)
2022-01-10 12:31:07,254 - Epoch: [29][  100/  100]    Loss 1.671078    Top1 53.980000    Top5 83.170000    
2022-01-10 12:31:07,304 - ==> Top1: 53.980    Top5: 83.170    Loss: 1.671

2022-01-10 12:31:07,306 - ==> Best [Top1: 53.980   Top5: 83.170   Sparsity:0.00   Params: 381792 on epoch: 29]
2022-01-10 12:31:07,306 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:31:07,332 - 

2022-01-10 12:31:07,332 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:31:10,823 - Epoch: [30][  250/  500]    Overall Loss 1.367907    Objective Loss 1.367907                                        LR 0.001000    Time 0.013952    
2022-01-10 12:31:14,040 - Epoch: [30][  500/  500]    Overall Loss 1.382042    Objective Loss 1.382042    Top1 62.500000    Top5 90.000000    LR 0.001000    Time 0.013408    
2022-01-10 12:31:14,089 - --- validate (epoch=30)-----------
2022-01-10 12:31:14,090 - 10000 samples (100 per mini-batch)
2022-01-10 12:31:14,880 - Epoch: [30][  100/  100]    Loss 1.706351    Top1 53.730000    Top5 82.450000    
2022-01-10 12:31:14,940 - ==> Top1: 53.730    Top5: 82.450    Loss: 1.706

2022-01-10 12:31:14,942 - ==> Best [Top1: 53.980   Top5: 83.170   Sparsity:0.00   Params: 381792 on epoch: 29]
2022-01-10 12:31:14,942 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:31:14,964 - 

2022-01-10 12:31:14,964 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:31:18,501 - Epoch: [31][  250/  500]    Overall Loss 1.345272    Objective Loss 1.345272                                        LR 0.001000    Time 0.014137    
2022-01-10 12:31:21,722 - Epoch: [31][  500/  500]    Overall Loss 1.357880    Objective Loss 1.357880    Top1 64.500000    Top5 91.500000    LR 0.001000    Time 0.013506    
2022-01-10 12:31:21,778 - --- validate (epoch=31)-----------
2022-01-10 12:31:21,779 - 10000 samples (100 per mini-batch)
2022-01-10 12:31:22,532 - Epoch: [31][  100/  100]    Loss 1.670113    Top1 54.340000    Top5 83.480000    
2022-01-10 12:31:22,586 - ==> Top1: 54.340    Top5: 83.480    Loss: 1.670

2022-01-10 12:31:22,588 - ==> Best [Top1: 54.340   Top5: 83.480   Sparsity:0.00   Params: 381792 on epoch: 31]
2022-01-10 12:31:22,588 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:31:22,607 - 

2022-01-10 12:31:22,607 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:31:26,159 - Epoch: [32][  250/  500]    Overall Loss 1.320390    Objective Loss 1.320390                                        LR 0.001000    Time 0.014200    
2022-01-10 12:31:29,480 - Epoch: [32][  500/  500]    Overall Loss 1.346487    Objective Loss 1.346487    Top1 61.000000    Top5 86.000000    LR 0.001000    Time 0.013738    
2022-01-10 12:31:29,537 - --- validate (epoch=32)-----------
2022-01-10 12:31:29,537 - 10000 samples (100 per mini-batch)
2022-01-10 12:31:30,269 - Epoch: [32][  100/  100]    Loss 1.644634    Top1 54.710000    Top5 83.390000    
2022-01-10 12:31:30,320 - ==> Top1: 54.710    Top5: 83.390    Loss: 1.645

2022-01-10 12:31:30,322 - ==> Best [Top1: 54.710   Top5: 83.390   Sparsity:0.00   Params: 381792 on epoch: 32]
2022-01-10 12:31:30,322 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:31:30,348 - 

2022-01-10 12:31:30,348 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:31:33,933 - Epoch: [33][  250/  500]    Overall Loss 1.316190    Objective Loss 1.316190                                        LR 0.001000    Time 0.014328    
2022-01-10 12:31:37,351 - Epoch: [33][  500/  500]    Overall Loss 1.334771    Objective Loss 1.334771    Top1 57.000000    Top5 89.500000    LR 0.001000    Time 0.013996    
2022-01-10 12:31:37,399 - --- validate (epoch=33)-----------
2022-01-10 12:31:37,399 - 10000 samples (100 per mini-batch)
2022-01-10 12:31:38,126 - Epoch: [33][  100/  100]    Loss 1.699406    Top1 53.540000    Top5 82.280000    
2022-01-10 12:31:38,175 - ==> Top1: 53.540    Top5: 82.280    Loss: 1.699

2022-01-10 12:31:38,177 - ==> Best [Top1: 54.710   Top5: 83.390   Sparsity:0.00   Params: 381792 on epoch: 32]
2022-01-10 12:31:38,177 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:31:38,199 - 

2022-01-10 12:31:38,199 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:31:41,732 - Epoch: [34][  250/  500]    Overall Loss 1.296606    Objective Loss 1.296606                                        LR 0.001000    Time 0.014120    
2022-01-10 12:31:44,982 - Epoch: [34][  500/  500]    Overall Loss 1.314563    Objective Loss 1.314563    Top1 55.500000    Top5 86.500000    LR 0.001000    Time 0.013556    
2022-01-10 12:31:45,039 - --- validate (epoch=34)-----------
2022-01-10 12:31:45,039 - 10000 samples (100 per mini-batch)
2022-01-10 12:31:45,783 - Epoch: [34][  100/  100]    Loss 1.635267    Top1 54.880000    Top5 83.790000    
2022-01-10 12:31:45,837 - ==> Top1: 54.880    Top5: 83.790    Loss: 1.635

2022-01-10 12:31:45,839 - ==> Best [Top1: 54.880   Top5: 83.790   Sparsity:0.00   Params: 381792 on epoch: 34]
2022-01-10 12:31:45,839 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:31:45,858 - 

2022-01-10 12:31:45,858 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:31:49,268 - Epoch: [35][  250/  500]    Overall Loss 1.274080    Objective Loss 1.274080                                        LR 0.001000    Time 0.013629    
2022-01-10 12:31:52,483 - Epoch: [35][  500/  500]    Overall Loss 1.296543    Objective Loss 1.296543    Top1 57.500000    Top5 84.500000    LR 0.001000    Time 0.013239    
2022-01-10 12:31:52,530 - --- validate (epoch=35)-----------
2022-01-10 12:31:52,531 - 10000 samples (100 per mini-batch)
2022-01-10 12:31:53,373 - Epoch: [35][  100/  100]    Loss 1.701072    Top1 53.550000    Top5 82.980000    
2022-01-10 12:31:53,422 - ==> Top1: 53.550    Top5: 82.980    Loss: 1.701

2022-01-10 12:31:53,424 - ==> Best [Top1: 54.880   Top5: 83.790   Sparsity:0.00   Params: 381792 on epoch: 34]
2022-01-10 12:31:53,424 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:31:53,446 - 

2022-01-10 12:31:53,446 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:31:56,934 - Epoch: [36][  250/  500]    Overall Loss 1.266178    Objective Loss 1.266178                                        LR 0.001000    Time 0.013940    
2022-01-10 12:32:00,160 - Epoch: [36][  500/  500]    Overall Loss 1.285420    Objective Loss 1.285420    Top1 69.000000    Top5 91.500000    LR 0.001000    Time 0.013408    
2022-01-10 12:32:00,217 - --- validate (epoch=36)-----------
2022-01-10 12:32:00,217 - 10000 samples (100 per mini-batch)
2022-01-10 12:32:00,937 - Epoch: [36][  100/  100]    Loss 1.680309    Top1 54.270000    Top5 83.030000    
2022-01-10 12:32:00,988 - ==> Top1: 54.270    Top5: 83.030    Loss: 1.680

2022-01-10 12:32:00,990 - ==> Best [Top1: 54.880   Top5: 83.790   Sparsity:0.00   Params: 381792 on epoch: 34]
2022-01-10 12:32:00,990 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:32:01,012 - 

2022-01-10 12:32:01,012 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:32:04,545 - Epoch: [37][  250/  500]    Overall Loss 1.259982    Objective Loss 1.259982                                        LR 0.001000    Time 0.014121    
2022-01-10 12:32:07,844 - Epoch: [37][  500/  500]    Overall Loss 1.274949    Objective Loss 1.274949    Top1 63.000000    Top5 89.500000    LR 0.001000    Time 0.013654    
2022-01-10 12:32:07,899 - --- validate (epoch=37)-----------
2022-01-10 12:32:07,899 - 10000 samples (100 per mini-batch)
2022-01-10 12:32:08,607 - Epoch: [37][  100/  100]    Loss 1.640069    Top1 54.890000    Top5 83.650000    
2022-01-10 12:32:08,661 - ==> Top1: 54.890    Top5: 83.650    Loss: 1.640

2022-01-10 12:32:08,662 - ==> Best [Top1: 54.890   Top5: 83.650   Sparsity:0.00   Params: 381792 on epoch: 37]
2022-01-10 12:32:08,662 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:32:08,682 - 

2022-01-10 12:32:08,682 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:32:12,137 - Epoch: [38][  250/  500]    Overall Loss 1.241267    Objective Loss 1.241267                                        LR 0.001000    Time 0.013808    
2022-01-10 12:32:15,381 - Epoch: [38][  500/  500]    Overall Loss 1.258516    Objective Loss 1.258516    Top1 66.000000    Top5 87.500000    LR 0.001000    Time 0.013389    
2022-01-10 12:32:15,434 - --- validate (epoch=38)-----------
2022-01-10 12:32:15,434 - 10000 samples (100 per mini-batch)
2022-01-10 12:32:16,267 - Epoch: [38][  100/  100]    Loss 1.681244    Top1 54.260000    Top5 83.190000    
2022-01-10 12:32:16,320 - ==> Top1: 54.260    Top5: 83.190    Loss: 1.681

2022-01-10 12:32:16,322 - ==> Best [Top1: 54.890   Top5: 83.650   Sparsity:0.00   Params: 381792 on epoch: 37]
2022-01-10 12:32:16,322 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:32:16,344 - 

2022-01-10 12:32:16,345 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:32:19,839 - Epoch: [39][  250/  500]    Overall Loss 1.244694    Objective Loss 1.244694                                        LR 0.001000    Time 0.013968    
2022-01-10 12:32:23,105 - Epoch: [39][  500/  500]    Overall Loss 1.253048    Objective Loss 1.253048    Top1 66.500000    Top5 91.000000    LR 0.001000    Time 0.013512    
2022-01-10 12:32:23,154 - --- validate (epoch=39)-----------
2022-01-10 12:32:23,154 - 10000 samples (100 per mini-batch)
2022-01-10 12:32:23,869 - Epoch: [39][  100/  100]    Loss 1.621430    Top1 55.580000    Top5 83.900000    
2022-01-10 12:32:23,923 - ==> Top1: 55.580    Top5: 83.900    Loss: 1.621

2022-01-10 12:32:23,925 - ==> Best [Top1: 55.580   Top5: 83.900   Sparsity:0.00   Params: 381792 on epoch: 39]
2022-01-10 12:32:23,925 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:32:23,951 - 

2022-01-10 12:32:23,951 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:32:27,532 - Epoch: [40][  250/  500]    Overall Loss 1.219239    Objective Loss 1.219239                                        LR 0.001000    Time 0.014314    
2022-01-10 12:32:30,840 - Epoch: [40][  500/  500]    Overall Loss 1.229735    Objective Loss 1.229735    Top1 58.000000    Top5 88.500000    LR 0.001000    Time 0.013770    
2022-01-10 12:32:30,888 - --- validate (epoch=40)-----------
2022-01-10 12:32:30,888 - 10000 samples (100 per mini-batch)
2022-01-10 12:32:31,693 - Epoch: [40][  100/  100]    Loss 1.661693    Top1 54.380000    Top5 83.480000    
2022-01-10 12:32:31,753 - ==> Top1: 54.380    Top5: 83.480    Loss: 1.662

2022-01-10 12:32:31,755 - ==> Best [Top1: 55.580   Top5: 83.900   Sparsity:0.00   Params: 381792 on epoch: 39]
2022-01-10 12:32:31,755 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:32:31,770 - 

2022-01-10 12:32:31,771 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:32:35,317 - Epoch: [41][  250/  500]    Overall Loss 1.215638    Objective Loss 1.215638                                        LR 0.001000    Time 0.014175    
2022-01-10 12:32:38,593 - Epoch: [41][  500/  500]    Overall Loss 1.219443    Objective Loss 1.219443    Top1 63.000000    Top5 89.500000    LR 0.001000    Time 0.013636    
2022-01-10 12:32:38,649 - --- validate (epoch=41)-----------
2022-01-10 12:32:38,650 - 10000 samples (100 per mini-batch)
2022-01-10 12:32:39,451 - Epoch: [41][  100/  100]    Loss 1.617707    Top1 55.500000    Top5 84.200000    
2022-01-10 12:32:39,502 - ==> Top1: 55.500    Top5: 84.200    Loss: 1.618

2022-01-10 12:32:39,504 - ==> Best [Top1: 55.580   Top5: 83.900   Sparsity:0.00   Params: 381792 on epoch: 39]
2022-01-10 12:32:39,504 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:32:39,526 - 

2022-01-10 12:32:39,527 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:32:43,106 - Epoch: [42][  250/  500]    Overall Loss 1.183261    Objective Loss 1.183261                                        LR 0.001000    Time 0.014308    
2022-01-10 12:32:46,648 - Epoch: [42][  500/  500]    Overall Loss 1.205685    Objective Loss 1.205685    Top1 68.500000    Top5 94.000000    LR 0.001000    Time 0.014233    
2022-01-10 12:32:46,706 - --- validate (epoch=42)-----------
2022-01-10 12:32:46,706 - 10000 samples (100 per mini-batch)
2022-01-10 12:32:47,471 - Epoch: [42][  100/  100]    Loss 1.600306    Top1 55.800000    Top5 84.460000    
2022-01-10 12:32:47,529 - ==> Top1: 55.800    Top5: 84.460    Loss: 1.600

2022-01-10 12:32:47,531 - ==> Best [Top1: 55.800   Top5: 84.460   Sparsity:0.00   Params: 381792 on epoch: 42]
2022-01-10 12:32:47,531 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:32:47,557 - 

2022-01-10 12:32:47,558 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:32:52,678 - Epoch: [43][  250/  500]    Overall Loss 1.172288    Objective Loss 1.172288                                        LR 0.001000    Time 0.020473    
2022-01-10 12:33:00,103 - Epoch: [43][  500/  500]    Overall Loss 1.194149    Objective Loss 1.194149    Top1 66.000000    Top5 90.000000    LR 0.001000    Time 0.025083    
2022-01-10 12:33:00,152 - --- validate (epoch=43)-----------
2022-01-10 12:33:00,152 - 10000 samples (100 per mini-batch)
2022-01-10 12:33:02,201 - Epoch: [43][  100/  100]    Loss 1.622667    Top1 56.120000    Top5 84.030000    
2022-01-10 12:33:02,256 - ==> Top1: 56.120    Top5: 84.030    Loss: 1.623

2022-01-10 12:33:02,267 - ==> Best [Top1: 56.120   Top5: 84.030   Sparsity:0.00   Params: 381792 on epoch: 43]
2022-01-10 12:33:02,267 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:33:02,294 - 

2022-01-10 12:33:02,294 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:33:09,812 - Epoch: [44][  250/  500]    Overall Loss 1.167662    Objective Loss 1.167662                                        LR 0.001000    Time 0.030064    
2022-01-10 12:33:17,206 - Epoch: [44][  500/  500]    Overall Loss 1.188058    Objective Loss 1.188058    Top1 71.500000    Top5 91.000000    LR 0.001000    Time 0.029814    
2022-01-10 12:33:17,262 - --- validate (epoch=44)-----------
2022-01-10 12:33:17,263 - 10000 samples (100 per mini-batch)
2022-01-10 12:33:19,312 - Epoch: [44][  100/  100]    Loss 1.629530    Top1 55.820000    Top5 84.020000    
2022-01-10 12:33:19,371 - ==> Top1: 55.820    Top5: 84.020    Loss: 1.630

2022-01-10 12:33:19,380 - ==> Best [Top1: 56.120   Top5: 84.030   Sparsity:0.00   Params: 381792 on epoch: 43]
2022-01-10 12:33:19,380 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:33:19,462 - 

2022-01-10 12:33:19,462 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:33:24,909 - Epoch: [45][  250/  500]    Overall Loss 1.154190    Objective Loss 1.154190                                        LR 0.001000    Time 0.021777    
2022-01-10 12:33:28,207 - Epoch: [45][  500/  500]    Overall Loss 1.174664    Objective Loss 1.174664    Top1 65.500000    Top5 91.000000    LR 0.001000    Time 0.017481    
2022-01-10 12:33:28,264 - --- validate (epoch=45)-----------
2022-01-10 12:33:28,264 - 10000 samples (100 per mini-batch)
2022-01-10 12:33:28,992 - Epoch: [45][  100/  100]    Loss 1.653247    Top1 55.010000    Top5 83.930000    
2022-01-10 12:33:29,042 - ==> Top1: 55.010    Top5: 83.930    Loss: 1.653

2022-01-10 12:33:29,044 - ==> Best [Top1: 56.120   Top5: 84.030   Sparsity:0.00   Params: 381792 on epoch: 43]
2022-01-10 12:33:29,044 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:33:29,067 - 

2022-01-10 12:33:29,067 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:33:32,619 - Epoch: [46][  250/  500]    Overall Loss 1.137597    Objective Loss 1.137597                                        LR 0.001000    Time 0.014198    
2022-01-10 12:33:35,931 - Epoch: [46][  500/  500]    Overall Loss 1.164682    Objective Loss 1.164682    Top1 63.000000    Top5 88.500000    LR 0.001000    Time 0.013719    
2022-01-10 12:33:35,980 - --- validate (epoch=46)-----------
2022-01-10 12:33:35,980 - 10000 samples (100 per mini-batch)
2022-01-10 12:33:36,734 - Epoch: [46][  100/  100]    Loss 1.623464    Top1 55.920000    Top5 84.200000    
2022-01-10 12:33:36,790 - ==> Top1: 55.920    Top5: 84.200    Loss: 1.623

2022-01-10 12:33:36,791 - ==> Best [Top1: 56.120   Top5: 84.030   Sparsity:0.00   Params: 381792 on epoch: 43]
2022-01-10 12:33:36,791 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:33:36,807 - 

2022-01-10 12:33:36,808 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:33:40,318 - Epoch: [47][  250/  500]    Overall Loss 1.132714    Objective Loss 1.132714                                        LR 0.001000    Time 0.014034    
2022-01-10 12:33:43,619 - Epoch: [47][  500/  500]    Overall Loss 1.150056    Objective Loss 1.150056    Top1 65.000000    Top5 92.000000    LR 0.001000    Time 0.013614    
2022-01-10 12:33:43,667 - --- validate (epoch=47)-----------
2022-01-10 12:33:43,667 - 10000 samples (100 per mini-batch)
2022-01-10 12:33:44,399 - Epoch: [47][  100/  100]    Loss 1.623000    Top1 55.860000    Top5 84.250000    
2022-01-10 12:33:44,448 - ==> Top1: 55.860    Top5: 84.250    Loss: 1.623

2022-01-10 12:33:44,449 - ==> Best [Top1: 56.120   Top5: 84.030   Sparsity:0.00   Params: 381792 on epoch: 43]
2022-01-10 12:33:44,450 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:33:44,472 - 

2022-01-10 12:33:44,472 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:33:48,041 - Epoch: [48][  250/  500]    Overall Loss 1.129651    Objective Loss 1.129651                                        LR 0.001000    Time 0.014265    
2022-01-10 12:33:51,368 - Epoch: [48][  500/  500]    Overall Loss 1.145129    Objective Loss 1.145129    Top1 68.000000    Top5 93.000000    LR 0.001000    Time 0.013782    
2022-01-10 12:33:51,417 - --- validate (epoch=48)-----------
2022-01-10 12:33:51,417 - 10000 samples (100 per mini-batch)
2022-01-10 12:33:52,174 - Epoch: [48][  100/  100]    Loss 1.610292    Top1 55.780000    Top5 84.530000    
2022-01-10 12:33:52,225 - ==> Top1: 55.780    Top5: 84.530    Loss: 1.610

2022-01-10 12:33:52,227 - ==> Best [Top1: 56.120   Top5: 84.030   Sparsity:0.00   Params: 381792 on epoch: 43]
2022-01-10 12:33:52,227 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:33:52,249 - 

2022-01-10 12:33:52,249 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:33:55,806 - Epoch: [49][  250/  500]    Overall Loss 1.101069    Objective Loss 1.101069                                        LR 0.001000    Time 0.014216    
2022-01-10 12:33:59,104 - Epoch: [49][  500/  500]    Overall Loss 1.132299    Objective Loss 1.132299    Top1 60.500000    Top5 91.500000    LR 0.001000    Time 0.013701    
2022-01-10 12:33:59,161 - --- validate (epoch=49)-----------
2022-01-10 12:33:59,162 - 10000 samples (100 per mini-batch)
2022-01-10 12:33:59,915 - Epoch: [49][  100/  100]    Loss 1.682214    Top1 54.590000    Top5 83.380000    
2022-01-10 12:33:59,963 - ==> Top1: 54.590    Top5: 83.380    Loss: 1.682

2022-01-10 12:33:59,965 - ==> Best [Top1: 56.120   Top5: 84.030   Sparsity:0.00   Params: 381792 on epoch: 43]
2022-01-10 12:33:59,965 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:33:59,987 - 

2022-01-10 12:33:59,988 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:34:03,506 - Epoch: [50][  250/  500]    Overall Loss 1.114616    Objective Loss 1.114616                                        LR 0.001000    Time 0.014065    
2022-01-10 12:34:06,828 - Epoch: [50][  500/  500]    Overall Loss 1.125257    Objective Loss 1.125257    Top1 63.500000    Top5 91.500000    LR 0.001000    Time 0.013671    
2022-01-10 12:34:06,882 - --- validate (epoch=50)-----------
2022-01-10 12:34:06,882 - 10000 samples (100 per mini-batch)
2022-01-10 12:34:07,585 - Epoch: [50][  100/  100]    Loss 1.659125    Top1 55.110000    Top5 83.890000    
2022-01-10 12:34:07,642 - ==> Top1: 55.110    Top5: 83.890    Loss: 1.659

2022-01-10 12:34:07,643 - ==> Best [Top1: 56.120   Top5: 84.030   Sparsity:0.00   Params: 381792 on epoch: 43]
2022-01-10 12:34:07,643 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:34:07,666 - 

2022-01-10 12:34:07,666 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:34:11,431 - Epoch: [51][  250/  500]    Overall Loss 1.093478    Objective Loss 1.093478                                        LR 0.001000    Time 0.015048    
2022-01-10 12:34:14,748 - Epoch: [51][  500/  500]    Overall Loss 1.116436    Objective Loss 1.116436    Top1 65.500000    Top5 88.500000    LR 0.001000    Time 0.014156    
2022-01-10 12:34:14,797 - --- validate (epoch=51)-----------
2022-01-10 12:34:14,797 - 10000 samples (100 per mini-batch)
2022-01-10 12:34:15,509 - Epoch: [51][  100/  100]    Loss 1.568650    Top1 57.220000    Top5 85.010000    
2022-01-10 12:34:15,560 - ==> Top1: 57.220    Top5: 85.010    Loss: 1.569

2022-01-10 12:34:15,562 - ==> Best [Top1: 57.220   Top5: 85.010   Sparsity:0.00   Params: 381792 on epoch: 51]
2022-01-10 12:34:15,562 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:34:15,587 - 

2022-01-10 12:34:15,588 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:34:19,110 - Epoch: [52][  250/  500]    Overall Loss 1.095380    Objective Loss 1.095380                                        LR 0.001000    Time 0.014082    
2022-01-10 12:34:22,374 - Epoch: [52][  500/  500]    Overall Loss 1.107688    Objective Loss 1.107688    Top1 68.500000    Top5 94.500000    LR 0.001000    Time 0.013564    
2022-01-10 12:34:22,421 - --- validate (epoch=52)-----------
2022-01-10 12:34:22,422 - 10000 samples (100 per mini-batch)
2022-01-10 12:34:23,124 - Epoch: [52][  100/  100]    Loss 1.618132    Top1 57.030000    Top5 84.320000    
2022-01-10 12:34:23,179 - ==> Top1: 57.030    Top5: 84.320    Loss: 1.618

2022-01-10 12:34:23,180 - ==> Best [Top1: 57.220   Top5: 85.010   Sparsity:0.00   Params: 381792 on epoch: 51]
2022-01-10 12:34:23,180 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:34:23,203 - 

2022-01-10 12:34:23,203 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:34:26,815 - Epoch: [53][  250/  500]    Overall Loss 1.092632    Objective Loss 1.092632                                        LR 0.001000    Time 0.014439    
2022-01-10 12:34:30,229 - Epoch: [53][  500/  500]    Overall Loss 1.100141    Objective Loss 1.100141    Top1 72.500000    Top5 93.500000    LR 0.001000    Time 0.014044    
2022-01-10 12:34:30,278 - --- validate (epoch=53)-----------
2022-01-10 12:34:30,278 - 10000 samples (100 per mini-batch)
2022-01-10 12:34:30,979 - Epoch: [53][  100/  100]    Loss 1.668989    Top1 55.700000    Top5 83.590000    
2022-01-10 12:34:31,030 - ==> Top1: 55.700    Top5: 83.590    Loss: 1.669

2022-01-10 12:34:31,031 - ==> Best [Top1: 57.220   Top5: 85.010   Sparsity:0.00   Params: 381792 on epoch: 51]
2022-01-10 12:34:31,032 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:34:31,054 - 

2022-01-10 12:34:31,054 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:34:34,605 - Epoch: [54][  250/  500]    Overall Loss 1.062271    Objective Loss 1.062271                                        LR 0.001000    Time 0.014195    
2022-01-10 12:34:38,088 - Epoch: [54][  500/  500]    Overall Loss 1.085685    Objective Loss 1.085685    Top1 68.500000    Top5 93.000000    LR 0.001000    Time 0.014060    
2022-01-10 12:34:38,143 - --- validate (epoch=54)-----------
2022-01-10 12:34:38,143 - 10000 samples (100 per mini-batch)
2022-01-10 12:34:38,892 - Epoch: [54][  100/  100]    Loss 1.615751    Top1 56.650000    Top5 84.570000    
2022-01-10 12:34:38,941 - ==> Top1: 56.650    Top5: 84.570    Loss: 1.616

2022-01-10 12:34:38,943 - ==> Best [Top1: 57.220   Top5: 85.010   Sparsity:0.00   Params: 381792 on epoch: 51]
2022-01-10 12:34:38,943 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:34:38,966 - 

2022-01-10 12:34:38,966 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:34:42,783 - Epoch: [55][  250/  500]    Overall Loss 1.071628    Objective Loss 1.071628                                        LR 0.001000    Time 0.015261    
2022-01-10 12:34:46,576 - Epoch: [55][  500/  500]    Overall Loss 1.084473    Objective Loss 1.084473    Top1 69.500000    Top5 91.500000    LR 0.001000    Time 0.015212    
2022-01-10 12:34:46,633 - --- validate (epoch=55)-----------
2022-01-10 12:34:46,633 - 10000 samples (100 per mini-batch)
2022-01-10 12:34:47,369 - Epoch: [55][  100/  100]    Loss 1.607427    Top1 56.470000    Top5 84.660000    
2022-01-10 12:34:47,422 - ==> Top1: 56.470    Top5: 84.660    Loss: 1.607

2022-01-10 12:34:47,423 - ==> Best [Top1: 57.220   Top5: 85.010   Sparsity:0.00   Params: 381792 on epoch: 51]
2022-01-10 12:34:47,423 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:34:47,445 - 

2022-01-10 12:34:47,446 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:34:50,914 - Epoch: [56][  250/  500]    Overall Loss 1.067655    Objective Loss 1.067655                                        LR 0.001000    Time 0.013863    
2022-01-10 12:34:54,198 - Epoch: [56][  500/  500]    Overall Loss 1.068235    Objective Loss 1.068235    Top1 66.000000    Top5 94.500000    LR 0.001000    Time 0.013496    
2022-01-10 12:34:54,255 - --- validate (epoch=56)-----------
2022-01-10 12:34:54,255 - 10000 samples (100 per mini-batch)
2022-01-10 12:34:54,991 - Epoch: [56][  100/  100]    Loss 1.607115    Top1 56.920000    Top5 84.650000    
2022-01-10 12:34:55,041 - ==> Top1: 56.920    Top5: 84.650    Loss: 1.607

2022-01-10 12:34:55,043 - ==> Best [Top1: 57.220   Top5: 85.010   Sparsity:0.00   Params: 381792 on epoch: 51]
2022-01-10 12:34:55,043 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:34:55,065 - 

2022-01-10 12:34:55,065 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:34:58,960 - Epoch: [57][  250/  500]    Overall Loss 1.042081    Objective Loss 1.042081                                        LR 0.001000    Time 0.015569    
2022-01-10 12:35:02,575 - Epoch: [57][  500/  500]    Overall Loss 1.057874    Objective Loss 1.057874    Top1 67.500000    Top5 91.000000    LR 0.001000    Time 0.015011    
2022-01-10 12:35:02,626 - --- validate (epoch=57)-----------
2022-01-10 12:35:02,626 - 10000 samples (100 per mini-batch)
2022-01-10 12:35:03,332 - Epoch: [57][  100/  100]    Loss 1.657011    Top1 56.320000    Top5 84.130000    
2022-01-10 12:35:03,385 - ==> Top1: 56.320    Top5: 84.130    Loss: 1.657

2022-01-10 12:35:03,386 - ==> Best [Top1: 57.220   Top5: 85.010   Sparsity:0.00   Params: 381792 on epoch: 51]
2022-01-10 12:35:03,386 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:35:03,401 - 

2022-01-10 12:35:03,401 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:35:06,951 - Epoch: [58][  250/  500]    Overall Loss 1.051671    Objective Loss 1.051671                                        LR 0.001000    Time 0.014192    
2022-01-10 12:35:10,289 - Epoch: [58][  500/  500]    Overall Loss 1.055071    Objective Loss 1.055071    Top1 67.500000    Top5 94.000000    LR 0.001000    Time 0.013767    
2022-01-10 12:35:10,338 - --- validate (epoch=58)-----------
2022-01-10 12:35:10,338 - 10000 samples (100 per mini-batch)
2022-01-10 12:35:11,054 - Epoch: [58][  100/  100]    Loss 1.598097    Top1 56.740000    Top5 84.960000    
2022-01-10 12:35:11,112 - ==> Top1: 56.740    Top5: 84.960    Loss: 1.598

2022-01-10 12:35:11,113 - ==> Best [Top1: 57.220   Top5: 85.010   Sparsity:0.00   Params: 381792 on epoch: 51]
2022-01-10 12:35:11,113 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:35:11,136 - 

2022-01-10 12:35:11,136 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:35:14,647 - Epoch: [59][  250/  500]    Overall Loss 1.016783    Objective Loss 1.016783                                        LR 0.001000    Time 0.014034    
2022-01-10 12:35:17,952 - Epoch: [59][  500/  500]    Overall Loss 1.038119    Objective Loss 1.038119    Top1 66.500000    Top5 92.000000    LR 0.001000    Time 0.013623    
2022-01-10 12:35:18,000 - --- validate (epoch=59)-----------
2022-01-10 12:35:18,001 - 10000 samples (100 per mini-batch)
2022-01-10 12:35:18,770 - Epoch: [59][  100/  100]    Loss 1.589159    Top1 57.550000    Top5 85.130000    
2022-01-10 12:35:18,828 - ==> Top1: 57.550    Top5: 85.130    Loss: 1.589

2022-01-10 12:35:18,830 - ==> Best [Top1: 57.550   Top5: 85.130   Sparsity:0.00   Params: 381792 on epoch: 59]
2022-01-10 12:35:18,830 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:35:18,856 - 

2022-01-10 12:35:18,856 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:35:22,447 - Epoch: [60][  250/  500]    Overall Loss 1.021732    Objective Loss 1.021732                                        LR 0.001000    Time 0.014356    
2022-01-10 12:35:25,741 - Epoch: [60][  500/  500]    Overall Loss 1.042063    Objective Loss 1.042063    Top1 69.500000    Top5 89.000000    LR 0.001000    Time 0.013763    
2022-01-10 12:35:25,798 - --- validate (epoch=60)-----------
2022-01-10 12:35:25,798 - 10000 samples (100 per mini-batch)
2022-01-10 12:35:26,555 - Epoch: [60][  100/  100]    Loss 1.623627    Top1 56.730000    Top5 84.430000    
2022-01-10 12:35:26,605 - ==> Top1: 56.730    Top5: 84.430    Loss: 1.624

2022-01-10 12:35:26,607 - ==> Best [Top1: 57.550   Top5: 85.130   Sparsity:0.00   Params: 381792 on epoch: 59]
2022-01-10 12:35:26,607 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:35:26,629 - 

2022-01-10 12:35:26,629 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:35:30,189 - Epoch: [61][  250/  500]    Overall Loss 1.018777    Objective Loss 1.018777                                        LR 0.001000    Time 0.014229    
2022-01-10 12:35:33,628 - Epoch: [61][  500/  500]    Overall Loss 1.027157    Objective Loss 1.027157    Top1 63.000000    Top5 94.500000    LR 0.001000    Time 0.013989    
2022-01-10 12:35:33,677 - --- validate (epoch=61)-----------
2022-01-10 12:35:33,677 - 10000 samples (100 per mini-batch)
2022-01-10 12:35:34,408 - Epoch: [61][  100/  100]    Loss 1.603325    Top1 57.070000    Top5 84.520000    
2022-01-10 12:35:34,461 - ==> Top1: 57.070    Top5: 84.520    Loss: 1.603

2022-01-10 12:35:34,462 - ==> Best [Top1: 57.550   Top5: 85.130   Sparsity:0.00   Params: 381792 on epoch: 59]
2022-01-10 12:35:34,463 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:35:34,485 - 

2022-01-10 12:35:34,485 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:35:37,982 - Epoch: [62][  250/  500]    Overall Loss 1.005007    Objective Loss 1.005007                                        LR 0.001000    Time 0.013977    
2022-01-10 12:35:41,214 - Epoch: [62][  500/  500]    Overall Loss 1.020384    Objective Loss 1.020384    Top1 68.500000    Top5 93.500000    LR 0.001000    Time 0.013448    
2022-01-10 12:35:41,271 - --- validate (epoch=62)-----------
2022-01-10 12:35:41,271 - 10000 samples (100 per mini-batch)
2022-01-10 12:35:41,980 - Epoch: [62][  100/  100]    Loss 1.649972    Top1 56.080000    Top5 83.800000    
2022-01-10 12:35:42,034 - ==> Top1: 56.080    Top5: 83.800    Loss: 1.650

2022-01-10 12:35:42,036 - ==> Best [Top1: 57.550   Top5: 85.130   Sparsity:0.00   Params: 381792 on epoch: 59]
2022-01-10 12:35:42,036 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:35:42,053 - 

2022-01-10 12:35:42,053 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:35:45,669 - Epoch: [63][  250/  500]    Overall Loss 0.999038    Objective Loss 0.999038                                        LR 0.001000    Time 0.014456    
2022-01-10 12:35:48,988 - Epoch: [63][  500/  500]    Overall Loss 1.017867    Objective Loss 1.017867    Top1 68.500000    Top5 94.000000    LR 0.001000    Time 0.013863    
2022-01-10 12:35:49,039 - --- validate (epoch=63)-----------
2022-01-10 12:35:49,039 - 10000 samples (100 per mini-batch)
2022-01-10 12:35:49,793 - Epoch: [63][  100/  100]    Loss 1.609031    Top1 56.870000    Top5 84.550000    
2022-01-10 12:35:49,844 - ==> Top1: 56.870    Top5: 84.550    Loss: 1.609

2022-01-10 12:35:49,846 - ==> Best [Top1: 57.550   Top5: 85.130   Sparsity:0.00   Params: 381792 on epoch: 59]
2022-01-10 12:35:49,846 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:35:49,869 - 

2022-01-10 12:35:49,869 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:35:53,451 - Epoch: [64][  250/  500]    Overall Loss 0.982929    Objective Loss 0.982929                                        LR 0.001000    Time 0.014317    
2022-01-10 12:35:56,774 - Epoch: [64][  500/  500]    Overall Loss 1.002325    Objective Loss 1.002325    Top1 69.500000    Top5 91.000000    LR 0.001000    Time 0.013802    
2022-01-10 12:35:56,829 - --- validate (epoch=64)-----------
2022-01-10 12:35:56,829 - 10000 samples (100 per mini-batch)
2022-01-10 12:35:57,574 - Epoch: [64][  100/  100]    Loss 1.615033    Top1 56.810000    Top5 84.740000    
2022-01-10 12:35:57,630 - ==> Top1: 56.810    Top5: 84.740    Loss: 1.615

2022-01-10 12:35:57,632 - ==> Best [Top1: 57.550   Top5: 85.130   Sparsity:0.00   Params: 381792 on epoch: 59]
2022-01-10 12:35:57,632 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:35:57,655 - 

2022-01-10 12:35:57,655 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:36:01,167 - Epoch: [65][  250/  500]    Overall Loss 0.978553    Objective Loss 0.978553                                        LR 0.001000    Time 0.014039    
2022-01-10 12:36:04,447 - Epoch: [65][  500/  500]    Overall Loss 0.997144    Objective Loss 0.997144    Top1 68.500000    Top5 94.000000    LR 0.001000    Time 0.013575    
2022-01-10 12:36:04,495 - --- validate (epoch=65)-----------
2022-01-10 12:36:04,495 - 10000 samples (100 per mini-batch)
2022-01-10 12:36:05,288 - Epoch: [65][  100/  100]    Loss 1.603481    Top1 57.520000    Top5 84.540000    
2022-01-10 12:36:05,343 - ==> Top1: 57.520    Top5: 84.540    Loss: 1.603

2022-01-10 12:36:05,345 - ==> Best [Top1: 57.550   Top5: 85.130   Sparsity:0.00   Params: 381792 on epoch: 59]
2022-01-10 12:36:05,345 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:36:05,361 - 

2022-01-10 12:36:05,361 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:36:08,965 - Epoch: [66][  250/  500]    Overall Loss 0.961385    Objective Loss 0.961385                                        LR 0.001000    Time 0.014407    
2022-01-10 12:36:12,173 - Epoch: [66][  500/  500]    Overall Loss 0.990826    Objective Loss 0.990826    Top1 73.500000    Top5 94.500000    LR 0.001000    Time 0.013615    
2022-01-10 12:36:12,231 - --- validate (epoch=66)-----------
2022-01-10 12:36:12,231 - 10000 samples (100 per mini-batch)
2022-01-10 12:36:12,962 - Epoch: [66][  100/  100]    Loss 1.629924    Top1 56.540000    Top5 84.420000    
2022-01-10 12:36:13,013 - ==> Top1: 56.540    Top5: 84.420    Loss: 1.630

2022-01-10 12:36:13,015 - ==> Best [Top1: 57.550   Top5: 85.130   Sparsity:0.00   Params: 381792 on epoch: 59]
2022-01-10 12:36:13,015 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:36:13,037 - 

2022-01-10 12:36:13,037 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:36:16,467 - Epoch: [67][  250/  500]    Overall Loss 0.975168    Objective Loss 0.975168                                        LR 0.001000    Time 0.013712    
2022-01-10 12:36:19,749 - Epoch: [67][  500/  500]    Overall Loss 0.986781    Objective Loss 0.986781    Top1 67.500000    Top5 89.000000    LR 0.001000    Time 0.013415    
2022-01-10 12:36:19,804 - --- validate (epoch=67)-----------
2022-01-10 12:36:19,804 - 10000 samples (100 per mini-batch)
2022-01-10 12:36:20,601 - Epoch: [67][  100/  100]    Loss 1.662146    Top1 56.040000    Top5 83.680000    
2022-01-10 12:36:20,660 - ==> Top1: 56.040    Top5: 83.680    Loss: 1.662

2022-01-10 12:36:20,661 - ==> Best [Top1: 57.550   Top5: 85.130   Sparsity:0.00   Params: 381792 on epoch: 59]
2022-01-10 12:36:20,661 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:36:20,684 - 

2022-01-10 12:36:20,684 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:36:24,124 - Epoch: [68][  250/  500]    Overall Loss 0.962435    Objective Loss 0.962435                                        LR 0.001000    Time 0.013751    
2022-01-10 12:36:27,367 - Epoch: [68][  500/  500]    Overall Loss 0.974152    Objective Loss 0.974152    Top1 64.500000    Top5 91.500000    LR 0.001000    Time 0.013357    
2022-01-10 12:36:27,424 - --- validate (epoch=68)-----------
2022-01-10 12:36:27,424 - 10000 samples (100 per mini-batch)
2022-01-10 12:36:28,149 - Epoch: [68][  100/  100]    Loss 1.597450    Top1 57.110000    Top5 84.930000    
2022-01-10 12:36:28,203 - ==> Top1: 57.110    Top5: 84.930    Loss: 1.597

2022-01-10 12:36:28,205 - ==> Best [Top1: 57.550   Top5: 85.130   Sparsity:0.00   Params: 381792 on epoch: 59]
2022-01-10 12:36:28,205 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:36:28,221 - 

2022-01-10 12:36:28,222 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:36:31,804 - Epoch: [69][  250/  500]    Overall Loss 0.953180    Objective Loss 0.953180                                        LR 0.001000    Time 0.014321    
2022-01-10 12:36:35,485 - Epoch: [69][  500/  500]    Overall Loss 0.970994    Objective Loss 0.970994    Top1 70.000000    Top5 91.000000    LR 0.001000    Time 0.014517    
2022-01-10 12:36:35,542 - --- validate (epoch=69)-----------
2022-01-10 12:36:35,543 - 10000 samples (100 per mini-batch)
2022-01-10 12:36:36,294 - Epoch: [69][  100/  100]    Loss 1.586656    Top1 56.850000    Top5 85.220000    
2022-01-10 12:36:36,351 - ==> Top1: 56.850    Top5: 85.220    Loss: 1.587

2022-01-10 12:36:36,352 - ==> Best [Top1: 57.550   Top5: 85.130   Sparsity:0.00   Params: 381792 on epoch: 59]
2022-01-10 12:36:36,352 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:36:36,375 - 

2022-01-10 12:36:36,375 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:36:40,058 - Epoch: [70][  250/  500]    Overall Loss 0.939815    Objective Loss 0.939815                                        LR 0.001000    Time 0.014723    
2022-01-10 12:36:43,750 - Epoch: [70][  500/  500]    Overall Loss 0.954868    Objective Loss 0.954868    Top1 72.000000    Top5 93.000000    LR 0.001000    Time 0.014742    
2022-01-10 12:36:43,801 - --- validate (epoch=70)-----------
2022-01-10 12:36:43,801 - 10000 samples (100 per mini-batch)
2022-01-10 12:36:44,539 - Epoch: [70][  100/  100]    Loss 1.595617    Top1 57.280000    Top5 84.970000    
2022-01-10 12:36:44,590 - ==> Top1: 57.280    Top5: 84.970    Loss: 1.596

2022-01-10 12:36:44,592 - ==> Best [Top1: 57.550   Top5: 85.130   Sparsity:0.00   Params: 381792 on epoch: 59]
2022-01-10 12:36:44,592 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:36:44,666 - 

2022-01-10 12:36:44,666 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:36:48,073 - Epoch: [71][  250/  500]    Overall Loss 0.954017    Objective Loss 0.954017                                        LR 0.001000    Time 0.013619    
2022-01-10 12:36:51,491 - Epoch: [71][  500/  500]    Overall Loss 0.958872    Objective Loss 0.958872    Top1 73.000000    Top5 95.500000    LR 0.001000    Time 0.013641    
2022-01-10 12:36:51,548 - --- validate (epoch=71)-----------
2022-01-10 12:36:51,548 - 10000 samples (100 per mini-batch)
2022-01-10 12:36:52,317 - Epoch: [71][  100/  100]    Loss 1.632448    Top1 56.470000    Top5 84.510000    
2022-01-10 12:36:52,365 - ==> Top1: 56.470    Top5: 84.510    Loss: 1.632

2022-01-10 12:36:52,367 - ==> Best [Top1: 57.550   Top5: 85.130   Sparsity:0.00   Params: 381792 on epoch: 59]
2022-01-10 12:36:52,367 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:36:52,389 - 

2022-01-10 12:36:52,389 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:36:56,156 - Epoch: [72][  250/  500]    Overall Loss 0.916685    Objective Loss 0.916685                                        LR 0.001000    Time 0.015057    
2022-01-10 12:36:59,577 - Epoch: [72][  500/  500]    Overall Loss 0.945032    Objective Loss 0.945032    Top1 75.500000    Top5 94.500000    LR 0.001000    Time 0.014368    
2022-01-10 12:36:59,635 - --- validate (epoch=72)-----------
2022-01-10 12:36:59,635 - 10000 samples (100 per mini-batch)
2022-01-10 12:37:00,358 - Epoch: [72][  100/  100]    Loss 1.610294    Top1 57.080000    Top5 85.150000    
2022-01-10 12:37:00,411 - ==> Top1: 57.080    Top5: 85.150    Loss: 1.610

2022-01-10 12:37:00,413 - ==> Best [Top1: 57.550   Top5: 85.130   Sparsity:0.00   Params: 381792 on epoch: 59]
2022-01-10 12:37:00,413 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:37:00,435 - 

2022-01-10 12:37:00,435 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:37:03,937 - Epoch: [73][  250/  500]    Overall Loss 0.940010    Objective Loss 0.940010                                        LR 0.001000    Time 0.013999    
2022-01-10 12:37:07,228 - Epoch: [73][  500/  500]    Overall Loss 0.945302    Objective Loss 0.945302    Top1 66.000000    Top5 92.000000    LR 0.001000    Time 0.013577    
2022-01-10 12:37:07,284 - --- validate (epoch=73)-----------
2022-01-10 12:37:07,284 - 10000 samples (100 per mini-batch)
2022-01-10 12:37:08,015 - Epoch: [73][  100/  100]    Loss 1.584075    Top1 57.660000    Top5 85.440000    
2022-01-10 12:37:08,068 - ==> Top1: 57.660    Top5: 85.440    Loss: 1.584

2022-01-10 12:37:08,070 - ==> Best [Top1: 57.660   Top5: 85.440   Sparsity:0.00   Params: 381792 on epoch: 73]
2022-01-10 12:37:08,070 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:37:08,090 - 

2022-01-10 12:37:08,090 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:37:11,659 - Epoch: [74][  250/  500]    Overall Loss 0.919030    Objective Loss 0.919030                                        LR 0.001000    Time 0.014270    
2022-01-10 12:37:14,998 - Epoch: [74][  500/  500]    Overall Loss 0.937580    Objective Loss 0.937580    Top1 69.000000    Top5 95.000000    LR 0.001000    Time 0.013809    
2022-01-10 12:37:15,046 - --- validate (epoch=74)-----------
2022-01-10 12:37:15,046 - 10000 samples (100 per mini-batch)
2022-01-10 12:37:15,783 - Epoch: [74][  100/  100]    Loss 1.637787    Top1 56.440000    Top5 84.620000    
2022-01-10 12:37:15,842 - ==> Top1: 56.440    Top5: 84.620    Loss: 1.638

2022-01-10 12:37:15,844 - ==> Best [Top1: 57.660   Top5: 85.440   Sparsity:0.00   Params: 381792 on epoch: 73]
2022-01-10 12:37:15,844 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:37:15,867 - 

2022-01-10 12:37:15,867 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:37:19,591 - Epoch: [75][  250/  500]    Overall Loss 0.909964    Objective Loss 0.909964                                        LR 0.001000    Time 0.014888    
2022-01-10 12:37:23,029 - Epoch: [75][  500/  500]    Overall Loss 0.927183    Objective Loss 0.927183    Top1 69.500000    Top5 91.500000    LR 0.001000    Time 0.014316    
2022-01-10 12:37:23,078 - --- validate (epoch=75)-----------
2022-01-10 12:37:23,078 - 10000 samples (100 per mini-batch)
2022-01-10 12:37:23,807 - Epoch: [75][  100/  100]    Loss 1.643215    Top1 57.150000    Top5 84.730000    
2022-01-10 12:37:23,862 - ==> Top1: 57.150    Top5: 84.730    Loss: 1.643

2022-01-10 12:37:23,864 - ==> Best [Top1: 57.660   Top5: 85.440   Sparsity:0.00   Params: 381792 on epoch: 73]
2022-01-10 12:37:23,864 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:37:23,887 - 

2022-01-10 12:37:23,887 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:37:27,471 - Epoch: [76][  250/  500]    Overall Loss 0.905013    Objective Loss 0.905013                                        LR 0.001000    Time 0.014325    
2022-01-10 12:37:30,731 - Epoch: [76][  500/  500]    Overall Loss 0.922480    Objective Loss 0.922480    Top1 66.000000    Top5 94.500000    LR 0.001000    Time 0.013679    
2022-01-10 12:37:30,785 - --- validate (epoch=76)-----------
2022-01-10 12:37:30,785 - 10000 samples (100 per mini-batch)
2022-01-10 12:37:31,512 - Epoch: [76][  100/  100]    Loss 1.574990    Top1 57.680000    Top5 85.230000    
2022-01-10 12:37:31,563 - ==> Top1: 57.680    Top5: 85.230    Loss: 1.575

2022-01-10 12:37:31,565 - ==> Best [Top1: 57.680   Top5: 85.230   Sparsity:0.00   Params: 381792 on epoch: 76]
2022-01-10 12:37:31,565 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:37:31,590 - 

2022-01-10 12:37:31,591 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:37:35,152 - Epoch: [77][  250/  500]    Overall Loss 0.896144    Objective Loss 0.896144                                        LR 0.001000    Time 0.014237    
2022-01-10 12:37:38,447 - Epoch: [77][  500/  500]    Overall Loss 0.914552    Objective Loss 0.914552    Top1 66.500000    Top5 94.000000    LR 0.001000    Time 0.013705    
2022-01-10 12:37:38,496 - --- validate (epoch=77)-----------
2022-01-10 12:37:38,496 - 10000 samples (100 per mini-batch)
2022-01-10 12:37:39,211 - Epoch: [77][  100/  100]    Loss 1.595850    Top1 58.200000    Top5 85.070000    
2022-01-10 12:37:39,267 - ==> Top1: 58.200    Top5: 85.070    Loss: 1.596

2022-01-10 12:37:39,269 - ==> Best [Top1: 58.200   Top5: 85.070   Sparsity:0.00   Params: 381792 on epoch: 77]
2022-01-10 12:37:39,269 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:37:39,295 - 

2022-01-10 12:37:39,295 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:37:42,848 - Epoch: [78][  250/  500]    Overall Loss 0.902189    Objective Loss 0.902189                                        LR 0.001000    Time 0.014206    
2022-01-10 12:37:46,126 - Epoch: [78][  500/  500]    Overall Loss 0.912309    Objective Loss 0.912309    Top1 72.000000    Top5 93.500000    LR 0.001000    Time 0.013654    
2022-01-10 12:37:46,181 - --- validate (epoch=78)-----------
2022-01-10 12:37:46,181 - 10000 samples (100 per mini-batch)
2022-01-10 12:37:46,912 - Epoch: [78][  100/  100]    Loss 1.584750    Top1 57.780000    Top5 85.470000    
2022-01-10 12:37:46,963 - ==> Top1: 57.780    Top5: 85.470    Loss: 1.585

2022-01-10 12:37:46,965 - ==> Best [Top1: 58.200   Top5: 85.070   Sparsity:0.00   Params: 381792 on epoch: 77]
2022-01-10 12:37:46,965 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:37:46,987 - 

2022-01-10 12:37:46,987 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:37:50,464 - Epoch: [79][  250/  500]    Overall Loss 0.887288    Objective Loss 0.887288                                        LR 0.001000    Time 0.013901    
2022-01-10 12:37:53,735 - Epoch: [79][  500/  500]    Overall Loss 0.904395    Objective Loss 0.904395    Top1 72.000000    Top5 96.000000    LR 0.001000    Time 0.013487    
2022-01-10 12:37:53,789 - --- validate (epoch=79)-----------
2022-01-10 12:37:53,789 - 10000 samples (100 per mini-batch)
2022-01-10 12:37:54,538 - Epoch: [79][  100/  100]    Loss 1.610010    Top1 57.470000    Top5 84.890000    
2022-01-10 12:37:54,595 - ==> Top1: 57.470    Top5: 84.890    Loss: 1.610

2022-01-10 12:37:54,597 - ==> Best [Top1: 58.200   Top5: 85.070   Sparsity:0.00   Params: 381792 on epoch: 77]
2022-01-10 12:37:54,597 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:37:54,619 - 

2022-01-10 12:37:54,619 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:37:58,112 - Epoch: [80][  250/  500]    Overall Loss 0.888161    Objective Loss 0.888161                                        LR 0.001000    Time 0.013961    
2022-01-10 12:38:01,304 - Epoch: [80][  500/  500]    Overall Loss 0.896629    Objective Loss 0.896629    Top1 77.500000    Top5 95.500000    LR 0.001000    Time 0.013362    
2022-01-10 12:38:01,354 - --- validate (epoch=80)-----------
2022-01-10 12:38:01,354 - 10000 samples (100 per mini-batch)
2022-01-10 12:38:02,058 - Epoch: [80][  100/  100]    Loss 1.600050    Top1 57.810000    Top5 85.190000    
2022-01-10 12:38:02,114 - ==> Top1: 57.810    Top5: 85.190    Loss: 1.600

2022-01-10 12:38:02,116 - ==> Best [Top1: 58.200   Top5: 85.070   Sparsity:0.00   Params: 381792 on epoch: 77]
2022-01-10 12:38:02,116 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:38:02,139 - 

2022-01-10 12:38:02,139 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:38:05,669 - Epoch: [81][  250/  500]    Overall Loss 0.871131    Objective Loss 0.871131                                        LR 0.001000    Time 0.014112    
2022-01-10 12:38:08,977 - Epoch: [81][  500/  500]    Overall Loss 0.890085    Objective Loss 0.890085    Top1 72.500000    Top5 93.000000    LR 0.001000    Time 0.013669    
2022-01-10 12:38:09,031 - --- validate (epoch=81)-----------
2022-01-10 12:38:09,032 - 10000 samples (100 per mini-batch)
2022-01-10 12:38:09,768 - Epoch: [81][  100/  100]    Loss 1.615447    Top1 57.710000    Top5 84.990000    
2022-01-10 12:38:09,819 - ==> Top1: 57.710    Top5: 84.990    Loss: 1.615

2022-01-10 12:38:09,820 - ==> Best [Top1: 58.200   Top5: 85.070   Sparsity:0.00   Params: 381792 on epoch: 77]
2022-01-10 12:38:09,821 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:38:09,843 - 

2022-01-10 12:38:09,843 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:38:13,360 - Epoch: [82][  250/  500]    Overall Loss 0.855782    Objective Loss 0.855782                                        LR 0.001000    Time 0.014057    
2022-01-10 12:38:16,989 - Epoch: [82][  500/  500]    Overall Loss 0.877898    Objective Loss 0.877898    Top1 73.500000    Top5 95.000000    LR 0.001000    Time 0.014284    
2022-01-10 12:38:17,041 - --- validate (epoch=82)-----------
2022-01-10 12:38:17,041 - 10000 samples (100 per mini-batch)
2022-01-10 12:38:17,749 - Epoch: [82][  100/  100]    Loss 1.617299    Top1 57.780000    Top5 85.110000    
2022-01-10 12:38:17,804 - ==> Top1: 57.780    Top5: 85.110    Loss: 1.617

2022-01-10 12:38:17,806 - ==> Best [Top1: 58.200   Top5: 85.070   Sparsity:0.00   Params: 381792 on epoch: 77]
2022-01-10 12:38:17,806 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:38:17,821 - 

2022-01-10 12:38:17,821 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:38:21,396 - Epoch: [83][  250/  500]    Overall Loss 0.870149    Objective Loss 0.870149                                        LR 0.001000    Time 0.014288    
2022-01-10 12:38:25,003 - Epoch: [83][  500/  500]    Overall Loss 0.884047    Objective Loss 0.884047    Top1 77.000000    Top5 96.000000    LR 0.001000    Time 0.014356    
2022-01-10 12:38:25,060 - --- validate (epoch=83)-----------
2022-01-10 12:38:25,060 - 10000 samples (100 per mini-batch)
2022-01-10 12:38:25,794 - Epoch: [83][  100/  100]    Loss 1.599562    Top1 57.570000    Top5 85.450000    
2022-01-10 12:38:25,845 - ==> Top1: 57.570    Top5: 85.450    Loss: 1.600

2022-01-10 12:38:25,846 - ==> Best [Top1: 58.200   Top5: 85.070   Sparsity:0.00   Params: 381792 on epoch: 77]
2022-01-10 12:38:25,846 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:38:25,869 - 

2022-01-10 12:38:25,869 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:38:29,819 - Epoch: [84][  250/  500]    Overall Loss 0.854095    Objective Loss 0.854095                                        LR 0.001000    Time 0.015789    
2022-01-10 12:38:33,282 - Epoch: [84][  500/  500]    Overall Loss 0.874697    Objective Loss 0.874697    Top1 73.000000    Top5 98.500000    LR 0.001000    Time 0.014817    
2022-01-10 12:38:33,330 - --- validate (epoch=84)-----------
2022-01-10 12:38:33,330 - 10000 samples (100 per mini-batch)
2022-01-10 12:38:34,138 - Epoch: [84][  100/  100]    Loss 1.743233    Top1 55.660000    Top5 83.300000    
2022-01-10 12:38:34,187 - ==> Top1: 55.660    Top5: 83.300    Loss: 1.743

2022-01-10 12:38:34,189 - ==> Best [Top1: 58.200   Top5: 85.070   Sparsity:0.00   Params: 381792 on epoch: 77]
2022-01-10 12:38:34,189 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:38:34,211 - 

2022-01-10 12:38:34,212 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:38:37,748 - Epoch: [85][  250/  500]    Overall Loss 0.844688    Objective Loss 0.844688                                        LR 0.001000    Time 0.014135    
2022-01-10 12:38:41,052 - Epoch: [85][  500/  500]    Overall Loss 0.860114    Objective Loss 0.860114    Top1 72.000000    Top5 95.000000    LR 0.001000    Time 0.013673    
2022-01-10 12:38:41,102 - --- validate (epoch=85)-----------
2022-01-10 12:38:41,102 - 10000 samples (100 per mini-batch)
2022-01-10 12:38:41,825 - Epoch: [85][  100/  100]    Loss 1.589222    Top1 58.370000    Top5 85.150000    
2022-01-10 12:38:41,878 - ==> Top1: 58.370    Top5: 85.150    Loss: 1.589

2022-01-10 12:38:41,880 - ==> Best [Top1: 58.370   Top5: 85.150   Sparsity:0.00   Params: 381792 on epoch: 85]
2022-01-10 12:38:41,880 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:38:41,906 - 

2022-01-10 12:38:41,906 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:38:45,511 - Epoch: [86][  250/  500]    Overall Loss 0.845266    Objective Loss 0.845266                                        LR 0.001000    Time 0.014410    
2022-01-10 12:38:48,838 - Epoch: [86][  500/  500]    Overall Loss 0.861050    Objective Loss 0.861050    Top1 71.000000    Top5 92.500000    LR 0.001000    Time 0.013855    
2022-01-10 12:38:48,888 - --- validate (epoch=86)-----------
2022-01-10 12:38:48,888 - 10000 samples (100 per mini-batch)
2022-01-10 12:38:49,644 - Epoch: [86][  100/  100]    Loss 1.616188    Top1 58.010000    Top5 85.080000    
2022-01-10 12:38:49,694 - ==> Top1: 58.010    Top5: 85.080    Loss: 1.616

2022-01-10 12:38:49,696 - ==> Best [Top1: 58.370   Top5: 85.150   Sparsity:0.00   Params: 381792 on epoch: 85]
2022-01-10 12:38:49,696 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:38:49,711 - 

2022-01-10 12:38:49,711 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:38:53,463 - Epoch: [87][  250/  500]    Overall Loss 0.839210    Objective Loss 0.839210                                        LR 0.001000    Time 0.014999    
2022-01-10 12:38:56,719 - Epoch: [87][  500/  500]    Overall Loss 0.856440    Objective Loss 0.856440    Top1 70.000000    Top5 94.500000    LR 0.001000    Time 0.014008    
2022-01-10 12:38:56,777 - --- validate (epoch=87)-----------
2022-01-10 12:38:56,777 - 10000 samples (100 per mini-batch)
2022-01-10 12:38:57,493 - Epoch: [87][  100/  100]    Loss 1.645772    Top1 57.470000    Top5 84.400000    
2022-01-10 12:38:57,547 - ==> Top1: 57.470    Top5: 84.400    Loss: 1.646

2022-01-10 12:38:57,549 - ==> Best [Top1: 58.370   Top5: 85.150   Sparsity:0.00   Params: 381792 on epoch: 85]
2022-01-10 12:38:57,549 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:38:57,572 - 

2022-01-10 12:38:57,572 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:39:01,103 - Epoch: [88][  250/  500]    Overall Loss 0.836271    Objective Loss 0.836271                                        LR 0.001000    Time 0.014112    
2022-01-10 12:39:04,313 - Epoch: [88][  500/  500]    Overall Loss 0.854605    Objective Loss 0.854605    Top1 72.000000    Top5 97.500000    LR 0.001000    Time 0.013474    
2022-01-10 12:39:04,363 - --- validate (epoch=88)-----------
2022-01-10 12:39:04,363 - 10000 samples (100 per mini-batch)
2022-01-10 12:39:05,088 - Epoch: [88][  100/  100]    Loss 1.594417    Top1 58.290000    Top5 85.670000    
2022-01-10 12:39:05,139 - ==> Top1: 58.290    Top5: 85.670    Loss: 1.594

2022-01-10 12:39:05,141 - ==> Best [Top1: 58.370   Top5: 85.150   Sparsity:0.00   Params: 381792 on epoch: 85]
2022-01-10 12:39:05,141 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:39:05,163 - 

2022-01-10 12:39:05,163 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:39:08,713 - Epoch: [89][  250/  500]    Overall Loss 0.831531    Objective Loss 0.831531                                        LR 0.001000    Time 0.014193    
2022-01-10 12:39:11,975 - Epoch: [89][  500/  500]    Overall Loss 0.845785    Objective Loss 0.845785    Top1 80.000000    Top5 98.000000    LR 0.001000    Time 0.013615    
2022-01-10 12:39:12,031 - --- validate (epoch=89)-----------
2022-01-10 12:39:12,031 - 10000 samples (100 per mini-batch)
2022-01-10 12:39:12,818 - Epoch: [89][  100/  100]    Loss 1.621950    Top1 58.290000    Top5 85.080000    
2022-01-10 12:39:12,869 - ==> Top1: 58.290    Top5: 85.080    Loss: 1.622

2022-01-10 12:39:12,871 - ==> Best [Top1: 58.370   Top5: 85.150   Sparsity:0.00   Params: 381792 on epoch: 85]
2022-01-10 12:39:12,871 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:39:12,893 - 

2022-01-10 12:39:12,893 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:39:16,450 - Epoch: [90][  250/  500]    Overall Loss 0.828466    Objective Loss 0.828466                                        LR 0.001000    Time 0.014219    
2022-01-10 12:39:19,733 - Epoch: [90][  500/  500]    Overall Loss 0.835409    Objective Loss 0.835409    Top1 76.000000    Top5 95.000000    LR 0.001000    Time 0.013671    
2022-01-10 12:39:19,788 - --- validate (epoch=90)-----------
2022-01-10 12:39:19,788 - 10000 samples (100 per mini-batch)
2022-01-10 12:39:20,505 - Epoch: [90][  100/  100]    Loss 1.684476    Top1 57.000000    Top5 84.180000    
2022-01-10 12:39:20,561 - ==> Top1: 57.000    Top5: 84.180    Loss: 1.684

2022-01-10 12:39:20,563 - ==> Best [Top1: 58.370   Top5: 85.150   Sparsity:0.00   Params: 381792 on epoch: 85]
2022-01-10 12:39:20,563 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:39:20,585 - 

2022-01-10 12:39:20,585 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:39:24,055 - Epoch: [91][  250/  500]    Overall Loss 0.824632    Objective Loss 0.824632                                        LR 0.001000    Time 0.013870    
2022-01-10 12:39:27,280 - Epoch: [91][  500/  500]    Overall Loss 0.834610    Objective Loss 0.834610    Top1 76.000000    Top5 98.000000    LR 0.001000    Time 0.013380    
2022-01-10 12:39:27,333 - --- validate (epoch=91)-----------
2022-01-10 12:39:27,333 - 10000 samples (100 per mini-batch)
2022-01-10 12:39:28,103 - Epoch: [91][  100/  100]    Loss 1.651042    Top1 57.520000    Top5 84.790000    
2022-01-10 12:39:28,161 - ==> Top1: 57.520    Top5: 84.790    Loss: 1.651

2022-01-10 12:39:28,163 - ==> Best [Top1: 58.370   Top5: 85.150   Sparsity:0.00   Params: 381792 on epoch: 85]
2022-01-10 12:39:28,163 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:39:28,185 - 

2022-01-10 12:39:28,185 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:39:31,810 - Epoch: [92][  250/  500]    Overall Loss 0.817456    Objective Loss 0.817456                                        LR 0.001000    Time 0.014491    
2022-01-10 12:39:35,083 - Epoch: [92][  500/  500]    Overall Loss 0.829301    Objective Loss 0.829301    Top1 79.500000    Top5 96.500000    LR 0.001000    Time 0.013788    
2022-01-10 12:39:35,131 - --- validate (epoch=92)-----------
2022-01-10 12:39:35,131 - 10000 samples (100 per mini-batch)
2022-01-10 12:39:35,895 - Epoch: [92][  100/  100]    Loss 1.651544    Top1 57.950000    Top5 84.920000    
2022-01-10 12:39:35,948 - ==> Top1: 57.950    Top5: 84.920    Loss: 1.652

2022-01-10 12:39:35,950 - ==> Best [Top1: 58.370   Top5: 85.150   Sparsity:0.00   Params: 381792 on epoch: 85]
2022-01-10 12:39:35,950 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:39:35,972 - 

2022-01-10 12:39:35,972 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:39:39,458 - Epoch: [93][  250/  500]    Overall Loss 0.795891    Objective Loss 0.795891                                        LR 0.001000    Time 0.013934    
2022-01-10 12:39:42,749 - Epoch: [93][  500/  500]    Overall Loss 0.826938    Objective Loss 0.826938    Top1 76.000000    Top5 95.500000    LR 0.001000    Time 0.013545    
2022-01-10 12:39:42,798 - --- validate (epoch=93)-----------
2022-01-10 12:39:42,798 - 10000 samples (100 per mini-batch)
2022-01-10 12:39:43,626 - Epoch: [93][  100/  100]    Loss 1.641107    Top1 57.590000    Top5 85.200000    
2022-01-10 12:39:43,678 - ==> Top1: 57.590    Top5: 85.200    Loss: 1.641

2022-01-10 12:39:43,680 - ==> Best [Top1: 58.370   Top5: 85.150   Sparsity:0.00   Params: 381792 on epoch: 85]
2022-01-10 12:39:43,680 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:39:43,702 - 

2022-01-10 12:39:43,702 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:39:47,237 - Epoch: [94][  250/  500]    Overall Loss 0.802339    Objective Loss 0.802339                                        LR 0.001000    Time 0.014131    
2022-01-10 12:39:50,611 - Epoch: [94][  500/  500]    Overall Loss 0.815248    Objective Loss 0.815248    Top1 75.000000    Top5 95.000000    LR 0.001000    Time 0.013810    
2022-01-10 12:39:50,666 - --- validate (epoch=94)-----------
2022-01-10 12:39:50,667 - 10000 samples (100 per mini-batch)
2022-01-10 12:39:51,402 - Epoch: [94][  100/  100]    Loss 1.647564    Top1 57.500000    Top5 85.210000    
2022-01-10 12:39:51,453 - ==> Top1: 57.500    Top5: 85.210    Loss: 1.648

2022-01-10 12:39:51,454 - ==> Best [Top1: 58.370   Top5: 85.150   Sparsity:0.00   Params: 381792 on epoch: 85]
2022-01-10 12:39:51,455 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:39:51,477 - 

2022-01-10 12:39:51,477 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:39:55,094 - Epoch: [95][  250/  500]    Overall Loss 0.790153    Objective Loss 0.790153                                        LR 0.001000    Time 0.014460    
2022-01-10 12:39:58,403 - Epoch: [95][  500/  500]    Overall Loss 0.804096    Objective Loss 0.804096    Top1 76.000000    Top5 97.000000    LR 0.001000    Time 0.013844    
2022-01-10 12:39:58,462 - --- validate (epoch=95)-----------
2022-01-10 12:39:58,463 - 10000 samples (100 per mini-batch)
2022-01-10 12:39:59,185 - Epoch: [95][  100/  100]    Loss 1.701906    Top1 56.280000    Top5 84.570000    
2022-01-10 12:39:59,244 - ==> Top1: 56.280    Top5: 84.570    Loss: 1.702

2022-01-10 12:39:59,246 - ==> Best [Top1: 58.370   Top5: 85.150   Sparsity:0.00   Params: 381792 on epoch: 85]
2022-01-10 12:39:59,246 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:39:59,261 - 

2022-01-10 12:39:59,261 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:40:02,964 - Epoch: [96][  250/  500]    Overall Loss 0.789269    Objective Loss 0.789269                                        LR 0.001000    Time 0.014802    
2022-01-10 12:40:06,636 - Epoch: [96][  500/  500]    Overall Loss 0.811602    Objective Loss 0.811602    Top1 75.000000    Top5 96.500000    LR 0.001000    Time 0.014741    
2022-01-10 12:40:06,691 - --- validate (epoch=96)-----------
2022-01-10 12:40:06,691 - 10000 samples (100 per mini-batch)
2022-01-10 12:40:07,529 - Epoch: [96][  100/  100]    Loss 1.643140    Top1 58.040000    Top5 84.910000    
2022-01-10 12:40:07,582 - ==> Top1: 58.040    Top5: 84.910    Loss: 1.643

2022-01-10 12:40:07,584 - ==> Best [Top1: 58.370   Top5: 85.150   Sparsity:0.00   Params: 381792 on epoch: 85]
2022-01-10 12:40:07,584 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:40:07,606 - 

2022-01-10 12:40:07,606 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:40:11,249 - Epoch: [97][  250/  500]    Overall Loss 0.788290    Objective Loss 0.788290                                        LR 0.001000    Time 0.014562    
2022-01-10 12:40:14,461 - Epoch: [97][  500/  500]    Overall Loss 0.800163    Objective Loss 0.800163    Top1 76.500000    Top5 95.500000    LR 0.001000    Time 0.013701    
2022-01-10 12:40:14,517 - --- validate (epoch=97)-----------
2022-01-10 12:40:14,518 - 10000 samples (100 per mini-batch)
2022-01-10 12:40:15,256 - Epoch: [97][  100/  100]    Loss 1.661527    Top1 57.660000    Top5 84.710000    
2022-01-10 12:40:15,316 - ==> Top1: 57.660    Top5: 84.710    Loss: 1.662

2022-01-10 12:40:15,318 - ==> Best [Top1: 58.370   Top5: 85.150   Sparsity:0.00   Params: 381792 on epoch: 85]
2022-01-10 12:40:15,318 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:40:15,341 - 

2022-01-10 12:40:15,341 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:40:19,035 - Epoch: [98][  250/  500]    Overall Loss 0.785128    Objective Loss 0.785128                                        LR 0.001000    Time 0.014767    
2022-01-10 12:40:22,458 - Epoch: [98][  500/  500]    Overall Loss 0.798337    Objective Loss 0.798337    Top1 73.500000    Top5 96.500000    LR 0.001000    Time 0.014226    
2022-01-10 12:40:22,512 - --- validate (epoch=98)-----------
2022-01-10 12:40:22,513 - 10000 samples (100 per mini-batch)
2022-01-10 12:40:23,226 - Epoch: [98][  100/  100]    Loss 1.657247    Top1 57.760000    Top5 84.670000    
2022-01-10 12:40:23,280 - ==> Top1: 57.760    Top5: 84.670    Loss: 1.657

2022-01-10 12:40:23,282 - ==> Best [Top1: 58.370   Top5: 85.150   Sparsity:0.00   Params: 381792 on epoch: 85]
2022-01-10 12:40:23,282 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:40:23,304 - 

2022-01-10 12:40:23,304 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:40:26,849 - Epoch: [99][  250/  500]    Overall Loss 0.766479    Objective Loss 0.766479                                        LR 0.001000    Time 0.014169    
2022-01-10 12:40:30,141 - Epoch: [99][  500/  500]    Overall Loss 0.791691    Objective Loss 0.791691    Top1 73.500000    Top5 93.500000    LR 0.001000    Time 0.013665    
2022-01-10 12:40:30,196 - --- validate (epoch=99)-----------
2022-01-10 12:40:30,196 - 10000 samples (100 per mini-batch)
2022-01-10 12:40:30,939 - Epoch: [99][  100/  100]    Loss 1.670358    Top1 57.740000    Top5 84.900000    
2022-01-10 12:40:30,992 - ==> Top1: 57.740    Top5: 84.900    Loss: 1.670

2022-01-10 12:40:30,995 - ==> Best [Top1: 58.370   Top5: 85.150   Sparsity:0.00   Params: 381792 on epoch: 85]
2022-01-10 12:40:30,995 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:40:31,017 - 

2022-01-10 12:40:31,017 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:40:34,843 - Epoch: [100][  250/  500]    Overall Loss 0.703235    Objective Loss 0.703235                                        LR 0.000250    Time 0.015292    
2022-01-10 12:40:38,178 - Epoch: [100][  500/  500]    Overall Loss 0.683553    Objective Loss 0.683553    Top1 81.000000    Top5 97.000000    LR 0.000250    Time 0.014312    
2022-01-10 12:40:38,228 - --- validate (epoch=100)-----------
2022-01-10 12:40:38,228 - 10000 samples (100 per mini-batch)
2022-01-10 12:40:38,957 - Epoch: [100][  100/  100]    Loss 1.550863    Top1 60.060000    Top5 85.890000    
2022-01-10 12:40:39,012 - ==> Top1: 60.060    Top5: 85.890    Loss: 1.551

2022-01-10 12:40:39,013 - ==> Best [Top1: 60.060   Top5: 85.890   Sparsity:0.00   Params: 381792 on epoch: 100]
2022-01-10 12:40:39,013 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:40:39,032 - 

2022-01-10 12:40:39,033 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:40:42,623 - Epoch: [101][  250/  500]    Overall Loss 0.659861    Objective Loss 0.659861                                        LR 0.000250    Time 0.014351    
2022-01-10 12:40:45,958 - Epoch: [101][  500/  500]    Overall Loss 0.661613    Objective Loss 0.661613    Top1 79.500000    Top5 96.000000    LR 0.000250    Time 0.013842    
2022-01-10 12:40:46,007 - --- validate (epoch=101)-----------
2022-01-10 12:40:46,007 - 10000 samples (100 per mini-batch)
2022-01-10 12:40:46,723 - Epoch: [101][  100/  100]    Loss 1.545725    Top1 59.880000    Top5 86.200000    
2022-01-10 12:40:46,772 - ==> Top1: 59.880    Top5: 86.200    Loss: 1.546

2022-01-10 12:40:46,773 - ==> Best [Top1: 60.060   Top5: 85.890   Sparsity:0.00   Params: 381792 on epoch: 100]
2022-01-10 12:40:46,773 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:40:46,796 - 

2022-01-10 12:40:46,796 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:40:50,504 - Epoch: [102][  250/  500]    Overall Loss 0.636974    Objective Loss 0.636974                                        LR 0.000250    Time 0.014823    
2022-01-10 12:40:53,913 - Epoch: [102][  500/  500]    Overall Loss 0.647200    Objective Loss 0.647200    Top1 75.500000    Top5 97.000000    LR 0.000250    Time 0.014225    
2022-01-10 12:40:53,969 - --- validate (epoch=102)-----------
2022-01-10 12:40:53,969 - 10000 samples (100 per mini-batch)
2022-01-10 12:40:54,751 - Epoch: [102][  100/  100]    Loss 1.552279    Top1 60.160000    Top5 85.930000    
2022-01-10 12:40:54,800 - ==> Top1: 60.160    Top5: 85.930    Loss: 1.552

2022-01-10 12:40:54,802 - ==> Best [Top1: 60.160   Top5: 85.930   Sparsity:0.00   Params: 381792 on epoch: 102]
2022-01-10 12:40:54,802 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:40:54,828 - 

2022-01-10 12:40:54,829 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:40:58,447 - Epoch: [103][  250/  500]    Overall Loss 0.626901    Objective Loss 0.626901                                        LR 0.000250    Time 0.014464    
2022-01-10 12:41:01,739 - Epoch: [103][  500/  500]    Overall Loss 0.637527    Objective Loss 0.637527    Top1 82.500000    Top5 96.500000    LR 0.000250    Time 0.013813    
2022-01-10 12:41:01,794 - --- validate (epoch=103)-----------
2022-01-10 12:41:01,794 - 10000 samples (100 per mini-batch)
2022-01-10 12:41:02,579 - Epoch: [103][  100/  100]    Loss 1.550178    Top1 60.170000    Top5 86.040000    
2022-01-10 12:41:02,633 - ==> Top1: 60.170    Top5: 86.040    Loss: 1.550

2022-01-10 12:41:02,635 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:41:02,635 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:41:02,654 - 

2022-01-10 12:41:02,655 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:41:06,311 - Epoch: [104][  250/  500]    Overall Loss 0.628771    Objective Loss 0.628771                                        LR 0.000250    Time 0.014616    
2022-01-10 12:41:09,710 - Epoch: [104][  500/  500]    Overall Loss 0.625373    Objective Loss 0.625373    Top1 80.000000    Top5 98.000000    LR 0.000250    Time 0.014103    
2022-01-10 12:41:09,767 - --- validate (epoch=104)-----------
2022-01-10 12:41:09,767 - 10000 samples (100 per mini-batch)
2022-01-10 12:41:10,471 - Epoch: [104][  100/  100]    Loss 1.560839    Top1 59.870000    Top5 86.100000    
2022-01-10 12:41:10,528 - ==> Top1: 59.870    Top5: 86.100    Loss: 1.561

2022-01-10 12:41:10,530 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:41:10,530 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:41:10,552 - 

2022-01-10 12:41:10,552 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:41:14,053 - Epoch: [105][  250/  500]    Overall Loss 0.618130    Objective Loss 0.618130                                        LR 0.000250    Time 0.013993    
2022-01-10 12:41:17,304 - Epoch: [105][  500/  500]    Overall Loss 0.621316    Objective Loss 0.621316    Top1 77.500000    Top5 98.000000    LR 0.000250    Time 0.013495    
2022-01-10 12:41:17,360 - --- validate (epoch=105)-----------
2022-01-10 12:41:17,360 - 10000 samples (100 per mini-batch)
2022-01-10 12:41:18,102 - Epoch: [105][  100/  100]    Loss 1.572085    Top1 59.560000    Top5 86.250000    
2022-01-10 12:41:18,153 - ==> Top1: 59.560    Top5: 86.250    Loss: 1.572

2022-01-10 12:41:18,154 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:41:18,155 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:41:18,176 - 

2022-01-10 12:41:18,177 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:41:21,822 - Epoch: [106][  250/  500]    Overall Loss 0.605980    Objective Loss 0.605980                                        LR 0.000250    Time 0.014573    
2022-01-10 12:41:25,109 - Epoch: [106][  500/  500]    Overall Loss 0.618478    Objective Loss 0.618478    Top1 74.000000    Top5 98.500000    LR 0.000250    Time 0.013855    
2022-01-10 12:41:25,158 - --- validate (epoch=106)-----------
2022-01-10 12:41:25,158 - 10000 samples (100 per mini-batch)
2022-01-10 12:41:25,897 - Epoch: [106][  100/  100]    Loss 1.565147    Top1 59.930000    Top5 86.060000    
2022-01-10 12:41:25,955 - ==> Top1: 59.930    Top5: 86.060    Loss: 1.565

2022-01-10 12:41:25,957 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:41:25,957 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:41:25,979 - 

2022-01-10 12:41:25,979 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:41:29,520 - Epoch: [107][  250/  500]    Overall Loss 0.613040    Objective Loss 0.613040                                        LR 0.000250    Time 0.014152    
2022-01-10 12:41:32,790 - Epoch: [107][  500/  500]    Overall Loss 0.615614    Objective Loss 0.615614    Top1 83.500000    Top5 97.500000    LR 0.000250    Time 0.013613    
2022-01-10 12:41:32,846 - --- validate (epoch=107)-----------
2022-01-10 12:41:32,846 - 10000 samples (100 per mini-batch)
2022-01-10 12:41:33,631 - Epoch: [107][  100/  100]    Loss 1.571423    Top1 60.110000    Top5 86.110000    
2022-01-10 12:41:33,685 - ==> Top1: 60.110    Top5: 86.110    Loss: 1.571

2022-01-10 12:41:33,687 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:41:33,687 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:41:33,703 - 

2022-01-10 12:41:33,703 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:41:37,370 - Epoch: [108][  250/  500]    Overall Loss 0.600328    Objective Loss 0.600328                                        LR 0.000250    Time 0.014660    
2022-01-10 12:41:40,606 - Epoch: [108][  500/  500]    Overall Loss 0.611075    Objective Loss 0.611075    Top1 80.000000    Top5 97.500000    LR 0.000250    Time 0.013797    
2022-01-10 12:41:40,655 - --- validate (epoch=108)-----------
2022-01-10 12:41:40,655 - 10000 samples (100 per mini-batch)
2022-01-10 12:41:41,389 - Epoch: [108][  100/  100]    Loss 1.581361    Top1 59.730000    Top5 85.980000    
2022-01-10 12:41:41,440 - ==> Top1: 59.730    Top5: 85.980    Loss: 1.581

2022-01-10 12:41:41,442 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:41:41,442 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:41:41,464 - 

2022-01-10 12:41:41,464 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:41:45,048 - Epoch: [109][  250/  500]    Overall Loss 0.598507    Objective Loss 0.598507                                        LR 0.000250    Time 0.014329    
2022-01-10 12:41:48,355 - Epoch: [109][  500/  500]    Overall Loss 0.606941    Objective Loss 0.606941    Top1 89.500000    Top5 98.500000    LR 0.000250    Time 0.013773    
2022-01-10 12:41:48,403 - --- validate (epoch=109)-----------
2022-01-10 12:41:48,403 - 10000 samples (100 per mini-batch)
2022-01-10 12:41:49,160 - Epoch: [109][  100/  100]    Loss 1.588565    Top1 59.520000    Top5 86.070000    
2022-01-10 12:41:49,210 - ==> Top1: 59.520    Top5: 86.070    Loss: 1.589

2022-01-10 12:41:49,212 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:41:49,212 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:41:49,234 - 

2022-01-10 12:41:49,234 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:41:52,903 - Epoch: [110][  250/  500]    Overall Loss 0.593195    Objective Loss 0.593195                                        LR 0.000250    Time 0.014667    
2022-01-10 12:41:56,239 - Epoch: [110][  500/  500]    Overall Loss 0.603764    Objective Loss 0.603764    Top1 82.500000    Top5 98.500000    LR 0.000250    Time 0.014002    
2022-01-10 12:41:56,287 - --- validate (epoch=110)-----------
2022-01-10 12:41:56,288 - 10000 samples (100 per mini-batch)
2022-01-10 12:41:57,068 - Epoch: [110][  100/  100]    Loss 1.591081    Top1 59.780000    Top5 86.120000    
2022-01-10 12:41:57,125 - ==> Top1: 59.780    Top5: 86.120    Loss: 1.591

2022-01-10 12:41:57,127 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:41:57,127 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:41:57,149 - 

2022-01-10 12:41:57,149 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:42:00,571 - Epoch: [111][  250/  500]    Overall Loss 0.597599    Objective Loss 0.597599                                        LR 0.000250    Time 0.013678    
2022-01-10 12:42:03,876 - Epoch: [111][  500/  500]    Overall Loss 0.598110    Objective Loss 0.598110    Top1 79.500000    Top5 98.000000    LR 0.000250    Time 0.013446    
2022-01-10 12:42:03,933 - --- validate (epoch=111)-----------
2022-01-10 12:42:03,933 - 10000 samples (100 per mini-batch)
2022-01-10 12:42:04,696 - Epoch: [111][  100/  100]    Loss 1.583471    Top1 59.980000    Top5 86.170000    
2022-01-10 12:42:04,751 - ==> Top1: 59.980    Top5: 86.170    Loss: 1.583

2022-01-10 12:42:04,753 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:42:04,753 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:42:04,775 - 

2022-01-10 12:42:04,775 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:42:08,272 - Epoch: [112][  250/  500]    Overall Loss 0.585989    Objective Loss 0.585989                                        LR 0.000250    Time 0.013977    
2022-01-10 12:42:11,516 - Epoch: [112][  500/  500]    Overall Loss 0.598413    Objective Loss 0.598413    Top1 81.500000    Top5 98.000000    LR 0.000250    Time 0.013474    
2022-01-10 12:42:11,570 - --- validate (epoch=112)-----------
2022-01-10 12:42:11,570 - 10000 samples (100 per mini-batch)
2022-01-10 12:42:12,268 - Epoch: [112][  100/  100]    Loss 1.594621    Top1 59.670000    Top5 85.870000    
2022-01-10 12:42:12,318 - ==> Top1: 59.670    Top5: 85.870    Loss: 1.595

2022-01-10 12:42:12,320 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:42:12,320 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:42:12,336 - 

2022-01-10 12:42:12,336 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:42:16,189 - Epoch: [113][  250/  500]    Overall Loss 0.585706    Objective Loss 0.585706                                        LR 0.000250    Time 0.015405    
2022-01-10 12:42:19,520 - Epoch: [113][  500/  500]    Overall Loss 0.592438    Objective Loss 0.592438    Top1 78.500000    Top5 98.500000    LR 0.000250    Time 0.014359    
2022-01-10 12:42:19,573 - --- validate (epoch=113)-----------
2022-01-10 12:42:19,573 - 10000 samples (100 per mini-batch)
2022-01-10 12:42:20,283 - Epoch: [113][  100/  100]    Loss 1.602161    Top1 59.660000    Top5 85.880000    
2022-01-10 12:42:20,337 - ==> Top1: 59.660    Top5: 85.880    Loss: 1.602

2022-01-10 12:42:20,339 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:42:20,339 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:42:20,361 - 

2022-01-10 12:42:20,361 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:42:23,802 - Epoch: [114][  250/  500]    Overall Loss 0.591018    Objective Loss 0.591018                                        LR 0.000250    Time 0.013754    
2022-01-10 12:42:27,100 - Epoch: [114][  500/  500]    Overall Loss 0.593137    Objective Loss 0.593137    Top1 85.500000    Top5 98.000000    LR 0.000250    Time 0.013468    
2022-01-10 12:42:27,154 - --- validate (epoch=114)-----------
2022-01-10 12:42:27,154 - 10000 samples (100 per mini-batch)
2022-01-10 12:42:27,881 - Epoch: [114][  100/  100]    Loss 1.592096    Top1 59.830000    Top5 85.870000    
2022-01-10 12:42:27,932 - ==> Top1: 59.830    Top5: 85.870    Loss: 1.592

2022-01-10 12:42:27,934 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:42:27,934 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:42:27,956 - 

2022-01-10 12:42:27,956 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:42:31,590 - Epoch: [115][  250/  500]    Overall Loss 0.585967    Objective Loss 0.585967                                        LR 0.000250    Time 0.014526    
2022-01-10 12:42:34,966 - Epoch: [115][  500/  500]    Overall Loss 0.586742    Objective Loss 0.586742    Top1 83.500000    Top5 98.000000    LR 0.000250    Time 0.014012    
2022-01-10 12:42:35,023 - --- validate (epoch=115)-----------
2022-01-10 12:42:35,023 - 10000 samples (100 per mini-batch)
2022-01-10 12:42:35,765 - Epoch: [115][  100/  100]    Loss 1.596223    Top1 59.930000    Top5 86.020000    
2022-01-10 12:42:35,816 - ==> Top1: 59.930    Top5: 86.020    Loss: 1.596

2022-01-10 12:42:35,818 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:42:35,818 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:42:35,840 - 

2022-01-10 12:42:35,840 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:42:39,245 - Epoch: [116][  250/  500]    Overall Loss 0.573645    Objective Loss 0.573645                                        LR 0.000250    Time 0.013607    
2022-01-10 12:42:42,446 - Epoch: [116][  500/  500]    Overall Loss 0.581380    Objective Loss 0.581380    Top1 81.500000    Top5 99.000000    LR 0.000250    Time 0.013202    
2022-01-10 12:42:42,511 - --- validate (epoch=116)-----------
2022-01-10 12:42:42,511 - 10000 samples (100 per mini-batch)
2022-01-10 12:42:43,270 - Epoch: [116][  100/  100]    Loss 1.604812    Top1 59.660000    Top5 85.820000    
2022-01-10 12:42:43,321 - ==> Top1: 59.660    Top5: 85.820    Loss: 1.605

2022-01-10 12:42:43,323 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:42:43,323 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:42:43,338 - 

2022-01-10 12:42:43,338 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:42:46,824 - Epoch: [117][  250/  500]    Overall Loss 0.568033    Objective Loss 0.568033                                        LR 0.000250    Time 0.013936    
2022-01-10 12:42:50,093 - Epoch: [117][  500/  500]    Overall Loss 0.580497    Objective Loss 0.580497    Top1 82.000000    Top5 98.500000    LR 0.000250    Time 0.013502    
2022-01-10 12:42:50,145 - --- validate (epoch=117)-----------
2022-01-10 12:42:50,145 - 10000 samples (100 per mini-batch)
2022-01-10 12:42:50,879 - Epoch: [117][  100/  100]    Loss 1.609685    Top1 59.350000    Top5 86.330000    
2022-01-10 12:42:50,935 - ==> Top1: 59.350    Top5: 86.330    Loss: 1.610

2022-01-10 12:42:50,937 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:42:50,937 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:42:50,959 - 

2022-01-10 12:42:50,959 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:42:54,545 - Epoch: [118][  250/  500]    Overall Loss 0.574487    Objective Loss 0.574487                                        LR 0.000250    Time 0.014332    
2022-01-10 12:42:57,801 - Epoch: [118][  500/  500]    Overall Loss 0.580443    Objective Loss 0.580443    Top1 81.500000    Top5 98.500000    LR 0.000250    Time 0.013675    
2022-01-10 12:42:57,850 - --- validate (epoch=118)-----------
2022-01-10 12:42:57,850 - 10000 samples (100 per mini-batch)
2022-01-10 12:42:58,579 - Epoch: [118][  100/  100]    Loss 1.601738    Top1 60.100000    Top5 86.000000    
2022-01-10 12:42:58,630 - ==> Top1: 60.100    Top5: 86.000    Loss: 1.602

2022-01-10 12:42:58,632 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:42:58,632 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:42:58,654 - 

2022-01-10 12:42:58,654 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:43:02,104 - Epoch: [119][  250/  500]    Overall Loss 0.582084    Objective Loss 0.582084                                        LR 0.000250    Time 0.013793    
2022-01-10 12:43:05,398 - Epoch: [119][  500/  500]    Overall Loss 0.581634    Objective Loss 0.581634    Top1 81.000000    Top5 97.000000    LR 0.000250    Time 0.013481    
2022-01-10 12:43:05,453 - --- validate (epoch=119)-----------
2022-01-10 12:43:05,453 - 10000 samples (100 per mini-batch)
2022-01-10 12:43:06,285 - Epoch: [119][  100/  100]    Loss 1.602171    Top1 60.100000    Top5 86.060000    
2022-01-10 12:43:06,341 - ==> Top1: 60.100    Top5: 86.060    Loss: 1.602

2022-01-10 12:43:06,343 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:43:06,343 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:43:06,366 - 

2022-01-10 12:43:06,366 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:43:09,837 - Epoch: [120][  250/  500]    Overall Loss 0.562121    Objective Loss 0.562121                                        LR 0.000250    Time 0.013875    
2022-01-10 12:43:13,166 - Epoch: [120][  500/  500]    Overall Loss 0.571747    Objective Loss 0.571747    Top1 79.500000    Top5 97.500000    LR 0.000250    Time 0.013593    
2022-01-10 12:43:13,221 - --- validate (epoch=120)-----------
2022-01-10 12:43:13,221 - 10000 samples (100 per mini-batch)
2022-01-10 12:43:13,945 - Epoch: [120][  100/  100]    Loss 1.632166    Top1 59.550000    Top5 85.890000    
2022-01-10 12:43:13,995 - ==> Top1: 59.550    Top5: 85.890    Loss: 1.632

2022-01-10 12:43:13,997 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:43:13,997 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:43:14,012 - 

2022-01-10 12:43:14,013 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:43:17,901 - Epoch: [121][  250/  500]    Overall Loss 0.572535    Objective Loss 0.572535                                        LR 0.000250    Time 0.015546    
2022-01-10 12:43:21,161 - Epoch: [121][  500/  500]    Overall Loss 0.574757    Objective Loss 0.574757    Top1 85.000000    Top5 99.000000    LR 0.000250    Time 0.014288    
2022-01-10 12:43:21,216 - --- validate (epoch=121)-----------
2022-01-10 12:43:21,216 - 10000 samples (100 per mini-batch)
2022-01-10 12:43:21,911 - Epoch: [121][  100/  100]    Loss 1.616947    Top1 59.570000    Top5 85.720000    
2022-01-10 12:43:21,961 - ==> Top1: 59.570    Top5: 85.720    Loss: 1.617

2022-01-10 12:43:21,963 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:43:21,963 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:43:21,985 - 

2022-01-10 12:43:21,985 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:43:25,407 - Epoch: [122][  250/  500]    Overall Loss 0.574119    Objective Loss 0.574119                                        LR 0.000250    Time 0.013677    
2022-01-10 12:43:28,616 - Epoch: [122][  500/  500]    Overall Loss 0.575780    Objective Loss 0.575780    Top1 83.000000    Top5 97.500000    LR 0.000250    Time 0.013252    
2022-01-10 12:43:28,673 - --- validate (epoch=122)-----------
2022-01-10 12:43:28,673 - 10000 samples (100 per mini-batch)
2022-01-10 12:43:29,469 - Epoch: [122][  100/  100]    Loss 1.609694    Top1 59.440000    Top5 85.800000    
2022-01-10 12:43:29,521 - ==> Top1: 59.440    Top5: 85.800    Loss: 1.610

2022-01-10 12:43:29,523 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:43:29,523 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:43:29,545 - 

2022-01-10 12:43:29,545 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:43:33,030 - Epoch: [123][  250/  500]    Overall Loss 0.568750    Objective Loss 0.568750                                        LR 0.000250    Time 0.013931    
2022-01-10 12:43:36,304 - Epoch: [123][  500/  500]    Overall Loss 0.571893    Objective Loss 0.571893    Top1 85.000000    Top5 98.500000    LR 0.000250    Time 0.013509    
2022-01-10 12:43:36,352 - --- validate (epoch=123)-----------
2022-01-10 12:43:36,352 - 10000 samples (100 per mini-batch)
2022-01-10 12:43:37,065 - Epoch: [123][  100/  100]    Loss 1.625111    Top1 59.260000    Top5 85.820000    
2022-01-10 12:43:37,115 - ==> Top1: 59.260    Top5: 85.820    Loss: 1.625

2022-01-10 12:43:37,117 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:43:37,117 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:43:37,139 - 

2022-01-10 12:43:37,139 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:43:40,888 - Epoch: [124][  250/  500]    Overall Loss 0.564527    Objective Loss 0.564527                                        LR 0.000250    Time 0.014987    
2022-01-10 12:43:44,646 - Epoch: [124][  500/  500]    Overall Loss 0.570673    Objective Loss 0.570673    Top1 79.500000    Top5 97.500000    LR 0.000250    Time 0.015005    
2022-01-10 12:43:44,695 - --- validate (epoch=124)-----------
2022-01-10 12:43:44,695 - 10000 samples (100 per mini-batch)
2022-01-10 12:43:45,447 - Epoch: [124][  100/  100]    Loss 1.627684    Top1 59.340000    Top5 85.940000    
2022-01-10 12:43:45,504 - ==> Top1: 59.340    Top5: 85.940    Loss: 1.628

2022-01-10 12:43:45,506 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:43:45,506 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:43:45,527 - 

2022-01-10 12:43:45,528 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:43:49,133 - Epoch: [125][  250/  500]    Overall Loss 0.562710    Objective Loss 0.562710                                        LR 0.000250    Time 0.014411    
2022-01-10 12:43:52,770 - Epoch: [125][  500/  500]    Overall Loss 0.565903    Objective Loss 0.565903    Top1 86.000000    Top5 99.000000    LR 0.000250    Time 0.014476    
2022-01-10 12:43:52,825 - --- validate (epoch=125)-----------
2022-01-10 12:43:52,825 - 10000 samples (100 per mini-batch)
2022-01-10 12:43:53,538 - Epoch: [125][  100/  100]    Loss 1.626178    Top1 59.950000    Top5 85.620000    
2022-01-10 12:43:53,594 - ==> Top1: 59.950    Top5: 85.620    Loss: 1.626

2022-01-10 12:43:53,596 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:43:53,596 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:43:53,618 - 

2022-01-10 12:43:53,618 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:43:57,174 - Epoch: [126][  250/  500]    Overall Loss 0.564391    Objective Loss 0.564391                                        LR 0.000250    Time 0.014213    
2022-01-10 12:44:00,432 - Epoch: [126][  500/  500]    Overall Loss 0.564307    Objective Loss 0.564307    Top1 84.000000    Top5 98.000000    LR 0.000250    Time 0.013619    
2022-01-10 12:44:00,480 - --- validate (epoch=126)-----------
2022-01-10 12:44:00,480 - 10000 samples (100 per mini-batch)
2022-01-10 12:44:01,223 - Epoch: [126][  100/  100]    Loss 1.606378    Top1 59.950000    Top5 86.390000    
2022-01-10 12:44:01,276 - ==> Top1: 59.950    Top5: 86.390    Loss: 1.606

2022-01-10 12:44:01,278 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:44:01,278 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:44:01,300 - 

2022-01-10 12:44:01,300 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:44:04,946 - Epoch: [127][  250/  500]    Overall Loss 0.557785    Objective Loss 0.557785                                        LR 0.000250    Time 0.014572    
2022-01-10 12:44:08,558 - Epoch: [127][  500/  500]    Overall Loss 0.563381    Objective Loss 0.563381    Top1 79.500000    Top5 96.500000    LR 0.000250    Time 0.014507    
2022-01-10 12:44:08,612 - --- validate (epoch=127)-----------
2022-01-10 12:44:08,612 - 10000 samples (100 per mini-batch)
2022-01-10 12:44:09,336 - Epoch: [127][  100/  100]    Loss 1.627889    Top1 59.600000    Top5 86.060000    
2022-01-10 12:44:09,392 - ==> Top1: 59.600    Top5: 86.060    Loss: 1.628

2022-01-10 12:44:09,394 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:44:09,394 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:44:09,417 - 

2022-01-10 12:44:09,417 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:44:12,867 - Epoch: [128][  250/  500]    Overall Loss 0.552102    Objective Loss 0.552102                                        LR 0.000250    Time 0.013792    
2022-01-10 12:44:16,077 - Epoch: [128][  500/  500]    Overall Loss 0.559902    Objective Loss 0.559902    Top1 82.000000    Top5 97.500000    LR 0.000250    Time 0.013313    
2022-01-10 12:44:16,132 - --- validate (epoch=128)-----------
2022-01-10 12:44:16,132 - 10000 samples (100 per mini-batch)
2022-01-10 12:44:16,865 - Epoch: [128][  100/  100]    Loss 1.624462    Top1 59.790000    Top5 85.940000    
2022-01-10 12:44:16,923 - ==> Top1: 59.790    Top5: 85.940    Loss: 1.624

2022-01-10 12:44:16,925 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:44:16,925 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:44:16,947 - 

2022-01-10 12:44:16,947 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:44:20,626 - Epoch: [129][  250/  500]    Overall Loss 0.555870    Objective Loss 0.555870                                        LR 0.000250    Time 0.014704    
2022-01-10 12:44:24,026 - Epoch: [129][  500/  500]    Overall Loss 0.557037    Objective Loss 0.557037    Top1 78.500000    Top5 97.000000    LR 0.000250    Time 0.014147    
2022-01-10 12:44:24,075 - --- validate (epoch=129)-----------
2022-01-10 12:44:24,075 - 10000 samples (100 per mini-batch)
2022-01-10 12:44:24,812 - Epoch: [129][  100/  100]    Loss 1.628395    Top1 60.130000    Top5 85.700000    
2022-01-10 12:44:24,875 - ==> Top1: 60.130    Top5: 85.700    Loss: 1.628

2022-01-10 12:44:24,876 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:44:24,876 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:44:24,893 - 

2022-01-10 12:44:24,893 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:44:28,430 - Epoch: [130][  250/  500]    Overall Loss 0.550889    Objective Loss 0.550889                                        LR 0.000250    Time 0.014140    
2022-01-10 12:44:31,678 - Epoch: [130][  500/  500]    Overall Loss 0.557545    Objective Loss 0.557545    Top1 83.500000    Top5 99.000000    LR 0.000250    Time 0.013562    
2022-01-10 12:44:31,735 - --- validate (epoch=130)-----------
2022-01-10 12:44:31,735 - 10000 samples (100 per mini-batch)
2022-01-10 12:44:32,512 - Epoch: [130][  100/  100]    Loss 1.637660    Top1 59.510000    Top5 85.780000    
2022-01-10 12:44:32,565 - ==> Top1: 59.510    Top5: 85.780    Loss: 1.638

2022-01-10 12:44:32,567 - ==> Best [Top1: 60.170   Top5: 86.040   Sparsity:0.00   Params: 381792 on epoch: 103]
2022-01-10 12:44:32,567 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:44:32,589 - 

2022-01-10 12:44:32,589 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:44:36,224 - Epoch: [131][  250/  500]    Overall Loss 0.545091    Objective Loss 0.545091                                        LR 0.000250    Time 0.014530    
2022-01-10 12:44:39,641 - Epoch: [131][  500/  500]    Overall Loss 0.551559    Objective Loss 0.551559    Top1 83.500000    Top5 98.000000    LR 0.000250    Time 0.014094    
2022-01-10 12:44:39,697 - --- validate (epoch=131)-----------
2022-01-10 12:44:39,697 - 10000 samples (100 per mini-batch)
2022-01-10 12:44:40,470 - Epoch: [131][  100/  100]    Loss 1.632696    Top1 60.180000    Top5 85.810000    
2022-01-10 12:44:40,527 - ==> Top1: 60.180    Top5: 85.810    Loss: 1.633

2022-01-10 12:44:40,529 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:44:40,529 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:44:40,554 - 

2022-01-10 12:44:40,555 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:44:44,286 - Epoch: [132][  250/  500]    Overall Loss 0.539556    Objective Loss 0.539556                                        LR 0.000250    Time 0.014915    
2022-01-10 12:44:47,555 - Epoch: [132][  500/  500]    Overall Loss 0.549218    Objective Loss 0.549218    Top1 79.500000    Top5 96.500000    LR 0.000250    Time 0.013993    
2022-01-10 12:44:47,606 - --- validate (epoch=132)-----------
2022-01-10 12:44:47,606 - 10000 samples (100 per mini-batch)
2022-01-10 12:44:48,298 - Epoch: [132][  100/  100]    Loss 1.634347    Top1 59.590000    Top5 86.180000    
2022-01-10 12:44:48,357 - ==> Top1: 59.590    Top5: 86.180    Loss: 1.634

2022-01-10 12:44:48,359 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:44:48,359 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:44:48,382 - 

2022-01-10 12:44:48,382 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:44:52,033 - Epoch: [133][  250/  500]    Overall Loss 0.546136    Objective Loss 0.546136                                        LR 0.000250    Time 0.014594    
2022-01-10 12:44:55,382 - Epoch: [133][  500/  500]    Overall Loss 0.547173    Objective Loss 0.547173    Top1 82.500000    Top5 96.000000    LR 0.000250    Time 0.013992    
2022-01-10 12:44:55,437 - --- validate (epoch=133)-----------
2022-01-10 12:44:55,438 - 10000 samples (100 per mini-batch)
2022-01-10 12:44:56,219 - Epoch: [133][  100/  100]    Loss 1.651238    Top1 59.510000    Top5 86.100000    
2022-01-10 12:44:56,273 - ==> Top1: 59.510    Top5: 86.100    Loss: 1.651

2022-01-10 12:44:56,275 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:44:56,275 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:44:56,291 - 

2022-01-10 12:44:56,291 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:44:59,718 - Epoch: [134][  250/  500]    Overall Loss 0.539343    Objective Loss 0.539343                                        LR 0.000250    Time 0.013698    
2022-01-10 12:45:03,031 - Epoch: [134][  500/  500]    Overall Loss 0.546073    Objective Loss 0.546073    Top1 81.500000    Top5 95.500000    LR 0.000250    Time 0.013473    
2022-01-10 12:45:03,088 - --- validate (epoch=134)-----------
2022-01-10 12:45:03,088 - 10000 samples (100 per mini-batch)
2022-01-10 12:45:03,834 - Epoch: [134][  100/  100]    Loss 1.663469    Top1 59.360000    Top5 85.810000    
2022-01-10 12:45:03,884 - ==> Top1: 59.360    Top5: 85.810    Loss: 1.663

2022-01-10 12:45:03,886 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:45:03,886 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:45:03,909 - 

2022-01-10 12:45:03,909 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:45:07,603 - Epoch: [135][  250/  500]    Overall Loss 0.537509    Objective Loss 0.537509                                        LR 0.000250    Time 0.014766    
2022-01-10 12:45:11,032 - Epoch: [135][  500/  500]    Overall Loss 0.545183    Objective Loss 0.545183    Top1 83.500000    Top5 99.000000    LR 0.000250    Time 0.014237    
2022-01-10 12:45:11,079 - --- validate (epoch=135)-----------
2022-01-10 12:45:11,079 - 10000 samples (100 per mini-batch)
2022-01-10 12:45:11,802 - Epoch: [135][  100/  100]    Loss 1.653022    Top1 59.610000    Top5 85.910000    
2022-01-10 12:45:11,860 - ==> Top1: 59.610    Top5: 85.910    Loss: 1.653

2022-01-10 12:45:11,862 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:45:11,862 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:45:11,884 - 

2022-01-10 12:45:11,884 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:45:15,419 - Epoch: [136][  250/  500]    Overall Loss 0.532842    Objective Loss 0.532842                                        LR 0.000250    Time 0.014131    
2022-01-10 12:45:18,856 - Epoch: [136][  500/  500]    Overall Loss 0.541140    Objective Loss 0.541140    Top1 84.500000    Top5 98.500000    LR 0.000250    Time 0.013935    
2022-01-10 12:45:18,905 - --- validate (epoch=136)-----------
2022-01-10 12:45:18,905 - 10000 samples (100 per mini-batch)
2022-01-10 12:45:19,619 - Epoch: [136][  100/  100]    Loss 1.648455    Top1 59.820000    Top5 85.940000    
2022-01-10 12:45:19,673 - ==> Top1: 59.820    Top5: 85.940    Loss: 1.648

2022-01-10 12:45:19,674 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:45:19,674 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:45:19,691 - 

2022-01-10 12:45:19,691 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:45:23,367 - Epoch: [137][  250/  500]    Overall Loss 0.527739    Objective Loss 0.527739                                        LR 0.000250    Time 0.014693    
2022-01-10 12:45:26,959 - Epoch: [137][  500/  500]    Overall Loss 0.531947    Objective Loss 0.531947    Top1 82.500000    Top5 97.500000    LR 0.000250    Time 0.014528    
2022-01-10 12:45:27,008 - --- validate (epoch=137)-----------
2022-01-10 12:45:27,008 - 10000 samples (100 per mini-batch)
2022-01-10 12:45:27,746 - Epoch: [137][  100/  100]    Loss 1.666240    Top1 59.470000    Top5 85.700000    
2022-01-10 12:45:27,801 - ==> Top1: 59.470    Top5: 85.700    Loss: 1.666

2022-01-10 12:45:27,803 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:45:27,803 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:45:27,825 - 

2022-01-10 12:45:27,826 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:45:31,652 - Epoch: [138][  250/  500]    Overall Loss 0.532995    Objective Loss 0.532995                                        LR 0.000250    Time 0.015297    
2022-01-10 12:45:34,932 - Epoch: [138][  500/  500]    Overall Loss 0.535996    Objective Loss 0.535996    Top1 77.500000    Top5 97.000000    LR 0.000250    Time 0.014205    
2022-01-10 12:45:34,984 - --- validate (epoch=138)-----------
2022-01-10 12:45:34,985 - 10000 samples (100 per mini-batch)
2022-01-10 12:45:35,689 - Epoch: [138][  100/  100]    Loss 1.663197    Top1 59.440000    Top5 85.730000    
2022-01-10 12:45:35,747 - ==> Top1: 59.440    Top5: 85.730    Loss: 1.663

2022-01-10 12:45:35,749 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:45:35,749 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:45:35,771 - 

2022-01-10 12:45:35,771 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:45:39,341 - Epoch: [139][  250/  500]    Overall Loss 0.535546    Objective Loss 0.535546                                        LR 0.000250    Time 0.014270    
2022-01-10 12:45:42,597 - Epoch: [139][  500/  500]    Overall Loss 0.542862    Objective Loss 0.542862    Top1 84.500000    Top5 98.000000    LR 0.000250    Time 0.013645    
2022-01-10 12:45:42,654 - --- validate (epoch=139)-----------
2022-01-10 12:45:42,655 - 10000 samples (100 per mini-batch)
2022-01-10 12:45:43,354 - Epoch: [139][  100/  100]    Loss 1.646206    Top1 60.050000    Top5 85.690000    
2022-01-10 12:45:43,409 - ==> Top1: 60.050    Top5: 85.690    Loss: 1.646

2022-01-10 12:45:43,411 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:45:43,411 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:45:43,433 - 

2022-01-10 12:45:43,433 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:45:46,852 - Epoch: [140][  250/  500]    Overall Loss 0.527382    Objective Loss 0.527382                                        LR 0.000250    Time 0.013665    
2022-01-10 12:45:50,068 - Epoch: [140][  500/  500]    Overall Loss 0.531122    Objective Loss 0.531122    Top1 85.000000    Top5 99.500000    LR 0.000250    Time 0.013261    
2022-01-10 12:45:50,123 - --- validate (epoch=140)-----------
2022-01-10 12:45:50,124 - 10000 samples (100 per mini-batch)
2022-01-10 12:45:50,874 - Epoch: [140][  100/  100]    Loss 1.653794    Top1 59.520000    Top5 85.960000    
2022-01-10 12:45:50,926 - ==> Top1: 59.520    Top5: 85.960    Loss: 1.654

2022-01-10 12:45:50,927 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:45:50,928 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:45:50,950 - 

2022-01-10 12:45:50,950 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:45:54,834 - Epoch: [141][  250/  500]    Overall Loss 0.517666    Objective Loss 0.517666                                        LR 0.000250    Time 0.015525    
2022-01-10 12:45:58,257 - Epoch: [141][  500/  500]    Overall Loss 0.529022    Objective Loss 0.529022    Top1 85.000000    Top5 98.500000    LR 0.000250    Time 0.014605    
2022-01-10 12:45:58,305 - --- validate (epoch=141)-----------
2022-01-10 12:45:58,305 - 10000 samples (100 per mini-batch)
2022-01-10 12:45:59,057 - Epoch: [141][  100/  100]    Loss 1.668065    Top1 59.260000    Top5 85.840000    
2022-01-10 12:45:59,109 - ==> Top1: 59.260    Top5: 85.840    Loss: 1.668

2022-01-10 12:45:59,111 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:45:59,111 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:45:59,134 - 

2022-01-10 12:45:59,134 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:46:02,728 - Epoch: [142][  250/  500]    Overall Loss 0.521851    Objective Loss 0.521851                                        LR 0.000250    Time 0.014367    
2022-01-10 12:46:06,047 - Epoch: [142][  500/  500]    Overall Loss 0.530804    Objective Loss 0.530804    Top1 84.000000    Top5 99.000000    LR 0.000250    Time 0.013817    
2022-01-10 12:46:06,102 - --- validate (epoch=142)-----------
2022-01-10 12:46:06,102 - 10000 samples (100 per mini-batch)
2022-01-10 12:46:06,869 - Epoch: [142][  100/  100]    Loss 1.660586    Top1 59.650000    Top5 85.870000    
2022-01-10 12:46:06,923 - ==> Top1: 59.650    Top5: 85.870    Loss: 1.661

2022-01-10 12:46:06,925 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:46:06,925 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:46:06,948 - 

2022-01-10 12:46:06,948 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:46:10,500 - Epoch: [143][  250/  500]    Overall Loss 0.523557    Objective Loss 0.523557                                        LR 0.000250    Time 0.014198    
2022-01-10 12:46:13,832 - Epoch: [143][  500/  500]    Overall Loss 0.529089    Objective Loss 0.529089    Top1 85.000000    Top5 98.000000    LR 0.000250    Time 0.013760    
2022-01-10 12:46:13,889 - --- validate (epoch=143)-----------
2022-01-10 12:46:13,889 - 10000 samples (100 per mini-batch)
2022-01-10 12:46:14,636 - Epoch: [143][  100/  100]    Loss 1.664445    Top1 59.690000    Top5 86.070000    
2022-01-10 12:46:14,688 - ==> Top1: 59.690    Top5: 86.070    Loss: 1.664

2022-01-10 12:46:14,690 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:46:14,690 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:46:14,712 - 

2022-01-10 12:46:14,712 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:46:18,337 - Epoch: [144][  250/  500]    Overall Loss 0.526646    Objective Loss 0.526646                                        LR 0.000250    Time 0.014490    
2022-01-10 12:46:21,733 - Epoch: [144][  500/  500]    Overall Loss 0.534346    Objective Loss 0.534346    Top1 83.000000    Top5 97.500000    LR 0.000250    Time 0.014033    
2022-01-10 12:46:21,783 - --- validate (epoch=144)-----------
2022-01-10 12:46:21,783 - 10000 samples (100 per mini-batch)
2022-01-10 12:46:22,489 - Epoch: [144][  100/  100]    Loss 1.670292    Top1 59.420000    Top5 85.690000    
2022-01-10 12:46:22,543 - ==> Top1: 59.420    Top5: 85.690    Loss: 1.670

2022-01-10 12:46:22,545 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:46:22,545 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:46:22,561 - 

2022-01-10 12:46:22,561 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:46:26,133 - Epoch: [145][  250/  500]    Overall Loss 0.514032    Objective Loss 0.514032                                        LR 0.000250    Time 0.014278    
2022-01-10 12:46:29,400 - Epoch: [145][  500/  500]    Overall Loss 0.525095    Objective Loss 0.525095    Top1 82.500000    Top5 97.500000    LR 0.000250    Time 0.013670    
2022-01-10 12:46:29,457 - --- validate (epoch=145)-----------
2022-01-10 12:46:29,457 - 10000 samples (100 per mini-batch)
2022-01-10 12:46:30,190 - Epoch: [145][  100/  100]    Loss 1.681341    Top1 59.280000    Top5 85.740000    
2022-01-10 12:46:30,241 - ==> Top1: 59.280    Top5: 85.740    Loss: 1.681

2022-01-10 12:46:30,243 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:46:30,243 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:46:30,265 - 

2022-01-10 12:46:30,265 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:46:33,763 - Epoch: [146][  250/  500]    Overall Loss 0.518523    Objective Loss 0.518523                                        LR 0.000250    Time 0.013982    
2022-01-10 12:46:37,386 - Epoch: [146][  500/  500]    Overall Loss 0.525682    Objective Loss 0.525682    Top1 85.500000    Top5 95.500000    LR 0.000250    Time 0.014232    
2022-01-10 12:46:37,441 - --- validate (epoch=146)-----------
2022-01-10 12:46:37,441 - 10000 samples (100 per mini-batch)
2022-01-10 12:46:38,199 - Epoch: [146][  100/  100]    Loss 1.665093    Top1 59.770000    Top5 85.920000    
2022-01-10 12:46:38,251 - ==> Top1: 59.770    Top5: 85.920    Loss: 1.665

2022-01-10 12:46:38,253 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:46:38,253 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:46:38,269 - 

2022-01-10 12:46:38,269 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:46:41,836 - Epoch: [147][  250/  500]    Overall Loss 0.525259    Objective Loss 0.525259                                        LR 0.000250    Time 0.014255    
2022-01-10 12:46:45,127 - Epoch: [147][  500/  500]    Overall Loss 0.524995    Objective Loss 0.524995    Top1 82.000000    Top5 96.500000    LR 0.000250    Time 0.013707    
2022-01-10 12:46:45,176 - --- validate (epoch=147)-----------
2022-01-10 12:46:45,176 - 10000 samples (100 per mini-batch)
2022-01-10 12:46:45,953 - Epoch: [147][  100/  100]    Loss 1.673752    Top1 59.870000    Top5 85.630000    
2022-01-10 12:46:46,004 - ==> Top1: 59.870    Top5: 85.630    Loss: 1.674

2022-01-10 12:46:46,006 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:46:46,006 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:46:46,029 - 

2022-01-10 12:46:46,029 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:46:49,566 - Epoch: [148][  250/  500]    Overall Loss 0.513349    Objective Loss 0.513349                                        LR 0.000250    Time 0.014138    
2022-01-10 12:46:52,834 - Epoch: [148][  500/  500]    Overall Loss 0.521532    Objective Loss 0.521532    Top1 81.500000    Top5 97.000000    LR 0.000250    Time 0.013602    
2022-01-10 12:46:52,891 - --- validate (epoch=148)-----------
2022-01-10 12:46:52,891 - 10000 samples (100 per mini-batch)
2022-01-10 12:46:53,685 - Epoch: [148][  100/  100]    Loss 1.674200    Top1 59.110000    Top5 86.240000    
2022-01-10 12:46:53,736 - ==> Top1: 59.110    Top5: 86.240    Loss: 1.674

2022-01-10 12:46:53,738 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:46:53,738 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:46:53,754 - 

2022-01-10 12:46:53,754 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:46:57,210 - Epoch: [149][  250/  500]    Overall Loss 0.515691    Objective Loss 0.515691                                        LR 0.000250    Time 0.013812    
2022-01-10 12:47:00,508 - Epoch: [149][  500/  500]    Overall Loss 0.517660    Objective Loss 0.517660    Top1 87.500000    Top5 97.500000    LR 0.000250    Time 0.013499    
2022-01-10 12:47:00,564 - --- validate (epoch=149)-----------
2022-01-10 12:47:00,564 - 10000 samples (100 per mini-batch)
2022-01-10 12:47:01,338 - Epoch: [149][  100/  100]    Loss 1.667407    Top1 59.270000    Top5 85.770000    
2022-01-10 12:47:01,391 - ==> Top1: 59.270    Top5: 85.770    Loss: 1.667

2022-01-10 12:47:01,393 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:47:01,393 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:47:01,415 - 

2022-01-10 12:47:01,416 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:47:05,002 - Epoch: [150][  250/  500]    Overall Loss 0.497664    Objective Loss 0.497664                                        LR 0.000063    Time 0.014335    
2022-01-10 12:47:08,454 - Epoch: [150][  500/  500]    Overall Loss 0.499043    Objective Loss 0.499043    Top1 86.000000    Top5 97.500000    LR 0.000063    Time 0.014068    
2022-01-10 12:47:08,517 - --- validate (epoch=150)-----------
2022-01-10 12:47:08,517 - 10000 samples (100 per mini-batch)
2022-01-10 12:47:09,297 - Epoch: [150][  100/  100]    Loss 1.650959    Top1 59.710000    Top5 86.170000    
2022-01-10 12:47:09,349 - ==> Top1: 59.710    Top5: 86.170    Loss: 1.651

2022-01-10 12:47:09,351 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:47:09,351 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:47:09,373 - 

2022-01-10 12:47:09,373 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:47:12,817 - Epoch: [151][  250/  500]    Overall Loss 0.480292    Objective Loss 0.480292                                        LR 0.000063    Time 0.013765    
2022-01-10 12:47:16,058 - Epoch: [151][  500/  500]    Overall Loss 0.490362    Objective Loss 0.490362    Top1 85.000000    Top5 100.000000    LR 0.000063    Time 0.013362    
2022-01-10 12:47:16,107 - --- validate (epoch=151)-----------
2022-01-10 12:47:16,107 - 10000 samples (100 per mini-batch)
2022-01-10 12:47:16,915 - Epoch: [151][  100/  100]    Loss 1.657519    Top1 59.730000    Top5 85.970000    
2022-01-10 12:47:16,972 - ==> Top1: 59.730    Top5: 85.970    Loss: 1.658

2022-01-10 12:47:16,974 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:47:16,974 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:47:16,996 - 

2022-01-10 12:47:16,997 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:47:20,470 - Epoch: [152][  250/  500]    Overall Loss 0.484366    Objective Loss 0.484366                                        LR 0.000063    Time 0.013885    
2022-01-10 12:47:23,733 - Epoch: [152][  500/  500]    Overall Loss 0.484414    Objective Loss 0.484414    Top1 83.000000    Top5 98.500000    LR 0.000063    Time 0.013463    
2022-01-10 12:47:23,789 - --- validate (epoch=152)-----------
2022-01-10 12:47:23,790 - 10000 samples (100 per mini-batch)
2022-01-10 12:47:24,546 - Epoch: [152][  100/  100]    Loss 1.659206    Top1 59.750000    Top5 85.880000    
2022-01-10 12:47:24,599 - ==> Top1: 59.750    Top5: 85.880    Loss: 1.659

2022-01-10 12:47:24,601 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:47:24,601 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:47:24,623 - 

2022-01-10 12:47:24,623 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:47:28,133 - Epoch: [153][  250/  500]    Overall Loss 0.482060    Objective Loss 0.482060                                        LR 0.000063    Time 0.014029    
2022-01-10 12:47:31,469 - Epoch: [153][  500/  500]    Overall Loss 0.484717    Objective Loss 0.484717    Top1 83.000000    Top5 98.000000    LR 0.000063    Time 0.013683    
2022-01-10 12:47:31,522 - --- validate (epoch=153)-----------
2022-01-10 12:47:31,522 - 10000 samples (100 per mini-batch)
2022-01-10 12:47:32,262 - Epoch: [153][  100/  100]    Loss 1.654895    Top1 59.800000    Top5 85.990000    
2022-01-10 12:47:32,313 - ==> Top1: 59.800    Top5: 85.990    Loss: 1.655

2022-01-10 12:47:32,315 - ==> Best [Top1: 60.180   Top5: 85.810   Sparsity:0.00   Params: 381792 on epoch: 131]
2022-01-10 12:47:32,315 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:47:32,338 - 

2022-01-10 12:47:32,338 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:47:35,827 - Epoch: [154][  250/  500]    Overall Loss 0.476400    Objective Loss 0.476400                                        LR 0.000063    Time 0.013947    
2022-01-10 12:47:39,199 - Epoch: [154][  500/  500]    Overall Loss 0.483690    Objective Loss 0.483690    Top1 80.500000    Top5 97.500000    LR 0.000063    Time 0.013714    
2022-01-10 12:47:39,249 - --- validate (epoch=154)-----------
2022-01-10 12:47:39,249 - 10000 samples (100 per mini-batch)
2022-01-10 12:47:40,071 - Epoch: [154][  100/  100]    Loss 1.655540    Top1 60.270000    Top5 85.920000    
2022-01-10 12:47:40,120 - ==> Top1: 60.270    Top5: 85.920    Loss: 1.656

2022-01-10 12:47:40,122 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:47:40,122 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:47:40,142 - 

2022-01-10 12:47:40,142 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:47:43,590 - Epoch: [155][  250/  500]    Overall Loss 0.483089    Objective Loss 0.483089                                        LR 0.000063    Time 0.013783    
2022-01-10 12:47:46,857 - Epoch: [155][  500/  500]    Overall Loss 0.486146    Objective Loss 0.486146    Top1 89.000000    Top5 100.000000    LR 0.000063    Time 0.013423    
2022-01-10 12:47:46,906 - --- validate (epoch=155)-----------
2022-01-10 12:47:46,906 - 10000 samples (100 per mini-batch)
2022-01-10 12:47:47,596 - Epoch: [155][  100/  100]    Loss 1.662438    Top1 59.860000    Top5 85.980000    
2022-01-10 12:47:47,654 - ==> Top1: 59.860    Top5: 85.980    Loss: 1.662

2022-01-10 12:47:47,656 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:47:47,656 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:47:47,678 - 

2022-01-10 12:47:47,678 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:47:51,228 - Epoch: [156][  250/  500]    Overall Loss 0.475874    Objective Loss 0.475874                                        LR 0.000063    Time 0.014188    
2022-01-10 12:47:54,506 - Epoch: [156][  500/  500]    Overall Loss 0.483758    Objective Loss 0.483758    Top1 79.500000    Top5 98.000000    LR 0.000063    Time 0.013647    
2022-01-10 12:47:54,555 - --- validate (epoch=156)-----------
2022-01-10 12:47:54,555 - 10000 samples (100 per mini-batch)
2022-01-10 12:47:55,258 - Epoch: [156][  100/  100]    Loss 1.657684    Top1 59.890000    Top5 85.820000    
2022-01-10 12:47:55,308 - ==> Top1: 59.890    Top5: 85.820    Loss: 1.658

2022-01-10 12:47:55,310 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:47:55,310 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:47:55,332 - 

2022-01-10 12:47:55,333 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:47:58,967 - Epoch: [157][  250/  500]    Overall Loss 0.481933    Objective Loss 0.481933                                        LR 0.000063    Time 0.014528    
2022-01-10 12:48:02,428 - Epoch: [157][  500/  500]    Overall Loss 0.477550    Objective Loss 0.477550    Top1 85.000000    Top5 98.500000    LR 0.000063    Time 0.014181    
2022-01-10 12:48:02,483 - --- validate (epoch=157)-----------
2022-01-10 12:48:02,483 - 10000 samples (100 per mini-batch)
2022-01-10 12:48:03,292 - Epoch: [157][  100/  100]    Loss 1.659677    Top1 59.940000    Top5 85.830000    
2022-01-10 12:48:03,348 - ==> Top1: 59.940    Top5: 85.830    Loss: 1.660

2022-01-10 12:48:03,350 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:48:03,350 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:48:03,372 - 

2022-01-10 12:48:03,372 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:48:06,968 - Epoch: [158][  250/  500]    Overall Loss 0.477284    Objective Loss 0.477284                                        LR 0.000063    Time 0.014374    
2022-01-10 12:48:10,276 - Epoch: [158][  500/  500]    Overall Loss 0.471350    Objective Loss 0.471350    Top1 87.000000    Top5 99.000000    LR 0.000063    Time 0.013800    
2022-01-10 12:48:10,331 - --- validate (epoch=158)-----------
2022-01-10 12:48:10,331 - 10000 samples (100 per mini-batch)
2022-01-10 12:48:11,071 - Epoch: [158][  100/  100]    Loss 1.658946    Top1 59.920000    Top5 85.920000    
2022-01-10 12:48:11,129 - ==> Top1: 59.920    Top5: 85.920    Loss: 1.659

2022-01-10 12:48:11,131 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:48:11,131 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:48:11,146 - 

2022-01-10 12:48:11,146 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:48:14,629 - Epoch: [159][  250/  500]    Overall Loss 0.473354    Objective Loss 0.473354                                        LR 0.000063    Time 0.013921    
2022-01-10 12:48:17,847 - Epoch: [159][  500/  500]    Overall Loss 0.475999    Objective Loss 0.475999    Top1 83.500000    Top5 99.500000    LR 0.000063    Time 0.013394    
2022-01-10 12:48:17,900 - --- validate (epoch=159)-----------
2022-01-10 12:48:17,900 - 10000 samples (100 per mini-batch)
2022-01-10 12:48:18,650 - Epoch: [159][  100/  100]    Loss 1.658537    Top1 60.020000    Top5 85.920000    
2022-01-10 12:48:18,700 - ==> Top1: 60.020    Top5: 85.920    Loss: 1.659

2022-01-10 12:48:18,702 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:48:18,702 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:48:18,724 - 

2022-01-10 12:48:18,724 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:48:22,283 - Epoch: [160][  250/  500]    Overall Loss 0.473973    Objective Loss 0.473973                                        LR 0.000063    Time 0.014225    
2022-01-10 12:48:25,564 - Epoch: [160][  500/  500]    Overall Loss 0.476951    Objective Loss 0.476951    Top1 79.500000    Top5 99.000000    LR 0.000063    Time 0.013671    
2022-01-10 12:48:25,612 - --- validate (epoch=160)-----------
2022-01-10 12:48:25,612 - 10000 samples (100 per mini-batch)
2022-01-10 12:48:26,450 - Epoch: [160][  100/  100]    Loss 1.661489    Top1 59.900000    Top5 85.880000    
2022-01-10 12:48:26,501 - ==> Top1: 59.900    Top5: 85.880    Loss: 1.661

2022-01-10 12:48:26,503 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:48:26,503 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:48:26,525 - 

2022-01-10 12:48:26,526 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:48:30,027 - Epoch: [161][  250/  500]    Overall Loss 0.466633    Objective Loss 0.466633                                        LR 0.000063    Time 0.013997    
2022-01-10 12:48:33,372 - Epoch: [161][  500/  500]    Overall Loss 0.473252    Objective Loss 0.473252    Top1 85.000000    Top5 98.500000    LR 0.000063    Time 0.013684    
2022-01-10 12:48:33,429 - --- validate (epoch=161)-----------
2022-01-10 12:48:33,429 - 10000 samples (100 per mini-batch)
2022-01-10 12:48:34,164 - Epoch: [161][  100/  100]    Loss 1.665025    Top1 59.910000    Top5 85.920000    
2022-01-10 12:48:34,222 - ==> Top1: 59.910    Top5: 85.920    Loss: 1.665

2022-01-10 12:48:34,223 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:48:34,224 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:48:34,246 - 

2022-01-10 12:48:34,246 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:48:37,862 - Epoch: [162][  250/  500]    Overall Loss 0.462701    Objective Loss 0.462701                                        LR 0.000063    Time 0.014453    
2022-01-10 12:48:41,118 - Epoch: [162][  500/  500]    Overall Loss 0.479000    Objective Loss 0.479000    Top1 84.000000    Top5 98.500000    LR 0.000063    Time 0.013735    
2022-01-10 12:48:41,177 - --- validate (epoch=162)-----------
2022-01-10 12:48:41,177 - 10000 samples (100 per mini-batch)
2022-01-10 12:48:41,932 - Epoch: [162][  100/  100]    Loss 1.665969    Top1 59.870000    Top5 85.700000    
2022-01-10 12:48:41,985 - ==> Top1: 59.870    Top5: 85.700    Loss: 1.666

2022-01-10 12:48:41,987 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:48:41,987 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:48:42,009 - 

2022-01-10 12:48:42,009 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:48:45,570 - Epoch: [163][  250/  500]    Overall Loss 0.469004    Objective Loss 0.469004                                        LR 0.000063    Time 0.014233    
2022-01-10 12:48:48,937 - Epoch: [163][  500/  500]    Overall Loss 0.472714    Objective Loss 0.472714    Top1 86.000000    Top5 98.500000    LR 0.000063    Time 0.013846    
2022-01-10 12:48:48,993 - --- validate (epoch=163)-----------
2022-01-10 12:48:48,994 - 10000 samples (100 per mini-batch)
2022-01-10 12:48:49,780 - Epoch: [163][  100/  100]    Loss 1.662473    Top1 59.950000    Top5 85.810000    
2022-01-10 12:48:49,837 - ==> Top1: 59.950    Top5: 85.810    Loss: 1.662

2022-01-10 12:48:49,839 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:48:49,839 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:48:49,861 - 

2022-01-10 12:48:49,861 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:48:53,257 - Epoch: [164][  250/  500]    Overall Loss 0.469222    Objective Loss 0.469222                                        LR 0.000063    Time 0.013573    
2022-01-10 12:48:56,481 - Epoch: [164][  500/  500]    Overall Loss 0.470388    Objective Loss 0.470388    Top1 86.000000    Top5 98.500000    LR 0.000063    Time 0.013231    
2022-01-10 12:48:56,536 - --- validate (epoch=164)-----------
2022-01-10 12:48:56,536 - 10000 samples (100 per mini-batch)
2022-01-10 12:48:57,259 - Epoch: [164][  100/  100]    Loss 1.669671    Top1 59.880000    Top5 85.800000    
2022-01-10 12:48:57,315 - ==> Top1: 59.880    Top5: 85.800    Loss: 1.670

2022-01-10 12:48:57,317 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:48:57,317 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:48:57,339 - 

2022-01-10 12:48:57,339 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:49:00,868 - Epoch: [165][  250/  500]    Overall Loss 0.471250    Objective Loss 0.471250                                        LR 0.000063    Time 0.014108    
2022-01-10 12:49:04,168 - Epoch: [165][  500/  500]    Overall Loss 0.472209    Objective Loss 0.472209    Top1 85.000000    Top5 98.500000    LR 0.000063    Time 0.013650    
2022-01-10 12:49:04,225 - --- validate (epoch=165)-----------
2022-01-10 12:49:04,225 - 10000 samples (100 per mini-batch)
2022-01-10 12:49:04,952 - Epoch: [165][  100/  100]    Loss 1.674312    Top1 59.720000    Top5 85.630000    
2022-01-10 12:49:05,002 - ==> Top1: 59.720    Top5: 85.630    Loss: 1.674

2022-01-10 12:49:05,004 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:49:05,004 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:49:05,026 - 

2022-01-10 12:49:05,026 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:49:08,628 - Epoch: [166][  250/  500]    Overall Loss 0.464145    Objective Loss 0.464145                                        LR 0.000063    Time 0.014398    
2022-01-10 12:49:12,036 - Epoch: [166][  500/  500]    Overall Loss 0.469967    Objective Loss 0.469967    Top1 85.500000    Top5 98.000000    LR 0.000063    Time 0.014011    
2022-01-10 12:49:12,092 - --- validate (epoch=166)-----------
2022-01-10 12:49:12,092 - 10000 samples (100 per mini-batch)
2022-01-10 12:49:12,841 - Epoch: [166][  100/  100]    Loss 1.672684    Top1 59.620000    Top5 85.680000    
2022-01-10 12:49:12,892 - ==> Top1: 59.620    Top5: 85.680    Loss: 1.673

2022-01-10 12:49:12,894 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:49:12,894 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:49:12,916 - 

2022-01-10 12:49:12,916 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:49:16,453 - Epoch: [167][  250/  500]    Overall Loss 0.472938    Objective Loss 0.472938                                        LR 0.000063    Time 0.014137    
2022-01-10 12:49:19,711 - Epoch: [167][  500/  500]    Overall Loss 0.471585    Objective Loss 0.471585    Top1 84.000000    Top5 97.500000    LR 0.000063    Time 0.013581    
2022-01-10 12:49:19,760 - --- validate (epoch=167)-----------
2022-01-10 12:49:19,760 - 10000 samples (100 per mini-batch)
2022-01-10 12:49:20,493 - Epoch: [167][  100/  100]    Loss 1.669403    Top1 59.910000    Top5 85.940000    
2022-01-10 12:49:20,545 - ==> Top1: 59.910    Top5: 85.940    Loss: 1.669

2022-01-10 12:49:20,547 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:49:20,547 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:49:20,569 - 

2022-01-10 12:49:20,569 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:49:24,098 - Epoch: [168][  250/  500]    Overall Loss 0.467337    Objective Loss 0.467337                                        LR 0.000063    Time 0.014105    
2022-01-10 12:49:27,347 - Epoch: [168][  500/  500]    Overall Loss 0.471318    Objective Loss 0.471318    Top1 80.500000    Top5 98.500000    LR 0.000063    Time 0.013546    
2022-01-10 12:49:27,403 - --- validate (epoch=168)-----------
2022-01-10 12:49:27,403 - 10000 samples (100 per mini-batch)
2022-01-10 12:49:28,139 - Epoch: [168][  100/  100]    Loss 1.673733    Top1 59.940000    Top5 85.930000    
2022-01-10 12:49:28,194 - ==> Top1: 59.940    Top5: 85.930    Loss: 1.674

2022-01-10 12:49:28,195 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:49:28,196 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:49:28,211 - 

2022-01-10 12:49:28,211 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:49:31,679 - Epoch: [169][  250/  500]    Overall Loss 0.468811    Objective Loss 0.468811                                        LR 0.000063    Time 0.013861    
2022-01-10 12:49:34,892 - Epoch: [169][  500/  500]    Overall Loss 0.472036    Objective Loss 0.472036    Top1 80.000000    Top5 96.000000    LR 0.000063    Time 0.013353    
2022-01-10 12:49:34,942 - --- validate (epoch=169)-----------
2022-01-10 12:49:34,942 - 10000 samples (100 per mini-batch)
2022-01-10 12:49:35,679 - Epoch: [169][  100/  100]    Loss 1.676108    Top1 59.970000    Top5 85.700000    
2022-01-10 12:49:35,732 - ==> Top1: 59.970    Top5: 85.700    Loss: 1.676

2022-01-10 12:49:35,734 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:49:35,734 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:49:35,757 - 

2022-01-10 12:49:35,757 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:49:39,378 - Epoch: [170][  250/  500]    Overall Loss 0.467545    Objective Loss 0.467545                                        LR 0.000063    Time 0.014474    
2022-01-10 12:49:42,690 - Epoch: [170][  500/  500]    Overall Loss 0.466518    Objective Loss 0.466518    Top1 78.000000    Top5 98.000000    LR 0.000063    Time 0.013857    
2022-01-10 12:49:42,745 - --- validate (epoch=170)-----------
2022-01-10 12:49:42,745 - 10000 samples (100 per mini-batch)
2022-01-10 12:49:43,465 - Epoch: [170][  100/  100]    Loss 1.671379    Top1 59.910000    Top5 85.650000    
2022-01-10 12:49:43,517 - ==> Top1: 59.910    Top5: 85.650    Loss: 1.671

2022-01-10 12:49:43,518 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:49:43,518 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:49:43,540 - 

2022-01-10 12:49:43,540 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:49:47,146 - Epoch: [171][  250/  500]    Overall Loss 0.465760    Objective Loss 0.465760                                        LR 0.000063    Time 0.014414    
2022-01-10 12:49:50,305 - Epoch: [171][  500/  500]    Overall Loss 0.468899    Objective Loss 0.468899    Top1 81.000000    Top5 97.000000    LR 0.000063    Time 0.013521    
2022-01-10 12:49:50,362 - --- validate (epoch=171)-----------
2022-01-10 12:49:50,363 - 10000 samples (100 per mini-batch)
2022-01-10 12:49:51,116 - Epoch: [171][  100/  100]    Loss 1.674377    Top1 59.880000    Top5 85.660000    
2022-01-10 12:49:51,171 - ==> Top1: 59.880    Top5: 85.660    Loss: 1.674

2022-01-10 12:49:51,172 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:49:51,173 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:49:51,195 - 

2022-01-10 12:49:51,195 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:49:54,599 - Epoch: [172][  250/  500]    Overall Loss 0.467658    Objective Loss 0.467658                                        LR 0.000063    Time 0.013607    
2022-01-10 12:49:57,802 - Epoch: [172][  500/  500]    Overall Loss 0.471865    Objective Loss 0.471865    Top1 86.000000    Top5 98.000000    LR 0.000063    Time 0.013206    
2022-01-10 12:49:57,859 - --- validate (epoch=172)-----------
2022-01-10 12:49:57,859 - 10000 samples (100 per mini-batch)
2022-01-10 12:49:58,621 - Epoch: [172][  100/  100]    Loss 1.674301    Top1 59.800000    Top5 85.770000    
2022-01-10 12:49:58,682 - ==> Top1: 59.800    Top5: 85.770    Loss: 1.674

2022-01-10 12:49:58,684 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:49:58,684 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:49:58,707 - 

2022-01-10 12:49:58,707 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:50:02,285 - Epoch: [173][  250/  500]    Overall Loss 0.458256    Objective Loss 0.458256                                        LR 0.000063    Time 0.014303    
2022-01-10 12:50:05,599 - Epoch: [173][  500/  500]    Overall Loss 0.465331    Objective Loss 0.465331    Top1 85.500000    Top5 98.000000    LR 0.000063    Time 0.013775    
2022-01-10 12:50:05,652 - --- validate (epoch=173)-----------
2022-01-10 12:50:05,652 - 10000 samples (100 per mini-batch)
2022-01-10 12:50:06,399 - Epoch: [173][  100/  100]    Loss 1.675931    Top1 59.640000    Top5 85.760000    
2022-01-10 12:50:06,452 - ==> Top1: 59.640    Top5: 85.760    Loss: 1.676

2022-01-10 12:50:06,454 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:50:06,454 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:50:06,477 - 

2022-01-10 12:50:06,477 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:50:10,072 - Epoch: [174][  250/  500]    Overall Loss 0.460194    Objective Loss 0.460194                                        LR 0.000063    Time 0.014373    
2022-01-10 12:50:13,395 - Epoch: [174][  500/  500]    Overall Loss 0.464580    Objective Loss 0.464580    Top1 86.000000    Top5 99.000000    LR 0.000063    Time 0.013828    
2022-01-10 12:50:13,453 - --- validate (epoch=174)-----------
2022-01-10 12:50:13,453 - 10000 samples (100 per mini-batch)
2022-01-10 12:50:14,166 - Epoch: [174][  100/  100]    Loss 1.675977    Top1 59.690000    Top5 85.680000    
2022-01-10 12:50:14,221 - ==> Top1: 59.690    Top5: 85.680    Loss: 1.676

2022-01-10 12:50:14,223 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:50:14,223 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:50:14,239 - 

2022-01-10 12:50:14,239 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:50:17,760 - Epoch: [175][  250/  500]    Overall Loss 0.461005    Objective Loss 0.461005                                        LR 0.000063    Time 0.014075    
2022-01-10 12:50:21,078 - Epoch: [175][  500/  500]    Overall Loss 0.464437    Objective Loss 0.464437    Top1 83.500000    Top5 98.500000    LR 0.000063    Time 0.013670    
2022-01-10 12:50:21,126 - --- validate (epoch=175)-----------
2022-01-10 12:50:21,126 - 10000 samples (100 per mini-batch)
2022-01-10 12:50:21,898 - Epoch: [175][  100/  100]    Loss 1.672473    Top1 59.790000    Top5 85.830000    
2022-01-10 12:50:21,950 - ==> Top1: 59.790    Top5: 85.830    Loss: 1.672

2022-01-10 12:50:21,952 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:50:21,952 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:50:21,974 - 

2022-01-10 12:50:21,974 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:50:25,592 - Epoch: [176][  250/  500]    Overall Loss 0.461293    Objective Loss 0.461293                                        LR 0.000063    Time 0.014461    
2022-01-10 12:50:28,855 - Epoch: [176][  500/  500]    Overall Loss 0.459832    Objective Loss 0.459832    Top1 87.500000    Top5 99.500000    LR 0.000063    Time 0.013752    
2022-01-10 12:50:28,912 - --- validate (epoch=176)-----------
2022-01-10 12:50:28,912 - 10000 samples (100 per mini-batch)
2022-01-10 12:50:29,666 - Epoch: [176][  100/  100]    Loss 1.679922    Top1 59.710000    Top5 85.760000    
2022-01-10 12:50:29,715 - ==> Top1: 59.710    Top5: 85.760    Loss: 1.680

2022-01-10 12:50:29,717 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:50:29,717 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:50:29,740 - 

2022-01-10 12:50:29,740 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:50:33,279 - Epoch: [177][  250/  500]    Overall Loss 0.460701    Objective Loss 0.460701                                        LR 0.000063    Time 0.014147    
2022-01-10 12:50:36,580 - Epoch: [177][  500/  500]    Overall Loss 0.468509    Objective Loss 0.468509    Top1 90.000000    Top5 100.000000    LR 0.000063    Time 0.013670    
2022-01-10 12:50:36,634 - --- validate (epoch=177)-----------
2022-01-10 12:50:36,634 - 10000 samples (100 per mini-batch)
2022-01-10 12:50:37,395 - Epoch: [177][  100/  100]    Loss 1.676915    Top1 59.740000    Top5 85.970000    
2022-01-10 12:50:37,449 - ==> Top1: 59.740    Top5: 85.970    Loss: 1.677

2022-01-10 12:50:37,451 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:50:37,451 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:50:37,467 - 

2022-01-10 12:50:37,467 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:50:41,052 - Epoch: [178][  250/  500]    Overall Loss 0.466705    Objective Loss 0.466705                                        LR 0.000063    Time 0.014328    
2022-01-10 12:50:44,378 - Epoch: [178][  500/  500]    Overall Loss 0.468006    Objective Loss 0.468006    Top1 88.500000    Top5 99.500000    LR 0.000063    Time 0.013814    
2022-01-10 12:50:44,434 - --- validate (epoch=178)-----------
2022-01-10 12:50:44,435 - 10000 samples (100 per mini-batch)
2022-01-10 12:50:45,164 - Epoch: [178][  100/  100]    Loss 1.680631    Top1 59.670000    Top5 85.830000    
2022-01-10 12:50:45,214 - ==> Top1: 59.670    Top5: 85.830    Loss: 1.681

2022-01-10 12:50:45,216 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:50:45,216 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:50:45,238 - 

2022-01-10 12:50:45,238 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:50:48,846 - Epoch: [179][  250/  500]    Overall Loss 0.464167    Objective Loss 0.464167                                        LR 0.000063    Time 0.014424    
2022-01-10 12:50:52,093 - Epoch: [179][  500/  500]    Overall Loss 0.465211    Objective Loss 0.465211    Top1 82.000000    Top5 99.000000    LR 0.000063    Time 0.013702    
2022-01-10 12:50:52,150 - --- validate (epoch=179)-----------
2022-01-10 12:50:52,150 - 10000 samples (100 per mini-batch)
2022-01-10 12:50:52,889 - Epoch: [179][  100/  100]    Loss 1.676812    Top1 59.710000    Top5 85.700000    
2022-01-10 12:50:52,948 - ==> Top1: 59.710    Top5: 85.700    Loss: 1.677

2022-01-10 12:50:52,950 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:50:52,950 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:50:52,972 - 

2022-01-10 12:50:52,972 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:50:56,564 - Epoch: [180][  250/  500]    Overall Loss 0.457426    Objective Loss 0.457426                                        LR 0.000063    Time 0.014357    
2022-01-10 12:51:00,201 - Epoch: [180][  500/  500]    Overall Loss 0.463946    Objective Loss 0.463946    Top1 86.500000    Top5 99.500000    LR 0.000063    Time 0.014448    
2022-01-10 12:51:00,262 - --- validate (epoch=180)-----------
2022-01-10 12:51:00,262 - 10000 samples (100 per mini-batch)
2022-01-10 12:51:00,979 - Epoch: [180][  100/  100]    Loss 1.680919    Top1 59.810000    Top5 85.690000    
2022-01-10 12:51:01,034 - ==> Top1: 59.810    Top5: 85.690    Loss: 1.681

2022-01-10 12:51:01,036 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:51:01,036 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:51:01,058 - 

2022-01-10 12:51:01,059 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:51:04,666 - Epoch: [181][  250/  500]    Overall Loss 0.466554    Objective Loss 0.466554                                        LR 0.000063    Time 0.014420    
2022-01-10 12:51:08,122 - Epoch: [181][  500/  500]    Overall Loss 0.464348    Objective Loss 0.464348    Top1 87.000000    Top5 98.500000    LR 0.000063    Time 0.014118    
2022-01-10 12:51:08,179 - --- validate (epoch=181)-----------
2022-01-10 12:51:08,179 - 10000 samples (100 per mini-batch)
2022-01-10 12:51:08,941 - Epoch: [181][  100/  100]    Loss 1.682017    Top1 59.590000    Top5 85.720000    
2022-01-10 12:51:08,991 - ==> Top1: 59.590    Top5: 85.720    Loss: 1.682

2022-01-10 12:51:08,993 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:51:08,993 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:51:09,015 - 

2022-01-10 12:51:09,015 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:51:12,661 - Epoch: [182][  250/  500]    Overall Loss 0.454736    Objective Loss 0.454736                                        LR 0.000063    Time 0.014574    
2022-01-10 12:51:15,975 - Epoch: [182][  500/  500]    Overall Loss 0.459324    Objective Loss 0.459324    Top1 87.500000    Top5 98.500000    LR 0.000063    Time 0.013912    
2022-01-10 12:51:16,030 - --- validate (epoch=182)-----------
2022-01-10 12:51:16,030 - 10000 samples (100 per mini-batch)
2022-01-10 12:51:16,766 - Epoch: [182][  100/  100]    Loss 1.683872    Top1 59.780000    Top5 85.720000    
2022-01-10 12:51:16,817 - ==> Top1: 59.780    Top5: 85.720    Loss: 1.684

2022-01-10 12:51:16,819 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:51:16,819 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:51:16,841 - 

2022-01-10 12:51:16,841 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:51:20,353 - Epoch: [183][  250/  500]    Overall Loss 0.456163    Objective Loss 0.456163                                        LR 0.000063    Time 0.014038    
2022-01-10 12:51:23,575 - Epoch: [183][  500/  500]    Overall Loss 0.460029    Objective Loss 0.460029    Top1 86.000000    Top5 99.500000    LR 0.000063    Time 0.013459    
2022-01-10 12:51:23,624 - --- validate (epoch=183)-----------
2022-01-10 12:51:23,624 - 10000 samples (100 per mini-batch)
2022-01-10 12:51:24,348 - Epoch: [183][  100/  100]    Loss 1.683907    Top1 59.690000    Top5 85.710000    
2022-01-10 12:51:24,402 - ==> Top1: 59.690    Top5: 85.710    Loss: 1.684

2022-01-10 12:51:24,404 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:51:24,404 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:51:24,426 - 

2022-01-10 12:51:24,426 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:51:27,923 - Epoch: [184][  250/  500]    Overall Loss 0.457634    Objective Loss 0.457634                                        LR 0.000063    Time 0.013978    
2022-01-10 12:51:31,525 - Epoch: [184][  500/  500]    Overall Loss 0.458933    Objective Loss 0.458933    Top1 87.000000    Top5 99.000000    LR 0.000063    Time 0.014189    
2022-01-10 12:51:31,581 - --- validate (epoch=184)-----------
2022-01-10 12:51:31,581 - 10000 samples (100 per mini-batch)
2022-01-10 12:51:32,308 - Epoch: [184][  100/  100]    Loss 1.688904    Top1 59.660000    Top5 85.580000    
2022-01-10 12:51:32,358 - ==> Top1: 59.660    Top5: 85.580    Loss: 1.689

2022-01-10 12:51:32,360 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:51:32,360 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:51:32,382 - 

2022-01-10 12:51:32,382 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:51:35,968 - Epoch: [185][  250/  500]    Overall Loss 0.453879    Objective Loss 0.453879                                        LR 0.000063    Time 0.014333    
2022-01-10 12:51:39,223 - Epoch: [185][  500/  500]    Overall Loss 0.458294    Objective Loss 0.458294    Top1 78.000000    Top5 97.500000    LR 0.000063    Time 0.013673    
2022-01-10 12:51:39,272 - --- validate (epoch=185)-----------
2022-01-10 12:51:39,272 - 10000 samples (100 per mini-batch)
2022-01-10 12:51:39,998 - Epoch: [185][  100/  100]    Loss 1.688200    Top1 59.720000    Top5 85.570000    
2022-01-10 12:51:40,052 - ==> Top1: 59.720    Top5: 85.570    Loss: 1.688

2022-01-10 12:51:40,054 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:51:40,054 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:51:40,070 - 

2022-01-10 12:51:40,070 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:51:43,519 - Epoch: [186][  250/  500]    Overall Loss 0.464172    Objective Loss 0.464172                                        LR 0.000063    Time 0.013788    
2022-01-10 12:51:46,700 - Epoch: [186][  500/  500]    Overall Loss 0.463575    Objective Loss 0.463575    Top1 87.500000    Top5 97.000000    LR 0.000063    Time 0.013251    
2022-01-10 12:51:46,747 - --- validate (epoch=186)-----------
2022-01-10 12:51:46,747 - 10000 samples (100 per mini-batch)
2022-01-10 12:51:47,535 - Epoch: [186][  100/  100]    Loss 1.692967    Top1 59.460000    Top5 85.460000    
2022-01-10 12:51:47,589 - ==> Top1: 59.460    Top5: 85.460    Loss: 1.693

2022-01-10 12:51:47,591 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:51:47,591 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:51:47,613 - 

2022-01-10 12:51:47,613 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:51:51,075 - Epoch: [187][  250/  500]    Overall Loss 0.453930    Objective Loss 0.453930                                        LR 0.000063    Time 0.013836    
2022-01-10 12:51:54,332 - Epoch: [187][  500/  500]    Overall Loss 0.458401    Objective Loss 0.458401    Top1 85.500000    Top5 98.500000    LR 0.000063    Time 0.013429    
2022-01-10 12:51:54,388 - --- validate (epoch=187)-----------
2022-01-10 12:51:54,388 - 10000 samples (100 per mini-batch)
2022-01-10 12:51:55,134 - Epoch: [187][  100/  100]    Loss 1.690656    Top1 59.340000    Top5 85.770000    
2022-01-10 12:51:55,192 - ==> Top1: 59.340    Top5: 85.770    Loss: 1.691

2022-01-10 12:51:55,193 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:51:55,193 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:51:55,216 - 

2022-01-10 12:51:55,216 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:51:58,775 - Epoch: [188][  250/  500]    Overall Loss 0.462226    Objective Loss 0.462226                                        LR 0.000063    Time 0.014225    
2022-01-10 12:52:02,109 - Epoch: [188][  500/  500]    Overall Loss 0.458160    Objective Loss 0.458160    Top1 89.500000    Top5 98.500000    LR 0.000063    Time 0.013776    
2022-01-10 12:52:02,158 - --- validate (epoch=188)-----------
2022-01-10 12:52:02,158 - 10000 samples (100 per mini-batch)
2022-01-10 12:52:02,936 - Epoch: [188][  100/  100]    Loss 1.686877    Top1 59.740000    Top5 85.590000    
2022-01-10 12:52:02,991 - ==> Top1: 59.740    Top5: 85.590    Loss: 1.687

2022-01-10 12:52:02,993 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:52:02,993 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:52:03,016 - 

2022-01-10 12:52:03,016 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:52:06,495 - Epoch: [189][  250/  500]    Overall Loss 0.454037    Objective Loss 0.454037                                        LR 0.000063    Time 0.013906    
2022-01-10 12:52:09,753 - Epoch: [189][  500/  500]    Overall Loss 0.455528    Objective Loss 0.455528    Top1 85.000000    Top5 99.500000    LR 0.000063    Time 0.013465    
2022-01-10 12:52:09,809 - --- validate (epoch=189)-----------
2022-01-10 12:52:09,809 - 10000 samples (100 per mini-batch)
2022-01-10 12:52:10,589 - Epoch: [189][  100/  100]    Loss 1.689750    Top1 59.470000    Top5 85.690000    
2022-01-10 12:52:10,640 - ==> Top1: 59.470    Top5: 85.690    Loss: 1.690

2022-01-10 12:52:10,641 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:52:10,641 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:52:10,664 - 

2022-01-10 12:52:10,664 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:52:14,202 - Epoch: [190][  250/  500]    Overall Loss 0.457597    Objective Loss 0.457597                                        LR 0.000063    Time 0.014142    
2022-01-10 12:52:17,489 - Epoch: [190][  500/  500]    Overall Loss 0.460128    Objective Loss 0.460128    Top1 84.000000    Top5 99.000000    LR 0.000063    Time 0.013641    
2022-01-10 12:52:17,544 - --- validate (epoch=190)-----------
2022-01-10 12:52:17,544 - 10000 samples (100 per mini-batch)
2022-01-10 12:52:18,264 - Epoch: [190][  100/  100]    Loss 1.682576    Top1 59.970000    Top5 85.730000    
2022-01-10 12:52:18,312 - ==> Top1: 59.970    Top5: 85.730    Loss: 1.683

2022-01-10 12:52:18,314 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:52:18,314 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:52:18,336 - 

2022-01-10 12:52:18,336 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:52:21,891 - Epoch: [191][  250/  500]    Overall Loss 0.452318    Objective Loss 0.452318                                        LR 0.000063    Time 0.014209    
2022-01-10 12:52:25,265 - Epoch: [191][  500/  500]    Overall Loss 0.452992    Objective Loss 0.452992    Top1 86.000000    Top5 98.000000    LR 0.000063    Time 0.013848    
2022-01-10 12:52:25,314 - --- validate (epoch=191)-----------
2022-01-10 12:52:25,314 - 10000 samples (100 per mini-batch)
2022-01-10 12:52:26,117 - Epoch: [191][  100/  100]    Loss 1.689669    Top1 59.800000    Top5 85.570000    
2022-01-10 12:52:26,172 - ==> Top1: 59.800    Top5: 85.570    Loss: 1.690

2022-01-10 12:52:26,174 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:52:26,174 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:52:26,191 - 

2022-01-10 12:52:26,191 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:52:29,677 - Epoch: [192][  250/  500]    Overall Loss 0.448186    Objective Loss 0.448186                                        LR 0.000063    Time 0.013933    
2022-01-10 12:52:32,932 - Epoch: [192][  500/  500]    Overall Loss 0.456781    Objective Loss 0.456781    Top1 80.500000    Top5 98.000000    LR 0.000063    Time 0.013474    
2022-01-10 12:52:32,988 - --- validate (epoch=192)-----------
2022-01-10 12:52:32,988 - 10000 samples (100 per mini-batch)
2022-01-10 12:52:33,822 - Epoch: [192][  100/  100]    Loss 1.691432    Top1 59.850000    Top5 85.730000    
2022-01-10 12:52:33,871 - ==> Top1: 59.850    Top5: 85.730    Loss: 1.691

2022-01-10 12:52:33,873 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:52:33,873 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:52:33,895 - 

2022-01-10 12:52:33,895 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:52:37,529 - Epoch: [193][  250/  500]    Overall Loss 0.452283    Objective Loss 0.452283                                        LR 0.000063    Time 0.014526    
2022-01-10 12:52:41,145 - Epoch: [193][  500/  500]    Overall Loss 0.455178    Objective Loss 0.455178    Top1 86.000000    Top5 99.000000    LR 0.000063    Time 0.014490    
2022-01-10 12:52:41,197 - --- validate (epoch=193)-----------
2022-01-10 12:52:41,198 - 10000 samples (100 per mini-batch)
2022-01-10 12:52:41,937 - Epoch: [193][  100/  100]    Loss 1.691170    Top1 59.750000    Top5 85.680000    
2022-01-10 12:52:41,991 - ==> Top1: 59.750    Top5: 85.680    Loss: 1.691

2022-01-10 12:52:41,993 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:52:41,993 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:52:42,008 - 

2022-01-10 12:52:42,008 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:52:45,535 - Epoch: [194][  250/  500]    Overall Loss 0.452755    Objective Loss 0.452755                                        LR 0.000063    Time 0.014096    
2022-01-10 12:52:48,811 - Epoch: [194][  500/  500]    Overall Loss 0.454549    Objective Loss 0.454549    Top1 88.500000    Top5 99.500000    LR 0.000063    Time 0.013597    
2022-01-10 12:52:48,867 - --- validate (epoch=194)-----------
2022-01-10 12:52:48,867 - 10000 samples (100 per mini-batch)
2022-01-10 12:52:49,613 - Epoch: [194][  100/  100]    Loss 1.691257    Top1 59.530000    Top5 85.470000    
2022-01-10 12:52:49,666 - ==> Top1: 59.530    Top5: 85.470    Loss: 1.691

2022-01-10 12:52:49,668 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:52:49,668 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:52:49,690 - 

2022-01-10 12:52:49,690 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:52:53,198 - Epoch: [195][  250/  500]    Overall Loss 0.449341    Objective Loss 0.449341                                        LR 0.000063    Time 0.014024    
2022-01-10 12:52:56,513 - Epoch: [195][  500/  500]    Overall Loss 0.455697    Objective Loss 0.455697    Top1 83.500000    Top5 99.500000    LR 0.000063    Time 0.013637    
2022-01-10 12:52:56,571 - --- validate (epoch=195)-----------
2022-01-10 12:52:56,571 - 10000 samples (100 per mini-batch)
2022-01-10 12:52:57,408 - Epoch: [195][  100/  100]    Loss 1.696841    Top1 59.690000    Top5 85.520000    
2022-01-10 12:52:57,461 - ==> Top1: 59.690    Top5: 85.520    Loss: 1.697

2022-01-10 12:52:57,463 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:52:57,463 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:52:57,485 - 

2022-01-10 12:52:57,485 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:53:01,230 - Epoch: [196][  250/  500]    Overall Loss 0.455252    Objective Loss 0.455252                                        LR 0.000063    Time 0.014970    
2022-01-10 12:53:04,872 - Epoch: [196][  500/  500]    Overall Loss 0.452472    Objective Loss 0.452472    Top1 84.000000    Top5 100.000000    LR 0.000063    Time 0.014764    
2022-01-10 12:53:04,921 - --- validate (epoch=196)-----------
2022-01-10 12:53:04,921 - 10000 samples (100 per mini-batch)
2022-01-10 12:53:05,637 - Epoch: [196][  100/  100]    Loss 1.697177    Top1 59.740000    Top5 85.570000    
2022-01-10 12:53:05,694 - ==> Top1: 59.740    Top5: 85.570    Loss: 1.697

2022-01-10 12:53:05,696 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:53:05,696 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:53:05,712 - 

2022-01-10 12:53:05,712 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:53:09,233 - Epoch: [197][  250/  500]    Overall Loss 0.448103    Objective Loss 0.448103                                        LR 0.000063    Time 0.014072    
2022-01-10 12:53:12,506 - Epoch: [197][  500/  500]    Overall Loss 0.455792    Objective Loss 0.455792    Top1 85.500000    Top5 98.500000    LR 0.000063    Time 0.013579    
2022-01-10 12:53:12,561 - --- validate (epoch=197)-----------
2022-01-10 12:53:12,561 - 10000 samples (100 per mini-batch)
2022-01-10 12:53:13,282 - Epoch: [197][  100/  100]    Loss 1.697122    Top1 59.960000    Top5 85.590000    
2022-01-10 12:53:13,333 - ==> Top1: 59.960    Top5: 85.590    Loss: 1.697

2022-01-10 12:53:13,334 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:53:13,335 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:53:13,357 - 

2022-01-10 12:53:13,357 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:53:16,819 - Epoch: [198][  250/  500]    Overall Loss 0.456596    Objective Loss 0.456596                                        LR 0.000063    Time 0.013838    
2022-01-10 12:53:20,333 - Epoch: [198][  500/  500]    Overall Loss 0.453771    Objective Loss 0.453771    Top1 84.000000    Top5 99.000000    LR 0.000063    Time 0.013944    
2022-01-10 12:53:20,382 - --- validate (epoch=198)-----------
2022-01-10 12:53:20,382 - 10000 samples (100 per mini-batch)
2022-01-10 12:53:21,232 - Epoch: [198][  100/  100]    Loss 1.701560    Top1 59.750000    Top5 85.550000    
2022-01-10 12:53:21,288 - ==> Top1: 59.750    Top5: 85.550    Loss: 1.702

2022-01-10 12:53:21,290 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:53:21,290 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:53:21,312 - 

2022-01-10 12:53:21,312 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:53:24,929 - Epoch: [199][  250/  500]    Overall Loss 0.452085    Objective Loss 0.452085                                        LR 0.000063    Time 0.014457    
2022-01-10 12:53:28,269 - Epoch: [199][  500/  500]    Overall Loss 0.454239    Objective Loss 0.454239    Top1 79.500000    Top5 98.000000    LR 0.000063    Time 0.013905    
2022-01-10 12:53:28,317 - --- validate (epoch=199)-----------
2022-01-10 12:53:28,317 - 10000 samples (100 per mini-batch)
2022-01-10 12:53:29,049 - Epoch: [199][  100/  100]    Loss 1.697167    Top1 59.420000    Top5 85.430000    
2022-01-10 12:53:29,102 - ==> Top1: 59.420    Top5: 85.430    Loss: 1.697

2022-01-10 12:53:29,103 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:53:29,104 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:53:29,126 - 

2022-01-10 12:53:29,126 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:53:32,642 - Epoch: [200][  250/  500]    Overall Loss 0.451068    Objective Loss 0.451068                                        LR 0.000016    Time 0.014055    
2022-01-10 12:53:35,904 - Epoch: [200][  500/  500]    Overall Loss 0.451250    Objective Loss 0.451250    Top1 87.000000    Top5 97.500000    LR 0.000016    Time 0.013549    
2022-01-10 12:53:35,962 - --- validate (epoch=200)-----------
2022-01-10 12:53:35,962 - 10000 samples (100 per mini-batch)
2022-01-10 12:53:36,712 - Epoch: [200][  100/  100]    Loss 1.698368    Top1 59.600000    Top5 85.620000    
2022-01-10 12:53:36,764 - ==> Top1: 59.600    Top5: 85.620    Loss: 1.698

2022-01-10 12:53:36,766 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:53:36,766 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:53:36,788 - 

2022-01-10 12:53:36,788 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:53:40,250 - Epoch: [201][  250/  500]    Overall Loss 0.440553    Objective Loss 0.440553                                        LR 0.000016    Time 0.013839    
2022-01-10 12:53:43,529 - Epoch: [201][  500/  500]    Overall Loss 0.443175    Objective Loss 0.443175    Top1 83.000000    Top5 98.500000    LR 0.000016    Time 0.013473    
2022-01-10 12:53:43,580 - --- validate (epoch=201)-----------
2022-01-10 12:53:43,581 - 10000 samples (100 per mini-batch)
2022-01-10 12:53:44,373 - Epoch: [201][  100/  100]    Loss 1.703162    Top1 59.370000    Top5 85.510000    
2022-01-10 12:53:44,424 - ==> Top1: 59.370    Top5: 85.510    Loss: 1.703

2022-01-10 12:53:44,426 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:53:44,426 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:53:44,441 - 

2022-01-10 12:53:44,441 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:53:47,942 - Epoch: [202][  250/  500]    Overall Loss 0.445531    Objective Loss 0.445531                                        LR 0.000016    Time 0.013995    
2022-01-10 12:53:51,301 - Epoch: [202][  500/  500]    Overall Loss 0.444174    Objective Loss 0.444174    Top1 87.000000    Top5 99.500000    LR 0.000016    Time 0.013713    
2022-01-10 12:53:51,351 - --- validate (epoch=202)-----------
2022-01-10 12:53:51,351 - 10000 samples (100 per mini-batch)
2022-01-10 12:53:52,094 - Epoch: [202][  100/  100]    Loss 1.694530    Top1 59.800000    Top5 85.670000    
2022-01-10 12:53:52,146 - ==> Top1: 59.800    Top5: 85.670    Loss: 1.695

2022-01-10 12:53:52,148 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:53:52,148 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:53:52,163 - 

2022-01-10 12:53:52,163 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:53:55,824 - Epoch: [203][  250/  500]    Overall Loss 0.446956    Objective Loss 0.446956                                        LR 0.000016    Time 0.014636    
2022-01-10 12:53:59,276 - Epoch: [203][  500/  500]    Overall Loss 0.445633    Objective Loss 0.445633    Top1 90.000000    Top5 99.000000    LR 0.000016    Time 0.014218    
2022-01-10 12:53:59,335 - --- validate (epoch=203)-----------
2022-01-10 12:53:59,335 - 10000 samples (100 per mini-batch)
2022-01-10 12:54:00,046 - Epoch: [203][  100/  100]    Loss 1.699148    Top1 59.930000    Top5 85.580000    
2022-01-10 12:54:00,103 - ==> Top1: 59.930    Top5: 85.580    Loss: 1.699

2022-01-10 12:54:00,105 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:54:00,105 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:54:00,127 - 

2022-01-10 12:54:00,127 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:54:03,592 - Epoch: [204][  250/  500]    Overall Loss 0.445002    Objective Loss 0.445002                                        LR 0.000016    Time 0.013848    
2022-01-10 12:54:06,862 - Epoch: [204][  500/  500]    Overall Loss 0.445739    Objective Loss 0.445739    Top1 88.500000    Top5 99.500000    LR 0.000016    Time 0.013460    
2022-01-10 12:54:06,917 - --- validate (epoch=204)-----------
2022-01-10 12:54:06,917 - 10000 samples (100 per mini-batch)
2022-01-10 12:54:07,654 - Epoch: [204][  100/  100]    Loss 1.699061    Top1 59.780000    Top5 85.480000    
2022-01-10 12:54:07,714 - ==> Top1: 59.780    Top5: 85.480    Loss: 1.699

2022-01-10 12:54:07,716 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:54:07,716 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:54:07,798 - 

2022-01-10 12:54:07,798 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:54:11,282 - Epoch: [205][  250/  500]    Overall Loss 0.448762    Objective Loss 0.448762                                        LR 0.000016    Time 0.013926    
2022-01-10 12:54:14,552 - Epoch: [205][  500/  500]    Overall Loss 0.448370    Objective Loss 0.448370    Top1 90.000000    Top5 99.500000    LR 0.000016    Time 0.013498    
2022-01-10 12:54:14,607 - --- validate (epoch=205)-----------
2022-01-10 12:54:14,607 - 10000 samples (100 per mini-batch)
2022-01-10 12:54:15,319 - Epoch: [205][  100/  100]    Loss 1.697708    Top1 59.770000    Top5 85.640000    
2022-01-10 12:54:15,377 - ==> Top1: 59.770    Top5: 85.640    Loss: 1.698

2022-01-10 12:54:15,379 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:54:15,379 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:54:15,401 - 

2022-01-10 12:54:15,401 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:54:19,161 - Epoch: [206][  250/  500]    Overall Loss 0.445196    Objective Loss 0.445196                                        LR 0.000016    Time 0.015028    
2022-01-10 12:54:22,834 - Epoch: [206][  500/  500]    Overall Loss 0.444080    Objective Loss 0.444080    Top1 82.000000    Top5 99.500000    LR 0.000016    Time 0.014857    
2022-01-10 12:54:22,882 - --- validate (epoch=206)-----------
2022-01-10 12:54:22,882 - 10000 samples (100 per mini-batch)
2022-01-10 12:54:23,632 - Epoch: [206][  100/  100]    Loss 1.697383    Top1 59.760000    Top5 85.550000    
2022-01-10 12:54:23,684 - ==> Top1: 59.760    Top5: 85.550    Loss: 1.697

2022-01-10 12:54:23,686 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:54:23,686 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:54:23,708 - 

2022-01-10 12:54:23,708 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:54:27,418 - Epoch: [207][  250/  500]    Overall Loss 0.441405    Objective Loss 0.441405                                        LR 0.000016    Time 0.014830    
2022-01-10 12:54:31,061 - Epoch: [207][  500/  500]    Overall Loss 0.444075    Objective Loss 0.444075    Top1 89.000000    Top5 99.000000    LR 0.000016    Time 0.014696    
2022-01-10 12:54:31,117 - --- validate (epoch=207)-----------
2022-01-10 12:54:31,118 - 10000 samples (100 per mini-batch)
2022-01-10 12:54:31,859 - Epoch: [207][  100/  100]    Loss 1.698244    Top1 59.900000    Top5 85.540000    
2022-01-10 12:54:31,914 - ==> Top1: 59.900    Top5: 85.540    Loss: 1.698

2022-01-10 12:54:31,917 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:54:31,917 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:54:31,939 - 

2022-01-10 12:54:31,939 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:54:35,563 - Epoch: [208][  250/  500]    Overall Loss 0.449699    Objective Loss 0.449699                                        LR 0.000016    Time 0.014489    
2022-01-10 12:54:39,326 - Epoch: [208][  500/  500]    Overall Loss 0.449718    Objective Loss 0.449718    Top1 89.000000    Top5 98.500000    LR 0.000016    Time 0.014765    
2022-01-10 12:54:39,384 - --- validate (epoch=208)-----------
2022-01-10 12:54:39,384 - 10000 samples (100 per mini-batch)
2022-01-10 12:54:40,124 - Epoch: [208][  100/  100]    Loss 1.697521    Top1 59.790000    Top5 85.480000    
2022-01-10 12:54:40,175 - ==> Top1: 59.790    Top5: 85.480    Loss: 1.698

2022-01-10 12:54:40,177 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:54:40,177 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:54:40,199 - 

2022-01-10 12:54:40,199 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:54:43,887 - Epoch: [209][  250/  500]    Overall Loss 0.435040    Objective Loss 0.435040                                        LR 0.000016    Time 0.014740    
2022-01-10 12:54:47,145 - Epoch: [209][  500/  500]    Overall Loss 0.442947    Objective Loss 0.442947    Top1 83.500000    Top5 98.000000    LR 0.000016    Time 0.013883    
2022-01-10 12:54:47,207 - --- validate (epoch=209)-----------
2022-01-10 12:54:47,207 - 10000 samples (100 per mini-batch)
2022-01-10 12:54:47,969 - Epoch: [209][  100/  100]    Loss 1.695265    Top1 59.690000    Top5 85.560000    
2022-01-10 12:54:48,026 - ==> Top1: 59.690    Top5: 85.560    Loss: 1.695

2022-01-10 12:54:48,028 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:54:48,028 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:54:48,050 - 

2022-01-10 12:54:48,050 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:54:51,607 - Epoch: [210][  250/  500]    Overall Loss 0.447417    Objective Loss 0.447417                                        LR 0.000016    Time 0.014218    
2022-01-10 12:54:55,228 - Epoch: [210][  500/  500]    Overall Loss 0.447020    Objective Loss 0.447020    Top1 88.500000    Top5 99.500000    LR 0.000016    Time 0.014347    
2022-01-10 12:54:55,284 - --- validate (epoch=210)-----------
2022-01-10 12:54:55,284 - 10000 samples (100 per mini-batch)
2022-01-10 12:54:56,053 - Epoch: [210][  100/  100]    Loss 1.695338    Top1 59.740000    Top5 85.570000    
2022-01-10 12:54:56,110 - ==> Top1: 59.740    Top5: 85.570    Loss: 1.695

2022-01-10 12:54:56,112 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:54:56,112 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:54:56,134 - 

2022-01-10 12:54:56,134 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:54:59,624 - Epoch: [211][  250/  500]    Overall Loss 0.445267    Objective Loss 0.445267                                        LR 0.000016    Time 0.013951    
2022-01-10 12:55:02,836 - Epoch: [211][  500/  500]    Overall Loss 0.448988    Objective Loss 0.448988    Top1 89.500000    Top5 99.500000    LR 0.000016    Time 0.013395    
2022-01-10 12:55:02,890 - --- validate (epoch=211)-----------
2022-01-10 12:55:02,890 - 10000 samples (100 per mini-batch)
2022-01-10 12:55:03,666 - Epoch: [211][  100/  100]    Loss 1.696439    Top1 59.560000    Top5 85.680000    
2022-01-10 12:55:03,719 - ==> Top1: 59.560    Top5: 85.680    Loss: 1.696

2022-01-10 12:55:03,721 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:55:03,721 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:55:03,743 - 

2022-01-10 12:55:03,744 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:55:07,210 - Epoch: [212][  250/  500]    Overall Loss 0.453035    Objective Loss 0.453035                                        LR 0.000016    Time 0.013856    
2022-01-10 12:55:10,477 - Epoch: [212][  500/  500]    Overall Loss 0.446408    Objective Loss 0.446408    Top1 87.000000    Top5 99.000000    LR 0.000016    Time 0.013459    
2022-01-10 12:55:10,532 - --- validate (epoch=212)-----------
2022-01-10 12:55:10,532 - 10000 samples (100 per mini-batch)
2022-01-10 12:55:11,244 - Epoch: [212][  100/  100]    Loss 1.697048    Top1 59.700000    Top5 85.680000    
2022-01-10 12:55:11,294 - ==> Top1: 59.700    Top5: 85.680    Loss: 1.697

2022-01-10 12:55:11,296 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:55:11,296 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:55:11,319 - 

2022-01-10 12:55:11,319 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:55:14,790 - Epoch: [213][  250/  500]    Overall Loss 0.439970    Objective Loss 0.439970                                        LR 0.000016    Time 0.013874    
2022-01-10 12:55:18,046 - Epoch: [213][  500/  500]    Overall Loss 0.442812    Objective Loss 0.442812    Top1 87.500000    Top5 99.000000    LR 0.000016    Time 0.013445    
2022-01-10 12:55:18,100 - --- validate (epoch=213)-----------
2022-01-10 12:55:18,100 - 10000 samples (100 per mini-batch)
2022-01-10 12:55:18,917 - Epoch: [213][  100/  100]    Loss 1.697198    Top1 59.780000    Top5 85.520000    
2022-01-10 12:55:18,968 - ==> Top1: 59.780    Top5: 85.520    Loss: 1.697

2022-01-10 12:55:18,970 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:55:18,970 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:55:18,993 - 

2022-01-10 12:55:18,993 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:55:22,885 - Epoch: [214][  250/  500]    Overall Loss 0.445037    Objective Loss 0.445037                                        LR 0.000016    Time 0.015558    
2022-01-10 12:55:26,584 - Epoch: [214][  500/  500]    Overall Loss 0.438889    Objective Loss 0.438889    Top1 87.000000    Top5 100.000000    LR 0.000016    Time 0.015174    
2022-01-10 12:55:26,639 - --- validate (epoch=214)-----------
2022-01-10 12:55:26,640 - 10000 samples (100 per mini-batch)
2022-01-10 12:55:27,437 - Epoch: [214][  100/  100]    Loss 1.697891    Top1 59.790000    Top5 85.500000    
2022-01-10 12:55:27,492 - ==> Top1: 59.790    Top5: 85.500    Loss: 1.698

2022-01-10 12:55:27,494 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:55:27,494 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:55:27,516 - 

2022-01-10 12:55:27,517 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:55:31,220 - Epoch: [215][  250/  500]    Overall Loss 0.444604    Objective Loss 0.444604                                        LR 0.000016    Time 0.014803    
2022-01-10 12:55:34,640 - Epoch: [215][  500/  500]    Overall Loss 0.443584    Objective Loss 0.443584    Top1 88.500000    Top5 98.000000    LR 0.000016    Time 0.014238    
2022-01-10 12:55:34,696 - --- validate (epoch=215)-----------
2022-01-10 12:55:34,697 - 10000 samples (100 per mini-batch)
2022-01-10 12:55:35,518 - Epoch: [215][  100/  100]    Loss 1.697184    Top1 59.710000    Top5 85.690000    
2022-01-10 12:55:35,577 - ==> Top1: 59.710    Top5: 85.690    Loss: 1.697

2022-01-10 12:55:35,579 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:55:35,579 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:55:35,602 - 

2022-01-10 12:55:35,602 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:55:39,248 - Epoch: [216][  250/  500]    Overall Loss 0.441726    Objective Loss 0.441726                                        LR 0.000016    Time 0.014575    
2022-01-10 12:55:42,685 - Epoch: [216][  500/  500]    Overall Loss 0.439856    Objective Loss 0.439856    Top1 86.500000    Top5 98.500000    LR 0.000016    Time 0.014158    
2022-01-10 12:55:42,733 - --- validate (epoch=216)-----------
2022-01-10 12:55:42,733 - 10000 samples (100 per mini-batch)
2022-01-10 12:55:43,510 - Epoch: [216][  100/  100]    Loss 1.697827    Top1 59.720000    Top5 85.640000    
2022-01-10 12:55:43,561 - ==> Top1: 59.720    Top5: 85.640    Loss: 1.698

2022-01-10 12:55:43,563 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:55:43,563 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:55:43,585 - 

2022-01-10 12:55:43,586 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:55:47,120 - Epoch: [217][  250/  500]    Overall Loss 0.440840    Objective Loss 0.440840                                        LR 0.000016    Time 0.014129    
2022-01-10 12:55:50,318 - Epoch: [217][  500/  500]    Overall Loss 0.442388    Objective Loss 0.442388    Top1 82.500000    Top5 99.500000    LR 0.000016    Time 0.013456    
2022-01-10 12:55:50,373 - --- validate (epoch=217)-----------
2022-01-10 12:55:50,373 - 10000 samples (100 per mini-batch)
2022-01-10 12:55:51,092 - Epoch: [217][  100/  100]    Loss 1.698108    Top1 59.630000    Top5 85.490000    
2022-01-10 12:55:51,159 - ==> Top1: 59.630    Top5: 85.490    Loss: 1.698

2022-01-10 12:55:51,161 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:55:51,161 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:55:51,179 - 

2022-01-10 12:55:51,179 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:55:55,041 - Epoch: [218][  250/  500]    Overall Loss 0.451327    Objective Loss 0.451327                                        LR 0.000016    Time 0.015438    
2022-01-10 12:55:58,424 - Epoch: [218][  500/  500]    Overall Loss 0.446431    Objective Loss 0.446431    Top1 87.500000    Top5 99.000000    LR 0.000016    Time 0.014480    
2022-01-10 12:55:58,479 - --- validate (epoch=218)-----------
2022-01-10 12:55:58,479 - 10000 samples (100 per mini-batch)
2022-01-10 12:55:59,241 - Epoch: [218][  100/  100]    Loss 1.699027    Top1 59.680000    Top5 85.550000    
2022-01-10 12:55:59,294 - ==> Top1: 59.680    Top5: 85.550    Loss: 1.699

2022-01-10 12:55:59,296 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:55:59,296 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:55:59,318 - 

2022-01-10 12:55:59,319 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:56:02,934 - Epoch: [219][  250/  500]    Overall Loss 0.438497    Objective Loss 0.438497                                        LR 0.000016    Time 0.014452    
2022-01-10 12:56:06,388 - Epoch: [219][  500/  500]    Overall Loss 0.439686    Objective Loss 0.439686    Top1 79.500000    Top5 98.500000    LR 0.000016    Time 0.014130    
2022-01-10 12:56:06,443 - --- validate (epoch=219)-----------
2022-01-10 12:56:06,443 - 10000 samples (100 per mini-batch)
2022-01-10 12:56:07,228 - Epoch: [219][  100/  100]    Loss 1.701865    Top1 59.700000    Top5 85.720000    
2022-01-10 12:56:07,279 - ==> Top1: 59.700    Top5: 85.720    Loss: 1.702

2022-01-10 12:56:07,281 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:56:07,281 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:56:07,303 - 

2022-01-10 12:56:07,303 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:56:10,924 - Epoch: [220][  250/  500]    Overall Loss 0.436873    Objective Loss 0.436873                                        LR 0.000016    Time 0.014476    
2022-01-10 12:56:14,271 - Epoch: [220][  500/  500]    Overall Loss 0.438790    Objective Loss 0.438790    Top1 86.000000    Top5 98.000000    LR 0.000016    Time 0.013928    
2022-01-10 12:56:14,325 - --- validate (epoch=220)-----------
2022-01-10 12:56:14,326 - 10000 samples (100 per mini-batch)
2022-01-10 12:56:15,104 - Epoch: [220][  100/  100]    Loss 1.702635    Top1 59.540000    Top5 85.590000    
2022-01-10 12:56:15,156 - ==> Top1: 59.540    Top5: 85.590    Loss: 1.703

2022-01-10 12:56:15,158 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:56:15,158 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:56:15,180 - 

2022-01-10 12:56:15,180 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:56:18,711 - Epoch: [221][  250/  500]    Overall Loss 0.438687    Objective Loss 0.438687                                        LR 0.000016    Time 0.014115    
2022-01-10 12:56:21,971 - Epoch: [221][  500/  500]    Overall Loss 0.438986    Objective Loss 0.438986    Top1 85.000000    Top5 98.500000    LR 0.000016    Time 0.013573    
2022-01-10 12:56:22,028 - --- validate (epoch=221)-----------
2022-01-10 12:56:22,028 - 10000 samples (100 per mini-batch)
2022-01-10 12:56:22,759 - Epoch: [221][  100/  100]    Loss 1.700729    Top1 59.630000    Top5 85.720000    
2022-01-10 12:56:22,807 - ==> Top1: 59.630    Top5: 85.720    Loss: 1.701

2022-01-10 12:56:22,809 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:56:22,809 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:56:22,831 - 

2022-01-10 12:56:22,831 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:56:26,432 - Epoch: [222][  250/  500]    Overall Loss 0.443003    Objective Loss 0.443003                                        LR 0.000016    Time 0.014393    
2022-01-10 12:56:29,721 - Epoch: [222][  500/  500]    Overall Loss 0.443944    Objective Loss 0.443944    Top1 85.500000    Top5 99.500000    LR 0.000016    Time 0.013770    
2022-01-10 12:56:29,778 - --- validate (epoch=222)-----------
2022-01-10 12:56:29,778 - 10000 samples (100 per mini-batch)
2022-01-10 12:56:30,516 - Epoch: [222][  100/  100]    Loss 1.694952    Top1 59.820000    Top5 85.560000    
2022-01-10 12:56:30,573 - ==> Top1: 59.820    Top5: 85.560    Loss: 1.695

2022-01-10 12:56:30,575 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:56:30,575 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:56:30,597 - 

2022-01-10 12:56:30,597 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:56:34,215 - Epoch: [223][  250/  500]    Overall Loss 0.444726    Objective Loss 0.444726                                        LR 0.000016    Time 0.014463    
2022-01-10 12:56:37,869 - Epoch: [223][  500/  500]    Overall Loss 0.441404    Objective Loss 0.441404    Top1 81.500000    Top5 98.000000    LR 0.000016    Time 0.014536    
2022-01-10 12:56:37,926 - --- validate (epoch=223)-----------
2022-01-10 12:56:37,927 - 10000 samples (100 per mini-batch)
2022-01-10 12:56:38,723 - Epoch: [223][  100/  100]    Loss 1.699571    Top1 59.830000    Top5 85.560000    
2022-01-10 12:56:38,775 - ==> Top1: 59.830    Top5: 85.560    Loss: 1.700

2022-01-10 12:56:38,777 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:56:38,777 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:56:38,799 - 

2022-01-10 12:56:38,799 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:56:42,333 - Epoch: [224][  250/  500]    Overall Loss 0.441041    Objective Loss 0.441041                                        LR 0.000016    Time 0.014127    
2022-01-10 12:56:45,595 - Epoch: [224][  500/  500]    Overall Loss 0.439609    Objective Loss 0.439609    Top1 90.500000    Top5 99.500000    LR 0.000016    Time 0.013584    
2022-01-10 12:56:45,652 - --- validate (epoch=224)-----------
2022-01-10 12:56:45,652 - 10000 samples (100 per mini-batch)
2022-01-10 12:56:46,375 - Epoch: [224][  100/  100]    Loss 1.702727    Top1 59.550000    Top5 85.490000    
2022-01-10 12:56:46,425 - ==> Top1: 59.550    Top5: 85.490    Loss: 1.703

2022-01-10 12:56:46,427 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:56:46,427 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:56:46,449 - 

2022-01-10 12:56:46,449 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:56:49,922 - Epoch: [225][  250/  500]    Overall Loss 0.430550    Objective Loss 0.430550                                        LR 0.000016    Time 0.013881    
2022-01-10 12:56:53,183 - Epoch: [225][  500/  500]    Overall Loss 0.440394    Objective Loss 0.440394    Top1 85.000000    Top5 100.000000    LR 0.000016    Time 0.013458    
2022-01-10 12:56:53,238 - --- validate (epoch=225)-----------
2022-01-10 12:56:53,238 - 10000 samples (100 per mini-batch)
2022-01-10 12:56:53,954 - Epoch: [225][  100/  100]    Loss 1.702443    Top1 59.430000    Top5 85.710000    
2022-01-10 12:56:54,008 - ==> Top1: 59.430    Top5: 85.710    Loss: 1.702

2022-01-10 12:56:54,009 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:56:54,010 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:56:54,025 - 

2022-01-10 12:56:54,025 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:56:57,701 - Epoch: [226][  250/  500]    Overall Loss 0.437577    Objective Loss 0.437577                                        LR 0.000016    Time 0.014691    
2022-01-10 12:57:01,027 - Epoch: [226][  500/  500]    Overall Loss 0.437293    Objective Loss 0.437293    Top1 85.500000    Top5 99.500000    LR 0.000016    Time 0.013994    
2022-01-10 12:57:01,082 - --- validate (epoch=226)-----------
2022-01-10 12:57:01,083 - 10000 samples (100 per mini-batch)
2022-01-10 12:57:01,853 - Epoch: [226][  100/  100]    Loss 1.698621    Top1 59.870000    Top5 85.710000    
2022-01-10 12:57:01,914 - ==> Top1: 59.870    Top5: 85.710    Loss: 1.699

2022-01-10 12:57:01,916 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:57:01,917 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:57:01,939 - 

2022-01-10 12:57:01,939 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:57:05,442 - Epoch: [227][  250/  500]    Overall Loss 0.438512    Objective Loss 0.438512                                        LR 0.000016    Time 0.014002    
2022-01-10 12:57:08,744 - Epoch: [227][  500/  500]    Overall Loss 0.437346    Objective Loss 0.437346    Top1 87.000000    Top5 99.000000    LR 0.000016    Time 0.013601    
2022-01-10 12:57:08,802 - --- validate (epoch=227)-----------
2022-01-10 12:57:08,802 - 10000 samples (100 per mini-batch)
2022-01-10 12:57:09,646 - Epoch: [227][  100/  100]    Loss 1.698706    Top1 59.590000    Top5 85.540000    
2022-01-10 12:57:09,700 - ==> Top1: 59.590    Top5: 85.540    Loss: 1.699

2022-01-10 12:57:09,702 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:57:09,702 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:57:09,724 - 

2022-01-10 12:57:09,724 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:57:13,227 - Epoch: [228][  250/  500]    Overall Loss 0.444406    Objective Loss 0.444406                                        LR 0.000016    Time 0.014003    
2022-01-10 12:57:16,522 - Epoch: [228][  500/  500]    Overall Loss 0.443367    Objective Loss 0.443367    Top1 89.000000    Top5 99.500000    LR 0.000016    Time 0.013587    
2022-01-10 12:57:16,577 - --- validate (epoch=228)-----------
2022-01-10 12:57:16,577 - 10000 samples (100 per mini-batch)
2022-01-10 12:57:17,272 - Epoch: [228][  100/  100]    Loss 1.702535    Top1 59.380000    Top5 85.750000    
2022-01-10 12:57:17,327 - ==> Top1: 59.380    Top5: 85.750    Loss: 1.703

2022-01-10 12:57:17,329 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:57:17,329 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:57:17,351 - 

2022-01-10 12:57:17,351 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:57:20,929 - Epoch: [229][  250/  500]    Overall Loss 0.445766    Objective Loss 0.445766                                        LR 0.000016    Time 0.014303    
2022-01-10 12:57:24,245 - Epoch: [229][  500/  500]    Overall Loss 0.444794    Objective Loss 0.444794    Top1 88.000000    Top5 99.000000    LR 0.000016    Time 0.013778    
2022-01-10 12:57:24,294 - --- validate (epoch=229)-----------
2022-01-10 12:57:24,294 - 10000 samples (100 per mini-batch)
2022-01-10 12:57:25,002 - Epoch: [229][  100/  100]    Loss 1.701166    Top1 59.640000    Top5 85.730000    
2022-01-10 12:57:25,057 - ==> Top1: 59.640    Top5: 85.730    Loss: 1.701

2022-01-10 12:57:25,059 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:57:25,059 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:57:25,082 - 

2022-01-10 12:57:25,082 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:57:28,478 - Epoch: [230][  250/  500]    Overall Loss 0.433492    Objective Loss 0.433492                                        LR 0.000016    Time 0.013576    
2022-01-10 12:57:31,795 - Epoch: [230][  500/  500]    Overall Loss 0.432727    Objective Loss 0.432727    Top1 87.500000    Top5 99.000000    LR 0.000016    Time 0.013419    
2022-01-10 12:57:31,851 - --- validate (epoch=230)-----------
2022-01-10 12:57:31,851 - 10000 samples (100 per mini-batch)
2022-01-10 12:57:32,559 - Epoch: [230][  100/  100]    Loss 1.698351    Top1 59.770000    Top5 85.690000    
2022-01-10 12:57:32,611 - ==> Top1: 59.770    Top5: 85.690    Loss: 1.698

2022-01-10 12:57:32,613 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:57:32,613 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:57:32,694 - 

2022-01-10 12:57:32,694 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:57:36,118 - Epoch: [231][  250/  500]    Overall Loss 0.446225    Objective Loss 0.446225                                        LR 0.000016    Time 0.013686    
2022-01-10 12:57:39,385 - Epoch: [231][  500/  500]    Overall Loss 0.441524    Objective Loss 0.441524    Top1 86.500000    Top5 99.000000    LR 0.000016    Time 0.013374    
2022-01-10 12:57:39,433 - --- validate (epoch=231)-----------
2022-01-10 12:57:39,433 - 10000 samples (100 per mini-batch)
2022-01-10 12:57:40,191 - Epoch: [231][  100/  100]    Loss 1.699515    Top1 59.580000    Top5 85.670000    
2022-01-10 12:57:40,243 - ==> Top1: 59.580    Top5: 85.670    Loss: 1.700

2022-01-10 12:57:40,245 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:57:40,245 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:57:40,261 - 

2022-01-10 12:57:40,261 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:57:43,869 - Epoch: [232][  250/  500]    Overall Loss 0.434354    Objective Loss 0.434354                                        LR 0.000016    Time 0.014424    
2022-01-10 12:57:47,195 - Epoch: [232][  500/  500]    Overall Loss 0.438566    Objective Loss 0.438566    Top1 90.000000    Top5 98.500000    LR 0.000016    Time 0.013861    
2022-01-10 12:57:47,244 - --- validate (epoch=232)-----------
2022-01-10 12:57:47,244 - 10000 samples (100 per mini-batch)
2022-01-10 12:57:47,990 - Epoch: [232][  100/  100]    Loss 1.701255    Top1 59.520000    Top5 85.710000    
2022-01-10 12:57:48,044 - ==> Top1: 59.520    Top5: 85.710    Loss: 1.701

2022-01-10 12:57:48,046 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:57:48,046 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:57:48,068 - 

2022-01-10 12:57:48,068 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:57:51,505 - Epoch: [233][  250/  500]    Overall Loss 0.436104    Objective Loss 0.436104                                        LR 0.000016    Time 0.013738    
2022-01-10 12:57:55,119 - Epoch: [233][  500/  500]    Overall Loss 0.438871    Objective Loss 0.438871    Top1 84.500000    Top5 98.000000    LR 0.000016    Time 0.014092    
2022-01-10 12:57:55,172 - --- validate (epoch=233)-----------
2022-01-10 12:57:55,172 - 10000 samples (100 per mini-batch)
2022-01-10 12:57:55,961 - Epoch: [233][  100/  100]    Loss 1.696333    Top1 59.650000    Top5 85.790000    
2022-01-10 12:57:56,018 - ==> Top1: 59.650    Top5: 85.790    Loss: 1.696

2022-01-10 12:57:56,020 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:57:56,020 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:57:56,042 - 

2022-01-10 12:57:56,043 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:57:59,717 - Epoch: [234][  250/  500]    Overall Loss 0.441505    Objective Loss 0.441505                                        LR 0.000016    Time 0.014690    
2022-01-10 12:58:02,966 - Epoch: [234][  500/  500]    Overall Loss 0.441019    Objective Loss 0.441019    Top1 88.000000    Top5 98.000000    LR 0.000016    Time 0.013838    
2022-01-10 12:58:03,022 - --- validate (epoch=234)-----------
2022-01-10 12:58:03,022 - 10000 samples (100 per mini-batch)
2022-01-10 12:58:03,764 - Epoch: [234][  100/  100]    Loss 1.697918    Top1 59.730000    Top5 85.630000    
2022-01-10 12:58:03,822 - ==> Top1: 59.730    Top5: 85.630    Loss: 1.698

2022-01-10 12:58:03,824 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:58:03,824 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:58:03,846 - 

2022-01-10 12:58:03,846 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:58:07,383 - Epoch: [235][  250/  500]    Overall Loss 0.437774    Objective Loss 0.437774                                        LR 0.000016    Time 0.014137    
2022-01-10 12:58:10,668 - Epoch: [235][  500/  500]    Overall Loss 0.441270    Objective Loss 0.441270    Top1 85.500000    Top5 98.500000    LR 0.000016    Time 0.013635    
2022-01-10 12:58:10,723 - --- validate (epoch=235)-----------
2022-01-10 12:58:10,723 - 10000 samples (100 per mini-batch)
2022-01-10 12:58:11,479 - Epoch: [235][  100/  100]    Loss 1.702458    Top1 59.630000    Top5 85.690000    
2022-01-10 12:58:11,532 - ==> Top1: 59.630    Top5: 85.690    Loss: 1.702

2022-01-10 12:58:11,534 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:58:11,534 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:58:11,550 - 

2022-01-10 12:58:11,551 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:58:15,123 - Epoch: [236][  250/  500]    Overall Loss 0.442288    Objective Loss 0.442288                                        LR 0.000016    Time 0.014279    
2022-01-10 12:58:18,506 - Epoch: [236][  500/  500]    Overall Loss 0.439062    Objective Loss 0.439062    Top1 90.500000    Top5 98.000000    LR 0.000016    Time 0.013902    
2022-01-10 12:58:18,555 - --- validate (epoch=236)-----------
2022-01-10 12:58:18,555 - 10000 samples (100 per mini-batch)
2022-01-10 12:58:19,335 - Epoch: [236][  100/  100]    Loss 1.700675    Top1 59.520000    Top5 85.670000    
2022-01-10 12:58:19,386 - ==> Top1: 59.520    Top5: 85.670    Loss: 1.701

2022-01-10 12:58:19,388 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:58:19,388 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:58:19,411 - 

2022-01-10 12:58:19,411 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:58:23,101 - Epoch: [237][  250/  500]    Overall Loss 0.436513    Objective Loss 0.436513                                        LR 0.000016    Time 0.014752    
2022-01-10 12:58:26,450 - Epoch: [237][  500/  500]    Overall Loss 0.435316    Objective Loss 0.435316    Top1 90.000000    Top5 99.500000    LR 0.000016    Time 0.014069    
2022-01-10 12:58:26,501 - --- validate (epoch=237)-----------
2022-01-10 12:58:26,501 - 10000 samples (100 per mini-batch)
2022-01-10 12:58:27,240 - Epoch: [237][  100/  100]    Loss 1.698958    Top1 59.610000    Top5 85.660000    
2022-01-10 12:58:27,297 - ==> Top1: 59.610    Top5: 85.660    Loss: 1.699

2022-01-10 12:58:27,299 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:58:27,299 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:58:27,321 - 

2022-01-10 12:58:27,321 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:58:31,111 - Epoch: [238][  250/  500]    Overall Loss 0.438209    Objective Loss 0.438209                                        LR 0.000016    Time 0.015149    
2022-01-10 12:58:34,788 - Epoch: [238][  500/  500]    Overall Loss 0.435773    Objective Loss 0.435773    Top1 83.500000    Top5 98.500000    LR 0.000016    Time 0.014924    
2022-01-10 12:58:34,842 - --- validate (epoch=238)-----------
2022-01-10 12:58:34,842 - 10000 samples (100 per mini-batch)
2022-01-10 12:58:35,540 - Epoch: [238][  100/  100]    Loss 1.699861    Top1 59.660000    Top5 85.740000    
2022-01-10 12:58:35,591 - ==> Top1: 59.660    Top5: 85.740    Loss: 1.700

2022-01-10 12:58:35,593 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:58:35,593 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:58:35,615 - 

2022-01-10 12:58:35,615 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:58:39,126 - Epoch: [239][  250/  500]    Overall Loss 0.434587    Objective Loss 0.434587                                        LR 0.000016    Time 0.014035    
2022-01-10 12:58:42,431 - Epoch: [239][  500/  500]    Overall Loss 0.437954    Objective Loss 0.437954    Top1 85.500000    Top5 98.500000    LR 0.000016    Time 0.013622    
2022-01-10 12:58:42,485 - --- validate (epoch=239)-----------
2022-01-10 12:58:42,486 - 10000 samples (100 per mini-batch)
2022-01-10 12:58:43,235 - Epoch: [239][  100/  100]    Loss 1.698821    Top1 59.560000    Top5 85.400000    
2022-01-10 12:58:43,286 - ==> Top1: 59.560    Top5: 85.400    Loss: 1.699

2022-01-10 12:58:43,288 - ==> Best [Top1: 60.270   Top5: 85.920   Sparsity:0.00   Params: 381792 on epoch: 154]
2022-01-10 12:58:43,288 - Saving checkpoint to: logs/2022.01.10-122710/checkpoint.pth.tar
2022-01-10 12:58:43,332 - 

2022-01-10 12:58:43,332 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:58:48,482 - Epoch: [240][  250/  500]    Overall Loss 4.253771    Objective Loss 4.253771                                        LR 0.000016    Time 0.020589    
2022-01-10 12:58:53,360 - Epoch: [240][  500/  500]    Overall Loss 3.730433    Objective Loss 3.730433    Top1 30.000000    Top5 64.500000    LR 0.000016    Time 0.020046    
2022-01-10 12:58:53,421 - --- validate (epoch=240)-----------
2022-01-10 12:58:53,421 - 10000 samples (100 per mini-batch)
2022-01-10 12:58:54,765 - Epoch: [240][  100/  100]    Loss 2.930661    Top1 31.040000    Top5 64.790000    
2022-01-10 12:58:54,816 - ==> Top1: 31.040    Top5: 64.790    Loss: 2.931

2022-01-10 12:58:54,818 - ==> Best [Top1: 31.040   Top5: 64.790   Sparsity:0.00   Params: 381792 on epoch: 240]
2022-01-10 12:58:54,818 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 12:58:54,832 - 

2022-01-10 12:58:54,832 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:58:59,975 - Epoch: [241][  250/  500]    Overall Loss 2.701259    Objective Loss 2.701259                                        LR 0.000016    Time 0.020563    
2022-01-10 12:59:04,840 - Epoch: [241][  500/  500]    Overall Loss 2.602021    Objective Loss 2.602021    Top1 41.500000    Top5 82.000000    LR 0.000016    Time 0.020006    
2022-01-10 12:59:04,895 - --- validate (epoch=241)-----------
2022-01-10 12:59:04,895 - 10000 samples (100 per mini-batch)
2022-01-10 12:59:06,226 - Epoch: [241][  100/  100]    Loss 2.536487    Top1 41.580000    Top5 74.380000    
2022-01-10 12:59:06,276 - ==> Top1: 41.580    Top5: 74.380    Loss: 2.536

2022-01-10 12:59:06,277 - ==> Best [Top1: 41.580   Top5: 74.380   Sparsity:0.00   Params: 381792 on epoch: 241]
2022-01-10 12:59:06,277 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 12:59:06,301 - 

2022-01-10 12:59:06,301 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:59:11,469 - Epoch: [242][  250/  500]    Overall Loss 2.410607    Objective Loss 2.410607                                        LR 0.000016    Time 0.020661    
2022-01-10 12:59:16,526 - Epoch: [242][  500/  500]    Overall Loss 2.368033    Objective Loss 2.368033    Top1 47.500000    Top5 78.500000    LR 0.000016    Time 0.020439    
2022-01-10 12:59:16,587 - --- validate (epoch=242)-----------
2022-01-10 12:59:16,587 - 10000 samples (100 per mini-batch)
2022-01-10 12:59:17,944 - Epoch: [242][  100/  100]    Loss 2.394903    Top1 45.020000    Top5 76.000000    
2022-01-10 12:59:17,993 - ==> Top1: 45.020    Top5: 76.000    Loss: 2.395

2022-01-10 12:59:17,995 - ==> Best [Top1: 45.020   Top5: 76.000   Sparsity:0.00   Params: 381792 on epoch: 242]
2022-01-10 12:59:17,995 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 12:59:18,019 - 

2022-01-10 12:59:18,019 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:59:23,213 - Epoch: [243][  250/  500]    Overall Loss 2.262877    Objective Loss 2.262877                                        LR 0.000016    Time 0.020765    
2022-01-10 12:59:28,107 - Epoch: [243][  500/  500]    Overall Loss 2.242107    Objective Loss 2.242107    Top1 52.500000    Top5 84.500000    LR 0.000016    Time 0.020167    
2022-01-10 12:59:28,163 - --- validate (epoch=243)-----------
2022-01-10 12:59:28,163 - 10000 samples (100 per mini-batch)
2022-01-10 12:59:29,598 - Epoch: [243][  100/  100]    Loss 2.326183    Top1 46.100000    Top5 77.090000    
2022-01-10 12:59:29,657 - ==> Top1: 46.100    Top5: 77.090    Loss: 2.326

2022-01-10 12:59:29,659 - ==> Best [Top1: 46.100   Top5: 77.090   Sparsity:0.00   Params: 381792 on epoch: 243]
2022-01-10 12:59:29,659 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 12:59:29,674 - 

2022-01-10 12:59:29,674 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:59:34,804 - Epoch: [244][  250/  500]    Overall Loss 2.162274    Objective Loss 2.162274                                        LR 0.000016    Time 0.020511    
2022-01-10 12:59:39,841 - Epoch: [244][  500/  500]    Overall Loss 2.155658    Objective Loss 2.155658    Top1 54.000000    Top5 83.500000    LR 0.000016    Time 0.020326    
2022-01-10 12:59:39,903 - --- validate (epoch=244)-----------
2022-01-10 12:59:39,903 - 10000 samples (100 per mini-batch)
2022-01-10 12:59:41,253 - Epoch: [244][  100/  100]    Loss 2.294676    Top1 46.820000    Top5 77.860000    
2022-01-10 12:59:41,304 - ==> Top1: 46.820    Top5: 77.860    Loss: 2.295

2022-01-10 12:59:41,306 - ==> Best [Top1: 46.820   Top5: 77.860   Sparsity:0.00   Params: 381792 on epoch: 244]
2022-01-10 12:59:41,306 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 12:59:41,329 - 

2022-01-10 12:59:41,330 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:59:46,373 - Epoch: [245][  250/  500]    Overall Loss 2.115319    Objective Loss 2.115319                                        LR 0.000016    Time 0.020162    
2022-01-10 12:59:51,313 - Epoch: [245][  500/  500]    Overall Loss 2.100792    Objective Loss 2.100792    Top1 50.000000    Top5 86.000000    LR 0.000016    Time 0.019956    
2022-01-10 12:59:51,376 - --- validate (epoch=245)-----------
2022-01-10 12:59:51,376 - 10000 samples (100 per mini-batch)
2022-01-10 12:59:52,738 - Epoch: [245][  100/  100]    Loss 2.228930    Top1 48.370000    Top5 79.020000    
2022-01-10 12:59:52,795 - ==> Top1: 48.370    Top5: 79.020    Loss: 2.229

2022-01-10 12:59:52,796 - ==> Best [Top1: 48.370   Top5: 79.020   Sparsity:0.00   Params: 381792 on epoch: 245]
2022-01-10 12:59:52,796 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 12:59:52,820 - 

2022-01-10 12:59:52,820 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 12:59:57,947 - Epoch: [246][  250/  500]    Overall Loss 2.060921    Objective Loss 2.060921                                        LR 0.000016    Time 0.020497    
2022-01-10 13:00:02,933 - Epoch: [246][  500/  500]    Overall Loss 2.064233    Objective Loss 2.064233    Top1 53.000000    Top5 86.000000    LR 0.000016    Time 0.020216    
2022-01-10 13:00:02,990 - --- validate (epoch=246)-----------
2022-01-10 13:00:02,991 - 10000 samples (100 per mini-batch)
2022-01-10 13:00:04,336 - Epoch: [246][  100/  100]    Loss 2.187340    Top1 49.600000    Top5 79.810000    
2022-01-10 13:00:04,388 - ==> Top1: 49.600    Top5: 79.810    Loss: 2.187

2022-01-10 13:00:04,389 - ==> Best [Top1: 49.600   Top5: 79.810   Sparsity:0.00   Params: 381792 on epoch: 246]
2022-01-10 13:00:04,389 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:00:04,413 - 

2022-01-10 13:00:04,413 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:00:09,734 - Epoch: [247][  250/  500]    Overall Loss 2.031192    Objective Loss 2.031192                                        LR 0.000016    Time 0.021271    
2022-01-10 13:00:14,648 - Epoch: [247][  500/  500]    Overall Loss 2.035178    Objective Loss 2.035178    Top1 62.000000    Top5 86.000000    LR 0.000016    Time 0.020459    
2022-01-10 13:00:14,710 - --- validate (epoch=247)-----------
2022-01-10 13:00:14,710 - 10000 samples (100 per mini-batch)
2022-01-10 13:00:16,026 - Epoch: [247][  100/  100]    Loss 2.187777    Top1 49.510000    Top5 79.290000    
2022-01-10 13:00:16,085 - ==> Top1: 49.510    Top5: 79.290    Loss: 2.188

2022-01-10 13:00:16,087 - ==> Best [Top1: 49.600   Top5: 79.810   Sparsity:0.00   Params: 381792 on epoch: 246]
2022-01-10 13:00:16,087 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:00:16,106 - 

2022-01-10 13:00:16,107 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:00:21,136 - Epoch: [248][  250/  500]    Overall Loss 2.012535    Objective Loss 2.012535                                        LR 0.000016    Time 0.020108    
2022-01-10 13:00:25,987 - Epoch: [248][  500/  500]    Overall Loss 2.010591    Objective Loss 2.010591    Top1 60.500000    Top5 88.500000    LR 0.000016    Time 0.019752    
2022-01-10 13:00:26,042 - --- validate (epoch=248)-----------
2022-01-10 13:00:26,042 - 10000 samples (100 per mini-batch)
2022-01-10 13:00:27,386 - Epoch: [248][  100/  100]    Loss 2.176463    Top1 49.690000    Top5 79.550000    
2022-01-10 13:00:27,443 - ==> Top1: 49.690    Top5: 79.550    Loss: 2.176

2022-01-10 13:00:27,444 - ==> Best [Top1: 49.690   Top5: 79.550   Sparsity:0.00   Params: 381792 on epoch: 248]
2022-01-10 13:00:27,444 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:00:27,468 - 

2022-01-10 13:00:27,468 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:00:32,812 - Epoch: [249][  250/  500]    Overall Loss 1.993031    Objective Loss 1.993031                                        LR 0.000016    Time 0.021364    
2022-01-10 13:00:37,893 - Epoch: [249][  500/  500]    Overall Loss 1.996087    Objective Loss 1.996087    Top1 50.000000    Top5 87.000000    LR 0.000016    Time 0.020839    
2022-01-10 13:00:37,950 - --- validate (epoch=249)-----------
2022-01-10 13:00:37,950 - 10000 samples (100 per mini-batch)
2022-01-10 13:00:39,306 - Epoch: [249][  100/  100]    Loss 2.136086    Top1 51.300000    Top5 80.400000    
2022-01-10 13:00:39,363 - ==> Top1: 51.300    Top5: 80.400    Loss: 2.136

2022-01-10 13:00:39,365 - ==> Best [Top1: 51.300   Top5: 80.400   Sparsity:0.00   Params: 381792 on epoch: 249]
2022-01-10 13:00:39,365 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:00:39,383 - 

2022-01-10 13:00:39,383 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:00:44,518 - Epoch: [250][  250/  500]    Overall Loss 1.969066    Objective Loss 1.969066                                        LR 0.000016    Time 0.020531    
2022-01-10 13:00:49,474 - Epoch: [250][  500/  500]    Overall Loss 1.975285    Objective Loss 1.975285    Top1 56.000000    Top5 87.000000    LR 0.000016    Time 0.020172    
2022-01-10 13:00:49,523 - --- validate (epoch=250)-----------
2022-01-10 13:00:49,523 - 10000 samples (100 per mini-batch)
2022-01-10 13:00:51,034 - Epoch: [250][  100/  100]    Loss 2.166720    Top1 48.990000    Top5 79.290000    
2022-01-10 13:00:51,091 - ==> Top1: 48.990    Top5: 79.290    Loss: 2.167

2022-01-10 13:00:51,093 - ==> Best [Top1: 51.300   Top5: 80.400   Sparsity:0.00   Params: 381792 on epoch: 249]
2022-01-10 13:00:51,093 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:00:51,113 - 

2022-01-10 13:00:51,113 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:00:56,227 - Epoch: [251][  250/  500]    Overall Loss 1.978351    Objective Loss 1.978351                                        LR 0.000016    Time 0.020446    
2022-01-10 13:01:01,086 - Epoch: [251][  500/  500]    Overall Loss 1.981061    Objective Loss 1.981061    Top1 59.500000    Top5 90.500000    LR 0.000016    Time 0.019937    
2022-01-10 13:01:01,143 - --- validate (epoch=251)-----------
2022-01-10 13:01:01,144 - 10000 samples (100 per mini-batch)
2022-01-10 13:01:02,492 - Epoch: [251][  100/  100]    Loss 2.139729    Top1 49.560000    Top5 79.880000    
2022-01-10 13:01:02,546 - ==> Top1: 49.560    Top5: 79.880    Loss: 2.140

2022-01-10 13:01:02,548 - ==> Best [Top1: 51.300   Top5: 80.400   Sparsity:0.00   Params: 381792 on epoch: 249]
2022-01-10 13:01:02,548 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:01:02,568 - 

2022-01-10 13:01:02,568 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:01:07,693 - Epoch: [252][  250/  500]    Overall Loss 1.943984    Objective Loss 1.943984                                        LR 0.000016    Time 0.020491    
2022-01-10 13:01:12,503 - Epoch: [252][  500/  500]    Overall Loss 1.940987    Objective Loss 1.940987    Top1 47.000000    Top5 77.500000    LR 0.000016    Time 0.019861    
2022-01-10 13:01:12,562 - --- validate (epoch=252)-----------
2022-01-10 13:01:12,562 - 10000 samples (100 per mini-batch)
2022-01-10 13:01:13,960 - Epoch: [252][  100/  100]    Loss 2.344583    Top1 44.440000    Top5 75.050000    
2022-01-10 13:01:14,018 - ==> Top1: 44.440    Top5: 75.050    Loss: 2.345

2022-01-10 13:01:14,020 - ==> Best [Top1: 51.300   Top5: 80.400   Sparsity:0.00   Params: 381792 on epoch: 249]
2022-01-10 13:01:14,020 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:01:14,039 - 

2022-01-10 13:01:14,040 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:01:19,109 - Epoch: [253][  250/  500]    Overall Loss 1.936145    Objective Loss 1.936145                                        LR 0.000016    Time 0.020269    
2022-01-10 13:01:24,011 - Epoch: [253][  500/  500]    Overall Loss 1.928939    Objective Loss 1.928939    Top1 47.000000    Top5 80.000000    LR 0.000016    Time 0.019932    
2022-01-10 13:01:24,064 - --- validate (epoch=253)-----------
2022-01-10 13:01:24,064 - 10000 samples (100 per mini-batch)
2022-01-10 13:01:25,496 - Epoch: [253][  100/  100]    Loss 2.138269    Top1 49.770000    Top5 79.900000    
2022-01-10 13:01:25,545 - ==> Top1: 49.770    Top5: 79.900    Loss: 2.138

2022-01-10 13:01:25,546 - ==> Best [Top1: 51.300   Top5: 80.400   Sparsity:0.00   Params: 381792 on epoch: 249]
2022-01-10 13:01:25,546 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:01:25,567 - 

2022-01-10 13:01:25,567 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:01:30,716 - Epoch: [254][  250/  500]    Overall Loss 1.912932    Objective Loss 1.912932                                        LR 0.000016    Time 0.020587    
2022-01-10 13:01:35,635 - Epoch: [254][  500/  500]    Overall Loss 1.916121    Objective Loss 1.916121    Top1 57.500000    Top5 86.500000    LR 0.000016    Time 0.020126    
2022-01-10 13:01:35,692 - --- validate (epoch=254)-----------
2022-01-10 13:01:35,692 - 10000 samples (100 per mini-batch)
2022-01-10 13:01:37,039 - Epoch: [254][  100/  100]    Loss 2.109427    Top1 50.010000    Top5 79.110000    
2022-01-10 13:01:37,097 - ==> Top1: 50.010    Top5: 79.110    Loss: 2.109

2022-01-10 13:01:37,099 - ==> Best [Top1: 51.300   Top5: 80.400   Sparsity:0.00   Params: 381792 on epoch: 249]
2022-01-10 13:01:37,099 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:01:37,119 - 

2022-01-10 13:01:37,119 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:01:42,281 - Epoch: [255][  250/  500]    Overall Loss 1.914955    Objective Loss 1.914955                                        LR 0.000016    Time 0.020639    
2022-01-10 13:01:47,156 - Epoch: [255][  500/  500]    Overall Loss 1.913400    Objective Loss 1.913400    Top1 60.000000    Top5 87.500000    LR 0.000016    Time 0.020064    
2022-01-10 13:01:47,213 - --- validate (epoch=255)-----------
2022-01-10 13:01:47,214 - 10000 samples (100 per mini-batch)
2022-01-10 13:01:48,801 - Epoch: [255][  100/  100]    Loss 2.101798    Top1 50.360000    Top5 80.280000    
2022-01-10 13:01:48,858 - ==> Top1: 50.360    Top5: 80.280    Loss: 2.102

2022-01-10 13:01:48,860 - ==> Best [Top1: 51.300   Top5: 80.400   Sparsity:0.00   Params: 381792 on epoch: 249]
2022-01-10 13:01:48,860 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:01:48,872 - 

2022-01-10 13:01:48,873 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:01:53,964 - Epoch: [256][  250/  500]    Overall Loss 1.913114    Objective Loss 1.913114                                        LR 0.000016    Time 0.020355    
2022-01-10 13:01:58,842 - Epoch: [256][  500/  500]    Overall Loss 1.905971    Objective Loss 1.905971    Top1 56.000000    Top5 88.000000    LR 0.000016    Time 0.019930    
2022-01-10 13:01:58,905 - --- validate (epoch=256)-----------
2022-01-10 13:01:58,905 - 10000 samples (100 per mini-batch)
2022-01-10 13:02:00,297 - Epoch: [256][  100/  100]    Loss 2.180369    Top1 48.080000    Top5 77.810000    
2022-01-10 13:02:00,354 - ==> Top1: 48.080    Top5: 77.810    Loss: 2.180

2022-01-10 13:02:00,355 - ==> Best [Top1: 51.300   Top5: 80.400   Sparsity:0.00   Params: 381792 on epoch: 249]
2022-01-10 13:02:00,355 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:02:00,375 - 

2022-01-10 13:02:00,375 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:02:05,528 - Epoch: [257][  250/  500]    Overall Loss 1.899554    Objective Loss 1.899554                                        LR 0.000016    Time 0.020599    
2022-01-10 13:02:10,411 - Epoch: [257][  500/  500]    Overall Loss 1.898377    Objective Loss 1.898377    Top1 55.000000    Top5 83.000000    LR 0.000016    Time 0.020062    
2022-01-10 13:02:10,468 - --- validate (epoch=257)-----------
2022-01-10 13:02:10,469 - 10000 samples (100 per mini-batch)
2022-01-10 13:02:11,835 - Epoch: [257][  100/  100]    Loss 2.136562    Top1 48.440000    Top5 78.710000    
2022-01-10 13:02:11,893 - ==> Top1: 48.440    Top5: 78.710    Loss: 2.137

2022-01-10 13:02:11,894 - ==> Best [Top1: 51.300   Top5: 80.400   Sparsity:0.00   Params: 381792 on epoch: 249]
2022-01-10 13:02:11,894 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:02:11,915 - 

2022-01-10 13:02:11,915 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:02:17,083 - Epoch: [258][  250/  500]    Overall Loss 1.906107    Objective Loss 1.906107                                        LR 0.000016    Time 0.020664    
2022-01-10 13:02:21,854 - Epoch: [258][  500/  500]    Overall Loss 1.899142    Objective Loss 1.899142    Top1 56.500000    Top5 87.000000    LR 0.000016    Time 0.019870    
2022-01-10 13:02:21,917 - --- validate (epoch=258)-----------
2022-01-10 13:02:21,917 - 10000 samples (100 per mini-batch)
2022-01-10 13:02:23,363 - Epoch: [258][  100/  100]    Loss 2.059036    Top1 51.050000    Top5 80.970000    
2022-01-10 13:02:23,419 - ==> Top1: 51.050    Top5: 80.970    Loss: 2.059

2022-01-10 13:02:23,420 - ==> Best [Top1: 51.300   Top5: 80.400   Sparsity:0.00   Params: 381792 on epoch: 249]
2022-01-10 13:02:23,420 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:02:23,440 - 

2022-01-10 13:02:23,440 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:02:28,593 - Epoch: [259][  250/  500]    Overall Loss 1.889701    Objective Loss 1.889701                                        LR 0.000016    Time 0.020599    
2022-01-10 13:02:33,561 - Epoch: [259][  500/  500]    Overall Loss 1.881363    Objective Loss 1.881363    Top1 59.500000    Top5 85.500000    LR 0.000016    Time 0.020233    
2022-01-10 13:02:33,617 - --- validate (epoch=259)-----------
2022-01-10 13:02:33,617 - 10000 samples (100 per mini-batch)
2022-01-10 13:02:35,058 - Epoch: [259][  100/  100]    Loss 2.083986    Top1 50.540000    Top5 79.670000    
2022-01-10 13:02:35,114 - ==> Top1: 50.540    Top5: 79.670    Loss: 2.084

2022-01-10 13:02:35,115 - ==> Best [Top1: 51.300   Top5: 80.400   Sparsity:0.00   Params: 381792 on epoch: 249]
2022-01-10 13:02:35,115 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:02:35,135 - 

2022-01-10 13:02:35,135 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:02:40,241 - Epoch: [260][  250/  500]    Overall Loss 1.862242    Objective Loss 1.862242                                        LR 0.000016    Time 0.020413    
2022-01-10 13:02:45,132 - Epoch: [260][  500/  500]    Overall Loss 1.856564    Objective Loss 1.856564    Top1 52.500000    Top5 84.500000    LR 0.000016    Time 0.019984    
2022-01-10 13:02:45,192 - --- validate (epoch=260)-----------
2022-01-10 13:02:45,192 - 10000 samples (100 per mini-batch)
2022-01-10 13:02:46,519 - Epoch: [260][  100/  100]    Loss 2.025647    Top1 51.240000    Top5 80.950000    
2022-01-10 13:02:46,572 - ==> Top1: 51.240    Top5: 80.950    Loss: 2.026

2022-01-10 13:02:46,574 - ==> Best [Top1: 51.300   Top5: 80.400   Sparsity:0.00   Params: 381792 on epoch: 249]
2022-01-10 13:02:46,574 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:02:46,594 - 

2022-01-10 13:02:46,594 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:02:51,808 - Epoch: [261][  250/  500]    Overall Loss 1.844179    Objective Loss 1.844179                                        LR 0.000016    Time 0.020846    
2022-01-10 13:02:56,768 - Epoch: [261][  500/  500]    Overall Loss 1.848290    Objective Loss 1.848290    Top1 54.500000    Top5 87.500000    LR 0.000016    Time 0.020340    
2022-01-10 13:02:56,823 - --- validate (epoch=261)-----------
2022-01-10 13:02:56,823 - 10000 samples (100 per mini-batch)
2022-01-10 13:02:58,282 - Epoch: [261][  100/  100]    Loss 2.082577    Top1 50.050000    Top5 79.670000    
2022-01-10 13:02:58,333 - ==> Top1: 50.050    Top5: 79.670    Loss: 2.083

2022-01-10 13:02:58,335 - ==> Best [Top1: 51.300   Top5: 80.400   Sparsity:0.00   Params: 381792 on epoch: 249]
2022-01-10 13:02:58,335 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:02:58,355 - 

2022-01-10 13:02:58,356 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:03:03,578 - Epoch: [262][  250/  500]    Overall Loss 1.855422    Objective Loss 1.855422                                        LR 0.000016    Time 0.020878    
2022-01-10 13:03:08,510 - Epoch: [262][  500/  500]    Overall Loss 1.845061    Objective Loss 1.845061    Top1 59.500000    Top5 89.000000    LR 0.000016    Time 0.020297    
2022-01-10 13:03:08,572 - --- validate (epoch=262)-----------
2022-01-10 13:03:08,572 - 10000 samples (100 per mini-batch)
2022-01-10 13:03:09,927 - Epoch: [262][  100/  100]    Loss 2.043994    Top1 51.080000    Top5 80.070000    
2022-01-10 13:03:09,984 - ==> Top1: 51.080    Top5: 80.070    Loss: 2.044

2022-01-10 13:03:09,986 - ==> Best [Top1: 51.300   Top5: 80.400   Sparsity:0.00   Params: 381792 on epoch: 249]
2022-01-10 13:03:09,986 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:03:10,007 - 

2022-01-10 13:03:10,007 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:03:15,242 - Epoch: [263][  250/  500]    Overall Loss 1.824697    Objective Loss 1.824697                                        LR 0.000016    Time 0.020931    
2022-01-10 13:03:20,192 - Epoch: [263][  500/  500]    Overall Loss 1.823158    Objective Loss 1.823158    Top1 56.000000    Top5 86.500000    LR 0.000016    Time 0.020360    
2022-01-10 13:03:20,255 - --- validate (epoch=263)-----------
2022-01-10 13:03:20,256 - 10000 samples (100 per mini-batch)
2022-01-10 13:03:21,608 - Epoch: [263][  100/  100]    Loss 2.005606    Top1 52.160000    Top5 81.780000    
2022-01-10 13:03:21,671 - ==> Top1: 52.160    Top5: 81.780    Loss: 2.006

2022-01-10 13:03:21,673 - ==> Best [Top1: 52.160   Top5: 81.780   Sparsity:0.00   Params: 381792 on epoch: 263]
2022-01-10 13:03:21,673 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:03:21,698 - 

2022-01-10 13:03:21,698 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:03:26,877 - Epoch: [264][  250/  500]    Overall Loss 1.822763    Objective Loss 1.822763                                        LR 0.000016    Time 0.020707    
2022-01-10 13:03:31,674 - Epoch: [264][  500/  500]    Overall Loss 1.821395    Objective Loss 1.821395    Top1 57.000000    Top5 86.000000    LR 0.000016    Time 0.019944    
2022-01-10 13:03:31,728 - --- validate (epoch=264)-----------
2022-01-10 13:03:31,728 - 10000 samples (100 per mini-batch)
2022-01-10 13:03:33,071 - Epoch: [264][  100/  100]    Loss 2.038037    Top1 51.590000    Top5 80.900000    
2022-01-10 13:03:33,129 - ==> Top1: 51.590    Top5: 80.900    Loss: 2.038

2022-01-10 13:03:33,131 - ==> Best [Top1: 52.160   Top5: 81.780   Sparsity:0.00   Params: 381792 on epoch: 263]
2022-01-10 13:03:33,131 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:03:33,150 - 

2022-01-10 13:03:33,151 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:03:38,313 - Epoch: [265][  250/  500]    Overall Loss 1.834203    Objective Loss 1.834203                                        LR 0.000016    Time 0.020639    
2022-01-10 13:03:43,368 - Epoch: [265][  500/  500]    Overall Loss 1.825211    Objective Loss 1.825211    Top1 53.500000    Top5 88.000000    LR 0.000016    Time 0.020423    
2022-01-10 13:03:43,426 - --- validate (epoch=265)-----------
2022-01-10 13:03:43,426 - 10000 samples (100 per mini-batch)
2022-01-10 13:03:44,774 - Epoch: [265][  100/  100]    Loss 2.019001    Top1 51.180000    Top5 80.810000    
2022-01-10 13:03:44,826 - ==> Top1: 51.180    Top5: 80.810    Loss: 2.019

2022-01-10 13:03:44,828 - ==> Best [Top1: 52.160   Top5: 81.780   Sparsity:0.00   Params: 381792 on epoch: 263]
2022-01-10 13:03:44,828 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:03:44,848 - 

2022-01-10 13:03:44,848 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:03:50,054 - Epoch: [266][  250/  500]    Overall Loss 1.810890    Objective Loss 1.810890                                        LR 0.000016    Time 0.020814    
2022-01-10 13:03:54,950 - Epoch: [266][  500/  500]    Overall Loss 1.818545    Objective Loss 1.818545    Top1 58.000000    Top5 86.500000    LR 0.000016    Time 0.020195    
2022-01-10 13:03:55,007 - --- validate (epoch=266)-----------
2022-01-10 13:03:55,007 - 10000 samples (100 per mini-batch)
2022-01-10 13:03:56,322 - Epoch: [266][  100/  100]    Loss 1.979036    Top1 52.260000    Top5 82.410000    
2022-01-10 13:03:56,374 - ==> Top1: 52.260    Top5: 82.410    Loss: 1.979

2022-01-10 13:03:56,375 - ==> Best [Top1: 52.260   Top5: 82.410   Sparsity:0.00   Params: 381792 on epoch: 266]
2022-01-10 13:03:56,376 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:03:56,399 - 

2022-01-10 13:03:56,400 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:04:01,600 - Epoch: [267][  250/  500]    Overall Loss 1.803339    Objective Loss 1.803339                                        LR 0.000016    Time 0.020793    
2022-01-10 13:04:06,449 - Epoch: [267][  500/  500]    Overall Loss 1.805073    Objective Loss 1.805073    Top1 58.000000    Top5 89.500000    LR 0.000016    Time 0.020090    
2022-01-10 13:04:06,512 - --- validate (epoch=267)-----------
2022-01-10 13:04:06,512 - 10000 samples (100 per mini-batch)
2022-01-10 13:04:07,861 - Epoch: [267][  100/  100]    Loss 2.018485    Top1 51.220000    Top5 81.020000    
2022-01-10 13:04:07,918 - ==> Top1: 51.220    Top5: 81.020    Loss: 2.018

2022-01-10 13:04:07,920 - ==> Best [Top1: 52.260   Top5: 82.410   Sparsity:0.00   Params: 381792 on epoch: 266]
2022-01-10 13:04:07,920 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:04:07,940 - 

2022-01-10 13:04:07,940 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:04:13,169 - Epoch: [268][  250/  500]    Overall Loss 1.805380    Objective Loss 1.805380                                        LR 0.000016    Time 0.020905    
2022-01-10 13:04:18,075 - Epoch: [268][  500/  500]    Overall Loss 1.795710    Objective Loss 1.795710    Top1 54.000000    Top5 85.500000    LR 0.000016    Time 0.020261    
2022-01-10 13:04:18,131 - --- validate (epoch=268)-----------
2022-01-10 13:04:18,131 - 10000 samples (100 per mini-batch)
2022-01-10 13:04:19,470 - Epoch: [268][  100/  100]    Loss 2.032595    Top1 49.930000    Top5 80.180000    
2022-01-10 13:04:19,519 - ==> Top1: 49.930    Top5: 80.180    Loss: 2.033

2022-01-10 13:04:19,520 - ==> Best [Top1: 52.260   Top5: 82.410   Sparsity:0.00   Params: 381792 on epoch: 266]
2022-01-10 13:04:19,520 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:04:19,533 - 

2022-01-10 13:04:19,533 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:04:24,619 - Epoch: [269][  250/  500]    Overall Loss 1.804297    Objective Loss 1.804297                                        LR 0.000016    Time 0.020334    
2022-01-10 13:04:29,553 - Epoch: [269][  500/  500]    Overall Loss 1.800398    Objective Loss 1.800398    Top1 56.500000    Top5 87.500000    LR 0.000016    Time 0.020031    
2022-01-10 13:04:29,609 - --- validate (epoch=269)-----------
2022-01-10 13:04:29,610 - 10000 samples (100 per mini-batch)
2022-01-10 13:04:30,944 - Epoch: [269][  100/  100]    Loss 2.037839    Top1 51.020000    Top5 79.580000    
2022-01-10 13:04:30,997 - ==> Top1: 51.020    Top5: 79.580    Loss: 2.038

2022-01-10 13:04:30,999 - ==> Best [Top1: 52.260   Top5: 82.410   Sparsity:0.00   Params: 381792 on epoch: 266]
2022-01-10 13:04:30,999 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:04:31,019 - 

2022-01-10 13:04:31,019 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:04:36,149 - Epoch: [270][  250/  500]    Overall Loss 1.772798    Objective Loss 1.772798                                        LR 0.000016    Time 0.020509    
2022-01-10 13:04:41,047 - Epoch: [270][  500/  500]    Overall Loss 1.774565    Objective Loss 1.774565    Top1 59.000000    Top5 90.000000    LR 0.000016    Time 0.020046    
2022-01-10 13:04:41,106 - --- validate (epoch=270)-----------
2022-01-10 13:04:41,106 - 10000 samples (100 per mini-batch)
2022-01-10 13:04:42,560 - Epoch: [270][  100/  100]    Loss 1.992319    Top1 51.580000    Top5 81.380000    
2022-01-10 13:04:42,615 - ==> Top1: 51.580    Top5: 81.380    Loss: 1.992

2022-01-10 13:04:42,617 - ==> Best [Top1: 52.260   Top5: 82.410   Sparsity:0.00   Params: 381792 on epoch: 266]
2022-01-10 13:04:42,617 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:04:42,637 - 

2022-01-10 13:04:42,637 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:04:47,842 - Epoch: [271][  250/  500]    Overall Loss 1.784617    Objective Loss 1.784617                                        LR 0.000016    Time 0.020805    
2022-01-10 13:04:52,906 - Epoch: [271][  500/  500]    Overall Loss 1.773129    Objective Loss 1.773129    Top1 61.500000    Top5 89.000000    LR 0.000016    Time 0.020526    
2022-01-10 13:04:52,964 - --- validate (epoch=271)-----------
2022-01-10 13:04:52,965 - 10000 samples (100 per mini-batch)
2022-01-10 13:04:54,330 - Epoch: [271][  100/  100]    Loss 2.102902    Top1 47.340000    Top5 78.550000    
2022-01-10 13:04:54,383 - ==> Top1: 47.340    Top5: 78.550    Loss: 2.103

2022-01-10 13:04:54,385 - ==> Best [Top1: 52.260   Top5: 82.410   Sparsity:0.00   Params: 381792 on epoch: 266]
2022-01-10 13:04:54,385 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:04:54,405 - 

2022-01-10 13:04:54,405 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:04:59,727 - Epoch: [272][  250/  500]    Overall Loss 1.774390    Objective Loss 1.774390                                        LR 0.000016    Time 0.021276    
2022-01-10 13:05:04,871 - Epoch: [272][  500/  500]    Overall Loss 1.779227    Objective Loss 1.779227    Top1 55.500000    Top5 85.000000    LR 0.000016    Time 0.020921    
2022-01-10 13:05:04,929 - --- validate (epoch=272)-----------
2022-01-10 13:05:04,929 - 10000 samples (100 per mini-batch)
2022-01-10 13:05:06,259 - Epoch: [272][  100/  100]    Loss 1.987901    Top1 51.660000    Top5 81.190000    
2022-01-10 13:05:06,319 - ==> Top1: 51.660    Top5: 81.190    Loss: 1.988

2022-01-10 13:05:06,320 - ==> Best [Top1: 52.260   Top5: 82.410   Sparsity:0.00   Params: 381792 on epoch: 266]
2022-01-10 13:05:06,320 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:05:06,341 - 

2022-01-10 13:05:06,341 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:05:11,615 - Epoch: [273][  250/  500]    Overall Loss 1.760719    Objective Loss 1.760719                                        LR 0.000016    Time 0.021087    
2022-01-10 13:05:16,577 - Epoch: [273][  500/  500]    Overall Loss 1.766306    Objective Loss 1.766306    Top1 50.500000    Top5 86.500000    LR 0.000016    Time 0.020462    
2022-01-10 13:05:16,633 - --- validate (epoch=273)-----------
2022-01-10 13:05:16,633 - 10000 samples (100 per mini-batch)
2022-01-10 13:05:18,090 - Epoch: [273][  100/  100]    Loss 2.006131    Top1 51.060000    Top5 80.730000    
2022-01-10 13:05:18,142 - ==> Top1: 51.060    Top5: 80.730    Loss: 2.006

2022-01-10 13:05:18,143 - ==> Best [Top1: 52.260   Top5: 82.410   Sparsity:0.00   Params: 381792 on epoch: 266]
2022-01-10 13:05:18,143 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:05:18,156 - 

2022-01-10 13:05:18,156 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:05:23,241 - Epoch: [274][  250/  500]    Overall Loss 1.765951    Objective Loss 1.765951                                        LR 0.000016    Time 0.020328    
2022-01-10 13:05:28,126 - Epoch: [274][  500/  500]    Overall Loss 1.756540    Objective Loss 1.756540    Top1 61.500000    Top5 86.000000    LR 0.000016    Time 0.019930    
2022-01-10 13:05:28,187 - --- validate (epoch=274)-----------
2022-01-10 13:05:28,187 - 10000 samples (100 per mini-batch)
2022-01-10 13:05:29,615 - Epoch: [274][  100/  100]    Loss 1.930936    Top1 53.250000    Top5 82.440000    
2022-01-10 13:05:29,673 - ==> Top1: 53.250    Top5: 82.440    Loss: 1.931

2022-01-10 13:05:29,675 - ==> Best [Top1: 53.250   Top5: 82.440   Sparsity:0.00   Params: 381792 on epoch: 274]
2022-01-10 13:05:29,675 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:05:29,698 - 

2022-01-10 13:05:29,699 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:05:34,891 - Epoch: [275][  250/  500]    Overall Loss 1.764104    Objective Loss 1.764104                                        LR 0.000016    Time 0.020757    
2022-01-10 13:05:39,828 - Epoch: [275][  500/  500]    Overall Loss 1.750799    Objective Loss 1.750799    Top1 51.500000    Top5 84.500000    LR 0.000016    Time 0.020247    
2022-01-10 13:05:39,885 - --- validate (epoch=275)-----------
2022-01-10 13:05:39,885 - 10000 samples (100 per mini-batch)
2022-01-10 13:05:41,232 - Epoch: [275][  100/  100]    Loss 2.004328    Top1 50.760000    Top5 80.730000    
2022-01-10 13:05:41,285 - ==> Top1: 50.760    Top5: 80.730    Loss: 2.004

2022-01-10 13:05:41,286 - ==> Best [Top1: 53.250   Top5: 82.440   Sparsity:0.00   Params: 381792 on epoch: 274]
2022-01-10 13:05:41,286 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:05:41,307 - 

2022-01-10 13:05:41,307 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:05:46,459 - Epoch: [276][  250/  500]    Overall Loss 1.750097    Objective Loss 1.750097                                        LR 0.000016    Time 0.020596    
2022-01-10 13:05:51,392 - Epoch: [276][  500/  500]    Overall Loss 1.740419    Objective Loss 1.740419    Top1 60.500000    Top5 86.000000    LR 0.000016    Time 0.020160    
2022-01-10 13:05:51,452 - --- validate (epoch=276)-----------
2022-01-10 13:05:51,452 - 10000 samples (100 per mini-batch)
2022-01-10 13:05:53,039 - Epoch: [276][  100/  100]    Loss 1.933870    Top1 52.870000    Top5 81.790000    
2022-01-10 13:05:53,089 - ==> Top1: 52.870    Top5: 81.790    Loss: 1.934

2022-01-10 13:05:53,091 - ==> Best [Top1: 53.250   Top5: 82.440   Sparsity:0.00   Params: 381792 on epoch: 274]
2022-01-10 13:05:53,091 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:05:53,111 - 

2022-01-10 13:05:53,111 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:05:58,281 - Epoch: [277][  250/  500]    Overall Loss 1.728126    Objective Loss 1.728126                                        LR 0.000016    Time 0.020668    
2022-01-10 13:06:03,324 - Epoch: [277][  500/  500]    Overall Loss 1.732613    Objective Loss 1.732613    Top1 64.500000    Top5 93.000000    LR 0.000016    Time 0.020416    
2022-01-10 13:06:03,387 - --- validate (epoch=277)-----------
2022-01-10 13:06:03,387 - 10000 samples (100 per mini-batch)
2022-01-10 13:06:04,750 - Epoch: [277][  100/  100]    Loss 1.919764    Top1 53.190000    Top5 82.070000    
2022-01-10 13:06:04,803 - ==> Top1: 53.190    Top5: 82.070    Loss: 1.920

2022-01-10 13:06:04,804 - ==> Best [Top1: 53.250   Top5: 82.440   Sparsity:0.00   Params: 381792 on epoch: 274]
2022-01-10 13:06:04,804 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:06:04,825 - 

2022-01-10 13:06:04,825 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:06:10,039 - Epoch: [278][  250/  500]    Overall Loss 1.732773    Objective Loss 1.732773                                        LR 0.000016    Time 0.020848    
2022-01-10 13:06:15,005 - Epoch: [278][  500/  500]    Overall Loss 1.724141    Objective Loss 1.724141    Top1 62.500000    Top5 90.000000    LR 0.000016    Time 0.020351    
2022-01-10 13:06:15,060 - --- validate (epoch=278)-----------
2022-01-10 13:06:15,060 - 10000 samples (100 per mini-batch)
2022-01-10 13:06:16,412 - Epoch: [278][  100/  100]    Loss 1.954275    Top1 51.270000    Top5 81.650000    
2022-01-10 13:06:16,463 - ==> Top1: 51.270    Top5: 81.650    Loss: 1.954

2022-01-10 13:06:16,464 - ==> Best [Top1: 53.250   Top5: 82.440   Sparsity:0.00   Params: 381792 on epoch: 274]
2022-01-10 13:06:16,464 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:06:16,477 - 

2022-01-10 13:06:16,477 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:06:21,576 - Epoch: [279][  250/  500]    Overall Loss 1.719416    Objective Loss 1.719416                                        LR 0.000016    Time 0.020386    
2022-01-10 13:06:26,418 - Epoch: [279][  500/  500]    Overall Loss 1.728587    Objective Loss 1.728587    Top1 61.500000    Top5 89.000000    LR 0.000016    Time 0.019872    
2022-01-10 13:06:26,479 - --- validate (epoch=279)-----------
2022-01-10 13:06:26,479 - 10000 samples (100 per mini-batch)
2022-01-10 13:06:27,970 - Epoch: [279][  100/  100]    Loss 2.020151    Top1 50.290000    Top5 80.050000    
2022-01-10 13:06:28,027 - ==> Top1: 50.290    Top5: 80.050    Loss: 2.020

2022-01-10 13:06:28,029 - ==> Best [Top1: 53.250   Top5: 82.440   Sparsity:0.00   Params: 381792 on epoch: 274]
2022-01-10 13:06:28,029 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:06:28,049 - 

2022-01-10 13:06:28,049 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:06:33,151 - Epoch: [280][  250/  500]    Overall Loss 1.741332    Objective Loss 1.741332                                        LR 0.000016    Time 0.020398    
2022-01-10 13:06:38,108 - Epoch: [280][  500/  500]    Overall Loss 1.736305    Objective Loss 1.736305    Top1 58.500000    Top5 90.500000    LR 0.000016    Time 0.020109    
2022-01-10 13:06:38,166 - --- validate (epoch=280)-----------
2022-01-10 13:06:38,167 - 10000 samples (100 per mini-batch)
2022-01-10 13:06:39,500 - Epoch: [280][  100/  100]    Loss 2.053215    Top1 48.790000    Top5 79.910000    
2022-01-10 13:06:39,554 - ==> Top1: 48.790    Top5: 79.910    Loss: 2.053

2022-01-10 13:06:39,556 - ==> Best [Top1: 53.250   Top5: 82.440   Sparsity:0.00   Params: 381792 on epoch: 274]
2022-01-10 13:06:39,556 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:06:39,576 - 

2022-01-10 13:06:39,576 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:06:44,743 - Epoch: [281][  250/  500]    Overall Loss 1.728436    Objective Loss 1.728436                                        LR 0.000016    Time 0.020658    
2022-01-10 13:06:49,635 - Epoch: [281][  500/  500]    Overall Loss 1.724839    Objective Loss 1.724839    Top1 67.000000    Top5 92.500000    LR 0.000016    Time 0.020108    
2022-01-10 13:06:49,699 - --- validate (epoch=281)-----------
2022-01-10 13:06:49,699 - 10000 samples (100 per mini-batch)
2022-01-10 13:06:51,171 - Epoch: [281][  100/  100]    Loss 1.887330    Top1 54.350000    Top5 82.830000    
2022-01-10 13:06:51,228 - ==> Top1: 54.350    Top5: 82.830    Loss: 1.887

2022-01-10 13:06:51,229 - ==> Best [Top1: 54.350   Top5: 82.830   Sparsity:0.00   Params: 381792 on epoch: 281]
2022-01-10 13:06:51,229 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:06:51,253 - 

2022-01-10 13:06:51,253 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:06:56,392 - Epoch: [282][  250/  500]    Overall Loss 1.707265    Objective Loss 1.707265                                        LR 0.000016    Time 0.020542    
2022-01-10 13:07:01,301 - Epoch: [282][  500/  500]    Overall Loss 1.714668    Objective Loss 1.714668    Top1 59.000000    Top5 90.000000    LR 0.000016    Time 0.020086    
2022-01-10 13:07:01,358 - --- validate (epoch=282)-----------
2022-01-10 13:07:01,358 - 10000 samples (100 per mini-batch)
2022-01-10 13:07:02,742 - Epoch: [282][  100/  100]    Loss 1.903944    Top1 53.330000    Top5 81.780000    
2022-01-10 13:07:02,799 - ==> Top1: 53.330    Top5: 81.780    Loss: 1.904

2022-01-10 13:07:02,800 - ==> Best [Top1: 54.350   Top5: 82.830   Sparsity:0.00   Params: 381792 on epoch: 281]
2022-01-10 13:07:02,800 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:07:02,820 - 

2022-01-10 13:07:02,820 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:07:07,913 - Epoch: [283][  250/  500]    Overall Loss 1.709923    Objective Loss 1.709923                                        LR 0.000016    Time 0.020361    
2022-01-10 13:07:12,757 - Epoch: [283][  500/  500]    Overall Loss 1.713363    Objective Loss 1.713363    Top1 59.000000    Top5 91.000000    LR 0.000016    Time 0.019865    
2022-01-10 13:07:12,821 - --- validate (epoch=283)-----------
2022-01-10 13:07:12,821 - 10000 samples (100 per mini-batch)
2022-01-10 13:07:14,164 - Epoch: [283][  100/  100]    Loss 1.991099    Top1 50.650000    Top5 80.170000    
2022-01-10 13:07:14,212 - ==> Top1: 50.650    Top5: 80.170    Loss: 1.991

2022-01-10 13:07:14,214 - ==> Best [Top1: 54.350   Top5: 82.830   Sparsity:0.00   Params: 381792 on epoch: 281]
2022-01-10 13:07:14,214 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:07:14,234 - 

2022-01-10 13:07:14,234 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:07:19,393 - Epoch: [284][  250/  500]    Overall Loss 1.721636    Objective Loss 1.721636                                        LR 0.000016    Time 0.020628    
2022-01-10 13:07:24,278 - Epoch: [284][  500/  500]    Overall Loss 1.718130    Objective Loss 1.718130    Top1 57.500000    Top5 90.000000    LR 0.000016    Time 0.020079    
2022-01-10 13:07:24,328 - --- validate (epoch=284)-----------
2022-01-10 13:07:24,328 - 10000 samples (100 per mini-batch)
2022-01-10 13:07:25,649 - Epoch: [284][  100/  100]    Loss 1.915977    Top1 52.720000    Top5 81.880000    
2022-01-10 13:07:25,698 - ==> Top1: 52.720    Top5: 81.880    Loss: 1.916

2022-01-10 13:07:25,700 - ==> Best [Top1: 54.350   Top5: 82.830   Sparsity:0.00   Params: 381792 on epoch: 281]
2022-01-10 13:07:25,700 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:07:25,720 - 

2022-01-10 13:07:25,720 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:07:30,758 - Epoch: [285][  250/  500]    Overall Loss 1.706311    Objective Loss 1.706311                                        LR 0.000016    Time 0.020140    
2022-01-10 13:07:35,692 - Epoch: [285][  500/  500]    Overall Loss 1.708529    Objective Loss 1.708529    Top1 60.500000    Top5 85.000000    LR 0.000016    Time 0.019934    
2022-01-10 13:07:35,755 - --- validate (epoch=285)-----------
2022-01-10 13:07:35,755 - 10000 samples (100 per mini-batch)
2022-01-10 13:07:37,132 - Epoch: [285][  100/  100]    Loss 1.994960    Top1 50.390000    Top5 80.300000    
2022-01-10 13:07:37,182 - ==> Top1: 50.390    Top5: 80.300    Loss: 1.995

2022-01-10 13:07:37,184 - ==> Best [Top1: 54.350   Top5: 82.830   Sparsity:0.00   Params: 381792 on epoch: 281]
2022-01-10 13:07:37,184 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:07:37,264 - 

2022-01-10 13:07:37,264 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:07:42,374 - Epoch: [286][  250/  500]    Overall Loss 1.699554    Objective Loss 1.699554                                        LR 0.000016    Time 0.020427    
2022-01-10 13:07:47,309 - Epoch: [286][  500/  500]    Overall Loss 1.699462    Objective Loss 1.699462    Top1 58.500000    Top5 88.000000    LR 0.000016    Time 0.020081    
2022-01-10 13:07:47,365 - --- validate (epoch=286)-----------
2022-01-10 13:07:47,365 - 10000 samples (100 per mini-batch)
2022-01-10 13:07:48,819 - Epoch: [286][  100/  100]    Loss 1.983684    Top1 50.090000    Top5 80.910000    
2022-01-10 13:07:48,872 - ==> Top1: 50.090    Top5: 80.910    Loss: 1.984

2022-01-10 13:07:48,874 - ==> Best [Top1: 54.350   Top5: 82.830   Sparsity:0.00   Params: 381792 on epoch: 281]
2022-01-10 13:07:48,874 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:07:48,894 - 

2022-01-10 13:07:48,894 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:07:54,270 - Epoch: [287][  250/  500]    Overall Loss 1.700554    Objective Loss 1.700554                                        LR 0.000016    Time 0.021491    
2022-01-10 13:07:59,031 - Epoch: [287][  500/  500]    Overall Loss 1.698677    Objective Loss 1.698677    Top1 59.500000    Top5 85.500000    LR 0.000016    Time 0.020264    
2022-01-10 13:07:59,089 - --- validate (epoch=287)-----------
2022-01-10 13:07:59,089 - 10000 samples (100 per mini-batch)
2022-01-10 13:08:00,478 - Epoch: [287][  100/  100]    Loss 1.908835    Top1 53.500000    Top5 81.710000    
2022-01-10 13:08:00,531 - ==> Top1: 53.500    Top5: 81.710    Loss: 1.909

2022-01-10 13:08:00,532 - ==> Best [Top1: 54.350   Top5: 82.830   Sparsity:0.00   Params: 381792 on epoch: 281]
2022-01-10 13:08:00,532 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:08:00,552 - 

2022-01-10 13:08:00,553 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:08:05,748 - Epoch: [288][  250/  500]    Overall Loss 1.701643    Objective Loss 1.701643                                        LR 0.000016    Time 0.020772    
2022-01-10 13:08:10,587 - Epoch: [288][  500/  500]    Overall Loss 1.699459    Objective Loss 1.699459    Top1 64.000000    Top5 91.500000    LR 0.000016    Time 0.020060    
2022-01-10 13:08:10,642 - --- validate (epoch=288)-----------
2022-01-10 13:08:10,642 - 10000 samples (100 per mini-batch)
2022-01-10 13:08:11,974 - Epoch: [288][  100/  100]    Loss 1.914460    Top1 52.040000    Top5 81.520000    
2022-01-10 13:08:12,029 - ==> Top1: 52.040    Top5: 81.520    Loss: 1.914

2022-01-10 13:08:12,031 - ==> Best [Top1: 54.350   Top5: 82.830   Sparsity:0.00   Params: 381792 on epoch: 281]
2022-01-10 13:08:12,031 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:08:12,051 - 

2022-01-10 13:08:12,051 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:08:17,132 - Epoch: [289][  250/  500]    Overall Loss 1.689334    Objective Loss 1.689334                                        LR 0.000016    Time 0.020312    
2022-01-10 13:08:21,961 - Epoch: [289][  500/  500]    Overall Loss 1.689641    Objective Loss 1.689641    Top1 56.500000    Top5 84.500000    LR 0.000016    Time 0.019811    
2022-01-10 13:08:22,018 - --- validate (epoch=289)-----------
2022-01-10 13:08:22,018 - 10000 samples (100 per mini-batch)
2022-01-10 13:08:23,341 - Epoch: [289][  100/  100]    Loss 1.969052    Top1 51.030000    Top5 80.000000    
2022-01-10 13:08:23,397 - ==> Top1: 51.030    Top5: 80.000    Loss: 1.969

2022-01-10 13:08:23,398 - ==> Best [Top1: 54.350   Top5: 82.830   Sparsity:0.00   Params: 381792 on epoch: 281]
2022-01-10 13:08:23,398 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:08:23,418 - 

2022-01-10 13:08:23,418 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:08:28,532 - Epoch: [290][  250/  500]    Overall Loss 1.687115    Objective Loss 1.687115                                        LR 0.000016    Time 0.020442    
2022-01-10 13:08:33,456 - Epoch: [290][  500/  500]    Overall Loss 1.683596    Objective Loss 1.683596    Top1 60.500000    Top5 85.500000    LR 0.000016    Time 0.020065    
2022-01-10 13:08:33,514 - --- validate (epoch=290)-----------
2022-01-10 13:08:33,514 - 10000 samples (100 per mini-batch)
2022-01-10 13:08:34,966 - Epoch: [290][  100/  100]    Loss 1.980062    Top1 49.890000    Top5 80.320000    
2022-01-10 13:08:35,025 - ==> Top1: 49.890    Top5: 80.320    Loss: 1.980

2022-01-10 13:08:35,027 - ==> Best [Top1: 54.350   Top5: 82.830   Sparsity:0.00   Params: 381792 on epoch: 281]
2022-01-10 13:08:35,027 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:08:35,047 - 

2022-01-10 13:08:35,047 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:08:40,176 - Epoch: [291][  250/  500]    Overall Loss 1.691119    Objective Loss 1.691119                                        LR 0.000016    Time 0.020505    
2022-01-10 13:08:45,129 - Epoch: [291][  500/  500]    Overall Loss 1.690498    Objective Loss 1.690498    Top1 57.500000    Top5 87.000000    LR 0.000016    Time 0.020155    
2022-01-10 13:08:45,186 - --- validate (epoch=291)-----------
2022-01-10 13:08:45,186 - 10000 samples (100 per mini-batch)
2022-01-10 13:08:46,541 - Epoch: [291][  100/  100]    Loss 1.906716    Top1 52.690000    Top5 81.920000    
2022-01-10 13:08:46,590 - ==> Top1: 52.690    Top5: 81.920    Loss: 1.907

2022-01-10 13:08:46,592 - ==> Best [Top1: 54.350   Top5: 82.830   Sparsity:0.00   Params: 381792 on epoch: 281]
2022-01-10 13:08:46,592 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:08:46,611 - 

2022-01-10 13:08:46,611 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:08:51,799 - Epoch: [292][  250/  500]    Overall Loss 1.687663    Objective Loss 1.687663                                        LR 0.000016    Time 0.020740    
2022-01-10 13:08:56,702 - Epoch: [292][  500/  500]    Overall Loss 1.683991    Objective Loss 1.683991    Top1 59.000000    Top5 88.000000    LR 0.000016    Time 0.020172    
2022-01-10 13:08:56,758 - --- validate (epoch=292)-----------
2022-01-10 13:08:56,758 - 10000 samples (100 per mini-batch)
2022-01-10 13:08:58,200 - Epoch: [292][  100/  100]    Loss 1.915268    Top1 51.920000    Top5 81.670000    
2022-01-10 13:08:58,254 - ==> Top1: 51.920    Top5: 81.670    Loss: 1.915

2022-01-10 13:08:58,256 - ==> Best [Top1: 54.350   Top5: 82.830   Sparsity:0.00   Params: 381792 on epoch: 281]
2022-01-10 13:08:58,256 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:08:58,275 - 

2022-01-10 13:08:58,276 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:09:03,425 - Epoch: [293][  250/  500]    Overall Loss 1.680521    Objective Loss 1.680521                                        LR 0.000016    Time 0.020587    
2022-01-10 13:09:08,294 - Epoch: [293][  500/  500]    Overall Loss 1.681144    Objective Loss 1.681144    Top1 55.000000    Top5 89.500000    LR 0.000016    Time 0.020029    
2022-01-10 13:09:08,352 - --- validate (epoch=293)-----------
2022-01-10 13:09:08,352 - 10000 samples (100 per mini-batch)
2022-01-10 13:09:09,785 - Epoch: [293][  100/  100]    Loss 1.973712    Top1 50.360000    Top5 80.220000    
2022-01-10 13:09:09,843 - ==> Top1: 50.360    Top5: 80.220    Loss: 1.974

2022-01-10 13:09:09,845 - ==> Best [Top1: 54.350   Top5: 82.830   Sparsity:0.00   Params: 381792 on epoch: 281]
2022-01-10 13:09:09,845 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:09:09,865 - 

2022-01-10 13:09:09,865 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:09:15,075 - Epoch: [294][  250/  500]    Overall Loss 1.675136    Objective Loss 1.675136                                        LR 0.000016    Time 0.020827    
2022-01-10 13:09:20,033 - Epoch: [294][  500/  500]    Overall Loss 1.672363    Objective Loss 1.672363    Top1 59.500000    Top5 87.000000    LR 0.000016    Time 0.020326    
2022-01-10 13:09:20,095 - --- validate (epoch=294)-----------
2022-01-10 13:09:20,095 - 10000 samples (100 per mini-batch)
2022-01-10 13:09:21,444 - Epoch: [294][  100/  100]    Loss 1.863024    Top1 53.650000    Top5 82.800000    
2022-01-10 13:09:21,502 - ==> Top1: 53.650    Top5: 82.800    Loss: 1.863

2022-01-10 13:09:21,503 - ==> Best [Top1: 54.350   Top5: 82.830   Sparsity:0.00   Params: 381792 on epoch: 281]
2022-01-10 13:09:21,503 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:09:21,523 - 

2022-01-10 13:09:21,523 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:09:26,721 - Epoch: [295][  250/  500]    Overall Loss 1.662777    Objective Loss 1.662777                                        LR 0.000016    Time 0.020777    
2022-01-10 13:09:31,783 - Epoch: [295][  500/  500]    Overall Loss 1.668879    Objective Loss 1.668879    Top1 60.500000    Top5 88.500000    LR 0.000016    Time 0.020509    
2022-01-10 13:09:31,832 - --- validate (epoch=295)-----------
2022-01-10 13:09:31,832 - 10000 samples (100 per mini-batch)
2022-01-10 13:09:33,232 - Epoch: [295][  100/  100]    Loss 1.846180    Top1 53.550000    Top5 82.950000    
2022-01-10 13:09:33,284 - ==> Top1: 53.550    Top5: 82.950    Loss: 1.846

2022-01-10 13:09:33,286 - ==> Best [Top1: 54.350   Top5: 82.830   Sparsity:0.00   Params: 381792 on epoch: 281]
2022-01-10 13:09:33,286 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:09:33,306 - 

2022-01-10 13:09:33,306 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:09:38,565 - Epoch: [296][  250/  500]    Overall Loss 1.682979    Objective Loss 1.682979                                        LR 0.000016    Time 0.021026    
2022-01-10 13:09:43,680 - Epoch: [296][  500/  500]    Overall Loss 1.674609    Objective Loss 1.674609    Top1 58.500000    Top5 85.000000    LR 0.000016    Time 0.020738    
2022-01-10 13:09:43,739 - --- validate (epoch=296)-----------
2022-01-10 13:09:43,739 - 10000 samples (100 per mini-batch)
2022-01-10 13:09:45,090 - Epoch: [296][  100/  100]    Loss 1.874335    Top1 53.120000    Top5 82.220000    
2022-01-10 13:09:45,140 - ==> Top1: 53.120    Top5: 82.220    Loss: 1.874

2022-01-10 13:09:45,141 - ==> Best [Top1: 54.350   Top5: 82.830   Sparsity:0.00   Params: 381792 on epoch: 281]
2022-01-10 13:09:45,141 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:09:45,162 - 

2022-01-10 13:09:45,162 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:09:50,284 - Epoch: [297][  250/  500]    Overall Loss 1.662004    Objective Loss 1.662004                                        LR 0.000016    Time 0.020478    
2022-01-10 13:09:55,173 - Epoch: [297][  500/  500]    Overall Loss 1.666562    Objective Loss 1.666562    Top1 59.000000    Top5 85.500000    LR 0.000016    Time 0.020014    
2022-01-10 13:09:55,230 - --- validate (epoch=297)-----------
2022-01-10 13:09:55,230 - 10000 samples (100 per mini-batch)
2022-01-10 13:09:56,550 - Epoch: [297][  100/  100]    Loss 2.062762    Top1 48.600000    Top5 78.530000    
2022-01-10 13:09:56,607 - ==> Top1: 48.600    Top5: 78.530    Loss: 2.063

2022-01-10 13:09:56,609 - ==> Best [Top1: 54.350   Top5: 82.830   Sparsity:0.00   Params: 381792 on epoch: 281]
2022-01-10 13:09:56,609 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:09:56,629 - 

2022-01-10 13:09:56,629 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:10:01,763 - Epoch: [298][  250/  500]    Overall Loss 1.659545    Objective Loss 1.659545                                        LR 0.000016    Time 0.020525    
2022-01-10 13:10:06,624 - Epoch: [298][  500/  500]    Overall Loss 1.666179    Objective Loss 1.666179    Top1 62.000000    Top5 91.000000    LR 0.000016    Time 0.019980    
2022-01-10 13:10:06,679 - --- validate (epoch=298)-----------
2022-01-10 13:10:06,679 - 10000 samples (100 per mini-batch)
2022-01-10 13:10:08,006 - Epoch: [298][  100/  100]    Loss 1.919794    Top1 51.220000    Top5 81.320000    
2022-01-10 13:10:08,063 - ==> Top1: 51.220    Top5: 81.320    Loss: 1.920

2022-01-10 13:10:08,065 - ==> Best [Top1: 54.350   Top5: 82.830   Sparsity:0.00   Params: 381792 on epoch: 281]
2022-01-10 13:10:08,065 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:10:08,085 - 

2022-01-10 13:10:08,085 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 13:10:13,193 - Epoch: [299][  250/  500]    Overall Loss 1.653698    Objective Loss 1.653698                                        LR 0.000016    Time 0.020421    
2022-01-10 13:10:18,083 - Epoch: [299][  500/  500]    Overall Loss 1.667810    Objective Loss 1.667810    Top1 62.500000    Top5 88.500000    LR 0.000016    Time 0.019986    
2022-01-10 13:10:18,138 - --- validate (epoch=299)-----------
2022-01-10 13:10:18,139 - 10000 samples (100 per mini-batch)
2022-01-10 13:10:19,524 - Epoch: [299][  100/  100]    Loss 1.963428    Top1 50.570000    Top5 80.680000    
2022-01-10 13:10:19,577 - ==> Top1: 50.570    Top5: 80.680    Loss: 1.963

2022-01-10 13:10:19,579 - ==> Best [Top1: 54.350   Top5: 82.830   Sparsity:0.00   Params: 381792 on epoch: 281]
2022-01-10 13:10:19,579 - Saving checkpoint to: logs/2022.01.10-122710/qat_checkpoint.pth.tar
2022-01-10 13:10:19,598 - --- test ---------------------
2022-01-10 13:10:19,599 - 10000 samples (100 per mini-batch)
2022-01-10 13:10:20,938 - Test: [  100/  100]    Loss 1.963428    Top1 50.570000    Top5 80.680000    
2022-01-10 13:10:20,994 - ==> Top1: 50.570    Top5: 80.680    Loss: 1.963

2022-01-10 13:10:20,996 - 
2022-01-10 13:10:20,996 - Log file for this run: /home/gorkemulkar/Workspace/Python/AI8X_GitHub/ai8x-training/logs/2022.01.10-122710/2022.01.10-122710.log
