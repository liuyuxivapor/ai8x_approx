2023-06-15 23:54:32,279 - Log file for this run: /home/alicangok/Projects/AI8X/ai8x-training/logs/2023.06.15-235432/2023.06.15-235432.log
2023-06-15 23:54:34,222 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2023-06-15 23:54:34,223 - Optimizer Args: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.001, 'amsgrad': False}
2023-06-15 23:56:50,660 - Dataset sizes:
	training=4544
	validation=1422
	test=1422
2023-06-15 23:56:50,661 - Reading compression schedule from: policies/schedule_kinetics.yaml
2023-06-15 23:56:50,666 - 

2023-06-15 23:56:50,666 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-15 23:57:01,610 - Epoch: [0][   10/  142]    Overall Loss 1.653768    Objective Loss 1.653768                                        LR 0.001000    Time 1.094249    
2023-06-15 23:57:06,119 - Epoch: [0][   20/  142]    Overall Loss 1.647963    Objective Loss 1.647963                                        LR 0.001000    Time 0.772520    
2023-06-15 23:57:10,494 - Epoch: [0][   30/  142]    Overall Loss 1.630685    Objective Loss 1.630685                                        LR 0.001000    Time 0.660819    
2023-06-15 23:57:16,521 - Epoch: [0][   40/  142]    Overall Loss 1.630117    Objective Loss 1.630117                                        LR 0.001000    Time 0.646261    
2023-06-15 23:57:21,540 - Epoch: [0][   50/  142]    Overall Loss 1.630430    Objective Loss 1.630430                                        LR 0.001000    Time 0.617359    
2023-06-15 23:57:26,722 - Epoch: [0][   60/  142]    Overall Loss 1.625352    Objective Loss 1.625352                                        LR 0.001000    Time 0.600825    
2023-06-15 23:57:31,983 - Epoch: [0][   70/  142]    Overall Loss 1.619128    Objective Loss 1.619128                                        LR 0.001000    Time 0.590134    
2023-06-15 23:57:37,091 - Epoch: [0][   80/  142]    Overall Loss 1.614226    Objective Loss 1.614226                                        LR 0.001000    Time 0.580199    
2023-06-15 23:57:42,083 - Epoch: [0][   90/  142]    Overall Loss 1.604580    Objective Loss 1.604580                                        LR 0.001000    Time 0.571185    
2023-06-15 23:57:47,155 - Epoch: [0][  100/  142]    Overall Loss 1.600641    Objective Loss 1.600641                                        LR 0.001000    Time 0.564781    
2023-06-15 23:57:52,128 - Epoch: [0][  110/  142]    Overall Loss 1.595988    Objective Loss 1.595988                                        LR 0.001000    Time 0.558631    
2023-06-15 23:57:57,374 - Epoch: [0][  120/  142]    Overall Loss 1.593338    Objective Loss 1.593338                                        LR 0.001000    Time 0.555789    
2023-06-15 23:58:02,203 - Epoch: [0][  130/  142]    Overall Loss 1.589145    Objective Loss 1.589145                                        LR 0.001000    Time 0.550169    
2023-06-15 23:58:07,035 - Epoch: [0][  140/  142]    Overall Loss 1.585555    Objective Loss 1.585555                                        LR 0.001000    Time 0.545381    
2023-06-15 23:58:07,762 - Epoch: [0][  142/  142]    Overall Loss 1.584075    Objective Loss 1.584075    Top1 34.375000    LR 0.001000    Time 0.542821    
2023-06-15 23:58:08,422 - --- validate (epoch=0)-----------
2023-06-15 23:58:08,423 - 1422 samples (32 per mini-batch)
2023-06-15 23:58:16,459 - Epoch: [0][   10/   45]    Loss 1.517357    Top1 35.312500    
2023-06-15 23:58:20,532 - Epoch: [0][   20/   45]    Loss 1.515928    Top1 37.500000    
2023-06-15 23:58:25,265 - Epoch: [0][   30/   45]    Loss 1.519761    Top1 38.541667    
2023-06-15 23:58:29,359 - Epoch: [0][   40/   45]    Loss 1.518761    Top1 37.578125    
2023-06-15 23:58:30,537 - Epoch: [0][   45/   45]    Loss 1.514411    Top1 37.623066    
2023-06-15 23:58:31,169 - ==> Top1: 37.623    Loss: 1.514

2023-06-15 23:58:31,175 - ==> Best [Top1: 37.623   Sparsity:0.00   Params: 375264 on epoch: 0]
2023-06-15 23:58:31,175 - Saving checkpoint to: logs/2023.06.15-235432/checkpoint.pth.tar
2023-06-15 23:58:31,196 - 

2023-06-15 23:58:31,196 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-15 23:58:39,705 - Epoch: [1][   10/  142]    Overall Loss 1.509983    Objective Loss 1.509983                                        LR 0.001000    Time 0.850790    
2023-06-15 23:58:44,187 - Epoch: [1][   20/  142]    Overall Loss 1.512352    Objective Loss 1.512352                                        LR 0.001000    Time 0.649417    
2023-06-15 23:58:48,588 - Epoch: [1][   30/  142]    Overall Loss 1.513977    Objective Loss 1.513977                                        LR 0.001000    Time 0.579606    
2023-06-15 23:58:52,967 - Epoch: [1][   40/  142]    Overall Loss 1.510474    Objective Loss 1.510474                                        LR 0.001000    Time 0.544149    
2023-06-15 23:58:57,962 - Epoch: [1][   50/  142]    Overall Loss 1.507048    Objective Loss 1.507048                                        LR 0.001000    Time 0.535199    
2023-06-15 23:59:02,509 - Epoch: [1][   60/  142]    Overall Loss 1.505390    Objective Loss 1.505390                                        LR 0.001000    Time 0.521773    
2023-06-15 23:59:07,301 - Epoch: [1][   70/  142]    Overall Loss 1.503206    Objective Loss 1.503206                                        LR 0.001000    Time 0.515678    
2023-06-15 23:59:11,666 - Epoch: [1][   80/  142]    Overall Loss 1.503242    Objective Loss 1.503242                                        LR 0.001000    Time 0.505766    
2023-06-15 23:59:16,246 - Epoch: [1][   90/  142]    Overall Loss 1.499763    Objective Loss 1.499763                                        LR 0.001000    Time 0.500444    
2023-06-15 23:59:20,669 - Epoch: [1][  100/  142]    Overall Loss 1.500056    Objective Loss 1.500056                                        LR 0.001000    Time 0.494614    
2023-06-15 23:59:25,399 - Epoch: [1][  110/  142]    Overall Loss 1.500226    Objective Loss 1.500226                                        LR 0.001000    Time 0.492642    
2023-06-15 23:59:29,998 - Epoch: [1][  120/  142]    Overall Loss 1.501561    Objective Loss 1.501561                                        LR 0.001000    Time 0.489902    
2023-06-15 23:59:34,672 - Epoch: [1][  130/  142]    Overall Loss 1.500112    Objective Loss 1.500112                                        LR 0.001000    Time 0.488165    
2023-06-15 23:59:38,733 - Epoch: [1][  140/  142]    Overall Loss 1.499351    Objective Loss 1.499351                                        LR 0.001000    Time 0.482299    
2023-06-15 23:59:39,446 - Epoch: [1][  142/  142]    Overall Loss 1.498992    Objective Loss 1.498992    Top1 46.875000    LR 0.001000    Time 0.480522    
2023-06-15 23:59:40,061 - --- validate (epoch=1)-----------
2023-06-15 23:59:40,062 - 1422 samples (32 per mini-batch)
2023-06-15 23:59:47,318 - Epoch: [1][   10/   45]    Loss 1.460923    Top1 43.750000    
2023-06-15 23:59:51,430 - Epoch: [1][   20/   45]    Loss 1.462292    Top1 41.562500    
2023-06-15 23:59:55,863 - Epoch: [1][   30/   45]    Loss 1.465979    Top1 40.937500    
2023-06-16 00:00:00,576 - Epoch: [1][   40/   45]    Loss 1.463514    Top1 41.171875    
2023-06-16 00:00:01,880 - Epoch: [1][   45/   45]    Loss 1.461503    Top1 40.717300    
2023-06-16 00:00:02,505 - ==> Top1: 40.717    Loss: 1.462

2023-06-16 00:00:02,507 - ==> Best [Top1: 40.717   Sparsity:0.00   Params: 375264 on epoch: 1]
2023-06-16 00:00:02,507 - Saving checkpoint to: logs/2023.06.15-235432/checkpoint.pth.tar
2023-06-16 00:00:02,535 - 

2023-06-16 00:00:02,535 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:00:10,819 - Epoch: [2][   10/  142]    Overall Loss 1.474265    Objective Loss 1.474265                                        LR 0.001000    Time 0.828242    
2023-06-16 00:00:15,311 - Epoch: [2][   20/  142]    Overall Loss 1.482709    Objective Loss 1.482709                                        LR 0.001000    Time 0.638632    
2023-06-16 00:00:19,531 - Epoch: [2][   30/  142]    Overall Loss 1.485770    Objective Loss 1.485770                                        LR 0.001000    Time 0.566384    
2023-06-16 00:00:23,935 - Epoch: [2][   40/  142]    Overall Loss 1.476729    Objective Loss 1.476729                                        LR 0.001000    Time 0.534874    
2023-06-16 00:00:28,661 - Epoch: [2][   50/  142]    Overall Loss 1.474156    Objective Loss 1.474156                                        LR 0.001000    Time 0.522384    
2023-06-16 00:00:33,283 - Epoch: [2][   60/  142]    Overall Loss 1.474092    Objective Loss 1.474092                                        LR 0.001000    Time 0.512337    
2023-06-16 00:00:37,549 - Epoch: [2][   70/  142]    Overall Loss 1.471311    Objective Loss 1.471311                                        LR 0.001000    Time 0.500083    
2023-06-16 00:00:42,892 - Epoch: [2][   80/  142]    Overall Loss 1.473096    Objective Loss 1.473096                                        LR 0.001000    Time 0.504345    
2023-06-16 00:00:47,012 - Epoch: [2][   90/  142]    Overall Loss 1.470324    Objective Loss 1.470324                                        LR 0.001000    Time 0.494082    
2023-06-16 00:00:51,868 - Epoch: [2][  100/  142]    Overall Loss 1.466572    Objective Loss 1.466572                                        LR 0.001000    Time 0.493223    
2023-06-16 00:00:56,642 - Epoch: [2][  110/  142]    Overall Loss 1.466166    Objective Loss 1.466166                                        LR 0.001000    Time 0.491772    
2023-06-16 00:01:00,650 - Epoch: [2][  120/  142]    Overall Loss 1.464344    Objective Loss 1.464344                                        LR 0.001000    Time 0.484189    
2023-06-16 00:01:06,217 - Epoch: [2][  130/  142]    Overall Loss 1.463269    Objective Loss 1.463269                                        LR 0.001000    Time 0.489757    
2023-06-16 00:01:10,241 - Epoch: [2][  140/  142]    Overall Loss 1.461510    Objective Loss 1.461510                                        LR 0.001000    Time 0.483511    
2023-06-16 00:01:10,954 - Epoch: [2][  142/  142]    Overall Loss 1.461221    Objective Loss 1.461221    Top1 45.312500    LR 0.001000    Time 0.481723    
2023-06-16 00:01:11,607 - --- validate (epoch=2)-----------
2023-06-16 00:01:11,607 - 1422 samples (32 per mini-batch)
2023-06-16 00:01:19,140 - Epoch: [2][   10/   45]    Loss 1.432604    Top1 40.937500    
2023-06-16 00:01:23,340 - Epoch: [2][   20/   45]    Loss 1.441126    Top1 39.375000    
2023-06-16 00:01:28,608 - Epoch: [2][   30/   45]    Loss 1.441679    Top1 39.166667    
2023-06-16 00:01:32,219 - Epoch: [2][   40/   45]    Loss 1.440474    Top1 40.234375    
2023-06-16 00:01:33,887 - Epoch: [2][   45/   45]    Loss 1.442306    Top1 39.943741    
2023-06-16 00:01:34,519 - ==> Top1: 39.944    Loss: 1.442

2023-06-16 00:01:34,521 - ==> Best [Top1: 40.717   Sparsity:0.00   Params: 375264 on epoch: 1]
2023-06-16 00:01:34,521 - Saving checkpoint to: logs/2023.06.15-235432/checkpoint.pth.tar
2023-06-16 00:01:34,545 - 

2023-06-16 00:01:34,545 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:01:42,814 - Epoch: [3][   10/  142]    Overall Loss 1.441604    Objective Loss 1.441604                                        LR 0.001000    Time 0.826727    
2023-06-16 00:01:47,387 - Epoch: [3][   20/  142]    Overall Loss 1.432970    Objective Loss 1.432970                                        LR 0.001000    Time 0.641954    
2023-06-16 00:01:52,081 - Epoch: [3][   30/  142]    Overall Loss 1.426261    Objective Loss 1.426261                                        LR 0.001000    Time 0.584393    
2023-06-16 00:01:56,637 - Epoch: [3][   40/  142]    Overall Loss 1.424677    Objective Loss 1.424677                                        LR 0.001000    Time 0.552173    
2023-06-16 00:02:01,161 - Epoch: [3][   50/  142]    Overall Loss 1.425354    Objective Loss 1.425354                                        LR 0.001000    Time 0.532191    
2023-06-16 00:02:05,581 - Epoch: [3][   60/  142]    Overall Loss 1.430333    Objective Loss 1.430333                                        LR 0.001000    Time 0.517144    
2023-06-16 00:02:10,140 - Epoch: [3][   70/  142]    Overall Loss 1.429803    Objective Loss 1.429803                                        LR 0.001000    Time 0.508381    
2023-06-16 00:02:14,602 - Epoch: [3][   80/  142]    Overall Loss 1.426921    Objective Loss 1.426921                                        LR 0.001000    Time 0.500593    
2023-06-16 00:02:19,272 - Epoch: [3][   90/  142]    Overall Loss 1.424746    Objective Loss 1.424746                                        LR 0.001000    Time 0.496840    
2023-06-16 00:02:23,849 - Epoch: [3][  100/  142]    Overall Loss 1.423680    Objective Loss 1.423680                                        LR 0.001000    Time 0.492917    
2023-06-16 00:02:28,789 - Epoch: [3][  110/  142]    Overall Loss 1.424681    Objective Loss 1.424681                                        LR 0.001000    Time 0.493005    
2023-06-16 00:02:34,000 - Epoch: [3][  120/  142]    Overall Loss 1.424844    Objective Loss 1.424844                                        LR 0.001000    Time 0.495338    
2023-06-16 00:02:38,395 - Epoch: [3][  130/  142]    Overall Loss 1.424252    Objective Loss 1.424252                                        LR 0.001000    Time 0.491034    
2023-06-16 00:02:43,012 - Epoch: [3][  140/  142]    Overall Loss 1.422437    Objective Loss 1.422437                                        LR 0.001000    Time 0.488938    
2023-06-16 00:02:43,732 - Epoch: [3][  142/  142]    Overall Loss 1.422488    Objective Loss 1.422488    Top1 43.750000    LR 0.001000    Time 0.487123    
2023-06-16 00:02:44,339 - --- validate (epoch=3)-----------
2023-06-16 00:02:44,340 - 1422 samples (32 per mini-batch)
2023-06-16 00:02:52,464 - Epoch: [3][   10/   45]    Loss 1.386356    Top1 47.187500    
2023-06-16 00:02:56,066 - Epoch: [3][   20/   45]    Loss 1.385775    Top1 47.031250    
2023-06-16 00:03:00,748 - Epoch: [3][   30/   45]    Loss 1.375760    Top1 47.083333    
2023-06-16 00:03:04,902 - Epoch: [3][   40/   45]    Loss 1.386902    Top1 46.718750    
2023-06-16 00:03:06,042 - Epoch: [3][   45/   45]    Loss 1.386250    Top1 46.835443    
2023-06-16 00:03:06,700 - ==> Top1: 46.835    Loss: 1.386

2023-06-16 00:03:06,703 - ==> Best [Top1: 46.835   Sparsity:0.00   Params: 375264 on epoch: 3]
2023-06-16 00:03:06,703 - Saving checkpoint to: logs/2023.06.15-235432/checkpoint.pth.tar
2023-06-16 00:03:06,730 - 

2023-06-16 00:03:06,730 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:03:15,000 - Epoch: [4][   10/  142]    Overall Loss 1.410189    Objective Loss 1.410189                                        LR 0.001000    Time 0.826847    
2023-06-16 00:03:19,786 - Epoch: [4][   20/  142]    Overall Loss 1.405971    Objective Loss 1.405971                                        LR 0.001000    Time 0.652659    
2023-06-16 00:03:24,466 - Epoch: [4][   30/  142]    Overall Loss 1.396783    Objective Loss 1.396783                                        LR 0.001000    Time 0.591081    
2023-06-16 00:03:28,961 - Epoch: [4][   40/  142]    Overall Loss 1.402546    Objective Loss 1.402546                                        LR 0.001000    Time 0.555649    
2023-06-16 00:03:34,155 - Epoch: [4][   50/  142]    Overall Loss 1.400355    Objective Loss 1.400355                                        LR 0.001000    Time 0.548382    
2023-06-16 00:03:38,467 - Epoch: [4][   60/  142]    Overall Loss 1.397740    Objective Loss 1.397740                                        LR 0.001000    Time 0.528839    
2023-06-16 00:03:42,874 - Epoch: [4][   70/  142]    Overall Loss 1.395749    Objective Loss 1.395749                                        LR 0.001000    Time 0.516232    
2023-06-16 00:03:47,458 - Epoch: [4][   80/  142]    Overall Loss 1.394344    Objective Loss 1.394344                                        LR 0.001000    Time 0.508992    
2023-06-16 00:03:52,238 - Epoch: [4][   90/  142]    Overall Loss 1.392867    Objective Loss 1.392867                                        LR 0.001000    Time 0.505541    
2023-06-16 00:03:56,920 - Epoch: [4][  100/  142]    Overall Loss 1.389620    Objective Loss 1.389620                                        LR 0.001000    Time 0.501790    
2023-06-16 00:04:01,526 - Epoch: [4][  110/  142]    Overall Loss 1.388347    Objective Loss 1.388347                                        LR 0.001000    Time 0.498040    
2023-06-16 00:04:05,939 - Epoch: [4][  120/  142]    Overall Loss 1.387458    Objective Loss 1.387458                                        LR 0.001000    Time 0.493302    
2023-06-16 00:04:10,486 - Epoch: [4][  130/  142]    Overall Loss 1.387750    Objective Loss 1.387750                                        LR 0.001000    Time 0.490320    
2023-06-16 00:04:14,715 - Epoch: [4][  140/  142]    Overall Loss 1.387477    Objective Loss 1.387477                                        LR 0.001000    Time 0.485502    
2023-06-16 00:04:15,440 - Epoch: [4][  142/  142]    Overall Loss 1.387198    Objective Loss 1.387198    Top1 54.687500    LR 0.001000    Time 0.483765    
2023-06-16 00:04:16,071 - --- validate (epoch=4)-----------
2023-06-16 00:04:16,072 - 1422 samples (32 per mini-batch)
2023-06-16 00:04:23,962 - Epoch: [4][   10/   45]    Loss 1.366176    Top1 45.000000    
2023-06-16 00:04:28,168 - Epoch: [4][   20/   45]    Loss 1.362204    Top1 45.000000    
2023-06-16 00:04:32,641 - Epoch: [4][   30/   45]    Loss 1.362962    Top1 45.833333    
2023-06-16 00:04:37,020 - Epoch: [4][   40/   45]    Loss 1.352346    Top1 46.718750    
2023-06-16 00:04:38,225 - Epoch: [4][   45/   45]    Loss 1.349176    Top1 47.046414    
2023-06-16 00:04:38,831 - ==> Top1: 47.046    Loss: 1.349

2023-06-16 00:04:38,833 - ==> Best [Top1: 47.046   Sparsity:0.00   Params: 375264 on epoch: 4]
2023-06-16 00:04:38,833 - Saving checkpoint to: logs/2023.06.15-235432/checkpoint.pth.tar
2023-06-16 00:04:38,860 - 

2023-06-16 00:04:38,860 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:04:47,191 - Epoch: [5][   10/  142]    Overall Loss 1.380923    Objective Loss 1.380923                                        LR 0.001000    Time 0.832981    
2023-06-16 00:04:51,699 - Epoch: [5][   20/  142]    Overall Loss 1.379373    Objective Loss 1.379373                                        LR 0.001000    Time 0.641806    
2023-06-16 00:04:56,211 - Epoch: [5][   30/  142]    Overall Loss 1.378651    Objective Loss 1.378651                                        LR 0.001000    Time 0.578228    
2023-06-16 00:05:00,445 - Epoch: [5][   40/  142]    Overall Loss 1.376018    Objective Loss 1.376018                                        LR 0.001000    Time 0.539492    
2023-06-16 00:05:05,067 - Epoch: [5][   50/  142]    Overall Loss 1.370209    Objective Loss 1.370209                                        LR 0.001000    Time 0.524025    
2023-06-16 00:05:09,986 - Epoch: [5][   60/  142]    Overall Loss 1.364756    Objective Loss 1.364756                                        LR 0.001000    Time 0.518650    
2023-06-16 00:05:14,502 - Epoch: [5][   70/  142]    Overall Loss 1.359596    Objective Loss 1.359596                                        LR 0.001000    Time 0.509059    
2023-06-16 00:05:18,991 - Epoch: [5][   80/  142]    Overall Loss 1.358579    Objective Loss 1.358579                                        LR 0.001000    Time 0.501517    
2023-06-16 00:05:23,507 - Epoch: [5][   90/  142]    Overall Loss 1.357769    Objective Loss 1.357769                                        LR 0.001000    Time 0.495956    
2023-06-16 00:05:28,001 - Epoch: [5][  100/  142]    Overall Loss 1.356934    Objective Loss 1.356934                                        LR 0.001000    Time 0.491296    
2023-06-16 00:05:32,335 - Epoch: [5][  110/  142]    Overall Loss 1.357088    Objective Loss 1.357088                                        LR 0.001000    Time 0.486020    
2023-06-16 00:05:36,920 - Epoch: [5][  120/  142]    Overall Loss 1.357514    Objective Loss 1.357514                                        LR 0.001000    Time 0.483718    
2023-06-16 00:05:41,424 - Epoch: [5][  130/  142]    Overall Loss 1.356694    Objective Loss 1.356694                                        LR 0.001000    Time 0.480592    
2023-06-16 00:05:45,579 - Epoch: [5][  140/  142]    Overall Loss 1.356749    Objective Loss 1.356749                                        LR 0.001000    Time 0.475941    
2023-06-16 00:05:46,302 - Epoch: [5][  142/  142]    Overall Loss 1.355981    Objective Loss 1.355981    Top1 54.687500    LR 0.001000    Time 0.474323    
2023-06-16 00:05:46,911 - --- validate (epoch=5)-----------
2023-06-16 00:05:46,912 - 1422 samples (32 per mini-batch)
2023-06-16 00:05:54,218 - Epoch: [5][   10/   45]    Loss 1.406310    Top1 45.000000    
2023-06-16 00:05:58,406 - Epoch: [5][   20/   45]    Loss 1.388244    Top1 45.625000    
2023-06-16 00:06:03,120 - Epoch: [5][   30/   45]    Loss 1.374778    Top1 46.250000    
2023-06-16 00:06:07,741 - Epoch: [5][   40/   45]    Loss 1.359126    Top1 47.421875    
2023-06-16 00:06:08,922 - Epoch: [5][   45/   45]    Loss 1.364100    Top1 47.187060    
2023-06-16 00:06:09,520 - ==> Top1: 47.187    Loss: 1.364

2023-06-16 00:06:09,522 - ==> Best [Top1: 47.187   Sparsity:0.00   Params: 375264 on epoch: 5]
2023-06-16 00:06:09,522 - Saving checkpoint to: logs/2023.06.15-235432/checkpoint.pth.tar
2023-06-16 00:06:09,546 - 

2023-06-16 00:06:09,547 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:06:18,140 - Epoch: [6][   10/  142]    Overall Loss 1.364672    Objective Loss 1.364672                                        LR 0.001000    Time 0.859154    
2023-06-16 00:06:22,700 - Epoch: [6][   20/  142]    Overall Loss 1.353164    Objective Loss 1.353164                                        LR 0.001000    Time 0.657516    
2023-06-16 00:06:27,206 - Epoch: [6][   30/  142]    Overall Loss 1.342351    Objective Loss 1.342351                                        LR 0.001000    Time 0.588519    
2023-06-16 00:06:32,123 - Epoch: [6][   40/  142]    Overall Loss 1.330084    Objective Loss 1.330084                                        LR 0.001000    Time 0.564290    
2023-06-16 00:06:36,779 - Epoch: [6][   50/  142]    Overall Loss 1.330924    Objective Loss 1.330924                                        LR 0.001000    Time 0.544529    
2023-06-16 00:06:41,221 - Epoch: [6][   60/  142]    Overall Loss 1.329658    Objective Loss 1.329658                                        LR 0.001000    Time 0.527791    
2023-06-16 00:06:46,046 - Epoch: [6][   70/  142]    Overall Loss 1.324199    Objective Loss 1.324199                                        LR 0.001000    Time 0.521312    
2023-06-16 00:06:50,779 - Epoch: [6][   80/  142]    Overall Loss 1.325896    Objective Loss 1.325896                                        LR 0.001000    Time 0.515301    
2023-06-16 00:06:55,476 - Epoch: [6][   90/  142]    Overall Loss 1.323554    Objective Loss 1.323554                                        LR 0.001000    Time 0.510212    
2023-06-16 00:07:00,144 - Epoch: [6][  100/  142]    Overall Loss 1.325374    Objective Loss 1.325374                                        LR 0.001000    Time 0.505868    
2023-06-16 00:07:04,879 - Epoch: [6][  110/  142]    Overall Loss 1.326410    Objective Loss 1.326410                                        LR 0.001000    Time 0.502909    
2023-06-16 00:07:09,519 - Epoch: [6][  120/  142]    Overall Loss 1.322126    Objective Loss 1.322126                                        LR 0.001000    Time 0.499659    
2023-06-16 00:07:14,608 - Epoch: [6][  130/  142]    Overall Loss 1.322082    Objective Loss 1.322082                                        LR 0.001000    Time 0.500363    
2023-06-16 00:07:18,741 - Epoch: [6][  140/  142]    Overall Loss 1.320123    Objective Loss 1.320123                                        LR 0.001000    Time 0.494141    
2023-06-16 00:07:19,457 - Epoch: [6][  142/  142]    Overall Loss 1.319469    Objective Loss 1.319469    Top1 62.500000    LR 0.001000    Time 0.492221    
2023-06-16 00:07:20,086 - --- validate (epoch=6)-----------
2023-06-16 00:07:20,087 - 1422 samples (32 per mini-batch)
2023-06-16 00:07:27,544 - Epoch: [6][   10/   45]    Loss 1.289828    Top1 56.875000    
2023-06-16 00:07:31,660 - Epoch: [6][   20/   45]    Loss 1.321982    Top1 53.750000    
2023-06-16 00:07:36,332 - Epoch: [6][   30/   45]    Loss 1.302936    Top1 56.145833    
2023-06-16 00:07:40,439 - Epoch: [6][   40/   45]    Loss 1.323668    Top1 54.296875    
2023-06-16 00:07:41,823 - Epoch: [6][   45/   45]    Loss 1.331132    Top1 54.289733    
2023-06-16 00:07:42,446 - ==> Top1: 54.290    Loss: 1.331

2023-06-16 00:07:42,448 - ==> Best [Top1: 54.290   Sparsity:0.00   Params: 375264 on epoch: 6]
2023-06-16 00:07:42,449 - Saving checkpoint to: logs/2023.06.15-235432/checkpoint.pth.tar
2023-06-16 00:07:42,473 - 

2023-06-16 00:07:42,473 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:07:50,720 - Epoch: [7][   10/  142]    Overall Loss 1.290989    Objective Loss 1.290989                                        LR 0.001000    Time 0.824616    
2023-06-16 00:07:55,128 - Epoch: [7][   20/  142]    Overall Loss 1.305481    Objective Loss 1.305481                                        LR 0.001000    Time 0.632662    
2023-06-16 00:07:59,860 - Epoch: [7][   30/  142]    Overall Loss 1.304550    Objective Loss 1.304550                                        LR 0.001000    Time 0.579449    
2023-06-16 00:08:04,455 - Epoch: [7][   40/  142]    Overall Loss 1.318501    Objective Loss 1.318501                                        LR 0.001000    Time 0.549440    
2023-06-16 00:08:09,115 - Epoch: [7][   50/  142]    Overall Loss 1.310773    Objective Loss 1.310773                                        LR 0.001000    Time 0.532725    
2023-06-16 00:08:13,601 - Epoch: [7][   60/  142]    Overall Loss 1.310214    Objective Loss 1.310214                                        LR 0.001000    Time 0.518693    
2023-06-16 00:08:17,984 - Epoch: [7][   70/  142]    Overall Loss 1.307377    Objective Loss 1.307377                                        LR 0.001000    Time 0.507191    
2023-06-16 00:08:22,505 - Epoch: [7][   80/  142]    Overall Loss 1.302628    Objective Loss 1.302628                                        LR 0.001000    Time 0.500289    
2023-06-16 00:08:27,038 - Epoch: [7][   90/  142]    Overall Loss 1.303424    Objective Loss 1.303424                                        LR 0.001000    Time 0.495056    
2023-06-16 00:08:31,678 - Epoch: [7][  100/  142]    Overall Loss 1.300924    Objective Loss 1.300924                                        LR 0.001000    Time 0.491947    
2023-06-16 00:08:36,359 - Epoch: [7][  110/  142]    Overall Loss 1.306185    Objective Loss 1.306185                                        LR 0.001000    Time 0.489770    
2023-06-16 00:08:41,418 - Epoch: [7][  120/  142]    Overall Loss 1.306707    Objective Loss 1.306707                                        LR 0.001000    Time 0.491102    
2023-06-16 00:08:46,164 - Epoch: [7][  130/  142]    Overall Loss 1.306298    Objective Loss 1.306298                                        LR 0.001000    Time 0.489826    
2023-06-16 00:08:50,771 - Epoch: [7][  140/  142]    Overall Loss 1.305602    Objective Loss 1.305602                                        LR 0.001000    Time 0.487742    
2023-06-16 00:08:51,488 - Epoch: [7][  142/  142]    Overall Loss 1.305132    Objective Loss 1.305132    Top1 54.687500    LR 0.001000    Time 0.485926    
2023-06-16 00:08:52,061 - --- validate (epoch=7)-----------
2023-06-16 00:08:52,061 - 1422 samples (32 per mini-batch)
2023-06-16 00:09:00,008 - Epoch: [7][   10/   45]    Loss 1.369399    Top1 46.562500    
2023-06-16 00:09:03,940 - Epoch: [7][   20/   45]    Loss 1.382165    Top1 46.250000    
2023-06-16 00:09:09,024 - Epoch: [7][   30/   45]    Loss 1.374596    Top1 46.875000    
2023-06-16 00:09:13,280 - Epoch: [7][   40/   45]    Loss 1.362714    Top1 47.656250    
2023-06-16 00:09:14,532 - Epoch: [7][   45/   45]    Loss 1.353842    Top1 48.312236    
2023-06-16 00:09:15,157 - ==> Top1: 48.312    Loss: 1.354

2023-06-16 00:09:15,160 - ==> Best [Top1: 54.290   Sparsity:0.00   Params: 375264 on epoch: 6]
2023-06-16 00:09:15,160 - Saving checkpoint to: logs/2023.06.15-235432/checkpoint.pth.tar
2023-06-16 00:09:15,183 - 

2023-06-16 00:09:15,183 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:09:23,537 - Epoch: [8][   10/  142]    Overall Loss 1.306296    Objective Loss 1.306296                                        LR 0.001000    Time 0.835268    
2023-06-16 00:09:28,064 - Epoch: [8][   20/  142]    Overall Loss 1.331324    Objective Loss 1.331324                                        LR 0.001000    Time 0.643938    
2023-06-16 00:09:32,973 - Epoch: [8][   30/  142]    Overall Loss 1.299202    Objective Loss 1.299202                                        LR 0.001000    Time 0.592859    
2023-06-16 00:09:37,500 - Epoch: [8][   40/  142]    Overall Loss 1.288752    Objective Loss 1.288752                                        LR 0.001000    Time 0.557797    
2023-06-16 00:09:42,010 - Epoch: [8][   50/  142]    Overall Loss 1.287718    Objective Loss 1.287718                                        LR 0.001000    Time 0.536417    
2023-06-16 00:09:46,854 - Epoch: [8][   60/  142]    Overall Loss 1.279225    Objective Loss 1.279225                                        LR 0.001000    Time 0.527731    
2023-06-16 00:09:50,993 - Epoch: [8][   70/  142]    Overall Loss 1.280582    Objective Loss 1.280582                                        LR 0.001000    Time 0.511458    
2023-06-16 00:09:55,883 - Epoch: [8][   80/  142]    Overall Loss 1.273864    Objective Loss 1.273864                                        LR 0.001000    Time 0.508644    
2023-06-16 00:10:00,340 - Epoch: [8][   90/  142]    Overall Loss 1.271175    Objective Loss 1.271175                                        LR 0.001000    Time 0.501631    
2023-06-16 00:10:05,078 - Epoch: [8][  100/  142]    Overall Loss 1.272531    Objective Loss 1.272531                                        LR 0.001000    Time 0.498842    
2023-06-16 00:10:09,658 - Epoch: [8][  110/  142]    Overall Loss 1.271904    Objective Loss 1.271904                                        LR 0.001000    Time 0.495116    
2023-06-16 00:10:14,215 - Epoch: [8][  120/  142]    Overall Loss 1.274237    Objective Loss 1.274237                                        LR 0.001000    Time 0.491819    
2023-06-16 00:10:19,057 - Epoch: [8][  130/  142]    Overall Loss 1.272978    Objective Loss 1.272978                                        LR 0.001000    Time 0.491228    
2023-06-16 00:10:23,377 - Epoch: [8][  140/  142]    Overall Loss 1.274364    Objective Loss 1.274364                                        LR 0.001000    Time 0.486987    
2023-06-16 00:10:24,098 - Epoch: [8][  142/  142]    Overall Loss 1.273893    Objective Loss 1.273893    Top1 60.937500    LR 0.001000    Time 0.485209    
2023-06-16 00:10:24,739 - --- validate (epoch=8)-----------
2023-06-16 00:10:24,739 - 1422 samples (32 per mini-batch)
2023-06-16 00:10:32,336 - Epoch: [8][   10/   45]    Loss 1.214653    Top1 63.437500    
2023-06-16 00:10:36,285 - Epoch: [8][   20/   45]    Loss 1.221008    Top1 60.468750    
2023-06-16 00:10:40,958 - Epoch: [8][   30/   45]    Loss 1.220331    Top1 60.833333    
2023-06-16 00:10:45,052 - Epoch: [8][   40/   45]    Loss 1.223210    Top1 59.765625    
2023-06-16 00:10:46,521 - Epoch: [8][   45/   45]    Loss 1.219187    Top1 59.563994    
2023-06-16 00:10:47,109 - ==> Top1: 59.564    Loss: 1.219

2023-06-16 00:10:47,111 - ==> Best [Top1: 59.564   Sparsity:0.00   Params: 375264 on epoch: 8]
2023-06-16 00:10:47,111 - Saving checkpoint to: logs/2023.06.15-235432/checkpoint.pth.tar
2023-06-16 00:10:47,139 - 

2023-06-16 00:10:47,139 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:10:55,741 - Epoch: [9][   10/  142]    Overall Loss 1.251908    Objective Loss 1.251908                                        LR 0.001000    Time 0.860026    
2023-06-16 00:11:00,256 - Epoch: [9][   20/  142]    Overall Loss 1.266103    Objective Loss 1.266103                                        LR 0.001000    Time 0.655698    
2023-06-16 00:11:04,707 - Epoch: [9][   30/  142]    Overall Loss 1.275356    Objective Loss 1.275356                                        LR 0.001000    Time 0.585461    
2023-06-16 00:11:09,211 - Epoch: [9][   40/  142]    Overall Loss 1.270587    Objective Loss 1.270587                                        LR 0.001000    Time 0.551690    
2023-06-16 00:11:13,584 - Epoch: [9][   50/  142]    Overall Loss 1.273049    Objective Loss 1.273049                                        LR 0.001000    Time 0.528792    
2023-06-16 00:11:17,934 - Epoch: [9][   60/  142]    Overall Loss 1.269445    Objective Loss 1.269445                                        LR 0.001000    Time 0.513144    
2023-06-16 00:11:22,857 - Epoch: [9][   70/  142]    Overall Loss 1.262216    Objective Loss 1.262216                                        LR 0.001000    Time 0.510138    
2023-06-16 00:11:27,764 - Epoch: [9][   80/  142]    Overall Loss 1.257347    Objective Loss 1.257347                                        LR 0.001000    Time 0.507701    
2023-06-16 00:11:32,384 - Epoch: [9][   90/  142]    Overall Loss 1.256972    Objective Loss 1.256972                                        LR 0.001000    Time 0.502613    
2023-06-16 00:11:37,025 - Epoch: [9][  100/  142]    Overall Loss 1.254666    Objective Loss 1.254666                                        LR 0.001000    Time 0.498745    
2023-06-16 00:11:41,471 - Epoch: [9][  110/  142]    Overall Loss 1.254459    Objective Loss 1.254459                                        LR 0.001000    Time 0.493819    
2023-06-16 00:11:46,331 - Epoch: [9][  120/  142]    Overall Loss 1.253351    Objective Loss 1.253351                                        LR 0.001000    Time 0.493157    
2023-06-16 00:11:50,682 - Epoch: [9][  130/  142]    Overall Loss 1.255398    Objective Loss 1.255398                                        LR 0.001000    Time 0.488679    
2023-06-16 00:11:54,867 - Epoch: [9][  140/  142]    Overall Loss 1.256580    Objective Loss 1.256580                                        LR 0.001000    Time 0.483663    
2023-06-16 00:11:55,583 - Epoch: [9][  142/  142]    Overall Loss 1.256310    Objective Loss 1.256310    Top1 51.562500    LR 0.001000    Time 0.481894    
2023-06-16 00:11:56,178 - --- validate (epoch=9)-----------
2023-06-16 00:11:56,178 - 1422 samples (32 per mini-batch)
2023-06-16 00:12:04,083 - Epoch: [9][   10/   45]    Loss 1.207878    Top1 59.062500    
2023-06-16 00:12:08,081 - Epoch: [9][   20/   45]    Loss 1.253664    Top1 54.531250    
2023-06-16 00:12:12,594 - Epoch: [9][   30/   45]    Loss 1.223818    Top1 56.041667    
2023-06-16 00:12:16,964 - Epoch: [9][   40/   45]    Loss 1.214593    Top1 57.734375    
2023-06-16 00:12:18,114 - Epoch: [9][   45/   45]    Loss 1.224580    Top1 57.665260    
2023-06-16 00:12:18,719 - ==> Top1: 57.665    Loss: 1.225

2023-06-16 00:12:18,721 - ==> Best [Top1: 59.564   Sparsity:0.00   Params: 375264 on epoch: 8]
2023-06-16 00:12:18,721 - Saving checkpoint to: logs/2023.06.15-235432/checkpoint.pth.tar
2023-06-16 00:12:18,767 - 

2023-06-16 00:12:18,767 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:12:28,092 - Epoch: [10][   10/  142]    Overall Loss 1.340276    Objective Loss 1.340276                                        LR 0.000500    Time 0.932387    
2023-06-16 00:12:33,151 - Epoch: [10][   20/  142]    Overall Loss 1.340126    Objective Loss 1.340126                                        LR 0.000500    Time 0.719111    
2023-06-16 00:12:38,220 - Epoch: [10][   30/  142]    Overall Loss 1.301602    Objective Loss 1.301602                                        LR 0.000500    Time 0.648365    
2023-06-16 00:12:43,175 - Epoch: [10][   40/  142]    Overall Loss 1.287926    Objective Loss 1.287926                                        LR 0.000500    Time 0.610135    
2023-06-16 00:12:48,181 - Epoch: [10][   50/  142]    Overall Loss 1.296287    Objective Loss 1.296287                                        LR 0.000500    Time 0.588216    
2023-06-16 00:12:53,238 - Epoch: [10][   60/  142]    Overall Loss 1.286875    Objective Loss 1.286875                                        LR 0.000500    Time 0.574457    
2023-06-16 00:12:58,165 - Epoch: [10][   70/  142]    Overall Loss 1.282295    Objective Loss 1.282295                                        LR 0.000500    Time 0.562765    
2023-06-16 00:13:03,136 - Epoch: [10][   80/  142]    Overall Loss 1.274832    Objective Loss 1.274832                                        LR 0.000500    Time 0.554549    
2023-06-16 00:13:08,214 - Epoch: [10][   90/  142]    Overall Loss 1.259396    Objective Loss 1.259396                                        LR 0.000500    Time 0.549345    
2023-06-16 00:13:13,205 - Epoch: [10][  100/  142]    Overall Loss 1.258414    Objective Loss 1.258414                                        LR 0.000500    Time 0.544314    
2023-06-16 00:13:18,254 - Epoch: [10][  110/  142]    Overall Loss 1.255951    Objective Loss 1.255951                                        LR 0.000500    Time 0.540729    
2023-06-16 00:13:23,254 - Epoch: [10][  120/  142]    Overall Loss 1.247954    Objective Loss 1.247954                                        LR 0.000500    Time 0.537327    
2023-06-16 00:13:28,261 - Epoch: [10][  130/  142]    Overall Loss 1.242480    Objective Loss 1.242480                                        LR 0.000500    Time 0.534506    
2023-06-16 00:13:32,902 - Epoch: [10][  140/  142]    Overall Loss 1.236508    Objective Loss 1.236508                                        LR 0.000500    Time 0.529473    
2023-06-16 00:13:33,743 - Epoch: [10][  142/  142]    Overall Loss 1.234828    Objective Loss 1.234828    Top1 65.625000    LR 0.000500    Time 0.527940    
2023-06-16 00:13:34,398 - --- validate (epoch=10)-----------
2023-06-16 00:13:34,399 - 1422 samples (32 per mini-batch)
2023-06-16 00:13:42,170 - Epoch: [10][   10/   45]    Loss 1.249961    Top1 56.875000    
2023-06-16 00:13:47,183 - Epoch: [10][   20/   45]    Loss 1.253462    Top1 56.562500    
2023-06-16 00:13:51,574 - Epoch: [10][   30/   45]    Loss 1.251464    Top1 56.666667    
2023-06-16 00:13:56,344 - Epoch: [10][   40/   45]    Loss 1.285280    Top1 56.250000    
2023-06-16 00:13:57,668 - Epoch: [10][   45/   45]    Loss 1.282672    Top1 55.766526    
2023-06-16 00:13:58,317 - ==> Top1: 55.767    Loss: 1.283

2023-06-16 00:13:58,319 - ==> Best [Top1: 55.767   Sparsity:0.00   Params: 375264 on epoch: 10]
2023-06-16 00:13:58,319 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:13:58,468 - 

2023-06-16 00:13:58,468 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:14:07,828 - Epoch: [11][   10/  142]    Overall Loss 1.190439    Objective Loss 1.190439                                        LR 0.000500    Time 0.935816    
2023-06-16 00:14:12,857 - Epoch: [11][   20/  142]    Overall Loss 1.222685    Objective Loss 1.222685                                        LR 0.000500    Time 0.719354    
2023-06-16 00:14:17,954 - Epoch: [11][   30/  142]    Overall Loss 1.224688    Objective Loss 1.224688                                        LR 0.000500    Time 0.649441    
2023-06-16 00:14:22,858 - Epoch: [11][   40/  142]    Overall Loss 1.229738    Objective Loss 1.229738                                        LR 0.000500    Time 0.609653    
2023-06-16 00:14:27,854 - Epoch: [11][   50/  142]    Overall Loss 1.222604    Objective Loss 1.222604                                        LR 0.000500    Time 0.587634    
2023-06-16 00:14:32,860 - Epoch: [11][   60/  142]    Overall Loss 1.235396    Objective Loss 1.235396                                        LR 0.000500    Time 0.573130    
2023-06-16 00:14:37,779 - Epoch: [11][   70/  142]    Overall Loss 1.238398    Objective Loss 1.238398                                        LR 0.000500    Time 0.561514    
2023-06-16 00:14:42,730 - Epoch: [11][   80/  142]    Overall Loss 1.229487    Objective Loss 1.229487                                        LR 0.000500    Time 0.553205    
2023-06-16 00:14:47,655 - Epoch: [11][   90/  142]    Overall Loss 1.222612    Objective Loss 1.222612                                        LR 0.000500    Time 0.546453    
2023-06-16 00:14:52,733 - Epoch: [11][  100/  142]    Overall Loss 1.213869    Objective Loss 1.213869                                        LR 0.000500    Time 0.542580    
2023-06-16 00:14:57,671 - Epoch: [11][  110/  142]    Overall Loss 1.203973    Objective Loss 1.203973                                        LR 0.000500    Time 0.538142    
2023-06-16 00:15:02,700 - Epoch: [11][  120/  142]    Overall Loss 1.188893    Objective Loss 1.188893                                        LR 0.000500    Time 0.535198    
2023-06-16 00:15:07,714 - Epoch: [11][  130/  142]    Overall Loss 1.180983    Objective Loss 1.180983                                        LR 0.000500    Time 0.532589    
2023-06-16 00:15:12,479 - Epoch: [11][  140/  142]    Overall Loss 1.180343    Objective Loss 1.180343                                        LR 0.000500    Time 0.528577    
2023-06-16 00:15:13,336 - Epoch: [11][  142/  142]    Overall Loss 1.178694    Objective Loss 1.178694    Top1 68.750000    LR 0.000500    Time 0.527171    
2023-06-16 00:15:13,979 - --- validate (epoch=11)-----------
2023-06-16 00:15:13,980 - 1422 samples (32 per mini-batch)
2023-06-16 00:15:22,238 - Epoch: [11][   10/   45]    Loss 1.203019    Top1 55.312500    
2023-06-16 00:15:27,077 - Epoch: [11][   20/   45]    Loss 1.168449    Top1 57.343750    
2023-06-16 00:15:31,239 - Epoch: [11][   30/   45]    Loss 1.151196    Top1 57.604167    
2023-06-16 00:15:35,492 - Epoch: [11][   40/   45]    Loss 1.145453    Top1 57.968750    
2023-06-16 00:15:37,029 - Epoch: [11][   45/   45]    Loss 1.154688    Top1 57.665260    
2023-06-16 00:15:37,656 - ==> Top1: 57.665    Loss: 1.155

2023-06-16 00:15:37,658 - ==> Best [Top1: 57.665   Sparsity:0.00   Params: 375264 on epoch: 11]
2023-06-16 00:15:37,658 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:15:37,683 - 

2023-06-16 00:15:37,683 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:15:46,988 - Epoch: [12][   10/  142]    Overall Loss 1.159337    Objective Loss 1.159337                                        LR 0.000500    Time 0.930292    
2023-06-16 00:15:52,008 - Epoch: [12][   20/  142]    Overall Loss 1.172230    Objective Loss 1.172230                                        LR 0.000500    Time 0.716098    
2023-06-16 00:15:56,953 - Epoch: [12][   30/  142]    Overall Loss 1.164851    Objective Loss 1.164851                                        LR 0.000500    Time 0.642225    
2023-06-16 00:16:02,020 - Epoch: [12][   40/  142]    Overall Loss 1.125807    Objective Loss 1.125807                                        LR 0.000500    Time 0.608330    
2023-06-16 00:16:07,151 - Epoch: [12][   50/  142]    Overall Loss 1.109732    Objective Loss 1.109732                                        LR 0.000500    Time 0.589266    
2023-06-16 00:16:12,154 - Epoch: [12][   60/  142]    Overall Loss 1.096692    Objective Loss 1.096692                                        LR 0.000500    Time 0.574433    
2023-06-16 00:16:17,216 - Epoch: [12][   70/  142]    Overall Loss 1.093119    Objective Loss 1.093119                                        LR 0.000500    Time 0.564670    
2023-06-16 00:16:22,378 - Epoch: [12][   80/  142]    Overall Loss 1.090823    Objective Loss 1.090823                                        LR 0.000500    Time 0.558606    
2023-06-16 00:16:27,320 - Epoch: [12][   90/  142]    Overall Loss 1.099253    Objective Loss 1.099253                                        LR 0.000500    Time 0.551450    
2023-06-16 00:16:32,340 - Epoch: [12][  100/  142]    Overall Loss 1.093169    Objective Loss 1.093169                                        LR 0.000500    Time 0.546497    
2023-06-16 00:16:37,421 - Epoch: [12][  110/  142]    Overall Loss 1.097203    Objective Loss 1.097203                                        LR 0.000500    Time 0.543003    
2023-06-16 00:16:42,496 - Epoch: [12][  120/  142]    Overall Loss 1.096649    Objective Loss 1.096649                                        LR 0.000500    Time 0.540036    
2023-06-16 00:16:47,426 - Epoch: [12][  130/  142]    Overall Loss 1.101341    Objective Loss 1.101341                                        LR 0.000500    Time 0.536416    
2023-06-16 00:16:52,095 - Epoch: [12][  140/  142]    Overall Loss 1.105133    Objective Loss 1.105133                                        LR 0.000500    Time 0.531444    
2023-06-16 00:16:52,936 - Epoch: [12][  142/  142]    Overall Loss 1.105158    Objective Loss 1.105158    Top1 57.812500    LR 0.000500    Time 0.529879    
2023-06-16 00:16:53,592 - --- validate (epoch=12)-----------
2023-06-16 00:16:53,593 - 1422 samples (32 per mini-batch)
2023-06-16 00:17:01,572 - Epoch: [12][   10/   45]    Loss 1.145638    Top1 59.062500    
2023-06-16 00:17:05,662 - Epoch: [12][   20/   45]    Loss 1.079497    Top1 61.406250    
2023-06-16 00:17:10,202 - Epoch: [12][   30/   45]    Loss 1.064200    Top1 61.145833    
2023-06-16 00:17:14,484 - Epoch: [12][   40/   45]    Loss 1.048594    Top1 61.796875    
2023-06-16 00:17:16,084 - Epoch: [12][   45/   45]    Loss 1.045978    Top1 62.447257    
2023-06-16 00:17:16,698 - ==> Top1: 62.447    Loss: 1.046

2023-06-16 00:17:16,700 - ==> Best [Top1: 62.447   Sparsity:0.00   Params: 375264 on epoch: 12]
2023-06-16 00:17:16,700 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:17:16,725 - 

2023-06-16 00:17:16,725 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:17:26,157 - Epoch: [13][   10/  142]    Overall Loss 1.070916    Objective Loss 1.070916                                        LR 0.000500    Time 0.943057    
2023-06-16 00:17:31,031 - Epoch: [13][   20/  142]    Overall Loss 1.100568    Objective Loss 1.100568                                        LR 0.000500    Time 0.715218    
2023-06-16 00:17:36,011 - Epoch: [13][   30/  142]    Overall Loss 1.103673    Objective Loss 1.103673                                        LR 0.000500    Time 0.642782    
2023-06-16 00:17:41,001 - Epoch: [13][   40/  142]    Overall Loss 1.093270    Objective Loss 1.093270                                        LR 0.000500    Time 0.606807    
2023-06-16 00:17:46,211 - Epoch: [13][   50/  142]    Overall Loss 1.091241    Objective Loss 1.091241                                        LR 0.000500    Time 0.589634    
2023-06-16 00:17:51,107 - Epoch: [13][   60/  142]    Overall Loss 1.093238    Objective Loss 1.093238                                        LR 0.000500    Time 0.572959    
2023-06-16 00:17:56,108 - Epoch: [13][   70/  142]    Overall Loss 1.108134    Objective Loss 1.108134                                        LR 0.000500    Time 0.561775    
2023-06-16 00:18:00,930 - Epoch: [13][   80/  142]    Overall Loss 1.097426    Objective Loss 1.097426                                        LR 0.000500    Time 0.551822    
2023-06-16 00:18:05,881 - Epoch: [13][   90/  142]    Overall Loss 1.088553    Objective Loss 1.088553                                        LR 0.000500    Time 0.545510    
2023-06-16 00:18:10,883 - Epoch: [13][  100/  142]    Overall Loss 1.098373    Objective Loss 1.098373                                        LR 0.000500    Time 0.540976    
2023-06-16 00:18:15,782 - Epoch: [13][  110/  142]    Overall Loss 1.096347    Objective Loss 1.096347                                        LR 0.000500    Time 0.536324    
2023-06-16 00:18:20,960 - Epoch: [13][  120/  142]    Overall Loss 1.092461    Objective Loss 1.092461                                        LR 0.000500    Time 0.534778    
2023-06-16 00:18:25,779 - Epoch: [13][  130/  142]    Overall Loss 1.095976    Objective Loss 1.095976                                        LR 0.000500    Time 0.530710    
2023-06-16 00:18:30,398 - Epoch: [13][  140/  142]    Overall Loss 1.098299    Objective Loss 1.098299                                        LR 0.000500    Time 0.525789    
2023-06-16 00:18:31,242 - Epoch: [13][  142/  142]    Overall Loss 1.098455    Objective Loss 1.098455    Top1 62.500000    LR 0.000500    Time 0.524327    
2023-06-16 00:18:31,885 - --- validate (epoch=13)-----------
2023-06-16 00:18:31,886 - 1422 samples (32 per mini-batch)
2023-06-16 00:18:40,010 - Epoch: [13][   10/   45]    Loss 1.104617    Top1 59.062500    
2023-06-16 00:18:44,101 - Epoch: [13][   20/   45]    Loss 1.127036    Top1 58.281250    
2023-06-16 00:18:48,511 - Epoch: [13][   30/   45]    Loss 1.120372    Top1 58.437500    
2023-06-16 00:18:53,195 - Epoch: [13][   40/   45]    Loss 1.093109    Top1 58.984375    
2023-06-16 00:18:54,665 - Epoch: [13][   45/   45]    Loss 1.091031    Top1 58.720113    
2023-06-16 00:18:55,258 - ==> Top1: 58.720    Loss: 1.091

2023-06-16 00:18:55,260 - ==> Best [Top1: 62.447   Sparsity:0.00   Params: 375264 on epoch: 12]
2023-06-16 00:18:55,260 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:18:55,274 - 

2023-06-16 00:18:55,274 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:19:04,686 - Epoch: [14][   10/  142]    Overall Loss 1.094706    Objective Loss 1.094706                                        LR 0.000500    Time 0.941101    
2023-06-16 00:19:09,672 - Epoch: [14][   20/  142]    Overall Loss 1.063898    Objective Loss 1.063898                                        LR 0.000500    Time 0.719805    
2023-06-16 00:19:14,748 - Epoch: [14][   30/  142]    Overall Loss 1.110749    Objective Loss 1.110749                                        LR 0.000500    Time 0.649061    
2023-06-16 00:19:19,712 - Epoch: [14][   40/  142]    Overall Loss 1.123196    Objective Loss 1.123196                                        LR 0.000500    Time 0.610881    
2023-06-16 00:19:24,751 - Epoch: [14][   50/  142]    Overall Loss 1.125567    Objective Loss 1.125567                                        LR 0.000500    Time 0.589473    
2023-06-16 00:19:29,724 - Epoch: [14][   60/  142]    Overall Loss 1.098842    Objective Loss 1.098842                                        LR 0.000500    Time 0.574105    
2023-06-16 00:19:34,800 - Epoch: [14][   70/  142]    Overall Loss 1.111832    Objective Loss 1.111832                                        LR 0.000500    Time 0.564597    
2023-06-16 00:19:39,819 - Epoch: [14][   80/  142]    Overall Loss 1.103113    Objective Loss 1.103113                                        LR 0.000500    Time 0.556743    
2023-06-16 00:19:44,794 - Epoch: [14][   90/  142]    Overall Loss 1.093996    Objective Loss 1.093996                                        LR 0.000500    Time 0.550155    
2023-06-16 00:19:49,908 - Epoch: [14][  100/  142]    Overall Loss 1.100681    Objective Loss 1.100681                                        LR 0.000500    Time 0.546272    
2023-06-16 00:19:54,820 - Epoch: [14][  110/  142]    Overall Loss 1.095643    Objective Loss 1.095643                                        LR 0.000500    Time 0.541265    
2023-06-16 00:19:59,926 - Epoch: [14][  120/  142]    Overall Loss 1.093201    Objective Loss 1.093201                                        LR 0.000500    Time 0.538701    
2023-06-16 00:20:04,870 - Epoch: [14][  130/  142]    Overall Loss 1.093118    Objective Loss 1.093118                                        LR 0.000500    Time 0.535290    
2023-06-16 00:20:09,455 - Epoch: [14][  140/  142]    Overall Loss 1.086075    Objective Loss 1.086075                                        LR 0.000500    Time 0.529800    
2023-06-16 00:20:10,308 - Epoch: [14][  142/  142]    Overall Loss 1.087791    Objective Loss 1.087791    Top1 57.812500    LR 0.000500    Time 0.528341    
2023-06-16 00:20:10,912 - --- validate (epoch=14)-----------
2023-06-16 00:20:10,913 - 1422 samples (32 per mini-batch)
2023-06-16 00:20:18,940 - Epoch: [14][   10/   45]    Loss 1.163612    Top1 61.562500    
2023-06-16 00:20:23,489 - Epoch: [14][   20/   45]    Loss 1.176083    Top1 61.250000    
2023-06-16 00:20:27,885 - Epoch: [14][   30/   45]    Loss 1.186588    Top1 60.937500    
2023-06-16 00:20:32,499 - Epoch: [14][   40/   45]    Loss 1.164041    Top1 61.015625    
2023-06-16 00:20:33,838 - Epoch: [14][   45/   45]    Loss 1.154050    Top1 61.181435    
2023-06-16 00:20:34,443 - ==> Top1: 61.181    Loss: 1.154

2023-06-16 00:20:34,445 - ==> Best [Top1: 62.447   Sparsity:0.00   Params: 375264 on epoch: 12]
2023-06-16 00:20:34,445 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:20:34,466 - 

2023-06-16 00:20:34,466 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:20:43,655 - Epoch: [15][   10/  142]    Overall Loss 0.942102    Objective Loss 0.942102                                        LR 0.000500    Time 0.918783    
2023-06-16 00:20:48,883 - Epoch: [15][   20/  142]    Overall Loss 0.983242    Objective Loss 0.983242                                        LR 0.000500    Time 0.720744    
2023-06-16 00:20:53,917 - Epoch: [15][   30/  142]    Overall Loss 1.007309    Objective Loss 1.007309                                        LR 0.000500    Time 0.648279    
2023-06-16 00:20:59,001 - Epoch: [15][   40/  142]    Overall Loss 1.016236    Objective Loss 1.016236                                        LR 0.000500    Time 0.613293    
2023-06-16 00:21:03,924 - Epoch: [15][   50/  142]    Overall Loss 1.013042    Objective Loss 1.013042                                        LR 0.000500    Time 0.589089    
2023-06-16 00:21:09,025 - Epoch: [15][   60/  142]    Overall Loss 1.003360    Objective Loss 1.003360                                        LR 0.000500    Time 0.575915    
2023-06-16 00:21:14,116 - Epoch: [15][   70/  142]    Overall Loss 1.000943    Objective Loss 1.000943                                        LR 0.000500    Time 0.566357    
2023-06-16 00:21:19,259 - Epoch: [15][   80/  142]    Overall Loss 0.994772    Objective Loss 0.994772                                        LR 0.000500    Time 0.559839    
2023-06-16 00:21:24,203 - Epoch: [15][   90/  142]    Overall Loss 1.005690    Objective Loss 1.005690                                        LR 0.000500    Time 0.552571    
2023-06-16 00:21:29,178 - Epoch: [15][  100/  142]    Overall Loss 1.008281    Objective Loss 1.008281                                        LR 0.000500    Time 0.547051    
2023-06-16 00:21:34,321 - Epoch: [15][  110/  142]    Overall Loss 1.006555    Objective Loss 1.006555                                        LR 0.000500    Time 0.544069    
2023-06-16 00:21:39,364 - Epoch: [15][  120/  142]    Overall Loss 1.018400    Objective Loss 1.018400                                        LR 0.000500    Time 0.540755    
2023-06-16 00:21:44,407 - Epoch: [15][  130/  142]    Overall Loss 1.020136    Objective Loss 1.020136                                        LR 0.000500    Time 0.537942    
2023-06-16 00:21:49,105 - Epoch: [15][  140/  142]    Overall Loss 1.017327    Objective Loss 1.017327                                        LR 0.000500    Time 0.533072    
2023-06-16 00:21:49,947 - Epoch: [15][  142/  142]    Overall Loss 1.019411    Objective Loss 1.019411    Top1 60.937500    LR 0.000500    Time 0.531497    
2023-06-16 00:21:50,582 - --- validate (epoch=15)-----------
2023-06-16 00:21:50,583 - 1422 samples (32 per mini-batch)
2023-06-16 00:21:58,486 - Epoch: [15][   10/   45]    Loss 1.062822    Top1 65.625000    
2023-06-16 00:22:02,581 - Epoch: [15][   20/   45]    Loss 1.044465    Top1 65.781250    
2023-06-16 00:22:07,743 - Epoch: [15][   30/   45]    Loss 1.048002    Top1 65.104167    
2023-06-16 00:22:11,423 - Epoch: [15][   40/   45]    Loss 1.037495    Top1 66.171875    
2023-06-16 00:22:12,892 - Epoch: [15][   45/   45]    Loss 1.043088    Top1 65.893108    
2023-06-16 00:22:13,509 - ==> Top1: 65.893    Loss: 1.043

2023-06-16 00:22:13,511 - ==> Best [Top1: 65.893   Sparsity:0.00   Params: 375264 on epoch: 15]
2023-06-16 00:22:13,511 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:22:13,533 - 

2023-06-16 00:22:13,533 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:22:22,987 - Epoch: [16][   10/  142]    Overall Loss 1.059715    Objective Loss 1.059715                                        LR 0.000500    Time 0.945331    
2023-06-16 00:22:27,876 - Epoch: [16][   20/  142]    Overall Loss 1.070877    Objective Loss 1.070877                                        LR 0.000500    Time 0.717085    
2023-06-16 00:22:32,934 - Epoch: [16][   30/  142]    Overall Loss 1.037525    Objective Loss 1.037525                                        LR 0.000500    Time 0.646625    
2023-06-16 00:22:37,991 - Epoch: [16][   40/  142]    Overall Loss 1.054925    Objective Loss 1.054925                                        LR 0.000500    Time 0.611369    
2023-06-16 00:22:42,936 - Epoch: [16][   50/  142]    Overall Loss 1.059849    Objective Loss 1.059849                                        LR 0.000500    Time 0.587995    
2023-06-16 00:22:47,839 - Epoch: [16][   60/  142]    Overall Loss 1.052773    Objective Loss 1.052773                                        LR 0.000500    Time 0.571705    
2023-06-16 00:22:52,868 - Epoch: [16][   70/  142]    Overall Loss 1.037625    Objective Loss 1.037625                                        LR 0.000500    Time 0.561859    
2023-06-16 00:22:57,863 - Epoch: [16][   80/  142]    Overall Loss 1.022433    Objective Loss 1.022433                                        LR 0.000500    Time 0.554056    
2023-06-16 00:23:02,986 - Epoch: [16][   90/  142]    Overall Loss 1.014826    Objective Loss 1.014826                                        LR 0.000500    Time 0.549416    
2023-06-16 00:23:07,912 - Epoch: [16][  100/  142]    Overall Loss 1.016144    Objective Loss 1.016144                                        LR 0.000500    Time 0.543720    
2023-06-16 00:23:12,810 - Epoch: [16][  110/  142]    Overall Loss 1.009547    Objective Loss 1.009547                                        LR 0.000500    Time 0.538819    
2023-06-16 00:23:17,921 - Epoch: [16][  120/  142]    Overall Loss 1.017184    Objective Loss 1.017184                                        LR 0.000500    Time 0.536503    
2023-06-16 00:23:22,806 - Epoch: [16][  130/  142]    Overall Loss 1.017731    Objective Loss 1.017731                                        LR 0.000500    Time 0.532807    
2023-06-16 00:23:27,480 - Epoch: [16][  140/  142]    Overall Loss 1.016558    Objective Loss 1.016558                                        LR 0.000500    Time 0.528127    
2023-06-16 00:23:28,322 - Epoch: [16][  142/  142]    Overall Loss 1.018941    Objective Loss 1.018941    Top1 70.312500    LR 0.000500    Time 0.526618    
2023-06-16 00:23:28,946 - --- validate (epoch=16)-----------
2023-06-16 00:23:28,947 - 1422 samples (32 per mini-batch)
2023-06-16 00:23:37,164 - Epoch: [16][   10/   45]    Loss 1.176363    Top1 61.562500    
2023-06-16 00:23:41,232 - Epoch: [16][   20/   45]    Loss 1.134424    Top1 61.562500    
2023-06-16 00:23:45,856 - Epoch: [16][   30/   45]    Loss 1.152633    Top1 61.041667    
2023-06-16 00:23:50,167 - Epoch: [16][   40/   45]    Loss 1.143934    Top1 62.109375    
2023-06-16 00:23:51,618 - Epoch: [16][   45/   45]    Loss 1.141854    Top1 62.095640    
2023-06-16 00:23:52,220 - ==> Top1: 62.096    Loss: 1.142

2023-06-16 00:23:52,223 - ==> Best [Top1: 65.893   Sparsity:0.00   Params: 375264 on epoch: 15]
2023-06-16 00:23:52,223 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:23:52,244 - 

2023-06-16 00:23:52,244 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:24:01,574 - Epoch: [17][   10/  142]    Overall Loss 0.969894    Objective Loss 0.969894                                        LR 0.000500    Time 0.932883    
2023-06-16 00:24:06,687 - Epoch: [17][   20/  142]    Overall Loss 0.981447    Objective Loss 0.981447                                        LR 0.000500    Time 0.722067    
2023-06-16 00:24:11,678 - Epoch: [17][   30/  142]    Overall Loss 0.975768    Objective Loss 0.975768                                        LR 0.000500    Time 0.647729    
2023-06-16 00:24:16,591 - Epoch: [17][   40/  142]    Overall Loss 0.993045    Objective Loss 0.993045                                        LR 0.000500    Time 0.608597    
2023-06-16 00:24:21,740 - Epoch: [17][   50/  142]    Overall Loss 1.020435    Objective Loss 1.020435                                        LR 0.000500    Time 0.589850    
2023-06-16 00:24:26,689 - Epoch: [17][   60/  142]    Overall Loss 1.023130    Objective Loss 1.023130                                        LR 0.000500    Time 0.574013    
2023-06-16 00:24:31,720 - Epoch: [17][   70/  142]    Overall Loss 1.021082    Objective Loss 1.021082                                        LR 0.000500    Time 0.563876    
2023-06-16 00:24:36,568 - Epoch: [17][   80/  142]    Overall Loss 1.015400    Objective Loss 1.015400                                        LR 0.000500    Time 0.553988    
2023-06-16 00:24:41,639 - Epoch: [17][   90/  142]    Overall Loss 1.013585    Objective Loss 1.013585                                        LR 0.000500    Time 0.548762    
2023-06-16 00:24:46,579 - Epoch: [17][  100/  142]    Overall Loss 1.015720    Objective Loss 1.015720                                        LR 0.000500    Time 0.543286    
2023-06-16 00:24:51,564 - Epoch: [17][  110/  142]    Overall Loss 1.002529    Objective Loss 1.002529                                        LR 0.000500    Time 0.539211    
2023-06-16 00:24:56,599 - Epoch: [17][  120/  142]    Overall Loss 1.006949    Objective Loss 1.006949                                        LR 0.000500    Time 0.536230    
2023-06-16 00:25:01,609 - Epoch: [17][  130/  142]    Overall Loss 1.013988    Objective Loss 1.013988                                        LR 0.000500    Time 0.533516    
2023-06-16 00:25:06,207 - Epoch: [17][  140/  142]    Overall Loss 1.013162    Objective Loss 1.013162                                        LR 0.000500    Time 0.528244    
2023-06-16 00:25:07,051 - Epoch: [17][  142/  142]    Overall Loss 1.011330    Objective Loss 1.011330    Top1 73.437500    LR 0.000500    Time 0.526748    
2023-06-16 00:25:07,660 - --- validate (epoch=17)-----------
2023-06-16 00:25:07,660 - 1422 samples (32 per mini-batch)
2023-06-16 00:25:15,900 - Epoch: [17][   10/   45]    Loss 1.080999    Top1 60.312500    
2023-06-16 00:25:20,032 - Epoch: [17][   20/   45]    Loss 1.031462    Top1 62.656250    
2023-06-16 00:25:24,607 - Epoch: [17][   30/   45]    Loss 1.021440    Top1 64.166667    
2023-06-16 00:25:28,868 - Epoch: [17][   40/   45]    Loss 0.999982    Top1 65.000000    
2023-06-16 00:25:30,320 - Epoch: [17][   45/   45]    Loss 1.009375    Top1 64.767932    
2023-06-16 00:25:30,961 - ==> Top1: 64.768    Loss: 1.009

2023-06-16 00:25:30,963 - ==> Best [Top1: 65.893   Sparsity:0.00   Params: 375264 on epoch: 15]
2023-06-16 00:25:30,963 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:25:30,984 - 

2023-06-16 00:25:30,984 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:25:40,429 - Epoch: [18][   10/  142]    Overall Loss 1.086912    Objective Loss 1.086912                                        LR 0.000500    Time 0.944374    
2023-06-16 00:25:45,505 - Epoch: [18][   20/  142]    Overall Loss 1.071817    Objective Loss 1.071817                                        LR 0.000500    Time 0.725920    
2023-06-16 00:25:50,652 - Epoch: [18][   30/  142]    Overall Loss 1.049186    Objective Loss 1.049186                                        LR 0.000500    Time 0.655494    
2023-06-16 00:25:55,684 - Epoch: [18][   40/  142]    Overall Loss 1.055188    Objective Loss 1.055188                                        LR 0.000500    Time 0.617421    
2023-06-16 00:26:00,914 - Epoch: [18][   50/  142]    Overall Loss 1.034171    Objective Loss 1.034171                                        LR 0.000500    Time 0.598519    
2023-06-16 00:26:05,782 - Epoch: [18][   60/  142]    Overall Loss 1.038368    Objective Loss 1.038368                                        LR 0.000500    Time 0.579886    
2023-06-16 00:26:10,727 - Epoch: [18][   70/  142]    Overall Loss 1.015410    Objective Loss 1.015410                                        LR 0.000500    Time 0.567678    
2023-06-16 00:26:15,766 - Epoch: [18][   80/  142]    Overall Loss 0.996330    Objective Loss 0.996330                                        LR 0.000500    Time 0.559699    
2023-06-16 00:26:21,114 - Epoch: [18][   90/  142]    Overall Loss 1.002240    Objective Loss 1.002240                                        LR 0.000500    Time 0.556920    
2023-06-16 00:26:26,195 - Epoch: [18][  100/  142]    Overall Loss 0.998463    Objective Loss 0.998463                                        LR 0.000500    Time 0.552030    
2023-06-16 00:26:31,238 - Epoch: [18][  110/  142]    Overall Loss 1.000730    Objective Loss 1.000730                                        LR 0.000500    Time 0.547685    
2023-06-16 00:26:36,121 - Epoch: [18][  120/  142]    Overall Loss 1.000507    Objective Loss 1.000507                                        LR 0.000500    Time 0.542731    
2023-06-16 00:26:41,415 - Epoch: [18][  130/  142]    Overall Loss 0.989175    Objective Loss 0.989175                                        LR 0.000500    Time 0.541707    
2023-06-16 00:26:46,162 - Epoch: [18][  140/  142]    Overall Loss 0.991993    Objective Loss 0.991993                                        LR 0.000500    Time 0.536914    
2023-06-16 00:26:47,015 - Epoch: [18][  142/  142]    Overall Loss 0.990459    Objective Loss 0.990459    Top1 68.750000    LR 0.000500    Time 0.535359    
2023-06-16 00:26:47,666 - --- validate (epoch=18)-----------
2023-06-16 00:26:47,667 - 1422 samples (32 per mini-batch)
2023-06-16 00:26:55,584 - Epoch: [18][   10/   45]    Loss 1.037112    Top1 62.187500    
2023-06-16 00:26:59,817 - Epoch: [18][   20/   45]    Loss 1.040248    Top1 61.093750    
2023-06-16 00:27:04,797 - Epoch: [18][   30/   45]    Loss 1.046265    Top1 60.104167    
2023-06-16 00:27:09,075 - Epoch: [18][   40/   45]    Loss 1.053144    Top1 60.781250    
2023-06-16 00:27:10,424 - Epoch: [18][   45/   45]    Loss 1.054345    Top1 61.392405    
2023-06-16 00:27:11,020 - ==> Top1: 61.392    Loss: 1.054

2023-06-16 00:27:11,022 - ==> Best [Top1: 65.893   Sparsity:0.00   Params: 375264 on epoch: 15]
2023-06-16 00:27:11,022 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:27:11,042 - 

2023-06-16 00:27:11,043 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:27:20,443 - Epoch: [19][   10/  142]    Overall Loss 1.045248    Objective Loss 1.045248                                        LR 0.000500    Time 0.939879    
2023-06-16 00:27:25,404 - Epoch: [19][   20/  142]    Overall Loss 1.029141    Objective Loss 1.029141                                        LR 0.000500    Time 0.717985    
2023-06-16 00:27:30,492 - Epoch: [19][   30/  142]    Overall Loss 1.007899    Objective Loss 1.007899                                        LR 0.000500    Time 0.648234    
2023-06-16 00:27:35,539 - Epoch: [19][   40/  142]    Overall Loss 1.009056    Objective Loss 1.009056                                        LR 0.000500    Time 0.612333    
2023-06-16 00:27:40,664 - Epoch: [19][   50/  142]    Overall Loss 1.029462    Objective Loss 1.029462                                        LR 0.000500    Time 0.592362    
2023-06-16 00:27:45,607 - Epoch: [19][   60/  142]    Overall Loss 1.038102    Objective Loss 1.038102                                        LR 0.000500    Time 0.576006    
2023-06-16 00:27:50,751 - Epoch: [19][   70/  142]    Overall Loss 1.026684    Objective Loss 1.026684                                        LR 0.000500    Time 0.567204    
2023-06-16 00:27:55,727 - Epoch: [19][   80/  142]    Overall Loss 1.024117    Objective Loss 1.024117                                        LR 0.000500    Time 0.558492    
2023-06-16 00:28:00,731 - Epoch: [19][   90/  142]    Overall Loss 1.007437    Objective Loss 1.007437                                        LR 0.000500    Time 0.552027    
2023-06-16 00:28:05,765 - Epoch: [19][  100/  142]    Overall Loss 1.011474    Objective Loss 1.011474                                        LR 0.000500    Time 0.547154    
2023-06-16 00:28:10,932 - Epoch: [19][  110/  142]    Overall Loss 1.007248    Objective Loss 1.007248                                        LR 0.000500    Time 0.544387    
2023-06-16 00:28:15,884 - Epoch: [19][  120/  142]    Overall Loss 1.001971    Objective Loss 1.001971                                        LR 0.000500    Time 0.540284    
2023-06-16 00:28:20,997 - Epoch: [19][  130/  142]    Overall Loss 0.999144    Objective Loss 0.999144                                        LR 0.000500    Time 0.538042    
2023-06-16 00:28:25,578 - Epoch: [19][  140/  142]    Overall Loss 1.002504    Objective Loss 1.002504                                        LR 0.000500    Time 0.532333    
2023-06-16 00:28:26,422 - Epoch: [19][  142/  142]    Overall Loss 1.004432    Objective Loss 1.004432    Top1 60.937500    LR 0.000500    Time 0.530774    
2023-06-16 00:28:27,072 - --- validate (epoch=19)-----------
2023-06-16 00:28:27,072 - 1422 samples (32 per mini-batch)
2023-06-16 00:28:35,209 - Epoch: [19][   10/   45]    Loss 1.017420    Top1 63.125000    
2023-06-16 00:28:39,321 - Epoch: [19][   20/   45]    Loss 1.052782    Top1 63.437500    
2023-06-16 00:28:43,374 - Epoch: [19][   30/   45]    Loss 1.054697    Top1 63.020833    
2023-06-16 00:28:47,887 - Epoch: [19][   40/   45]    Loss 1.074170    Top1 61.640625    
2023-06-16 00:28:49,327 - Epoch: [19][   45/   45]    Loss 1.077249    Top1 61.181435    
2023-06-16 00:28:49,961 - ==> Top1: 61.181    Loss: 1.077

2023-06-16 00:28:49,963 - ==> Best [Top1: 65.893   Sparsity:0.00   Params: 375264 on epoch: 15]
2023-06-16 00:28:49,963 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:28:49,984 - 

2023-06-16 00:28:49,984 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:28:59,370 - Epoch: [20][   10/  142]    Overall Loss 1.008106    Objective Loss 1.008106                                        LR 0.000250    Time 0.938389    
2023-06-16 00:29:04,209 - Epoch: [20][   20/  142]    Overall Loss 0.961496    Objective Loss 0.961496                                        LR 0.000250    Time 0.711146    
2023-06-16 00:29:09,129 - Epoch: [20][   30/  142]    Overall Loss 0.941413    Objective Loss 0.941413                                        LR 0.000250    Time 0.638059    
2023-06-16 00:29:14,007 - Epoch: [20][   40/  142]    Overall Loss 0.931423    Objective Loss 0.931423                                        LR 0.000250    Time 0.600489    
2023-06-16 00:29:18,962 - Epoch: [20][   50/  142]    Overall Loss 0.929929    Objective Loss 0.929929                                        LR 0.000250    Time 0.579471    
2023-06-16 00:29:23,923 - Epoch: [20][   60/  142]    Overall Loss 0.923174    Objective Loss 0.923174                                        LR 0.000250    Time 0.565567    
2023-06-16 00:29:28,842 - Epoch: [20][   70/  142]    Overall Loss 0.910039    Objective Loss 0.910039                                        LR 0.000250    Time 0.555045    
2023-06-16 00:29:33,721 - Epoch: [20][   80/  142]    Overall Loss 0.924094    Objective Loss 0.924094                                        LR 0.000250    Time 0.546637    
2023-06-16 00:29:38,705 - Epoch: [20][   90/  142]    Overall Loss 0.923758    Objective Loss 0.923758                                        LR 0.000250    Time 0.541275    
2023-06-16 00:29:43,646 - Epoch: [20][  100/  142]    Overall Loss 0.917068    Objective Loss 0.917068                                        LR 0.000250    Time 0.536553    
2023-06-16 00:29:48,605 - Epoch: [20][  110/  142]    Overall Loss 0.921464    Objective Loss 0.921464                                        LR 0.000250    Time 0.532852    
2023-06-16 00:29:53,597 - Epoch: [20][  120/  142]    Overall Loss 0.914481    Objective Loss 0.914481                                        LR 0.000250    Time 0.530038    
2023-06-16 00:29:58,444 - Epoch: [20][  130/  142]    Overall Loss 0.918316    Objective Loss 0.918316                                        LR 0.000250    Time 0.526549    
2023-06-16 00:30:03,164 - Epoch: [20][  140/  142]    Overall Loss 0.914810    Objective Loss 0.914810                                        LR 0.000250    Time 0.522645    
2023-06-16 00:30:04,019 - Epoch: [20][  142/  142]    Overall Loss 0.918604    Objective Loss 0.918604    Top1 59.375000    LR 0.000250    Time 0.521308    
2023-06-16 00:30:04,667 - --- validate (epoch=20)-----------
2023-06-16 00:30:04,667 - 1422 samples (32 per mini-batch)
2023-06-16 00:30:12,558 - Epoch: [20][   10/   45]    Loss 1.115477    Top1 61.250000    
2023-06-16 00:30:16,605 - Epoch: [20][   20/   45]    Loss 1.115214    Top1 63.125000    
2023-06-16 00:30:20,798 - Epoch: [20][   30/   45]    Loss 1.140607    Top1 62.708333    
2023-06-16 00:30:25,230 - Epoch: [20][   40/   45]    Loss 1.162566    Top1 62.500000    
2023-06-16 00:30:26,647 - Epoch: [20][   45/   45]    Loss 1.194446    Top1 62.165963    
2023-06-16 00:30:27,297 - ==> Top1: 62.166    Loss: 1.194

2023-06-16 00:30:27,299 - ==> Best [Top1: 65.893   Sparsity:0.00   Params: 375264 on epoch: 15]
2023-06-16 00:30:27,299 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:30:27,320 - 

2023-06-16 00:30:27,320 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:30:36,440 - Epoch: [21][   10/  142]    Overall Loss 0.914089    Objective Loss 0.914089                                        LR 0.000250    Time 0.911875    
2023-06-16 00:30:41,390 - Epoch: [21][   20/  142]    Overall Loss 0.966123    Objective Loss 0.966123                                        LR 0.000250    Time 0.703433    
2023-06-16 00:30:46,322 - Epoch: [21][   30/  142]    Overall Loss 0.963129    Objective Loss 0.963129                                        LR 0.000250    Time 0.633323    
2023-06-16 00:30:51,272 - Epoch: [21][   40/  142]    Overall Loss 0.933992    Objective Loss 0.933992                                        LR 0.000250    Time 0.598737    
2023-06-16 00:30:56,223 - Epoch: [21][   50/  142]    Overall Loss 0.928627    Objective Loss 0.928627                                        LR 0.000250    Time 0.577993    
2023-06-16 00:31:01,194 - Epoch: [21][   60/  142]    Overall Loss 0.918625    Objective Loss 0.918625                                        LR 0.000250    Time 0.564494    
2023-06-16 00:31:06,031 - Epoch: [21][   70/  142]    Overall Loss 0.913658    Objective Loss 0.913658                                        LR 0.000250    Time 0.552949    
2023-06-16 00:31:10,998 - Epoch: [21][   80/  142]    Overall Loss 0.897094    Objective Loss 0.897094                                        LR 0.000250    Time 0.545906    
2023-06-16 00:31:15,931 - Epoch: [21][   90/  142]    Overall Loss 0.886039    Objective Loss 0.886039                                        LR 0.000250    Time 0.540056    
2023-06-16 00:31:21,006 - Epoch: [21][  100/  142]    Overall Loss 0.899848    Objective Loss 0.899848                                        LR 0.000250    Time 0.536794    
2023-06-16 00:31:25,956 - Epoch: [21][  110/  142]    Overall Loss 0.901022    Objective Loss 0.901022                                        LR 0.000250    Time 0.532990    
2023-06-16 00:31:30,694 - Epoch: [21][  120/  142]    Overall Loss 0.905943    Objective Loss 0.905943                                        LR 0.000250    Time 0.528052    
2023-06-16 00:31:35,657 - Epoch: [21][  130/  142]    Overall Loss 0.904693    Objective Loss 0.904693                                        LR 0.000250    Time 0.525602    
2023-06-16 00:31:40,362 - Epoch: [21][  140/  142]    Overall Loss 0.912087    Objective Loss 0.912087                                        LR 0.000250    Time 0.521664    
2023-06-16 00:31:41,222 - Epoch: [21][  142/  142]    Overall Loss 0.910906    Objective Loss 0.910906    Top1 70.312500    LR 0.000250    Time 0.520373    
2023-06-16 00:31:41,838 - --- validate (epoch=21)-----------
2023-06-16 00:31:41,839 - 1422 samples (32 per mini-batch)
2023-06-16 00:31:49,563 - Epoch: [21][   10/   45]    Loss 1.135752    Top1 64.062500    
2023-06-16 00:31:54,198 - Epoch: [21][   20/   45]    Loss 1.053569    Top1 66.406250    
2023-06-16 00:31:58,734 - Epoch: [21][   30/   45]    Loss 1.031139    Top1 66.875000    
2023-06-16 00:32:02,684 - Epoch: [21][   40/   45]    Loss 1.053976    Top1 65.937500    
2023-06-16 00:32:04,271 - Epoch: [21][   45/   45]    Loss 1.063945    Top1 65.682138    
2023-06-16 00:32:04,925 - ==> Top1: 65.682    Loss: 1.064

2023-06-16 00:32:04,927 - ==> Best [Top1: 65.893   Sparsity:0.00   Params: 375264 on epoch: 15]
2023-06-16 00:32:04,927 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:32:04,941 - 

2023-06-16 00:32:04,941 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:32:14,186 - Epoch: [22][   10/  142]    Overall Loss 0.837410    Objective Loss 0.837410                                        LR 0.000250    Time 0.924352    
2023-06-16 00:32:19,111 - Epoch: [22][   20/  142]    Overall Loss 0.870291    Objective Loss 0.870291                                        LR 0.000250    Time 0.708422    
2023-06-16 00:32:24,134 - Epoch: [22][   30/  142]    Overall Loss 0.900071    Objective Loss 0.900071                                        LR 0.000250    Time 0.639677    
2023-06-16 00:32:28,983 - Epoch: [22][   40/  142]    Overall Loss 0.891026    Objective Loss 0.891026                                        LR 0.000250    Time 0.600984    
2023-06-16 00:32:33,928 - Epoch: [22][   50/  142]    Overall Loss 0.891336    Objective Loss 0.891336                                        LR 0.000250    Time 0.579678    
2023-06-16 00:32:38,841 - Epoch: [22][   60/  142]    Overall Loss 0.885056    Objective Loss 0.885056                                        LR 0.000250    Time 0.564936    
2023-06-16 00:32:43,798 - Epoch: [22][   70/  142]    Overall Loss 0.881054    Objective Loss 0.881054                                        LR 0.000250    Time 0.555028    
2023-06-16 00:32:48,655 - Epoch: [22][   80/  142]    Overall Loss 0.872934    Objective Loss 0.872934                                        LR 0.000250    Time 0.546365    
2023-06-16 00:32:53,616 - Epoch: [22][   90/  142]    Overall Loss 0.870902    Objective Loss 0.870902                                        LR 0.000250    Time 0.540768    
2023-06-16 00:32:58,593 - Epoch: [22][  100/  142]    Overall Loss 0.878829    Objective Loss 0.878829                                        LR 0.000250    Time 0.536454    
2023-06-16 00:33:03,582 - Epoch: [22][  110/  142]    Overall Loss 0.888222    Objective Loss 0.888222                                        LR 0.000250    Time 0.533032    
2023-06-16 00:33:08,465 - Epoch: [22][  120/  142]    Overall Loss 0.884603    Objective Loss 0.884603                                        LR 0.000250    Time 0.529303    
2023-06-16 00:33:13,442 - Epoch: [22][  130/  142]    Overall Loss 0.884481    Objective Loss 0.884481                                        LR 0.000250    Time 0.526863    
2023-06-16 00:33:17,996 - Epoch: [22][  140/  142]    Overall Loss 0.884666    Objective Loss 0.884666                                        LR 0.000250    Time 0.521758    
2023-06-16 00:33:18,851 - Epoch: [22][  142/  142]    Overall Loss 0.884284    Objective Loss 0.884284    Top1 71.875000    LR 0.000250    Time 0.520431    
2023-06-16 00:33:19,477 - --- validate (epoch=22)-----------
2023-06-16 00:33:19,477 - 1422 samples (32 per mini-batch)
2023-06-16 00:33:27,632 - Epoch: [22][   10/   45]    Loss 1.074092    Top1 64.375000    
2023-06-16 00:33:32,447 - Epoch: [22][   20/   45]    Loss 1.040705    Top1 66.250000    
2023-06-16 00:33:36,643 - Epoch: [22][   30/   45]    Loss 0.985174    Top1 68.020833    
2023-06-16 00:33:40,750 - Epoch: [22][   40/   45]    Loss 0.970074    Top1 68.515625    
2023-06-16 00:33:42,196 - Epoch: [22][   45/   45]    Loss 0.966866    Top1 68.424754    
2023-06-16 00:33:42,836 - ==> Top1: 68.425    Loss: 0.967

2023-06-16 00:33:42,838 - ==> Best [Top1: 68.425   Sparsity:0.00   Params: 375264 on epoch: 22]
2023-06-16 00:33:42,838 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:33:42,864 - 

2023-06-16 00:33:42,864 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:33:52,171 - Epoch: [23][   10/  142]    Overall Loss 0.873751    Objective Loss 0.873751                                        LR 0.000250    Time 0.930611    
2023-06-16 00:33:57,072 - Epoch: [23][   20/  142]    Overall Loss 0.874399    Objective Loss 0.874399                                        LR 0.000250    Time 0.710306    
2023-06-16 00:34:02,004 - Epoch: [23][   30/  142]    Overall Loss 0.911186    Objective Loss 0.911186                                        LR 0.000250    Time 0.637915    
2023-06-16 00:34:07,064 - Epoch: [23][   40/  142]    Overall Loss 0.927930    Objective Loss 0.927930                                        LR 0.000250    Time 0.604901    
2023-06-16 00:34:11,885 - Epoch: [23][   50/  142]    Overall Loss 0.918873    Objective Loss 0.918873                                        LR 0.000250    Time 0.580341    
2023-06-16 00:34:16,959 - Epoch: [23][   60/  142]    Overall Loss 0.922938    Objective Loss 0.922938                                        LR 0.000250    Time 0.568181    
2023-06-16 00:34:21,901 - Epoch: [23][   70/  142]    Overall Loss 0.921856    Objective Loss 0.921856                                        LR 0.000250    Time 0.557603    
2023-06-16 00:34:26,962 - Epoch: [23][   80/  142]    Overall Loss 0.913916    Objective Loss 0.913916                                        LR 0.000250    Time 0.551155    
2023-06-16 00:34:31,865 - Epoch: [23][   90/  142]    Overall Loss 0.908290    Objective Loss 0.908290                                        LR 0.000250    Time 0.544387    
2023-06-16 00:34:36,956 - Epoch: [23][  100/  142]    Overall Loss 0.907175    Objective Loss 0.907175                                        LR 0.000250    Time 0.540850    
2023-06-16 00:34:41,899 - Epoch: [23][  110/  142]    Overall Loss 0.903937    Objective Loss 0.903937                                        LR 0.000250    Time 0.536616    
2023-06-16 00:34:46,880 - Epoch: [23][  120/  142]    Overall Loss 0.904660    Objective Loss 0.904660                                        LR 0.000250    Time 0.533395    
2023-06-16 00:34:51,775 - Epoch: [23][  130/  142]    Overall Loss 0.898603    Objective Loss 0.898603                                        LR 0.000250    Time 0.530017    
2023-06-16 00:34:56,382 - Epoch: [23][  140/  142]    Overall Loss 0.892250    Objective Loss 0.892250                                        LR 0.000250    Time 0.525063    
2023-06-16 00:34:57,235 - Epoch: [23][  142/  142]    Overall Loss 0.892076    Objective Loss 0.892076    Top1 64.062500    LR 0.000250    Time 0.523674    
2023-06-16 00:34:57,893 - --- validate (epoch=23)-----------
2023-06-16 00:34:57,893 - 1422 samples (32 per mini-batch)
2023-06-16 00:35:05,645 - Epoch: [23][   10/   45]    Loss 1.051132    Top1 59.687500    
2023-06-16 00:35:09,830 - Epoch: [23][   20/   45]    Loss 0.986474    Top1 61.718750    
2023-06-16 00:35:14,045 - Epoch: [23][   30/   45]    Loss 0.958656    Top1 63.958333    
2023-06-16 00:35:18,436 - Epoch: [23][   40/   45]    Loss 0.932779    Top1 65.859375    
2023-06-16 00:35:19,871 - Epoch: [23][   45/   45]    Loss 0.946005    Top1 65.893108    
2023-06-16 00:35:20,529 - ==> Top1: 65.893    Loss: 0.946

2023-06-16 00:35:20,531 - ==> Best [Top1: 68.425   Sparsity:0.00   Params: 375264 on epoch: 22]
2023-06-16 00:35:20,531 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:35:20,552 - 

2023-06-16 00:35:20,552 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:35:29,958 - Epoch: [24][   10/  142]    Overall Loss 0.853729    Objective Loss 0.853729                                        LR 0.000250    Time 0.940480    
2023-06-16 00:35:34,929 - Epoch: [24][   20/  142]    Overall Loss 0.788878    Objective Loss 0.788878                                        LR 0.000250    Time 0.718747    
2023-06-16 00:35:39,975 - Epoch: [24][   30/  142]    Overall Loss 0.841316    Objective Loss 0.841316                                        LR 0.000250    Time 0.647348    
2023-06-16 00:35:45,008 - Epoch: [24][   40/  142]    Overall Loss 0.852180    Objective Loss 0.852180                                        LR 0.000250    Time 0.611319    
2023-06-16 00:35:49,944 - Epoch: [24][   50/  142]    Overall Loss 0.852407    Objective Loss 0.852407                                        LR 0.000250    Time 0.587773    
2023-06-16 00:35:54,996 - Epoch: [24][   60/  142]    Overall Loss 0.841780    Objective Loss 0.841780                                        LR 0.000250    Time 0.574007    
2023-06-16 00:36:00,001 - Epoch: [24][   70/  142]    Overall Loss 0.859028    Objective Loss 0.859028                                        LR 0.000250    Time 0.563489    
2023-06-16 00:36:05,121 - Epoch: [24][   80/  142]    Overall Loss 0.863548    Objective Loss 0.863548                                        LR 0.000250    Time 0.557044    
2023-06-16 00:36:10,062 - Epoch: [24][   90/  142]    Overall Loss 0.866917    Objective Loss 0.866917                                        LR 0.000250    Time 0.550043    
2023-06-16 00:36:15,028 - Epoch: [24][  100/  142]    Overall Loss 0.860078    Objective Loss 0.860078                                        LR 0.000250    Time 0.544692    
2023-06-16 00:36:20,015 - Epoch: [24][  110/  142]    Overall Loss 0.868825    Objective Loss 0.868825                                        LR 0.000250    Time 0.540508    
2023-06-16 00:36:25,062 - Epoch: [24][  120/  142]    Overall Loss 0.877532    Objective Loss 0.877532                                        LR 0.000250    Time 0.537521    
2023-06-16 00:36:30,181 - Epoch: [24][  130/  142]    Overall Loss 0.880170    Objective Loss 0.880170                                        LR 0.000250    Time 0.535546    
2023-06-16 00:36:34,813 - Epoch: [24][  140/  142]    Overall Loss 0.879825    Objective Loss 0.879825                                        LR 0.000250    Time 0.530376    
2023-06-16 00:36:35,656 - Epoch: [24][  142/  142]    Overall Loss 0.881258    Objective Loss 0.881258    Top1 65.625000    LR 0.000250    Time 0.528838    
2023-06-16 00:36:36,312 - --- validate (epoch=24)-----------
2023-06-16 00:36:36,312 - 1422 samples (32 per mini-batch)
2023-06-16 00:36:44,313 - Epoch: [24][   10/   45]    Loss 0.874015    Top1 67.187500    
2023-06-16 00:36:48,826 - Epoch: [24][   20/   45]    Loss 0.886663    Top1 67.500000    
2023-06-16 00:36:53,289 - Epoch: [24][   30/   45]    Loss 0.940048    Top1 67.083333    
2023-06-16 00:36:57,570 - Epoch: [24][   40/   45]    Loss 0.918765    Top1 68.437500    
2023-06-16 00:36:58,926 - Epoch: [24][   45/   45]    Loss 0.934315    Top1 67.862166    
2023-06-16 00:36:59,579 - ==> Top1: 67.862    Loss: 0.934

2023-06-16 00:36:59,581 - ==> Best [Top1: 68.425   Sparsity:0.00   Params: 375264 on epoch: 22]
2023-06-16 00:36:59,581 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:36:59,603 - 

2023-06-16 00:36:59,603 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:37:08,809 - Epoch: [25][   10/  142]    Overall Loss 0.861795    Objective Loss 0.861795                                        LR 0.000250    Time 0.920551    
2023-06-16 00:37:13,751 - Epoch: [25][   20/  142]    Overall Loss 0.855798    Objective Loss 0.855798                                        LR 0.000250    Time 0.707300    
2023-06-16 00:37:18,669 - Epoch: [25][   30/  142]    Overall Loss 0.892729    Objective Loss 0.892729                                        LR 0.000250    Time 0.635454    
2023-06-16 00:37:23,595 - Epoch: [25][   40/  142]    Overall Loss 0.874099    Objective Loss 0.874099                                        LR 0.000250    Time 0.599750    
2023-06-16 00:37:28,583 - Epoch: [25][   50/  142]    Overall Loss 0.885720    Objective Loss 0.885720                                        LR 0.000250    Time 0.579534    
2023-06-16 00:37:33,494 - Epoch: [25][   60/  142]    Overall Loss 0.873080    Objective Loss 0.873080                                        LR 0.000250    Time 0.564796    
2023-06-16 00:37:38,510 - Epoch: [25][   70/  142]    Overall Loss 0.859273    Objective Loss 0.859273                                        LR 0.000250    Time 0.555754    
2023-06-16 00:37:43,444 - Epoch: [25][   80/  142]    Overall Loss 0.846424    Objective Loss 0.846424                                        LR 0.000250    Time 0.547956    
2023-06-16 00:37:48,363 - Epoch: [25][   90/  142]    Overall Loss 0.863050    Objective Loss 0.863050                                        LR 0.000250    Time 0.541721    
2023-06-16 00:37:53,224 - Epoch: [25][  100/  142]    Overall Loss 0.861641    Objective Loss 0.861641                                        LR 0.000250    Time 0.536157    
2023-06-16 00:37:58,037 - Epoch: [25][  110/  142]    Overall Loss 0.861567    Objective Loss 0.861567                                        LR 0.000250    Time 0.531159    
2023-06-16 00:38:03,067 - Epoch: [25][  120/  142]    Overall Loss 0.862769    Objective Loss 0.862769                                        LR 0.000250    Time 0.528808    
2023-06-16 00:38:07,987 - Epoch: [25][  130/  142]    Overall Loss 0.865180    Objective Loss 0.865180                                        LR 0.000250    Time 0.525978    
2023-06-16 00:38:12,562 - Epoch: [25][  140/  142]    Overall Loss 0.865094    Objective Loss 0.865094                                        LR 0.000250    Time 0.521084    
2023-06-16 00:38:13,405 - Epoch: [25][  142/  142]    Overall Loss 0.864521    Objective Loss 0.864521    Top1 68.750000    LR 0.000250    Time 0.519680    
2023-06-16 00:38:14,054 - --- validate (epoch=25)-----------
2023-06-16 00:38:14,055 - 1422 samples (32 per mini-batch)
2023-06-16 00:38:21,867 - Epoch: [25][   10/   45]    Loss 1.008524    Top1 68.125000    
2023-06-16 00:38:26,477 - Epoch: [25][   20/   45]    Loss 0.957241    Top1 68.281250    
2023-06-16 00:38:30,974 - Epoch: [25][   30/   45]    Loss 0.954170    Top1 68.229167    
2023-06-16 00:38:35,528 - Epoch: [25][   40/   45]    Loss 0.955796    Top1 68.515625    
2023-06-16 00:38:36,930 - Epoch: [25][   45/   45]    Loss 0.935534    Top1 69.127989    
2023-06-16 00:38:37,579 - ==> Top1: 69.128    Loss: 0.936

2023-06-16 00:38:37,581 - ==> Best [Top1: 69.128   Sparsity:0.00   Params: 375264 on epoch: 25]
2023-06-16 00:38:37,581 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:38:37,606 - 

2023-06-16 00:38:37,606 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:38:46,527 - Epoch: [26][   10/  142]    Overall Loss 0.896463    Objective Loss 0.896463                                        LR 0.000250    Time 0.892027    
2023-06-16 00:38:51,578 - Epoch: [26][   20/  142]    Overall Loss 0.916490    Objective Loss 0.916490                                        LR 0.000250    Time 0.698528    
2023-06-16 00:38:56,528 - Epoch: [26][   30/  142]    Overall Loss 0.891619    Objective Loss 0.891619                                        LR 0.000250    Time 0.630669    
2023-06-16 00:39:01,387 - Epoch: [26][   40/  142]    Overall Loss 0.886489    Objective Loss 0.886489                                        LR 0.000250    Time 0.594447    
2023-06-16 00:39:06,267 - Epoch: [26][   50/  142]    Overall Loss 0.899900    Objective Loss 0.899900                                        LR 0.000250    Time 0.573156    
2023-06-16 00:39:11,143 - Epoch: [26][   60/  142]    Overall Loss 0.880196    Objective Loss 0.880196                                        LR 0.000250    Time 0.558873    
2023-06-16 00:39:16,177 - Epoch: [26][   70/  142]    Overall Loss 0.872640    Objective Loss 0.872640                                        LR 0.000250    Time 0.550955    
2023-06-16 00:39:20,999 - Epoch: [26][   80/  142]    Overall Loss 0.857466    Objective Loss 0.857466                                        LR 0.000250    Time 0.542342    
2023-06-16 00:39:26,053 - Epoch: [26][   90/  142]    Overall Loss 0.862726    Objective Loss 0.862726                                        LR 0.000250    Time 0.538238    
2023-06-16 00:39:30,838 - Epoch: [26][  100/  142]    Overall Loss 0.872763    Objective Loss 0.872763                                        LR 0.000250    Time 0.532255    
2023-06-16 00:39:35,807 - Epoch: [26][  110/  142]    Overall Loss 0.868148    Objective Loss 0.868148                                        LR 0.000250    Time 0.529032    
2023-06-16 00:39:40,766 - Epoch: [26][  120/  142]    Overall Loss 0.867008    Objective Loss 0.867008                                        LR 0.000250    Time 0.526267    
2023-06-16 00:39:45,744 - Epoch: [26][  130/  142]    Overall Loss 0.857500    Objective Loss 0.857500                                        LR 0.000250    Time 0.524074    
2023-06-16 00:39:50,436 - Epoch: [26][  140/  142]    Overall Loss 0.858263    Objective Loss 0.858263                                        LR 0.000250    Time 0.520148    
2023-06-16 00:39:51,277 - Epoch: [26][  142/  142]    Overall Loss 0.859551    Objective Loss 0.859551    Top1 64.062500    LR 0.000250    Time 0.518747    
2023-06-16 00:39:51,919 - --- validate (epoch=26)-----------
2023-06-16 00:39:51,920 - 1422 samples (32 per mini-batch)
2023-06-16 00:39:59,861 - Epoch: [26][   10/   45]    Loss 0.971599    Top1 62.187500    
2023-06-16 00:40:04,819 - Epoch: [26][   20/   45]    Loss 0.952200    Top1 63.281250    
2023-06-16 00:40:09,011 - Epoch: [26][   30/   45]    Loss 0.936179    Top1 64.583333    
2023-06-16 00:40:13,441 - Epoch: [26][   40/   45]    Loss 0.914808    Top1 65.078125    
2023-06-16 00:40:14,810 - Epoch: [26][   45/   45]    Loss 0.904097    Top1 65.260197    
2023-06-16 00:40:15,388 - ==> Top1: 65.260    Loss: 0.904

2023-06-16 00:40:15,390 - ==> Best [Top1: 69.128   Sparsity:0.00   Params: 375264 on epoch: 25]
2023-06-16 00:40:15,390 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:40:15,411 - 

2023-06-16 00:40:15,411 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:40:24,728 - Epoch: [27][   10/  142]    Overall Loss 0.762537    Objective Loss 0.762537                                        LR 0.000250    Time 0.931594    
2023-06-16 00:40:29,762 - Epoch: [27][   20/  142]    Overall Loss 0.796441    Objective Loss 0.796441                                        LR 0.000250    Time 0.714815    
2023-06-16 00:40:34,753 - Epoch: [27][   30/  142]    Overall Loss 0.838150    Objective Loss 0.838150                                        LR 0.000250    Time 0.642899    
2023-06-16 00:40:39,792 - Epoch: [27][   40/  142]    Overall Loss 0.842731    Objective Loss 0.842731                                        LR 0.000250    Time 0.608136    
2023-06-16 00:40:44,883 - Epoch: [27][   50/  142]    Overall Loss 0.851688    Objective Loss 0.851688                                        LR 0.000250    Time 0.588309    
2023-06-16 00:40:49,855 - Epoch: [27][   60/  142]    Overall Loss 0.845992    Objective Loss 0.845992                                        LR 0.000250    Time 0.573118    
2023-06-16 00:40:54,894 - Epoch: [27][   70/  142]    Overall Loss 0.865592    Objective Loss 0.865592                                        LR 0.000250    Time 0.563220    
2023-06-16 00:40:59,969 - Epoch: [27][   80/  142]    Overall Loss 0.864758    Objective Loss 0.864758                                        LR 0.000250    Time 0.556255    
2023-06-16 00:41:04,905 - Epoch: [27][   90/  142]    Overall Loss 0.863456    Objective Loss 0.863456                                        LR 0.000250    Time 0.549291    
2023-06-16 00:41:09,964 - Epoch: [27][  100/  142]    Overall Loss 0.859693    Objective Loss 0.859693                                        LR 0.000250    Time 0.544946    
2023-06-16 00:41:14,952 - Epoch: [27][  110/  142]    Overall Loss 0.849372    Objective Loss 0.849372                                        LR 0.000250    Time 0.540743    
2023-06-16 00:41:20,071 - Epoch: [27][  120/  142]    Overall Loss 0.850221    Objective Loss 0.850221                                        LR 0.000250    Time 0.538337    
2023-06-16 00:41:25,065 - Epoch: [27][  130/  142]    Overall Loss 0.841792    Objective Loss 0.841792                                        LR 0.000250    Time 0.535337    
2023-06-16 00:41:29,773 - Epoch: [27][  140/  142]    Overall Loss 0.839896    Objective Loss 0.839896                                        LR 0.000250    Time 0.530722    
2023-06-16 00:41:30,628 - Epoch: [27][  142/  142]    Overall Loss 0.839275    Objective Loss 0.839275    Top1 78.125000    LR 0.000250    Time 0.529272    
2023-06-16 00:41:31,290 - --- validate (epoch=27)-----------
2023-06-16 00:41:31,291 - 1422 samples (32 per mini-batch)
2023-06-16 00:41:39,259 - Epoch: [27][   10/   45]    Loss 0.914222    Top1 69.375000    
2023-06-16 00:41:43,542 - Epoch: [27][   20/   45]    Loss 0.936257    Top1 68.125000    
2023-06-16 00:41:47,871 - Epoch: [27][   30/   45]    Loss 0.956448    Top1 67.916667    
2023-06-16 00:41:51,930 - Epoch: [27][   40/   45]    Loss 0.946839    Top1 67.890625    
2023-06-16 00:41:53,542 - Epoch: [27][   45/   45]    Loss 0.948101    Top1 68.002813    
2023-06-16 00:41:54,192 - ==> Top1: 68.003    Loss: 0.948

2023-06-16 00:41:54,194 - ==> Best [Top1: 69.128   Sparsity:0.00   Params: 375264 on epoch: 25]
2023-06-16 00:41:54,194 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:41:54,208 - 

2023-06-16 00:41:54,208 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:42:03,313 - Epoch: [28][   10/  142]    Overall Loss 0.797570    Objective Loss 0.797570                                        LR 0.000250    Time 0.910395    
2023-06-16 00:42:08,265 - Epoch: [28][   20/  142]    Overall Loss 0.777081    Objective Loss 0.777081                                        LR 0.000250    Time 0.702745    
2023-06-16 00:42:13,149 - Epoch: [28][   30/  142]    Overall Loss 0.781055    Objective Loss 0.781055                                        LR 0.000250    Time 0.631266    
2023-06-16 00:42:18,111 - Epoch: [28][   40/  142]    Overall Loss 0.789327    Objective Loss 0.789327                                        LR 0.000250    Time 0.597490    
2023-06-16 00:42:22,977 - Epoch: [28][   50/  142]    Overall Loss 0.790761    Objective Loss 0.790761                                        LR 0.000250    Time 0.575300    
2023-06-16 00:42:27,944 - Epoch: [28][   60/  142]    Overall Loss 0.790818    Objective Loss 0.790818                                        LR 0.000250    Time 0.562199    
2023-06-16 00:42:32,778 - Epoch: [28][   70/  142]    Overall Loss 0.785811    Objective Loss 0.785811                                        LR 0.000250    Time 0.550938    
2023-06-16 00:42:37,759 - Epoch: [28][   80/  142]    Overall Loss 0.795522    Objective Loss 0.795522                                        LR 0.000250    Time 0.544318    
2023-06-16 00:42:42,645 - Epoch: [28][   90/  142]    Overall Loss 0.804270    Objective Loss 0.804270                                        LR 0.000250    Time 0.538118    
2023-06-16 00:42:47,711 - Epoch: [28][  100/  142]    Overall Loss 0.819752    Objective Loss 0.819752                                        LR 0.000250    Time 0.534961    
2023-06-16 00:42:52,660 - Epoch: [28][  110/  142]    Overall Loss 0.819514    Objective Loss 0.819514                                        LR 0.000250    Time 0.531314    
2023-06-16 00:42:57,502 - Epoch: [28][  120/  142]    Overall Loss 0.819107    Objective Loss 0.819107                                        LR 0.000250    Time 0.527387    
2023-06-16 00:43:02,617 - Epoch: [28][  130/  142]    Overall Loss 0.818298    Objective Loss 0.818298                                        LR 0.000250    Time 0.526162    
2023-06-16 00:43:07,205 - Epoch: [28][  140/  142]    Overall Loss 0.824945    Objective Loss 0.824945                                        LR 0.000250    Time 0.521343    
2023-06-16 00:43:08,044 - Epoch: [28][  142/  142]    Overall Loss 0.823230    Objective Loss 0.823230    Top1 84.375000    LR 0.000250    Time 0.519910    
2023-06-16 00:43:08,661 - --- validate (epoch=28)-----------
2023-06-16 00:43:08,662 - 1422 samples (32 per mini-batch)
2023-06-16 00:43:16,660 - Epoch: [28][   10/   45]    Loss 1.050050    Top1 66.875000    
2023-06-16 00:43:21,251 - Epoch: [28][   20/   45]    Loss 1.038424    Top1 65.468750    
2023-06-16 00:43:25,571 - Epoch: [28][   30/   45]    Loss 1.010217    Top1 66.250000    
2023-06-16 00:43:30,240 - Epoch: [28][   40/   45]    Loss 1.008851    Top1 66.250000    
2023-06-16 00:43:31,599 - Epoch: [28][   45/   45]    Loss 1.000662    Top1 66.033755    
2023-06-16 00:43:32,221 - ==> Top1: 66.034    Loss: 1.001

2023-06-16 00:43:32,223 - ==> Best [Top1: 69.128   Sparsity:0.00   Params: 375264 on epoch: 25]
2023-06-16 00:43:32,223 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:43:32,244 - 

2023-06-16 00:43:32,244 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:43:41,475 - Epoch: [29][   10/  142]    Overall Loss 0.867779    Objective Loss 0.867779                                        LR 0.000250    Time 0.922952    
2023-06-16 00:43:46,425 - Epoch: [29][   20/  142]    Overall Loss 0.857259    Objective Loss 0.857259                                        LR 0.000250    Time 0.708966    
2023-06-16 00:43:51,383 - Epoch: [29][   30/  142]    Overall Loss 0.845580    Objective Loss 0.845580                                        LR 0.000250    Time 0.637898    
2023-06-16 00:43:56,317 - Epoch: [29][   40/  142]    Overall Loss 0.826424    Objective Loss 0.826424                                        LR 0.000250    Time 0.601741    
2023-06-16 00:44:01,235 - Epoch: [29][   50/  142]    Overall Loss 0.818067    Objective Loss 0.818067                                        LR 0.000250    Time 0.579764    
2023-06-16 00:44:06,165 - Epoch: [29][   60/  142]    Overall Loss 0.832079    Objective Loss 0.832079                                        LR 0.000250    Time 0.565282    
2023-06-16 00:44:11,160 - Epoch: [29][   70/  142]    Overall Loss 0.837717    Objective Loss 0.837717                                        LR 0.000250    Time 0.555879    
2023-06-16 00:44:16,054 - Epoch: [29][   80/  142]    Overall Loss 0.824723    Objective Loss 0.824723                                        LR 0.000250    Time 0.547562    
2023-06-16 00:44:21,149 - Epoch: [29][   90/  142]    Overall Loss 0.816427    Objective Loss 0.816427                                        LR 0.000250    Time 0.543323    
2023-06-16 00:44:26,105 - Epoch: [29][  100/  142]    Overall Loss 0.811285    Objective Loss 0.811285                                        LR 0.000250    Time 0.538546    
2023-06-16 00:44:30,962 - Epoch: [29][  110/  142]    Overall Loss 0.816601    Objective Loss 0.816601                                        LR 0.000250    Time 0.533744    
2023-06-16 00:44:35,926 - Epoch: [29][  120/  142]    Overall Loss 0.813679    Objective Loss 0.813679                                        LR 0.000250    Time 0.530626    
2023-06-16 00:44:40,852 - Epoch: [29][  130/  142]    Overall Loss 0.810987    Objective Loss 0.810987                                        LR 0.000250    Time 0.527696    
2023-06-16 00:44:45,504 - Epoch: [29][  140/  142]    Overall Loss 0.813345    Objective Loss 0.813345                                        LR 0.000250    Time 0.523227    
2023-06-16 00:44:46,359 - Epoch: [29][  142/  142]    Overall Loss 0.815973    Objective Loss 0.815973    Top1 64.062500    LR 0.000250    Time 0.521878    
2023-06-16 00:44:47,010 - --- validate (epoch=29)-----------
2023-06-16 00:44:47,011 - 1422 samples (32 per mini-batch)
2023-06-16 00:44:54,918 - Epoch: [29][   10/   45]    Loss 0.880348    Top1 69.375000    
2023-06-16 00:44:58,934 - Epoch: [29][   20/   45]    Loss 0.843740    Top1 70.000000    
2023-06-16 00:45:03,486 - Epoch: [29][   30/   45]    Loss 0.868735    Top1 69.375000    
2023-06-16 00:45:07,949 - Epoch: [29][   40/   45]    Loss 0.906101    Top1 68.593750    
2023-06-16 00:45:09,268 - Epoch: [29][   45/   45]    Loss 0.909018    Top1 69.057665    
2023-06-16 00:45:09,930 - ==> Top1: 69.058    Loss: 0.909

2023-06-16 00:45:09,932 - ==> Best [Top1: 69.128   Sparsity:0.00   Params: 375264 on epoch: 25]
2023-06-16 00:45:09,932 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:45:09,953 - 

2023-06-16 00:45:09,953 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:45:19,282 - Epoch: [30][   10/  142]    Overall Loss 0.777163    Objective Loss 0.777163                                        LR 0.000250    Time 0.932768    
2023-06-16 00:45:24,316 - Epoch: [30][   20/  142]    Overall Loss 0.779173    Objective Loss 0.779173                                        LR 0.000250    Time 0.718007    
2023-06-16 00:45:29,433 - Epoch: [30][   30/  142]    Overall Loss 0.800103    Objective Loss 0.800103                                        LR 0.000250    Time 0.649228    
2023-06-16 00:45:34,301 - Epoch: [30][   40/  142]    Overall Loss 0.792166    Objective Loss 0.792166                                        LR 0.000250    Time 0.608621    
2023-06-16 00:45:39,439 - Epoch: [30][   50/  142]    Overall Loss 0.796571    Objective Loss 0.796571                                        LR 0.000250    Time 0.589650    
2023-06-16 00:45:44,422 - Epoch: [30][   60/  142]    Overall Loss 0.803520    Objective Loss 0.803520                                        LR 0.000250    Time 0.574412    
2023-06-16 00:45:49,441 - Epoch: [30][   70/  142]    Overall Loss 0.803719    Objective Loss 0.803719                                        LR 0.000250    Time 0.564042    
2023-06-16 00:45:54,379 - Epoch: [30][   80/  142]    Overall Loss 0.805114    Objective Loss 0.805114                                        LR 0.000250    Time 0.555251    
2023-06-16 00:45:59,312 - Epoch: [30][   90/  142]    Overall Loss 0.805541    Objective Loss 0.805541                                        LR 0.000250    Time 0.548370    
2023-06-16 00:46:04,391 - Epoch: [30][  100/  142]    Overall Loss 0.801412    Objective Loss 0.801412                                        LR 0.000250    Time 0.544313    
2023-06-16 00:46:09,305 - Epoch: [30][  110/  142]    Overall Loss 0.801122    Objective Loss 0.801122                                        LR 0.000250    Time 0.539500    
2023-06-16 00:46:14,243 - Epoch: [30][  120/  142]    Overall Loss 0.803796    Objective Loss 0.803796                                        LR 0.000250    Time 0.535692    
2023-06-16 00:46:19,151 - Epoch: [30][  130/  142]    Overall Loss 0.802727    Objective Loss 0.802727                                        LR 0.000250    Time 0.532233    
2023-06-16 00:46:23,836 - Epoch: [30][  140/  142]    Overall Loss 0.804914    Objective Loss 0.804914                                        LR 0.000250    Time 0.527674    
2023-06-16 00:46:24,679 - Epoch: [30][  142/  142]    Overall Loss 0.809593    Objective Loss 0.809593    Top1 76.562500    LR 0.000250    Time 0.526183    
2023-06-16 00:46:25,340 - --- validate (epoch=30)-----------
2023-06-16 00:46:25,341 - 1422 samples (32 per mini-batch)
2023-06-16 00:46:33,313 - Epoch: [30][   10/   45]    Loss 0.763887    Top1 73.750000    
2023-06-16 00:46:37,953 - Epoch: [30][   20/   45]    Loss 0.822552    Top1 72.187500    
2023-06-16 00:46:42,065 - Epoch: [30][   30/   45]    Loss 0.862283    Top1 72.083333    
2023-06-16 00:46:46,577 - Epoch: [30][   40/   45]    Loss 0.889423    Top1 71.250000    
2023-06-16 00:46:48,182 - Epoch: [30][   45/   45]    Loss 0.883423    Top1 70.815752    
2023-06-16 00:46:48,818 - ==> Top1: 70.816    Loss: 0.883

2023-06-16 00:46:48,820 - ==> Best [Top1: 70.816   Sparsity:0.00   Params: 375264 on epoch: 30]
2023-06-16 00:46:48,820 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:46:48,845 - 

2023-06-16 00:46:48,845 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:46:58,084 - Epoch: [31][   10/  142]    Overall Loss 0.754853    Objective Loss 0.754853                                        LR 0.000250    Time 0.923705    
2023-06-16 00:47:03,268 - Epoch: [31][   20/  142]    Overall Loss 0.762085    Objective Loss 0.762085                                        LR 0.000250    Time 0.721022    
2023-06-16 00:47:08,128 - Epoch: [31][   30/  142]    Overall Loss 0.786686    Objective Loss 0.786686                                        LR 0.000250    Time 0.642664    
2023-06-16 00:47:13,128 - Epoch: [31][   40/  142]    Overall Loss 0.804795    Objective Loss 0.804795                                        LR 0.000250    Time 0.606984    
2023-06-16 00:47:18,185 - Epoch: [31][   50/  142]    Overall Loss 0.808600    Objective Loss 0.808600                                        LR 0.000250    Time 0.586725    
2023-06-16 00:47:23,188 - Epoch: [31][   60/  142]    Overall Loss 0.806383    Objective Loss 0.806383                                        LR 0.000250    Time 0.572310    
2023-06-16 00:47:28,193 - Epoch: [31][   70/  142]    Overall Loss 0.807376    Objective Loss 0.807376                                        LR 0.000250    Time 0.562038    
2023-06-16 00:47:33,184 - Epoch: [31][   80/  142]    Overall Loss 0.815458    Objective Loss 0.815458                                        LR 0.000250    Time 0.554161    
2023-06-16 00:47:38,269 - Epoch: [31][   90/  142]    Overall Loss 0.821120    Objective Loss 0.821120                                        LR 0.000250    Time 0.549079    
2023-06-16 00:47:43,340 - Epoch: [31][  100/  142]    Overall Loss 0.816131    Objective Loss 0.816131                                        LR 0.000250    Time 0.544875    
2023-06-16 00:47:48,337 - Epoch: [31][  110/  142]    Overall Loss 0.819126    Objective Loss 0.819126                                        LR 0.000250    Time 0.540764    
2023-06-16 00:47:53,316 - Epoch: [31][  120/  142]    Overall Loss 0.820233    Objective Loss 0.820233                                        LR 0.000250    Time 0.537186    
2023-06-16 00:47:58,420 - Epoch: [31][  130/  142]    Overall Loss 0.822767    Objective Loss 0.822767                                        LR 0.000250    Time 0.535122    
2023-06-16 00:48:02,990 - Epoch: [31][  140/  142]    Overall Loss 0.817876    Objective Loss 0.817876                                        LR 0.000250    Time 0.529541    
2023-06-16 00:48:03,837 - Epoch: [31][  142/  142]    Overall Loss 0.817308    Objective Loss 0.817308    Top1 79.687500    LR 0.000250    Time 0.528046    
2023-06-16 00:48:04,473 - --- validate (epoch=31)-----------
2023-06-16 00:48:04,474 - 1422 samples (32 per mini-batch)
2023-06-16 00:48:12,442 - Epoch: [31][   10/   45]    Loss 0.965667    Top1 71.562500    
2023-06-16 00:48:16,645 - Epoch: [31][   20/   45]    Loss 0.976789    Top1 71.250000    
2023-06-16 00:48:20,683 - Epoch: [31][   30/   45]    Loss 1.001516    Top1 70.000000    
2023-06-16 00:48:25,489 - Epoch: [31][   40/   45]    Loss 1.021013    Top1 69.140625    
2023-06-16 00:48:26,878 - Epoch: [31][   45/   45]    Loss 1.017579    Top1 68.706048    
2023-06-16 00:48:27,516 - ==> Top1: 68.706    Loss: 1.018

2023-06-16 00:48:27,518 - ==> Best [Top1: 70.816   Sparsity:0.00   Params: 375264 on epoch: 30]
2023-06-16 00:48:27,518 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:48:27,539 - 

2023-06-16 00:48:27,539 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:48:36,943 - Epoch: [32][   10/  142]    Overall Loss 0.833033    Objective Loss 0.833033                                        LR 0.000250    Time 0.940280    
2023-06-16 00:48:41,987 - Epoch: [32][   20/  142]    Overall Loss 0.824560    Objective Loss 0.824560                                        LR 0.000250    Time 0.722278    
2023-06-16 00:48:47,020 - Epoch: [32][   30/  142]    Overall Loss 0.782797    Objective Loss 0.782797                                        LR 0.000250    Time 0.649257    
2023-06-16 00:48:51,968 - Epoch: [32][   40/  142]    Overall Loss 0.785891    Objective Loss 0.785891                                        LR 0.000250    Time 0.610631    
2023-06-16 00:48:57,294 - Epoch: [32][   50/  142]    Overall Loss 0.788724    Objective Loss 0.788724                                        LR 0.000250    Time 0.595016    
2023-06-16 00:49:02,278 - Epoch: [32][   60/  142]    Overall Loss 0.780837    Objective Loss 0.780837                                        LR 0.000250    Time 0.578902    
2023-06-16 00:49:07,354 - Epoch: [32][   70/  142]    Overall Loss 0.786595    Objective Loss 0.786595                                        LR 0.000250    Time 0.568704    
2023-06-16 00:49:12,461 - Epoch: [32][   80/  142]    Overall Loss 0.793033    Objective Loss 0.793033                                        LR 0.000250    Time 0.561447    
2023-06-16 00:49:17,526 - Epoch: [32][   90/  142]    Overall Loss 0.806757    Objective Loss 0.806757                                        LR 0.000250    Time 0.555345    
2023-06-16 00:49:22,516 - Epoch: [32][  100/  142]    Overall Loss 0.813895    Objective Loss 0.813895                                        LR 0.000250    Time 0.549703    
2023-06-16 00:49:27,569 - Epoch: [32][  110/  142]    Overall Loss 0.819895    Objective Loss 0.819895                                        LR 0.000250    Time 0.545658    
2023-06-16 00:49:32,649 - Epoch: [32][  120/  142]    Overall Loss 0.815981    Objective Loss 0.815981                                        LR 0.000250    Time 0.542515    
2023-06-16 00:49:37,831 - Epoch: [32][  130/  142]    Overall Loss 0.825223    Objective Loss 0.825223                                        LR 0.000250    Time 0.540642    
2023-06-16 00:49:42,442 - Epoch: [32][  140/  142]    Overall Loss 0.824730    Objective Loss 0.824730                                        LR 0.000250    Time 0.534953    
2023-06-16 00:49:43,288 - Epoch: [32][  142/  142]    Overall Loss 0.826286    Objective Loss 0.826286    Top1 68.750000    LR 0.000250    Time 0.533375    
2023-06-16 00:49:43,917 - --- validate (epoch=32)-----------
2023-06-16 00:49:43,917 - 1422 samples (32 per mini-batch)
2023-06-16 00:49:51,761 - Epoch: [32][   10/   45]    Loss 0.934098    Top1 65.312500    
2023-06-16 00:49:56,261 - Epoch: [32][   20/   45]    Loss 0.974854    Top1 65.781250    
2023-06-16 00:50:01,165 - Epoch: [32][   30/   45]    Loss 0.960539    Top1 67.812500    
2023-06-16 00:50:05,530 - Epoch: [32][   40/   45]    Loss 0.972125    Top1 67.890625    
2023-06-16 00:50:06,849 - Epoch: [32][   45/   45]    Loss 0.975560    Top1 67.862166    
2023-06-16 00:50:07,487 - ==> Top1: 67.862    Loss: 0.976

2023-06-16 00:50:07,489 - ==> Best [Top1: 70.816   Sparsity:0.00   Params: 375264 on epoch: 30]
2023-06-16 00:50:07,489 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:50:07,510 - 

2023-06-16 00:50:07,510 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:50:16,863 - Epoch: [33][   10/  142]    Overall Loss 0.803306    Objective Loss 0.803306                                        LR 0.000250    Time 0.935185    
2023-06-16 00:50:21,910 - Epoch: [33][   20/  142]    Overall Loss 0.800630    Objective Loss 0.800630                                        LR 0.000250    Time 0.719886    
2023-06-16 00:50:26,768 - Epoch: [33][   30/  142]    Overall Loss 0.827678    Objective Loss 0.827678                                        LR 0.000250    Time 0.641844    
2023-06-16 00:50:31,633 - Epoch: [33][   40/  142]    Overall Loss 0.832753    Objective Loss 0.832753                                        LR 0.000250    Time 0.603015    
2023-06-16 00:50:36,560 - Epoch: [33][   50/  142]    Overall Loss 0.833053    Objective Loss 0.833053                                        LR 0.000250    Time 0.580934    
2023-06-16 00:50:41,540 - Epoch: [33][   60/  142]    Overall Loss 0.826711    Objective Loss 0.826711                                        LR 0.000250    Time 0.567100    
2023-06-16 00:50:46,451 - Epoch: [33][   70/  142]    Overall Loss 0.819883    Objective Loss 0.819883                                        LR 0.000250    Time 0.556244    
2023-06-16 00:50:51,357 - Epoch: [33][   80/  142]    Overall Loss 0.821451    Objective Loss 0.821451                                        LR 0.000250    Time 0.548027    
2023-06-16 00:50:56,235 - Epoch: [33][   90/  142]    Overall Loss 0.823778    Objective Loss 0.823778                                        LR 0.000250    Time 0.541330    
2023-06-16 00:51:01,253 - Epoch: [33][  100/  142]    Overall Loss 0.831074    Objective Loss 0.831074                                        LR 0.000250    Time 0.537380    
2023-06-16 00:51:06,346 - Epoch: [33][  110/  142]    Overall Loss 0.823094    Objective Loss 0.823094                                        LR 0.000250    Time 0.534819    
2023-06-16 00:51:11,163 - Epoch: [33][  120/  142]    Overall Loss 0.816649    Objective Loss 0.816649                                        LR 0.000250    Time 0.530385    
2023-06-16 00:51:16,148 - Epoch: [33][  130/  142]    Overall Loss 0.812186    Objective Loss 0.812186                                        LR 0.000250    Time 0.527886    
2023-06-16 00:51:20,782 - Epoch: [33][  140/  142]    Overall Loss 0.811905    Objective Loss 0.811905                                        LR 0.000250    Time 0.523279    
2023-06-16 00:51:21,623 - Epoch: [33][  142/  142]    Overall Loss 0.808874    Objective Loss 0.808874    Top1 79.687500    LR 0.000250    Time 0.521824    
2023-06-16 00:51:22,271 - --- validate (epoch=33)-----------
2023-06-16 00:51:22,272 - 1422 samples (32 per mini-batch)
2023-06-16 00:51:30,173 - Epoch: [33][   10/   45]    Loss 0.969110    Top1 71.250000    
2023-06-16 00:51:34,670 - Epoch: [33][   20/   45]    Loss 0.974600    Top1 68.593750    
2023-06-16 00:51:39,196 - Epoch: [33][   30/   45]    Loss 0.954899    Top1 68.645833    
2023-06-16 00:51:43,202 - Epoch: [33][   40/   45]    Loss 0.946049    Top1 68.125000    
2023-06-16 00:51:44,599 - Epoch: [33][   45/   45]    Loss 0.968976    Top1 68.143460    
2023-06-16 00:51:45,258 - ==> Top1: 68.143    Loss: 0.969

2023-06-16 00:51:45,261 - ==> Best [Top1: 70.816   Sparsity:0.00   Params: 375264 on epoch: 30]
2023-06-16 00:51:45,261 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:51:45,282 - 

2023-06-16 00:51:45,282 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:51:54,593 - Epoch: [34][   10/  142]    Overall Loss 0.839550    Objective Loss 0.839550                                        LR 0.000250    Time 0.931059    
2023-06-16 00:51:59,578 - Epoch: [34][   20/  142]    Overall Loss 0.740537    Objective Loss 0.740537                                        LR 0.000250    Time 0.714717    
2023-06-16 00:52:04,560 - Epoch: [34][   30/  142]    Overall Loss 0.784980    Objective Loss 0.784980                                        LR 0.000250    Time 0.642545    
2023-06-16 00:52:09,539 - Epoch: [34][   40/  142]    Overall Loss 0.807105    Objective Loss 0.807105                                        LR 0.000250    Time 0.606362    
2023-06-16 00:52:14,422 - Epoch: [34][   50/  142]    Overall Loss 0.814213    Objective Loss 0.814213                                        LR 0.000250    Time 0.582732    
2023-06-16 00:52:19,360 - Epoch: [34][   60/  142]    Overall Loss 0.824724    Objective Loss 0.824724                                        LR 0.000250    Time 0.567910    
2023-06-16 00:52:24,291 - Epoch: [34][   70/  142]    Overall Loss 0.808394    Objective Loss 0.808394                                        LR 0.000250    Time 0.557217    
2023-06-16 00:52:29,260 - Epoch: [34][   80/  142]    Overall Loss 0.807835    Objective Loss 0.807835                                        LR 0.000250    Time 0.549665    
2023-06-16 00:52:34,192 - Epoch: [34][   90/  142]    Overall Loss 0.813961    Objective Loss 0.813961                                        LR 0.000250    Time 0.543388    
2023-06-16 00:52:39,186 - Epoch: [34][  100/  142]    Overall Loss 0.817397    Objective Loss 0.817397                                        LR 0.000250    Time 0.538983    
2023-06-16 00:52:44,106 - Epoch: [34][  110/  142]    Overall Loss 0.816113    Objective Loss 0.816113                                        LR 0.000250    Time 0.534704    
2023-06-16 00:52:48,931 - Epoch: [34][  120/  142]    Overall Loss 0.804910    Objective Loss 0.804910                                        LR 0.000250    Time 0.530353    
2023-06-16 00:52:53,936 - Epoch: [34][  130/  142]    Overall Loss 0.800053    Objective Loss 0.800053                                        LR 0.000250    Time 0.528047    
2023-06-16 00:52:58,582 - Epoch: [34][  140/  142]    Overall Loss 0.798609    Objective Loss 0.798609                                        LR 0.000250    Time 0.523517    
2023-06-16 00:52:59,436 - Epoch: [34][  142/  142]    Overall Loss 0.798657    Objective Loss 0.798657    Top1 59.375000    LR 0.000250    Time 0.522150    
2023-06-16 00:53:00,080 - --- validate (epoch=34)-----------
2023-06-16 00:53:00,081 - 1422 samples (32 per mini-batch)
2023-06-16 00:53:08,166 - Epoch: [34][   10/   45]    Loss 0.863977    Top1 70.000000    
2023-06-16 00:53:12,535 - Epoch: [34][   20/   45]    Loss 0.809642    Top1 71.562500    
2023-06-16 00:53:17,307 - Epoch: [34][   30/   45]    Loss 0.884715    Top1 69.479167    
2023-06-16 00:53:21,552 - Epoch: [34][   40/   45]    Loss 0.884769    Top1 69.765625    
2023-06-16 00:53:23,005 - Epoch: [34][   45/   45]    Loss 0.902061    Top1 68.846695    
2023-06-16 00:53:23,639 - ==> Top1: 68.847    Loss: 0.902

2023-06-16 00:53:23,641 - ==> Best [Top1: 70.816   Sparsity:0.00   Params: 375264 on epoch: 30]
2023-06-16 00:53:23,641 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:53:23,662 - 

2023-06-16 00:53:23,662 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:53:33,079 - Epoch: [35][   10/  142]    Overall Loss 0.838669    Objective Loss 0.838669                                        LR 0.000125    Time 0.941638    
2023-06-16 00:53:37,987 - Epoch: [35][   20/  142]    Overall Loss 0.747393    Objective Loss 0.747393                                        LR 0.000125    Time 0.716165    
2023-06-16 00:53:42,956 - Epoch: [35][   30/  142]    Overall Loss 0.749190    Objective Loss 0.749190                                        LR 0.000125    Time 0.643041    
2023-06-16 00:53:47,818 - Epoch: [35][   40/  142]    Overall Loss 0.790890    Objective Loss 0.790890                                        LR 0.000125    Time 0.603816    
2023-06-16 00:53:52,669 - Epoch: [35][   50/  142]    Overall Loss 0.783119    Objective Loss 0.783119                                        LR 0.000125    Time 0.580066    
2023-06-16 00:53:57,664 - Epoch: [35][   60/  142]    Overall Loss 0.766908    Objective Loss 0.766908                                        LR 0.000125    Time 0.566632    
2023-06-16 00:54:02,692 - Epoch: [35][   70/  142]    Overall Loss 0.772602    Objective Loss 0.772602                                        LR 0.000125    Time 0.557503    
2023-06-16 00:54:07,505 - Epoch: [35][   80/  142]    Overall Loss 0.749981    Objective Loss 0.749981                                        LR 0.000125    Time 0.547962    
2023-06-16 00:54:12,465 - Epoch: [35][   90/  142]    Overall Loss 0.747363    Objective Loss 0.747363                                        LR 0.000125    Time 0.542193    
2023-06-16 00:54:17,442 - Epoch: [35][  100/  142]    Overall Loss 0.749469    Objective Loss 0.749469                                        LR 0.000125    Time 0.537729    
2023-06-16 00:54:22,352 - Epoch: [35][  110/  142]    Overall Loss 0.757037    Objective Loss 0.757037                                        LR 0.000125    Time 0.533483    
2023-06-16 00:54:27,364 - Epoch: [35][  120/  142]    Overall Loss 0.752383    Objective Loss 0.752383                                        LR 0.000125    Time 0.530781    
2023-06-16 00:54:32,247 - Epoch: [35][  130/  142]    Overall Loss 0.756687    Objective Loss 0.756687                                        LR 0.000125    Time 0.527509    
2023-06-16 00:54:36,922 - Epoch: [35][  140/  142]    Overall Loss 0.754454    Objective Loss 0.754454                                        LR 0.000125    Time 0.523221    
2023-06-16 00:54:37,763 - Epoch: [35][  142/  142]    Overall Loss 0.754401    Objective Loss 0.754401    Top1 76.562500    LR 0.000125    Time 0.521769    
2023-06-16 00:54:38,419 - --- validate (epoch=35)-----------
2023-06-16 00:54:38,419 - 1422 samples (32 per mini-batch)
2023-06-16 00:54:46,578 - Epoch: [35][   10/   45]    Loss 0.902234    Top1 68.750000    
2023-06-16 00:54:50,669 - Epoch: [35][   20/   45]    Loss 0.917681    Top1 68.906250    
2023-06-16 00:54:54,963 - Epoch: [35][   30/   45]    Loss 0.905279    Top1 68.541667    
2023-06-16 00:54:59,558 - Epoch: [35][   40/   45]    Loss 0.893526    Top1 68.359375    
2023-06-16 00:55:01,418 - Epoch: [35][   45/   45]    Loss 0.873615    Top1 68.987342    
2023-06-16 00:55:02,060 - ==> Top1: 68.987    Loss: 0.874

2023-06-16 00:55:02,062 - ==> Best [Top1: 70.816   Sparsity:0.00   Params: 375264 on epoch: 30]
2023-06-16 00:55:02,062 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:55:02,084 - 

2023-06-16 00:55:02,084 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:55:11,183 - Epoch: [36][   10/  142]    Overall Loss 0.827040    Objective Loss 0.827040                                        LR 0.000125    Time 0.909811    
2023-06-16 00:55:16,263 - Epoch: [36][   20/  142]    Overall Loss 0.799099    Objective Loss 0.799099                                        LR 0.000125    Time 0.708879    
2023-06-16 00:55:21,208 - Epoch: [36][   30/  142]    Overall Loss 0.748470    Objective Loss 0.748470                                        LR 0.000125    Time 0.637400    
2023-06-16 00:55:25,953 - Epoch: [36][   40/  142]    Overall Loss 0.739734    Objective Loss 0.739734                                        LR 0.000125    Time 0.596661    
2023-06-16 00:55:30,941 - Epoch: [36][   50/  142]    Overall Loss 0.746670    Objective Loss 0.746670                                        LR 0.000125    Time 0.577087    
2023-06-16 00:55:35,821 - Epoch: [36][   60/  142]    Overall Loss 0.731548    Objective Loss 0.731548                                        LR 0.000125    Time 0.562227    
2023-06-16 00:55:40,811 - Epoch: [36][   70/  142]    Overall Loss 0.733164    Objective Loss 0.733164                                        LR 0.000125    Time 0.553178    
2023-06-16 00:55:45,633 - Epoch: [36][   80/  142]    Overall Loss 0.734287    Objective Loss 0.734287                                        LR 0.000125    Time 0.544294    
2023-06-16 00:55:50,565 - Epoch: [36][   90/  142]    Overall Loss 0.725442    Objective Loss 0.725442                                        LR 0.000125    Time 0.538617    
2023-06-16 00:55:55,486 - Epoch: [36][  100/  142]    Overall Loss 0.720927    Objective Loss 0.720927                                        LR 0.000125    Time 0.533955    
2023-06-16 00:56:00,310 - Epoch: [36][  110/  142]    Overall Loss 0.724100    Objective Loss 0.724100                                        LR 0.000125    Time 0.529270    
2023-06-16 00:56:05,218 - Epoch: [36][  120/  142]    Overall Loss 0.723858    Objective Loss 0.723858                                        LR 0.000125    Time 0.526054    
2023-06-16 00:56:10,223 - Epoch: [36][  130/  142]    Overall Loss 0.728107    Objective Loss 0.728107                                        LR 0.000125    Time 0.524085    
2023-06-16 00:56:14,700 - Epoch: [36][  140/  142]    Overall Loss 0.734341    Objective Loss 0.734341                                        LR 0.000125    Time 0.518625    
2023-06-16 00:56:15,541 - Epoch: [36][  142/  142]    Overall Loss 0.738226    Objective Loss 0.738226    Top1 67.187500    LR 0.000125    Time 0.517240    
2023-06-16 00:56:16,185 - --- validate (epoch=36)-----------
2023-06-16 00:56:16,186 - 1422 samples (32 per mini-batch)
2023-06-16 00:56:24,588 - Epoch: [36][   10/   45]    Loss 0.906318    Top1 68.437500    
2023-06-16 00:56:28,566 - Epoch: [36][   20/   45]    Loss 0.870219    Top1 70.000000    
2023-06-16 00:56:33,290 - Epoch: [36][   30/   45]    Loss 0.863006    Top1 71.666667    
2023-06-16 00:56:37,380 - Epoch: [36][   40/   45]    Loss 0.898805    Top1 70.937500    
2023-06-16 00:56:38,699 - Epoch: [36][   45/   45]    Loss 0.912019    Top1 70.886076    
2023-06-16 00:56:39,360 - ==> Top1: 70.886    Loss: 0.912

2023-06-16 00:56:39,362 - ==> Best [Top1: 70.886   Sparsity:0.00   Params: 375264 on epoch: 36]
2023-06-16 00:56:39,362 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:56:39,387 - 

2023-06-16 00:56:39,387 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:56:48,827 - Epoch: [37][   10/  142]    Overall Loss 0.786684    Objective Loss 0.786684                                        LR 0.000125    Time 0.943842    
2023-06-16 00:56:53,713 - Epoch: [37][   20/  142]    Overall Loss 0.708663    Objective Loss 0.708663                                        LR 0.000125    Time 0.716194    
2023-06-16 00:56:58,787 - Epoch: [37][   30/  142]    Overall Loss 0.689994    Objective Loss 0.689994                                        LR 0.000125    Time 0.646568    
2023-06-16 00:57:03,766 - Epoch: [37][   40/  142]    Overall Loss 0.729969    Objective Loss 0.729969                                        LR 0.000125    Time 0.609404    
2023-06-16 00:57:08,741 - Epoch: [37][   50/  142]    Overall Loss 0.712679    Objective Loss 0.712679                                        LR 0.000125    Time 0.587000    
2023-06-16 00:57:13,769 - Epoch: [37][   60/  142]    Overall Loss 0.705465    Objective Loss 0.705465                                        LR 0.000125    Time 0.572961    
2023-06-16 00:57:18,807 - Epoch: [37][   70/  142]    Overall Loss 0.707433    Objective Loss 0.707433                                        LR 0.000125    Time 0.563076    
2023-06-16 00:57:23,691 - Epoch: [37][   80/  142]    Overall Loss 0.716040    Objective Loss 0.716040                                        LR 0.000125    Time 0.553738    
2023-06-16 00:57:28,715 - Epoch: [37][   90/  142]    Overall Loss 0.722533    Objective Loss 0.722533                                        LR 0.000125    Time 0.548019    
2023-06-16 00:57:33,739 - Epoch: [37][  100/  142]    Overall Loss 0.715245    Objective Loss 0.715245                                        LR 0.000125    Time 0.543450    
2023-06-16 00:57:38,713 - Epoch: [37][  110/  142]    Overall Loss 0.718273    Objective Loss 0.718273                                        LR 0.000125    Time 0.539261    
2023-06-16 00:57:43,684 - Epoch: [37][  120/  142]    Overall Loss 0.725550    Objective Loss 0.725550                                        LR 0.000125    Time 0.535747    
2023-06-16 00:57:48,749 - Epoch: [37][  130/  142]    Overall Loss 0.733412    Objective Loss 0.733412                                        LR 0.000125    Time 0.533494    
2023-06-16 00:57:53,377 - Epoch: [37][  140/  142]    Overall Loss 0.737187    Objective Loss 0.737187                                        LR 0.000125    Time 0.528439    
2023-06-16 00:57:54,220 - Epoch: [37][  142/  142]    Overall Loss 0.735499    Objective Loss 0.735499    Top1 76.562500    LR 0.000125    Time 0.526931    
2023-06-16 00:57:54,879 - --- validate (epoch=37)-----------
2023-06-16 00:57:54,880 - 1422 samples (32 per mini-batch)
2023-06-16 00:58:03,095 - Epoch: [37][   10/   45]    Loss 0.996352    Top1 70.312500    
2023-06-16 00:58:07,108 - Epoch: [37][   20/   45]    Loss 1.047801    Top1 69.062500    
2023-06-16 00:58:11,911 - Epoch: [37][   30/   45]    Loss 1.020117    Top1 69.062500    
2023-06-16 00:58:16,122 - Epoch: [37][   40/   45]    Loss 1.024514    Top1 69.609375    
2023-06-16 00:58:17,488 - Epoch: [37][   45/   45]    Loss 1.021343    Top1 69.901547    
2023-06-16 00:58:18,108 - ==> Top1: 69.902    Loss: 1.021

2023-06-16 00:58:18,110 - ==> Best [Top1: 70.886   Sparsity:0.00   Params: 375264 on epoch: 36]
2023-06-16 00:58:18,110 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:58:18,131 - 

2023-06-16 00:58:18,131 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 00:58:27,462 - Epoch: [38][   10/  142]    Overall Loss 0.757271    Objective Loss 0.757271                                        LR 0.000125    Time 0.933002    
2023-06-16 00:58:32,539 - Epoch: [38][   20/  142]    Overall Loss 0.722619    Objective Loss 0.722619                                        LR 0.000125    Time 0.720278    
2023-06-16 00:58:37,594 - Epoch: [38][   30/  142]    Overall Loss 0.738084    Objective Loss 0.738084                                        LR 0.000125    Time 0.648666    
2023-06-16 00:58:42,602 - Epoch: [38][   40/  142]    Overall Loss 0.714099    Objective Loss 0.714099                                        LR 0.000125    Time 0.611695    
2023-06-16 00:58:47,672 - Epoch: [38][   50/  142]    Overall Loss 0.707161    Objective Loss 0.707161                                        LR 0.000125    Time 0.590752    
2023-06-16 00:58:52,729 - Epoch: [38][   60/  142]    Overall Loss 0.697397    Objective Loss 0.697397                                        LR 0.000125    Time 0.576558    
2023-06-16 00:58:57,739 - Epoch: [38][   70/  142]    Overall Loss 0.720043    Objective Loss 0.720043                                        LR 0.000125    Time 0.565765    
2023-06-16 00:59:02,708 - Epoch: [38][   80/  142]    Overall Loss 0.718489    Objective Loss 0.718489                                        LR 0.000125    Time 0.557143    
2023-06-16 00:59:07,679 - Epoch: [38][   90/  142]    Overall Loss 0.711289    Objective Loss 0.711289                                        LR 0.000125    Time 0.550475    
2023-06-16 00:59:12,746 - Epoch: [38][  100/  142]    Overall Loss 0.722870    Objective Loss 0.722870                                        LR 0.000125    Time 0.546084    
2023-06-16 00:59:17,745 - Epoch: [38][  110/  142]    Overall Loss 0.723362    Objective Loss 0.723362                                        LR 0.000125    Time 0.541882    
2023-06-16 00:59:22,735 - Epoch: [38][  120/  142]    Overall Loss 0.716465    Objective Loss 0.716465                                        LR 0.000125    Time 0.538305    
2023-06-16 00:59:27,739 - Epoch: [38][  130/  142]    Overall Loss 0.714036    Objective Loss 0.714036                                        LR 0.000125    Time 0.535389    
2023-06-16 00:59:32,370 - Epoch: [38][  140/  142]    Overall Loss 0.715933    Objective Loss 0.715933                                        LR 0.000125    Time 0.530219    
2023-06-16 00:59:33,212 - Epoch: [38][  142/  142]    Overall Loss 0.719573    Objective Loss 0.719573    Top1 73.437500    LR 0.000125    Time 0.528683    
2023-06-16 00:59:33,868 - --- validate (epoch=38)-----------
2023-06-16 00:59:33,869 - 1422 samples (32 per mini-batch)
2023-06-16 00:59:42,167 - Epoch: [38][   10/   45]    Loss 0.810246    Top1 70.937500    
2023-06-16 00:59:46,448 - Epoch: [38][   20/   45]    Loss 0.784045    Top1 71.875000    
2023-06-16 00:59:51,753 - Epoch: [38][   30/   45]    Loss 0.817589    Top1 71.770833    
2023-06-16 00:59:56,084 - Epoch: [38][   40/   45]    Loss 0.869058    Top1 70.859375    
2023-06-16 00:59:57,426 - Epoch: [38][   45/   45]    Loss 0.870080    Top1 71.167370    
2023-06-16 00:59:58,075 - ==> Top1: 71.167    Loss: 0.870

2023-06-16 00:59:58,077 - ==> Best [Top1: 71.167   Sparsity:0.00   Params: 375264 on epoch: 38]
2023-06-16 00:59:58,077 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 00:59:58,102 - 

2023-06-16 00:59:58,102 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:00:07,430 - Epoch: [39][   10/  142]    Overall Loss 0.673980    Objective Loss 0.673980                                        LR 0.000125    Time 0.932667    
2023-06-16 01:00:12,416 - Epoch: [39][   20/  142]    Overall Loss 0.680804    Objective Loss 0.680804                                        LR 0.000125    Time 0.715585    
2023-06-16 01:00:17,359 - Epoch: [39][   30/  142]    Overall Loss 0.707305    Objective Loss 0.707305                                        LR 0.000125    Time 0.641800    
2023-06-16 01:00:22,306 - Epoch: [39][   40/  142]    Overall Loss 0.708928    Objective Loss 0.708928                                        LR 0.000125    Time 0.605010    
2023-06-16 01:00:27,275 - Epoch: [39][   50/  142]    Overall Loss 0.704054    Objective Loss 0.704054                                        LR 0.000125    Time 0.583381    
2023-06-16 01:00:32,126 - Epoch: [39][   60/  142]    Overall Loss 0.713064    Objective Loss 0.713064                                        LR 0.000125    Time 0.566980    
2023-06-16 01:00:37,269 - Epoch: [39][   70/  142]    Overall Loss 0.719100    Objective Loss 0.719100                                        LR 0.000125    Time 0.559446    
2023-06-16 01:00:42,120 - Epoch: [39][   80/  142]    Overall Loss 0.714701    Objective Loss 0.714701                                        LR 0.000125    Time 0.550144    
2023-06-16 01:00:47,089 - Epoch: [39][   90/  142]    Overall Loss 0.719181    Objective Loss 0.719181                                        LR 0.000125    Time 0.544229    
2023-06-16 01:00:51,972 - Epoch: [39][  100/  142]    Overall Loss 0.717416    Objective Loss 0.717416                                        LR 0.000125    Time 0.538626    
2023-06-16 01:00:56,853 - Epoch: [39][  110/  142]    Overall Loss 0.720007    Objective Loss 0.720007                                        LR 0.000125    Time 0.534028    
2023-06-16 01:01:01,865 - Epoch: [39][  120/  142]    Overall Loss 0.713524    Objective Loss 0.713524                                        LR 0.000125    Time 0.531290    
2023-06-16 01:01:06,846 - Epoch: [39][  130/  142]    Overall Loss 0.717456    Objective Loss 0.717456                                        LR 0.000125    Time 0.528731    
2023-06-16 01:01:11,409 - Epoch: [39][  140/  142]    Overall Loss 0.721266    Objective Loss 0.721266                                        LR 0.000125    Time 0.523556    
2023-06-16 01:01:12,251 - Epoch: [39][  142/  142]    Overall Loss 0.721677    Objective Loss 0.721677    Top1 79.687500    LR 0.000125    Time 0.522111    
2023-06-16 01:01:12,905 - --- validate (epoch=39)-----------
2023-06-16 01:01:12,905 - 1422 samples (32 per mini-batch)
2023-06-16 01:01:20,790 - Epoch: [39][   10/   45]    Loss 1.006839    Top1 66.250000    
2023-06-16 01:01:24,983 - Epoch: [39][   20/   45]    Loss 0.976225    Top1 67.343750    
2023-06-16 01:01:29,729 - Epoch: [39][   30/   45]    Loss 0.943073    Top1 68.854167    
2023-06-16 01:01:34,238 - Epoch: [39][   40/   45]    Loss 0.926135    Top1 69.375000    
2023-06-16 01:01:35,601 - Epoch: [39][   45/   45]    Loss 0.934940    Top1 68.987342    
2023-06-16 01:01:36,209 - ==> Top1: 68.987    Loss: 0.935

2023-06-16 01:01:36,211 - ==> Best [Top1: 71.167   Sparsity:0.00   Params: 375264 on epoch: 38]
2023-06-16 01:01:36,211 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:01:36,232 - 

2023-06-16 01:01:36,233 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:01:45,673 - Epoch: [40][   10/  142]    Overall Loss 0.691263    Objective Loss 0.691263                                        LR 0.000125    Time 0.943922    
2023-06-16 01:01:50,527 - Epoch: [40][   20/  142]    Overall Loss 0.786658    Objective Loss 0.786658                                        LR 0.000125    Time 0.714615    
2023-06-16 01:01:55,614 - Epoch: [40][   30/  142]    Overall Loss 0.744616    Objective Loss 0.744616                                        LR 0.000125    Time 0.645965    
2023-06-16 01:02:00,504 - Epoch: [40][   40/  142]    Overall Loss 0.722631    Objective Loss 0.722631                                        LR 0.000125    Time 0.606712    
2023-06-16 01:02:05,527 - Epoch: [40][   50/  142]    Overall Loss 0.709117    Objective Loss 0.709117                                        LR 0.000125    Time 0.585815    
2023-06-16 01:02:10,394 - Epoch: [40][   60/  142]    Overall Loss 0.712192    Objective Loss 0.712192                                        LR 0.000125    Time 0.569301    
2023-06-16 01:02:15,492 - Epoch: [40][   70/  142]    Overall Loss 0.718208    Objective Loss 0.718208                                        LR 0.000125    Time 0.560793    
2023-06-16 01:02:20,256 - Epoch: [40][   80/  142]    Overall Loss 0.708345    Objective Loss 0.708345                                        LR 0.000125    Time 0.550232    
2023-06-16 01:02:25,406 - Epoch: [40][   90/  142]    Overall Loss 0.709106    Objective Loss 0.709106                                        LR 0.000125    Time 0.546314    
2023-06-16 01:02:30,426 - Epoch: [40][  100/  142]    Overall Loss 0.704684    Objective Loss 0.704684                                        LR 0.000125    Time 0.541874    
2023-06-16 01:02:35,590 - Epoch: [40][  110/  142]    Overall Loss 0.704835    Objective Loss 0.704835                                        LR 0.000125    Time 0.539555    
2023-06-16 01:02:40,506 - Epoch: [40][  120/  142]    Overall Loss 0.703749    Objective Loss 0.703749                                        LR 0.000125    Time 0.535560    
2023-06-16 01:02:45,468 - Epoch: [40][  130/  142]    Overall Loss 0.700542    Objective Loss 0.700542                                        LR 0.000125    Time 0.532525    
2023-06-16 01:02:50,107 - Epoch: [40][  140/  142]    Overall Loss 0.704759    Objective Loss 0.704759                                        LR 0.000125    Time 0.527622    
2023-06-16 01:02:50,965 - Epoch: [40][  142/  142]    Overall Loss 0.704288    Objective Loss 0.704288    Top1 73.437500    LR 0.000125    Time 0.526228    
2023-06-16 01:02:51,608 - --- validate (epoch=40)-----------
2023-06-16 01:02:51,608 - 1422 samples (32 per mini-batch)
2023-06-16 01:02:59,552 - Epoch: [40][   10/   45]    Loss 0.894201    Top1 71.250000    
2023-06-16 01:03:04,055 - Epoch: [40][   20/   45]    Loss 0.893638    Top1 71.875000    
2023-06-16 01:03:08,165 - Epoch: [40][   30/   45]    Loss 0.926399    Top1 70.416667    
2023-06-16 01:03:12,571 - Epoch: [40][   40/   45]    Loss 0.950919    Top1 69.609375    
2023-06-16 01:03:14,019 - Epoch: [40][   45/   45]    Loss 0.947008    Top1 70.182841    
2023-06-16 01:03:14,604 - ==> Top1: 70.183    Loss: 0.947

2023-06-16 01:03:14,606 - ==> Best [Top1: 71.167   Sparsity:0.00   Params: 375264 on epoch: 38]
2023-06-16 01:03:14,606 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:03:14,627 - 

2023-06-16 01:03:14,627 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:03:23,847 - Epoch: [41][   10/  142]    Overall Loss 0.746216    Objective Loss 0.746216                                        LR 0.000125    Time 0.921808    
2023-06-16 01:03:28,919 - Epoch: [41][   20/  142]    Overall Loss 0.736148    Objective Loss 0.736148                                        LR 0.000125    Time 0.714486    
2023-06-16 01:03:33,827 - Epoch: [41][   30/  142]    Overall Loss 0.726105    Objective Loss 0.726105                                        LR 0.000125    Time 0.639884    
2023-06-16 01:03:38,949 - Epoch: [41][   40/  142]    Overall Loss 0.712628    Objective Loss 0.712628                                        LR 0.000125    Time 0.607961    
2023-06-16 01:03:43,852 - Epoch: [41][   50/  142]    Overall Loss 0.717773    Objective Loss 0.717773                                        LR 0.000125    Time 0.584414    
2023-06-16 01:03:48,907 - Epoch: [41][   60/  142]    Overall Loss 0.708574    Objective Loss 0.708574                                        LR 0.000125    Time 0.571247    
2023-06-16 01:03:53,925 - Epoch: [41][   70/  142]    Overall Loss 0.717730    Objective Loss 0.717730                                        LR 0.000125    Time 0.561321    
2023-06-16 01:03:58,991 - Epoch: [41][   80/  142]    Overall Loss 0.718879    Objective Loss 0.718879                                        LR 0.000125    Time 0.554475    
2023-06-16 01:04:04,048 - Epoch: [41][   90/  142]    Overall Loss 0.713411    Objective Loss 0.713411                                        LR 0.000125    Time 0.548464    
2023-06-16 01:04:09,054 - Epoch: [41][  100/  142]    Overall Loss 0.706912    Objective Loss 0.706912                                        LR 0.000125    Time 0.543675    
2023-06-16 01:04:14,060 - Epoch: [41][  110/  142]    Overall Loss 0.704186    Objective Loss 0.704186                                        LR 0.000125    Time 0.539751    
2023-06-16 01:04:19,071 - Epoch: [41][  120/  142]    Overall Loss 0.705309    Objective Loss 0.705309                                        LR 0.000125    Time 0.536526    
2023-06-16 01:04:23,948 - Epoch: [41][  130/  142]    Overall Loss 0.711849    Objective Loss 0.711849                                        LR 0.000125    Time 0.532767    
2023-06-16 01:04:28,656 - Epoch: [41][  140/  142]    Overall Loss 0.708942    Objective Loss 0.708942                                        LR 0.000125    Time 0.528338    
2023-06-16 01:04:29,505 - Epoch: [41][  142/  142]    Overall Loss 0.708838    Objective Loss 0.708838    Top1 73.437500    LR 0.000125    Time 0.526871    
2023-06-16 01:04:30,115 - --- validate (epoch=41)-----------
2023-06-16 01:04:30,116 - 1422 samples (32 per mini-batch)
2023-06-16 01:04:38,000 - Epoch: [41][   10/   45]    Loss 0.841560    Top1 71.562500    
2023-06-16 01:04:42,259 - Epoch: [41][   20/   45]    Loss 0.818928    Top1 72.812500    
2023-06-16 01:04:47,113 - Epoch: [41][   30/   45]    Loss 0.850415    Top1 72.708333    
2023-06-16 01:04:51,397 - Epoch: [41][   40/   45]    Loss 0.868449    Top1 71.953125    
2023-06-16 01:04:52,765 - Epoch: [41][   45/   45]    Loss 0.887691    Top1 71.518987    
2023-06-16 01:04:53,401 - ==> Top1: 71.519    Loss: 0.888

2023-06-16 01:04:53,403 - ==> Best [Top1: 71.519   Sparsity:0.00   Params: 375264 on epoch: 41]
2023-06-16 01:04:53,403 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:04:53,428 - 

2023-06-16 01:04:53,428 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:05:02,757 - Epoch: [42][   10/  142]    Overall Loss 0.723933    Objective Loss 0.723933                                        LR 0.000125    Time 0.932801    
2023-06-16 01:05:07,787 - Epoch: [42][   20/  142]    Overall Loss 0.753722    Objective Loss 0.753722                                        LR 0.000125    Time 0.717874    
2023-06-16 01:05:12,826 - Epoch: [42][   30/  142]    Overall Loss 0.713593    Objective Loss 0.713593                                        LR 0.000125    Time 0.646541    
2023-06-16 01:05:17,813 - Epoch: [42][   40/  142]    Overall Loss 0.708550    Objective Loss 0.708550                                        LR 0.000125    Time 0.609557    
2023-06-16 01:05:22,840 - Epoch: [42][   50/  142]    Overall Loss 0.702700    Objective Loss 0.702700                                        LR 0.000125    Time 0.588166    
2023-06-16 01:05:27,808 - Epoch: [42][   60/  142]    Overall Loss 0.705428    Objective Loss 0.705428                                        LR 0.000125    Time 0.572928    
2023-06-16 01:05:32,732 - Epoch: [42][   70/  142]    Overall Loss 0.705642    Objective Loss 0.705642                                        LR 0.000125    Time 0.561423    
2023-06-16 01:05:37,716 - Epoch: [42][   80/  142]    Overall Loss 0.710853    Objective Loss 0.710853                                        LR 0.000125    Time 0.553536    
2023-06-16 01:05:42,758 - Epoch: [42][   90/  142]    Overall Loss 0.717101    Objective Loss 0.717101                                        LR 0.000125    Time 0.548049    
2023-06-16 01:05:47,732 - Epoch: [42][  100/  142]    Overall Loss 0.707179    Objective Loss 0.707179                                        LR 0.000125    Time 0.542976    
2023-06-16 01:05:52,758 - Epoch: [42][  110/  142]    Overall Loss 0.706826    Objective Loss 0.706826                                        LR 0.000125    Time 0.539302    
2023-06-16 01:05:57,708 - Epoch: [42][  120/  142]    Overall Loss 0.701792    Objective Loss 0.701792                                        LR 0.000125    Time 0.535605    
2023-06-16 01:06:02,737 - Epoch: [42][  130/  142]    Overall Loss 0.696787    Objective Loss 0.696787                                        LR 0.000125    Time 0.533081    
2023-06-16 01:06:07,388 - Epoch: [42][  140/  142]    Overall Loss 0.700492    Objective Loss 0.700492                                        LR 0.000125    Time 0.528220    
2023-06-16 01:06:08,244 - Epoch: [42][  142/  142]    Overall Loss 0.699948    Objective Loss 0.699948    Top1 81.250000    LR 0.000125    Time 0.526812    
2023-06-16 01:06:08,891 - --- validate (epoch=42)-----------
2023-06-16 01:06:08,891 - 1422 samples (32 per mini-batch)
2023-06-16 01:06:16,910 - Epoch: [42][   10/   45]    Loss 0.814919    Top1 71.250000    
2023-06-16 01:06:21,036 - Epoch: [42][   20/   45]    Loss 0.896184    Top1 70.156250    
2023-06-16 01:06:25,818 - Epoch: [42][   30/   45]    Loss 0.901011    Top1 71.041667    
2023-06-16 01:06:29,931 - Epoch: [42][   40/   45]    Loss 0.891575    Top1 71.484375    
2023-06-16 01:06:31,371 - Epoch: [42][   45/   45]    Loss 0.911736    Top1 71.237693    
2023-06-16 01:06:32,029 - ==> Top1: 71.238    Loss: 0.912

2023-06-16 01:06:32,031 - ==> Best [Top1: 71.519   Sparsity:0.00   Params: 375264 on epoch: 41]
2023-06-16 01:06:32,031 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:06:32,052 - 

2023-06-16 01:06:32,053 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:06:41,285 - Epoch: [43][   10/  142]    Overall Loss 0.668228    Objective Loss 0.668228                                        LR 0.000125    Time 0.923143    
2023-06-16 01:06:46,280 - Epoch: [43][   20/  142]    Overall Loss 0.649025    Objective Loss 0.649025                                        LR 0.000125    Time 0.711278    
2023-06-16 01:06:51,090 - Epoch: [43][   30/  142]    Overall Loss 0.636261    Objective Loss 0.636261                                        LR 0.000125    Time 0.634506    
2023-06-16 01:06:56,214 - Epoch: [43][   40/  142]    Overall Loss 0.659491    Objective Loss 0.659491                                        LR 0.000125    Time 0.603968    
2023-06-16 01:07:01,042 - Epoch: [43][   50/  142]    Overall Loss 0.660969    Objective Loss 0.660969                                        LR 0.000125    Time 0.579712    
2023-06-16 01:07:06,090 - Epoch: [43][   60/  142]    Overall Loss 0.673400    Objective Loss 0.673400                                        LR 0.000125    Time 0.567223    
2023-06-16 01:07:11,062 - Epoch: [43][   70/  142]    Overall Loss 0.676549    Objective Loss 0.676549                                        LR 0.000125    Time 0.557214    
2023-06-16 01:07:15,923 - Epoch: [43][   80/  142]    Overall Loss 0.673267    Objective Loss 0.673267                                        LR 0.000125    Time 0.548308    
2023-06-16 01:07:20,810 - Epoch: [43][   90/  142]    Overall Loss 0.672524    Objective Loss 0.672524                                        LR 0.000125    Time 0.541685    
2023-06-16 01:07:25,776 - Epoch: [43][  100/  142]    Overall Loss 0.670923    Objective Loss 0.670923                                        LR 0.000125    Time 0.537171    
2023-06-16 01:07:30,712 - Epoch: [43][  110/  142]    Overall Loss 0.681286    Objective Loss 0.681286                                        LR 0.000125    Time 0.533208    
2023-06-16 01:07:35,709 - Epoch: [43][  120/  142]    Overall Loss 0.677571    Objective Loss 0.677571                                        LR 0.000125    Time 0.530410    
2023-06-16 01:07:40,622 - Epoch: [43][  130/  142]    Overall Loss 0.682285    Objective Loss 0.682285                                        LR 0.000125    Time 0.527397    
2023-06-16 01:07:45,357 - Epoch: [43][  140/  142]    Overall Loss 0.687716    Objective Loss 0.687716                                        LR 0.000125    Time 0.523541    
2023-06-16 01:07:46,212 - Epoch: [43][  142/  142]    Overall Loss 0.687864    Objective Loss 0.687864    Top1 84.375000    LR 0.000125    Time 0.522186    
2023-06-16 01:07:46,845 - --- validate (epoch=43)-----------
2023-06-16 01:07:46,846 - 1422 samples (32 per mini-batch)
2023-06-16 01:07:55,051 - Epoch: [43][   10/   45]    Loss 0.954381    Top1 70.312500    
2023-06-16 01:07:59,393 - Epoch: [43][   20/   45]    Loss 0.957867    Top1 70.468750    
2023-06-16 01:08:04,024 - Epoch: [43][   30/   45]    Loss 0.954302    Top1 70.208333    
2023-06-16 01:08:07,916 - Epoch: [43][   40/   45]    Loss 0.953210    Top1 70.390625    
2023-06-16 01:08:09,958 - Epoch: [43][   45/   45]    Loss 0.930672    Top1 70.886076    
2023-06-16 01:08:10,617 - ==> Top1: 70.886    Loss: 0.931

2023-06-16 01:08:10,619 - ==> Best [Top1: 71.519   Sparsity:0.00   Params: 375264 on epoch: 41]
2023-06-16 01:08:10,619 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:08:10,640 - 

2023-06-16 01:08:10,640 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:08:20,030 - Epoch: [44][   10/  142]    Overall Loss 0.608337    Objective Loss 0.608337                                        LR 0.000125    Time 0.938844    
2023-06-16 01:08:24,987 - Epoch: [44][   20/  142]    Overall Loss 0.626344    Objective Loss 0.626344                                        LR 0.000125    Time 0.717256    
2023-06-16 01:08:29,993 - Epoch: [44][   30/  142]    Overall Loss 0.621564    Objective Loss 0.621564                                        LR 0.000125    Time 0.645017    
2023-06-16 01:08:35,024 - Epoch: [44][   40/  142]    Overall Loss 0.636825    Objective Loss 0.636825                                        LR 0.000125    Time 0.609519    
2023-06-16 01:08:39,974 - Epoch: [44][   50/  142]    Overall Loss 0.648566    Objective Loss 0.648566                                        LR 0.000125    Time 0.586597    
2023-06-16 01:08:45,022 - Epoch: [44][   60/  142]    Overall Loss 0.666864    Objective Loss 0.666864                                        LR 0.000125    Time 0.572969    
2023-06-16 01:08:50,011 - Epoch: [44][   70/  142]    Overall Loss 0.683114    Objective Loss 0.683114                                        LR 0.000125    Time 0.562367    
2023-06-16 01:08:55,044 - Epoch: [44][   80/  142]    Overall Loss 0.680390    Objective Loss 0.680390                                        LR 0.000125    Time 0.554976    
2023-06-16 01:08:59,990 - Epoch: [44][   90/  142]    Overall Loss 0.681064    Objective Loss 0.681064                                        LR 0.000125    Time 0.548266    
2023-06-16 01:09:04,954 - Epoch: [44][  100/  142]    Overall Loss 0.692542    Objective Loss 0.692542                                        LR 0.000125    Time 0.543071    
2023-06-16 01:09:09,929 - Epoch: [44][  110/  142]    Overall Loss 0.689449    Objective Loss 0.689449                                        LR 0.000125    Time 0.538921    
2023-06-16 01:09:14,994 - Epoch: [44][  120/  142]    Overall Loss 0.694606    Objective Loss 0.694606                                        LR 0.000125    Time 0.536220    
2023-06-16 01:09:20,165 - Epoch: [44][  130/  142]    Overall Loss 0.701586    Objective Loss 0.701586                                        LR 0.000125    Time 0.534745    
2023-06-16 01:09:24,728 - Epoch: [44][  140/  142]    Overall Loss 0.702344    Objective Loss 0.702344                                        LR 0.000125    Time 0.529137    
2023-06-16 01:09:25,586 - Epoch: [44][  142/  142]    Overall Loss 0.699809    Objective Loss 0.699809    Top1 85.937500    LR 0.000125    Time 0.527728    
2023-06-16 01:09:26,228 - --- validate (epoch=44)-----------
2023-06-16 01:09:26,229 - 1422 samples (32 per mini-batch)
2023-06-16 01:09:33,905 - Epoch: [44][   10/   45]    Loss 0.654958    Top1 77.187500    
2023-06-16 01:09:38,841 - Epoch: [44][   20/   45]    Loss 0.813044    Top1 72.031250    
2023-06-16 01:09:43,686 - Epoch: [44][   30/   45]    Loss 0.794560    Top1 72.916667    
2023-06-16 01:09:48,225 - Epoch: [44][   40/   45]    Loss 0.807915    Top1 72.812500    
2023-06-16 01:09:49,589 - Epoch: [44][   45/   45]    Loss 0.819982    Top1 72.644163    
2023-06-16 01:09:50,248 - ==> Top1: 72.644    Loss: 0.820

2023-06-16 01:09:50,250 - ==> Best [Top1: 72.644   Sparsity:0.00   Params: 375264 on epoch: 44]
2023-06-16 01:09:50,250 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:09:50,269 - 

2023-06-16 01:09:50,269 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:09:59,786 - Epoch: [45][   10/  142]    Overall Loss 0.548384    Objective Loss 0.548384                                        LR 0.000125    Time 0.951583    
2023-06-16 01:10:04,863 - Epoch: [45][   20/  142]    Overall Loss 0.607149    Objective Loss 0.607149                                        LR 0.000125    Time 0.729637    
2023-06-16 01:10:09,851 - Epoch: [45][   30/  142]    Overall Loss 0.639210    Objective Loss 0.639210                                        LR 0.000125    Time 0.652661    
2023-06-16 01:10:14,960 - Epoch: [45][   40/  142]    Overall Loss 0.682165    Objective Loss 0.682165                                        LR 0.000125    Time 0.617196    
2023-06-16 01:10:19,962 - Epoch: [45][   50/  142]    Overall Loss 0.687935    Objective Loss 0.687935                                        LR 0.000125    Time 0.593796    
2023-06-16 01:10:25,124 - Epoch: [45][   60/  142]    Overall Loss 0.699428    Objective Loss 0.699428                                        LR 0.000125    Time 0.580856    
2023-06-16 01:10:30,107 - Epoch: [45][   70/  142]    Overall Loss 0.695372    Objective Loss 0.695372                                        LR 0.000125    Time 0.569050    
2023-06-16 01:10:35,101 - Epoch: [45][   80/  142]    Overall Loss 0.687975    Objective Loss 0.687975                                        LR 0.000125    Time 0.560335    
2023-06-16 01:10:40,134 - Epoch: [45][   90/  142]    Overall Loss 0.697399    Objective Loss 0.697399                                        LR 0.000125    Time 0.553998    
2023-06-16 01:10:45,198 - Epoch: [45][  100/  142]    Overall Loss 0.692576    Objective Loss 0.692576                                        LR 0.000125    Time 0.549232    
2023-06-16 01:10:50,242 - Epoch: [45][  110/  142]    Overall Loss 0.691843    Objective Loss 0.691843                                        LR 0.000125    Time 0.545150    
2023-06-16 01:10:55,282 - Epoch: [45][  120/  142]    Overall Loss 0.699033    Objective Loss 0.699033                                        LR 0.000125    Time 0.541719    
2023-06-16 01:11:00,278 - Epoch: [45][  130/  142]    Overall Loss 0.687169    Objective Loss 0.687169                                        LR 0.000125    Time 0.538471    
2023-06-16 01:11:05,057 - Epoch: [45][  140/  142]    Overall Loss 0.696158    Objective Loss 0.696158                                        LR 0.000125    Time 0.534140    
2023-06-16 01:11:05,904 - Epoch: [45][  142/  142]    Overall Loss 0.698302    Objective Loss 0.698302    Top1 60.937500    LR 0.000125    Time 0.532583    
2023-06-16 01:11:06,541 - --- validate (epoch=45)-----------
2023-06-16 01:11:06,542 - 1422 samples (32 per mini-batch)
2023-06-16 01:11:14,478 - Epoch: [45][   10/   45]    Loss 0.808091    Top1 70.937500    
2023-06-16 01:11:19,290 - Epoch: [45][   20/   45]    Loss 0.871238    Top1 69.687500    
2023-06-16 01:11:24,245 - Epoch: [45][   30/   45]    Loss 0.864836    Top1 70.104167    
2023-06-16 01:11:28,711 - Epoch: [45][   40/   45]    Loss 0.877393    Top1 70.390625    
2023-06-16 01:11:30,182 - Epoch: [45][   45/   45]    Loss 0.877396    Top1 70.534459    
2023-06-16 01:11:30,833 - ==> Top1: 70.534    Loss: 0.877

2023-06-16 01:11:30,835 - ==> Best [Top1: 72.644   Sparsity:0.00   Params: 375264 on epoch: 44]
2023-06-16 01:11:30,835 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:11:30,857 - 

2023-06-16 01:11:30,857 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:11:40,061 - Epoch: [46][   10/  142]    Overall Loss 0.730339    Objective Loss 0.730339                                        LR 0.000125    Time 0.920217    
2023-06-16 01:11:45,023 - Epoch: [46][   20/  142]    Overall Loss 0.730393    Objective Loss 0.730393                                        LR 0.000125    Time 0.708178    
2023-06-16 01:11:50,010 - Epoch: [46][   30/  142]    Overall Loss 0.727040    Objective Loss 0.727040                                        LR 0.000125    Time 0.638348    
2023-06-16 01:11:54,972 - Epoch: [46][   40/  142]    Overall Loss 0.700441    Objective Loss 0.700441                                        LR 0.000125    Time 0.602784    
2023-06-16 01:12:00,015 - Epoch: [46][   50/  142]    Overall Loss 0.704395    Objective Loss 0.704395                                        LR 0.000125    Time 0.583073    
2023-06-16 01:12:04,944 - Epoch: [46][   60/  142]    Overall Loss 0.703591    Objective Loss 0.703591                                        LR 0.000125    Time 0.568043    
2023-06-16 01:12:09,971 - Epoch: [46][   70/  142]    Overall Loss 0.698734    Objective Loss 0.698734                                        LR 0.000125    Time 0.558704    
2023-06-16 01:12:14,928 - Epoch: [46][   80/  142]    Overall Loss 0.691693    Objective Loss 0.691693                                        LR 0.000125    Time 0.550816    
2023-06-16 01:12:20,004 - Epoch: [46][   90/  142]    Overall Loss 0.693176    Objective Loss 0.693176                                        LR 0.000125    Time 0.546008    
2023-06-16 01:12:25,044 - Epoch: [46][  100/  142]    Overall Loss 0.690381    Objective Loss 0.690381                                        LR 0.000125    Time 0.541796    
2023-06-16 01:12:30,075 - Epoch: [46][  110/  142]    Overall Loss 0.699375    Objective Loss 0.699375                                        LR 0.000125    Time 0.538274    
2023-06-16 01:12:35,090 - Epoch: [46][  120/  142]    Overall Loss 0.695553    Objective Loss 0.695553                                        LR 0.000125    Time 0.535208    
2023-06-16 01:12:40,129 - Epoch: [46][  130/  142]    Overall Loss 0.688767    Objective Loss 0.688767                                        LR 0.000125    Time 0.532792    
2023-06-16 01:12:44,731 - Epoch: [46][  140/  142]    Overall Loss 0.689051    Objective Loss 0.689051                                        LR 0.000125    Time 0.527603    
2023-06-16 01:12:45,571 - Epoch: [46][  142/  142]    Overall Loss 0.686922    Objective Loss 0.686922    Top1 79.687500    LR 0.000125    Time 0.526086    
2023-06-16 01:12:46,200 - --- validate (epoch=46)-----------
2023-06-16 01:12:46,200 - 1422 samples (32 per mini-batch)
2023-06-16 01:12:54,425 - Epoch: [46][   10/   45]    Loss 0.833375    Top1 76.250000    
2023-06-16 01:12:58,446 - Epoch: [46][   20/   45]    Loss 0.843921    Top1 75.000000    
2023-06-16 01:13:02,535 - Epoch: [46][   30/   45]    Loss 0.879058    Top1 74.062500    
2023-06-16 01:13:06,658 - Epoch: [46][   40/   45]    Loss 0.878311    Top1 73.515625    
2023-06-16 01:13:08,168 - Epoch: [46][   45/   45]    Loss 0.892483    Top1 73.066104    
2023-06-16 01:13:08,819 - ==> Top1: 73.066    Loss: 0.892

2023-06-16 01:13:08,821 - ==> Best [Top1: 73.066   Sparsity:0.00   Params: 375264 on epoch: 46]
2023-06-16 01:13:08,821 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:13:08,845 - 

2023-06-16 01:13:08,846 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:13:18,148 - Epoch: [47][   10/  142]    Overall Loss 0.605850    Objective Loss 0.605850                                        LR 0.000125    Time 0.930103    
2023-06-16 01:13:23,106 - Epoch: [47][   20/  142]    Overall Loss 0.676377    Objective Loss 0.676377                                        LR 0.000125    Time 0.712952    
2023-06-16 01:13:28,090 - Epoch: [47][   30/  142]    Overall Loss 0.668547    Objective Loss 0.668547                                        LR 0.000125    Time 0.641387    
2023-06-16 01:13:32,969 - Epoch: [47][   40/  142]    Overall Loss 0.678653    Objective Loss 0.678653                                        LR 0.000125    Time 0.602999    
2023-06-16 01:13:37,912 - Epoch: [47][   50/  142]    Overall Loss 0.660776    Objective Loss 0.660776                                        LR 0.000125    Time 0.581241    
2023-06-16 01:13:42,894 - Epoch: [47][   60/  142]    Overall Loss 0.668703    Objective Loss 0.668703                                        LR 0.000125    Time 0.567391    
2023-06-16 01:13:47,879 - Epoch: [47][   70/  142]    Overall Loss 0.660043    Objective Loss 0.660043                                        LR 0.000125    Time 0.557550    
2023-06-16 01:13:52,954 - Epoch: [47][   80/  142]    Overall Loss 0.653412    Objective Loss 0.653412                                        LR 0.000125    Time 0.551274    
2023-06-16 01:13:57,977 - Epoch: [47][   90/  142]    Overall Loss 0.662843    Objective Loss 0.662843                                        LR 0.000125    Time 0.545836    
2023-06-16 01:14:02,874 - Epoch: [47][  100/  142]    Overall Loss 0.659705    Objective Loss 0.659705                                        LR 0.000125    Time 0.540210    
2023-06-16 01:14:07,810 - Epoch: [47][  110/  142]    Overall Loss 0.663001    Objective Loss 0.663001                                        LR 0.000125    Time 0.535976    
2023-06-16 01:14:12,858 - Epoch: [47][  120/  142]    Overall Loss 0.665940    Objective Loss 0.665940                                        LR 0.000125    Time 0.533367    
2023-06-16 01:14:17,844 - Epoch: [47][  130/  142]    Overall Loss 0.671006    Objective Loss 0.671006                                        LR 0.000125    Time 0.530694    
2023-06-16 01:14:22,509 - Epoch: [47][  140/  142]    Overall Loss 0.674734    Objective Loss 0.674734                                        LR 0.000125    Time 0.526106    
2023-06-16 01:14:23,352 - Epoch: [47][  142/  142]    Overall Loss 0.677082    Objective Loss 0.677082    Top1 71.875000    LR 0.000125    Time 0.524631    
2023-06-16 01:14:24,013 - --- validate (epoch=47)-----------
2023-06-16 01:14:24,013 - 1422 samples (32 per mini-batch)
2023-06-16 01:14:31,798 - Epoch: [47][   10/   45]    Loss 0.865911    Top1 73.437500    
2023-06-16 01:14:36,520 - Epoch: [47][   20/   45]    Loss 0.880004    Top1 73.437500    
2023-06-16 01:14:41,576 - Epoch: [47][   30/   45]    Loss 0.915140    Top1 71.666667    
2023-06-16 01:14:45,735 - Epoch: [47][   40/   45]    Loss 0.912164    Top1 71.093750    
2023-06-16 01:14:47,297 - Epoch: [47][   45/   45]    Loss 0.890959    Top1 71.589311    
2023-06-16 01:14:47,947 - ==> Top1: 71.589    Loss: 0.891

2023-06-16 01:14:47,949 - ==> Best [Top1: 73.066   Sparsity:0.00   Params: 375264 on epoch: 46]
2023-06-16 01:14:47,949 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:14:47,970 - 

2023-06-16 01:14:47,970 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:14:57,184 - Epoch: [48][   10/  142]    Overall Loss 0.693013    Objective Loss 0.693013                                        LR 0.000125    Time 0.921242    
2023-06-16 01:15:02,014 - Epoch: [48][   20/  142]    Overall Loss 0.667705    Objective Loss 0.667705                                        LR 0.000125    Time 0.702068    
2023-06-16 01:15:06,967 - Epoch: [48][   30/  142]    Overall Loss 0.671088    Objective Loss 0.671088                                        LR 0.000125    Time 0.633126    
2023-06-16 01:15:11,760 - Epoch: [48][   40/  142]    Overall Loss 0.679290    Objective Loss 0.679290                                        LR 0.000125    Time 0.594646    
2023-06-16 01:15:16,744 - Epoch: [48][   50/  142]    Overall Loss 0.687437    Objective Loss 0.687437                                        LR 0.000125    Time 0.575382    
2023-06-16 01:15:21,575 - Epoch: [48][   60/  142]    Overall Loss 0.690094    Objective Loss 0.690094                                        LR 0.000125    Time 0.560003    
2023-06-16 01:15:26,597 - Epoch: [48][   70/  142]    Overall Loss 0.678904    Objective Loss 0.678904                                        LR 0.000125    Time 0.551736    
2023-06-16 01:15:31,378 - Epoch: [48][   80/  142]    Overall Loss 0.676255    Objective Loss 0.676255                                        LR 0.000125    Time 0.542528    
2023-06-16 01:15:36,387 - Epoch: [48][   90/  142]    Overall Loss 0.688823    Objective Loss 0.688823                                        LR 0.000125    Time 0.537889    
2023-06-16 01:15:41,314 - Epoch: [48][  100/  142]    Overall Loss 0.690318    Objective Loss 0.690318                                        LR 0.000125    Time 0.533370    
2023-06-16 01:15:46,225 - Epoch: [48][  110/  142]    Overall Loss 0.692804    Objective Loss 0.692804                                        LR 0.000125    Time 0.529518    
2023-06-16 01:15:51,081 - Epoch: [48][  120/  142]    Overall Loss 0.694347    Objective Loss 0.694347                                        LR 0.000125    Time 0.525854    
2023-06-16 01:15:56,007 - Epoch: [48][  130/  142]    Overall Loss 0.692024    Objective Loss 0.692024                                        LR 0.000125    Time 0.523294    
2023-06-16 01:16:00,526 - Epoch: [48][  140/  142]    Overall Loss 0.689335    Objective Loss 0.689335                                        LR 0.000125    Time 0.518188    
2023-06-16 01:16:01,369 - Epoch: [48][  142/  142]    Overall Loss 0.687247    Objective Loss 0.687247    Top1 82.812500    LR 0.000125    Time 0.516826    
2023-06-16 01:16:01,979 - --- validate (epoch=48)-----------
2023-06-16 01:16:01,979 - 1422 samples (32 per mini-batch)
2023-06-16 01:16:09,836 - Epoch: [48][   10/   45]    Loss 0.951642    Top1 70.937500    
2023-06-16 01:16:14,233 - Epoch: [48][   20/   45]    Loss 0.900285    Top1 71.562500    
2023-06-16 01:16:19,068 - Epoch: [48][   30/   45]    Loss 0.873721    Top1 72.500000    
2023-06-16 01:16:23,455 - Epoch: [48][   40/   45]    Loss 0.877717    Top1 72.187500    
2023-06-16 01:16:24,794 - Epoch: [48][   45/   45]    Loss 0.867927    Top1 71.729958    
2023-06-16 01:16:25,396 - ==> Top1: 71.730    Loss: 0.868

2023-06-16 01:16:25,398 - ==> Best [Top1: 73.066   Sparsity:0.00   Params: 375264 on epoch: 46]
2023-06-16 01:16:25,399 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:16:25,420 - 

2023-06-16 01:16:25,420 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:16:34,667 - Epoch: [49][   10/  142]    Overall Loss 0.651660    Objective Loss 0.651660                                        LR 0.000125    Time 0.924587    
2023-06-16 01:16:39,722 - Epoch: [49][   20/  142]    Overall Loss 0.656501    Objective Loss 0.656501                                        LR 0.000125    Time 0.715040    
2023-06-16 01:16:44,682 - Epoch: [49][   30/  142]    Overall Loss 0.642670    Objective Loss 0.642670                                        LR 0.000125    Time 0.641987    
2023-06-16 01:16:49,617 - Epoch: [49][   40/  142]    Overall Loss 0.637874    Objective Loss 0.637874                                        LR 0.000125    Time 0.604863    
2023-06-16 01:16:54,528 - Epoch: [49][   50/  142]    Overall Loss 0.642202    Objective Loss 0.642202                                        LR 0.000125    Time 0.582104    
2023-06-16 01:16:59,545 - Epoch: [49][   60/  142]    Overall Loss 0.653366    Objective Loss 0.653366                                        LR 0.000125    Time 0.568684    
2023-06-16 01:17:04,525 - Epoch: [49][   70/  142]    Overall Loss 0.654682    Objective Loss 0.654682                                        LR 0.000125    Time 0.558581    
2023-06-16 01:17:09,581 - Epoch: [49][   80/  142]    Overall Loss 0.654198    Objective Loss 0.654198                                        LR 0.000125    Time 0.551955    
2023-06-16 01:17:14,535 - Epoch: [49][   90/  142]    Overall Loss 0.650024    Objective Loss 0.650024                                        LR 0.000125    Time 0.545660    
2023-06-16 01:17:19,369 - Epoch: [49][  100/  142]    Overall Loss 0.645388    Objective Loss 0.645388                                        LR 0.000125    Time 0.539432    
2023-06-16 01:17:24,408 - Epoch: [49][  110/  142]    Overall Loss 0.647516    Objective Loss 0.647516                                        LR 0.000125    Time 0.536200    
2023-06-16 01:17:29,272 - Epoch: [49][  120/  142]    Overall Loss 0.653850    Objective Loss 0.653850                                        LR 0.000125    Time 0.532039    
2023-06-16 01:17:34,251 - Epoch: [49][  130/  142]    Overall Loss 0.661623    Objective Loss 0.661623                                        LR 0.000125    Time 0.529410    
2023-06-16 01:17:38,863 - Epoch: [49][  140/  142]    Overall Loss 0.668466    Objective Loss 0.668466                                        LR 0.000125    Time 0.524536    
2023-06-16 01:17:39,703 - Epoch: [49][  142/  142]    Overall Loss 0.672027    Objective Loss 0.672027    Top1 68.750000    LR 0.000125    Time 0.523063    
2023-06-16 01:17:40,359 - --- validate (epoch=49)-----------
2023-06-16 01:17:40,360 - 1422 samples (32 per mini-batch)
2023-06-16 01:17:48,747 - Epoch: [49][   10/   45]    Loss 0.797093    Top1 73.125000    
2023-06-16 01:17:52,105 - Epoch: [49][   20/   45]    Loss 0.876641    Top1 71.875000    
2023-06-16 01:17:57,057 - Epoch: [49][   30/   45]    Loss 0.890885    Top1 71.666667    
2023-06-16 01:18:01,505 - Epoch: [49][   40/   45]    Loss 0.901002    Top1 71.875000    
2023-06-16 01:18:02,973 - Epoch: [49][   45/   45]    Loss 0.904080    Top1 72.011252    
2023-06-16 01:18:03,627 - ==> Top1: 72.011    Loss: 0.904

2023-06-16 01:18:03,629 - ==> Best [Top1: 73.066   Sparsity:0.00   Params: 375264 on epoch: 46]
2023-06-16 01:18:03,630 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:18:03,651 - 

2023-06-16 01:18:03,651 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:18:13,217 - Epoch: [50][   10/  142]    Overall Loss 0.670860    Objective Loss 0.670860                                        LR 0.000063    Time 0.956454    
2023-06-16 01:18:18,234 - Epoch: [50][   20/  142]    Overall Loss 0.595409    Objective Loss 0.595409                                        LR 0.000063    Time 0.729079    
2023-06-16 01:18:23,273 - Epoch: [50][   30/  142]    Overall Loss 0.627095    Objective Loss 0.627095                                        LR 0.000063    Time 0.653983    
2023-06-16 01:18:28,331 - Epoch: [50][   40/  142]    Overall Loss 0.628555    Objective Loss 0.628555                                        LR 0.000063    Time 0.616911    
2023-06-16 01:18:33,371 - Epoch: [50][   50/  142]    Overall Loss 0.610458    Objective Loss 0.610458                                        LR 0.000063    Time 0.594334    
2023-06-16 01:18:38,424 - Epoch: [50][   60/  142]    Overall Loss 0.613451    Objective Loss 0.613451                                        LR 0.000063    Time 0.579487    
2023-06-16 01:18:43,399 - Epoch: [50][   70/  142]    Overall Loss 0.616212    Objective Loss 0.616212                                        LR 0.000063    Time 0.567759    
2023-06-16 01:18:48,467 - Epoch: [50][   80/  142]    Overall Loss 0.617123    Objective Loss 0.617123                                        LR 0.000063    Time 0.560130    
2023-06-16 01:18:53,592 - Epoch: [50][   90/  142]    Overall Loss 0.617915    Objective Loss 0.617915                                        LR 0.000063    Time 0.554836    
2023-06-16 01:18:58,591 - Epoch: [50][  100/  142]    Overall Loss 0.623435    Objective Loss 0.623435                                        LR 0.000063    Time 0.549339    
2023-06-16 01:19:03,766 - Epoch: [50][  110/  142]    Overall Loss 0.626063    Objective Loss 0.626063                                        LR 0.000063    Time 0.546437    
2023-06-16 01:19:08,796 - Epoch: [50][  120/  142]    Overall Loss 0.626647    Objective Loss 0.626647                                        LR 0.000063    Time 0.542814    
2023-06-16 01:19:13,830 - Epoch: [50][  130/  142]    Overall Loss 0.630307    Objective Loss 0.630307                                        LR 0.000063    Time 0.539778    
2023-06-16 01:19:18,523 - Epoch: [50][  140/  142]    Overall Loss 0.633972    Objective Loss 0.633972                                        LR 0.000063    Time 0.534739    
2023-06-16 01:19:19,377 - Epoch: [50][  142/  142]    Overall Loss 0.632063    Objective Loss 0.632063    Top1 82.812500    LR 0.000063    Time 0.533221    
2023-06-16 01:19:20,042 - --- validate (epoch=50)-----------
2023-06-16 01:19:20,043 - 1422 samples (32 per mini-batch)
2023-06-16 01:19:28,290 - Epoch: [50][   10/   45]    Loss 0.867346    Top1 70.937500    
2023-06-16 01:19:32,230 - Epoch: [50][   20/   45]    Loss 0.884985    Top1 71.718750    
2023-06-16 01:19:36,499 - Epoch: [50][   30/   45]    Loss 0.829173    Top1 72.916667    
2023-06-16 01:19:41,265 - Epoch: [50][   40/   45]    Loss 0.861059    Top1 72.421875    
2023-06-16 01:19:42,648 - Epoch: [50][   45/   45]    Loss 0.852955    Top1 72.925457    
2023-06-16 01:19:43,298 - ==> Top1: 72.925    Loss: 0.853

2023-06-16 01:19:43,300 - ==> Best [Top1: 73.066   Sparsity:0.00   Params: 375264 on epoch: 46]
2023-06-16 01:19:43,300 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:19:43,321 - 

2023-06-16 01:19:43,321 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:19:52,611 - Epoch: [51][   10/  142]    Overall Loss 0.638913    Objective Loss 0.638913                                        LR 0.000063    Time 0.928835    
2023-06-16 01:19:57,509 - Epoch: [51][   20/  142]    Overall Loss 0.663301    Objective Loss 0.663301                                        LR 0.000063    Time 0.709311    
2023-06-16 01:20:02,433 - Epoch: [51][   30/  142]    Overall Loss 0.650033    Objective Loss 0.650033                                        LR 0.000063    Time 0.636965    
2023-06-16 01:20:07,530 - Epoch: [51][   40/  142]    Overall Loss 0.641457    Objective Loss 0.641457                                        LR 0.000063    Time 0.605139    
2023-06-16 01:20:12,469 - Epoch: [51][   50/  142]    Overall Loss 0.634175    Objective Loss 0.634175                                        LR 0.000063    Time 0.582875    
2023-06-16 01:20:17,588 - Epoch: [51][   60/  142]    Overall Loss 0.630969    Objective Loss 0.630969                                        LR 0.000063    Time 0.571038    
2023-06-16 01:20:22,424 - Epoch: [51][   70/  142]    Overall Loss 0.631184    Objective Loss 0.631184                                        LR 0.000063    Time 0.558549    
2023-06-16 01:20:27,426 - Epoch: [51][   80/  142]    Overall Loss 0.634880    Objective Loss 0.634880                                        LR 0.000063    Time 0.551240    
2023-06-16 01:20:32,406 - Epoch: [51][   90/  142]    Overall Loss 0.631525    Objective Loss 0.631525                                        LR 0.000063    Time 0.545325    
2023-06-16 01:20:37,489 - Epoch: [51][  100/  142]    Overall Loss 0.632162    Objective Loss 0.632162                                        LR 0.000063    Time 0.541609    
2023-06-16 01:20:42,394 - Epoch: [51][  110/  142]    Overall Loss 0.639038    Objective Loss 0.639038                                        LR 0.000063    Time 0.536956    
2023-06-16 01:20:47,403 - Epoch: [51][  120/  142]    Overall Loss 0.634476    Objective Loss 0.634476                                        LR 0.000063    Time 0.533946    
2023-06-16 01:20:52,350 - Epoch: [51][  130/  142]    Overall Loss 0.640814    Objective Loss 0.640814                                        LR 0.000063    Time 0.530927    
2023-06-16 01:20:57,032 - Epoch: [51][  140/  142]    Overall Loss 0.642690    Objective Loss 0.642690                                        LR 0.000063    Time 0.526440    
2023-06-16 01:20:57,874 - Epoch: [51][  142/  142]    Overall Loss 0.643922    Objective Loss 0.643922    Top1 76.562500    LR 0.000063    Time 0.524954    
2023-06-16 01:20:58,529 - --- validate (epoch=51)-----------
2023-06-16 01:20:58,530 - 1422 samples (32 per mini-batch)
2023-06-16 01:21:06,540 - Epoch: [51][   10/   45]    Loss 0.906966    Top1 69.687500    
2023-06-16 01:21:10,582 - Epoch: [51][   20/   45]    Loss 0.840751    Top1 71.875000    
2023-06-16 01:21:14,915 - Epoch: [51][   30/   45]    Loss 0.816652    Top1 72.500000    
2023-06-16 01:21:19,568 - Epoch: [51][   40/   45]    Loss 0.806842    Top1 73.515625    
2023-06-16 01:21:21,097 - Epoch: [51][   45/   45]    Loss 0.785876    Top1 73.628692    
2023-06-16 01:21:21,692 - ==> Top1: 73.629    Loss: 0.786

2023-06-16 01:21:21,694 - ==> Best [Top1: 73.629   Sparsity:0.00   Params: 375264 on epoch: 51]
2023-06-16 01:21:21,694 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:21:21,719 - 

2023-06-16 01:21:21,719 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:21:30,978 - Epoch: [52][   10/  142]    Overall Loss 0.608938    Objective Loss 0.608938                                        LR 0.000063    Time 0.925765    
2023-06-16 01:21:35,842 - Epoch: [52][   20/  142]    Overall Loss 0.591904    Objective Loss 0.591904                                        LR 0.000063    Time 0.706068    
2023-06-16 01:21:40,773 - Epoch: [52][   30/  142]    Overall Loss 0.639062    Objective Loss 0.639062                                        LR 0.000063    Time 0.635066    
2023-06-16 01:21:45,597 - Epoch: [52][   40/  142]    Overall Loss 0.650276    Objective Loss 0.650276                                        LR 0.000063    Time 0.596889    
2023-06-16 01:21:50,603 - Epoch: [52][   50/  142]    Overall Loss 0.646187    Objective Loss 0.646187                                        LR 0.000063    Time 0.577616    
2023-06-16 01:21:55,415 - Epoch: [52][   60/  142]    Overall Loss 0.639859    Objective Loss 0.639859                                        LR 0.000063    Time 0.561531    
2023-06-16 01:22:00,310 - Epoch: [52][   70/  142]    Overall Loss 0.631025    Objective Loss 0.631025                                        LR 0.000063    Time 0.551230    
2023-06-16 01:22:05,106 - Epoch: [52][   80/  142]    Overall Loss 0.623266    Objective Loss 0.623266                                        LR 0.000063    Time 0.542269    
2023-06-16 01:22:10,027 - Epoch: [52][   90/  142]    Overall Loss 0.617977    Objective Loss 0.617977                                        LR 0.000063    Time 0.536692    
2023-06-16 01:22:15,025 - Epoch: [52][  100/  142]    Overall Loss 0.619274    Objective Loss 0.619274                                        LR 0.000063    Time 0.532993    
2023-06-16 01:22:19,848 - Epoch: [52][  110/  142]    Overall Loss 0.618338    Objective Loss 0.618338                                        LR 0.000063    Time 0.528379    
2023-06-16 01:22:24,742 - Epoch: [52][  120/  142]    Overall Loss 0.614855    Objective Loss 0.614855                                        LR 0.000063    Time 0.525128    
2023-06-16 01:22:29,624 - Epoch: [52][  130/  142]    Overall Loss 0.623142    Objective Loss 0.623142                                        LR 0.000063    Time 0.522283    
2023-06-16 01:22:34,250 - Epoch: [52][  140/  142]    Overall Loss 0.619083    Objective Loss 0.619083                                        LR 0.000063    Time 0.518020    
2023-06-16 01:22:35,104 - Epoch: [52][  142/  142]    Overall Loss 0.618885    Objective Loss 0.618885    Top1 78.125000    LR 0.000063    Time 0.516736    
2023-06-16 01:22:35,735 - --- validate (epoch=52)-----------
2023-06-16 01:22:35,736 - 1422 samples (32 per mini-batch)
2023-06-16 01:22:43,900 - Epoch: [52][   10/   45]    Loss 0.682992    Top1 73.125000    
2023-06-16 01:22:48,002 - Epoch: [52][   20/   45]    Loss 0.744911    Top1 72.968750    
2023-06-16 01:22:52,422 - Epoch: [52][   30/   45]    Loss 0.730166    Top1 73.958333    
2023-06-16 01:22:57,270 - Epoch: [52][   40/   45]    Loss 0.763014    Top1 73.593750    
2023-06-16 01:22:58,569 - Epoch: [52][   45/   45]    Loss 0.777474    Top1 73.206751    
2023-06-16 01:22:59,208 - ==> Top1: 73.207    Loss: 0.777

2023-06-16 01:22:59,210 - ==> Best [Top1: 73.629   Sparsity:0.00   Params: 375264 on epoch: 51]
2023-06-16 01:22:59,210 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:22:59,231 - 

2023-06-16 01:22:59,231 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:23:08,514 - Epoch: [53][   10/  142]    Overall Loss 0.600538    Objective Loss 0.600538                                        LR 0.000063    Time 0.928190    
2023-06-16 01:23:13,441 - Epoch: [53][   20/  142]    Overall Loss 0.583421    Objective Loss 0.583421                                        LR 0.000063    Time 0.710414    
2023-06-16 01:23:18,486 - Epoch: [53][   30/  142]    Overall Loss 0.594620    Objective Loss 0.594620                                        LR 0.000063    Time 0.641764    
2023-06-16 01:23:23,451 - Epoch: [53][   40/  142]    Overall Loss 0.612206    Objective Loss 0.612206                                        LR 0.000063    Time 0.605437    
2023-06-16 01:23:28,458 - Epoch: [53][   50/  142]    Overall Loss 0.616155    Objective Loss 0.616155                                        LR 0.000063    Time 0.584478    
2023-06-16 01:23:33,311 - Epoch: [53][   60/  142]    Overall Loss 0.616220    Objective Loss 0.616220                                        LR 0.000063    Time 0.567935    
2023-06-16 01:23:38,338 - Epoch: [53][   70/  142]    Overall Loss 0.616135    Objective Loss 0.616135                                        LR 0.000063    Time 0.558605    
2023-06-16 01:23:43,319 - Epoch: [53][   80/  142]    Overall Loss 0.610706    Objective Loss 0.610706                                        LR 0.000063    Time 0.551040    
2023-06-16 01:23:48,306 - Epoch: [53][   90/  142]    Overall Loss 0.609241    Objective Loss 0.609241                                        LR 0.000063    Time 0.545214    
2023-06-16 01:23:53,287 - Epoch: [53][  100/  142]    Overall Loss 0.606627    Objective Loss 0.606627                                        LR 0.000063    Time 0.540497    
2023-06-16 01:23:58,317 - Epoch: [53][  110/  142]    Overall Loss 0.610048    Objective Loss 0.610048                                        LR 0.000063    Time 0.537085    
2023-06-16 01:24:03,191 - Epoch: [53][  120/  142]    Overall Loss 0.616135    Objective Loss 0.616135                                        LR 0.000063    Time 0.532943    
2023-06-16 01:24:08,240 - Epoch: [53][  130/  142]    Overall Loss 0.618922    Objective Loss 0.618922                                        LR 0.000063    Time 0.530779    
2023-06-16 01:24:12,907 - Epoch: [53][  140/  142]    Overall Loss 0.618550    Objective Loss 0.618550                                        LR 0.000063    Time 0.526199    
2023-06-16 01:24:13,753 - Epoch: [53][  142/  142]    Overall Loss 0.621212    Objective Loss 0.621212    Top1 71.875000    LR 0.000063    Time 0.524743    
2023-06-16 01:24:14,419 - --- validate (epoch=53)-----------
2023-06-16 01:24:14,419 - 1422 samples (32 per mini-batch)
2023-06-16 01:24:22,365 - Epoch: [53][   10/   45]    Loss 0.856661    Top1 71.250000    
2023-06-16 01:24:26,976 - Epoch: [53][   20/   45]    Loss 0.829056    Top1 71.562500    
2023-06-16 01:24:31,455 - Epoch: [53][   30/   45]    Loss 0.818536    Top1 71.562500    
2023-06-16 01:24:35,894 - Epoch: [53][   40/   45]    Loss 0.836421    Top1 71.953125    
2023-06-16 01:24:37,243 - Epoch: [53][   45/   45]    Loss 0.820966    Top1 72.573840    
2023-06-16 01:24:37,875 - ==> Top1: 72.574    Loss: 0.821

2023-06-16 01:24:37,877 - ==> Best [Top1: 73.629   Sparsity:0.00   Params: 375264 on epoch: 51]
2023-06-16 01:24:37,877 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:24:37,898 - 

2023-06-16 01:24:37,899 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:24:47,271 - Epoch: [54][   10/  142]    Overall Loss 0.621898    Objective Loss 0.621898                                        LR 0.000063    Time 0.937131    
2023-06-16 01:24:52,213 - Epoch: [54][   20/  142]    Overall Loss 0.614901    Objective Loss 0.614901                                        LR 0.000063    Time 0.715643    
2023-06-16 01:24:57,145 - Epoch: [54][   30/  142]    Overall Loss 0.606533    Objective Loss 0.606533                                        LR 0.000063    Time 0.641464    
2023-06-16 01:25:02,194 - Epoch: [54][   40/  142]    Overall Loss 0.598897    Objective Loss 0.598897                                        LR 0.000063    Time 0.607324    
2023-06-16 01:25:07,222 - Epoch: [54][   50/  142]    Overall Loss 0.597058    Objective Loss 0.597058                                        LR 0.000063    Time 0.586406    
2023-06-16 01:25:12,241 - Epoch: [54][   60/  142]    Overall Loss 0.615793    Objective Loss 0.615793                                        LR 0.000063    Time 0.572317    
2023-06-16 01:25:17,258 - Epoch: [54][   70/  142]    Overall Loss 0.615243    Objective Loss 0.615243                                        LR 0.000063    Time 0.562215    
2023-06-16 01:25:22,280 - Epoch: [54][   80/  142]    Overall Loss 0.616450    Objective Loss 0.616450                                        LR 0.000063    Time 0.554703    
2023-06-16 01:25:27,321 - Epoch: [54][   90/  142]    Overall Loss 0.628686    Objective Loss 0.628686                                        LR 0.000063    Time 0.549074    
2023-06-16 01:25:32,303 - Epoch: [54][  100/  142]    Overall Loss 0.619502    Objective Loss 0.619502                                        LR 0.000063    Time 0.543982    
2023-06-16 01:25:37,336 - Epoch: [54][  110/  142]    Overall Loss 0.628575    Objective Loss 0.628575                                        LR 0.000063    Time 0.540281    
2023-06-16 01:25:42,400 - Epoch: [54][  120/  142]    Overall Loss 0.635131    Objective Loss 0.635131                                        LR 0.000063    Time 0.537449    
2023-06-16 01:25:47,281 - Epoch: [54][  130/  142]    Overall Loss 0.630632    Objective Loss 0.630632                                        LR 0.000063    Time 0.533653    
2023-06-16 01:25:51,975 - Epoch: [54][  140/  142]    Overall Loss 0.629898    Objective Loss 0.629898                                        LR 0.000063    Time 0.529058    
2023-06-16 01:25:52,829 - Epoch: [54][  142/  142]    Overall Loss 0.630465    Objective Loss 0.630465    Top1 75.000000    LR 0.000063    Time 0.527621    
2023-06-16 01:25:53,451 - --- validate (epoch=54)-----------
2023-06-16 01:25:53,452 - 1422 samples (32 per mini-batch)
2023-06-16 01:26:01,558 - Epoch: [54][   10/   45]    Loss 0.820835    Top1 73.437500    
2023-06-16 01:26:05,944 - Epoch: [54][   20/   45]    Loss 0.868198    Top1 72.343750    
2023-06-16 01:26:10,390 - Epoch: [54][   30/   45]    Loss 0.869061    Top1 71.666667    
2023-06-16 01:26:14,716 - Epoch: [54][   40/   45]    Loss 0.876866    Top1 72.187500    
2023-06-16 01:26:16,056 - Epoch: [54][   45/   45]    Loss 0.858008    Top1 72.573840    
2023-06-16 01:26:16,678 - ==> Top1: 72.574    Loss: 0.858

2023-06-16 01:26:16,680 - ==> Best [Top1: 73.629   Sparsity:0.00   Params: 375264 on epoch: 51]
2023-06-16 01:26:16,680 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:26:16,695 - 

2023-06-16 01:26:16,695 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:26:26,204 - Epoch: [55][   10/  142]    Overall Loss 0.645672    Objective Loss 0.645672                                        LR 0.000063    Time 0.950797    
2023-06-16 01:26:31,177 - Epoch: [55][   20/  142]    Overall Loss 0.610127    Objective Loss 0.610127                                        LR 0.000063    Time 0.724013    
2023-06-16 01:26:36,222 - Epoch: [55][   30/  142]    Overall Loss 0.604016    Objective Loss 0.604016                                        LR 0.000063    Time 0.650830    
2023-06-16 01:26:41,177 - Epoch: [55][   40/  142]    Overall Loss 0.611817    Objective Loss 0.611817                                        LR 0.000063    Time 0.611979    
2023-06-16 01:26:46,315 - Epoch: [55][   50/  142]    Overall Loss 0.639047    Objective Loss 0.639047                                        LR 0.000063    Time 0.592331    
2023-06-16 01:26:51,262 - Epoch: [55][   60/  142]    Overall Loss 0.643722    Objective Loss 0.643722                                        LR 0.000063    Time 0.576053    
2023-06-16 01:26:56,220 - Epoch: [55][   70/  142]    Overall Loss 0.646782    Objective Loss 0.646782                                        LR 0.000063    Time 0.564581    
2023-06-16 01:27:01,354 - Epoch: [55][   80/  142]    Overall Loss 0.643853    Objective Loss 0.643853                                        LR 0.000063    Time 0.558181    
2023-06-16 01:27:06,280 - Epoch: [55][   90/  142]    Overall Loss 0.639181    Objective Loss 0.639181                                        LR 0.000063    Time 0.550882    
2023-06-16 01:27:11,333 - Epoch: [55][  100/  142]    Overall Loss 0.632299    Objective Loss 0.632299                                        LR 0.000063    Time 0.546321    
2023-06-16 01:27:16,406 - Epoch: [55][  110/  142]    Overall Loss 0.638022    Objective Loss 0.638022                                        LR 0.000063    Time 0.542767    
2023-06-16 01:27:21,289 - Epoch: [55][  120/  142]    Overall Loss 0.638700    Objective Loss 0.638700                                        LR 0.000063    Time 0.538226    
2023-06-16 01:27:26,382 - Epoch: [55][  130/  142]    Overall Loss 0.637900    Objective Loss 0.637900                                        LR 0.000063    Time 0.535995    
2023-06-16 01:27:30,989 - Epoch: [55][  140/  142]    Overall Loss 0.635890    Objective Loss 0.635890                                        LR 0.000063    Time 0.530611    
2023-06-16 01:27:31,831 - Epoch: [55][  142/  142]    Overall Loss 0.635221    Objective Loss 0.635221    Top1 79.687500    LR 0.000063    Time 0.529069    
2023-06-16 01:27:32,489 - --- validate (epoch=55)-----------
2023-06-16 01:27:32,489 - 1422 samples (32 per mini-batch)
2023-06-16 01:27:40,715 - Epoch: [55][   10/   45]    Loss 0.837007    Top1 73.750000    
2023-06-16 01:27:44,695 - Epoch: [55][   20/   45]    Loss 0.856617    Top1 72.343750    
2023-06-16 01:27:49,647 - Epoch: [55][   30/   45]    Loss 0.838730    Top1 72.916667    
2023-06-16 01:27:54,125 - Epoch: [55][   40/   45]    Loss 0.828757    Top1 72.812500    
2023-06-16 01:27:55,450 - Epoch: [55][   45/   45]    Loss 0.851925    Top1 72.151899    
2023-06-16 01:27:56,086 - ==> Top1: 72.152    Loss: 0.852

2023-06-16 01:27:56,088 - ==> Best [Top1: 73.629   Sparsity:0.00   Params: 375264 on epoch: 51]
2023-06-16 01:27:56,088 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:27:56,102 - 

2023-06-16 01:27:56,102 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:28:05,586 - Epoch: [56][   10/  142]    Overall Loss 0.545985    Objective Loss 0.545985                                        LR 0.000063    Time 0.948291    
2023-06-16 01:28:10,548 - Epoch: [56][   20/  142]    Overall Loss 0.524376    Objective Loss 0.524376                                        LR 0.000063    Time 0.722188    
2023-06-16 01:28:15,567 - Epoch: [56][   30/  142]    Overall Loss 0.551578    Objective Loss 0.551578                                        LR 0.000063    Time 0.648756    
2023-06-16 01:28:20,551 - Epoch: [56][   40/  142]    Overall Loss 0.578315    Objective Loss 0.578315                                        LR 0.000063    Time 0.611132    
2023-06-16 01:28:25,636 - Epoch: [56][   50/  142]    Overall Loss 0.578729    Objective Loss 0.578729                                        LR 0.000063    Time 0.590600    
2023-06-16 01:28:30,615 - Epoch: [56][   60/  142]    Overall Loss 0.585511    Objective Loss 0.585511                                        LR 0.000063    Time 0.575144    
2023-06-16 01:28:35,534 - Epoch: [56][   70/  142]    Overall Loss 0.608282    Objective Loss 0.608282                                        LR 0.000063    Time 0.563241    
2023-06-16 01:28:40,475 - Epoch: [56][   80/  142]    Overall Loss 0.613785    Objective Loss 0.613785                                        LR 0.000063    Time 0.554593    
2023-06-16 01:28:45,514 - Epoch: [56][   90/  142]    Overall Loss 0.613502    Objective Loss 0.613502                                        LR 0.000063    Time 0.548955    
2023-06-16 01:28:50,437 - Epoch: [56][  100/  142]    Overall Loss 0.619898    Objective Loss 0.619898                                        LR 0.000063    Time 0.543286    
2023-06-16 01:28:55,428 - Epoch: [56][  110/  142]    Overall Loss 0.625997    Objective Loss 0.625997                                        LR 0.000063    Time 0.539257    
2023-06-16 01:29:00,505 - Epoch: [56][  120/  142]    Overall Loss 0.622863    Objective Loss 0.622863                                        LR 0.000063    Time 0.536628    
2023-06-16 01:29:05,558 - Epoch: [56][  130/  142]    Overall Loss 0.622073    Objective Loss 0.622073                                        LR 0.000063    Time 0.534213    
2023-06-16 01:29:10,172 - Epoch: [56][  140/  142]    Overall Loss 0.624232    Objective Loss 0.624232                                        LR 0.000063    Time 0.529007    
2023-06-16 01:29:11,015 - Epoch: [56][  142/  142]    Overall Loss 0.624232    Objective Loss 0.624232    Top1 79.687500    LR 0.000063    Time 0.527490    
2023-06-16 01:29:11,630 - --- validate (epoch=56)-----------
2023-06-16 01:29:11,631 - 1422 samples (32 per mini-batch)
2023-06-16 01:29:19,932 - Epoch: [56][   10/   45]    Loss 0.858529    Top1 72.812500    
2023-06-16 01:29:24,005 - Epoch: [56][   20/   45]    Loss 0.874184    Top1 72.187500    
2023-06-16 01:29:28,414 - Epoch: [56][   30/   45]    Loss 0.851190    Top1 72.395833    
2023-06-16 01:29:32,752 - Epoch: [56][   40/   45]    Loss 0.835680    Top1 71.953125    
2023-06-16 01:29:34,123 - Epoch: [56][   45/   45]    Loss 0.817775    Top1 72.433193    
2023-06-16 01:29:34,626 - ==> Top1: 72.433    Loss: 0.818

2023-06-16 01:29:34,628 - ==> Best [Top1: 73.629   Sparsity:0.00   Params: 375264 on epoch: 51]
2023-06-16 01:29:34,628 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:29:34,649 - 

2023-06-16 01:29:34,649 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:29:44,324 - Epoch: [57][   10/  142]    Overall Loss 0.597569    Objective Loss 0.597569                                        LR 0.000063    Time 0.967322    
2023-06-16 01:29:49,214 - Epoch: [57][   20/  142]    Overall Loss 0.575945    Objective Loss 0.575945                                        LR 0.000063    Time 0.728125    
2023-06-16 01:29:54,249 - Epoch: [57][   30/  142]    Overall Loss 0.568330    Objective Loss 0.568330                                        LR 0.000063    Time 0.653256    
2023-06-16 01:29:59,219 - Epoch: [57][   40/  142]    Overall Loss 0.588918    Objective Loss 0.588918                                        LR 0.000063    Time 0.614173    
2023-06-16 01:30:04,324 - Epoch: [57][   50/  142]    Overall Loss 0.593391    Objective Loss 0.593391                                        LR 0.000063    Time 0.593422    
2023-06-16 01:30:09,311 - Epoch: [57][   60/  142]    Overall Loss 0.595580    Objective Loss 0.595580                                        LR 0.000063    Time 0.577627    
2023-06-16 01:30:14,432 - Epoch: [57][   70/  142]    Overall Loss 0.601148    Objective Loss 0.601148                                        LR 0.000063    Time 0.568264    
2023-06-16 01:30:19,427 - Epoch: [57][   80/  142]    Overall Loss 0.603527    Objective Loss 0.603527                                        LR 0.000063    Time 0.559664    
2023-06-16 01:30:24,545 - Epoch: [57][   90/  142]    Overall Loss 0.601739    Objective Loss 0.601739                                        LR 0.000063    Time 0.554339    
2023-06-16 01:30:29,539 - Epoch: [57][  100/  142]    Overall Loss 0.597719    Objective Loss 0.597719                                        LR 0.000063    Time 0.548839    
2023-06-16 01:30:34,467 - Epoch: [57][  110/  142]    Overall Loss 0.606456    Objective Loss 0.606456                                        LR 0.000063    Time 0.543732    
2023-06-16 01:30:39,578 - Epoch: [57][  120/  142]    Overall Loss 0.602404    Objective Loss 0.602404                                        LR 0.000063    Time 0.541016    
2023-06-16 01:30:44,601 - Epoch: [57][  130/  142]    Overall Loss 0.605979    Objective Loss 0.605979                                        LR 0.000063    Time 0.538030    
2023-06-16 01:30:49,294 - Epoch: [57][  140/  142]    Overall Loss 0.610266    Objective Loss 0.610266                                        LR 0.000063    Time 0.533117    
2023-06-16 01:30:50,151 - Epoch: [57][  142/  142]    Overall Loss 0.609521    Objective Loss 0.609521    Top1 82.812500    LR 0.000063    Time 0.531644    
2023-06-16 01:30:50,798 - --- validate (epoch=57)-----------
2023-06-16 01:30:50,799 - 1422 samples (32 per mini-batch)
2023-06-16 01:30:58,826 - Epoch: [57][   10/   45]    Loss 1.048974    Top1 70.312500    
2023-06-16 01:31:03,433 - Epoch: [57][   20/   45]    Loss 0.967961    Top1 69.843750    
2023-06-16 01:31:07,752 - Epoch: [57][   30/   45]    Loss 0.938318    Top1 70.937500    
2023-06-16 01:31:12,064 - Epoch: [57][   40/   45]    Loss 0.886356    Top1 72.109375    
2023-06-16 01:31:13,439 - Epoch: [57][   45/   45]    Loss 0.874064    Top1 72.573840    
2023-06-16 01:31:14,092 - ==> Top1: 72.574    Loss: 0.874

2023-06-16 01:31:14,094 - ==> Best [Top1: 73.629   Sparsity:0.00   Params: 375264 on epoch: 51]
2023-06-16 01:31:14,094 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:31:14,115 - 

2023-06-16 01:31:14,115 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:31:23,577 - Epoch: [58][   10/  142]    Overall Loss 0.631095    Objective Loss 0.631095                                        LR 0.000063    Time 0.946141    
2023-06-16 01:31:28,464 - Epoch: [58][   20/  142]    Overall Loss 0.612371    Objective Loss 0.612371                                        LR 0.000063    Time 0.717392    
2023-06-16 01:31:33,404 - Epoch: [58][   30/  142]    Overall Loss 0.604765    Objective Loss 0.604765                                        LR 0.000063    Time 0.642907    
2023-06-16 01:31:38,412 - Epoch: [58][   40/  142]    Overall Loss 0.576660    Objective Loss 0.576660                                        LR 0.000063    Time 0.607352    
2023-06-16 01:31:43,273 - Epoch: [58][   50/  142]    Overall Loss 0.571331    Objective Loss 0.571331                                        LR 0.000063    Time 0.583103    
2023-06-16 01:31:48,268 - Epoch: [58][   60/  142]    Overall Loss 0.577206    Objective Loss 0.577206                                        LR 0.000063    Time 0.569156    
2023-06-16 01:31:53,236 - Epoch: [58][   70/  142]    Overall Loss 0.582016    Objective Loss 0.582016                                        LR 0.000063    Time 0.558802    
2023-06-16 01:31:58,239 - Epoch: [58][   80/  142]    Overall Loss 0.588588    Objective Loss 0.588588                                        LR 0.000063    Time 0.551483    
2023-06-16 01:32:03,121 - Epoch: [58][   90/  142]    Overall Loss 0.581853    Objective Loss 0.581853                                        LR 0.000063    Time 0.544448    
2023-06-16 01:32:08,125 - Epoch: [58][  100/  142]    Overall Loss 0.592706    Objective Loss 0.592706                                        LR 0.000063    Time 0.540040    
2023-06-16 01:32:13,027 - Epoch: [58][  110/  142]    Overall Loss 0.589092    Objective Loss 0.589092                                        LR 0.000063    Time 0.535497    
2023-06-16 01:32:18,020 - Epoch: [58][  120/  142]    Overall Loss 0.591270    Objective Loss 0.591270                                        LR 0.000063    Time 0.532476    
2023-06-16 01:32:22,969 - Epoch: [58][  130/  142]    Overall Loss 0.589719    Objective Loss 0.589719                                        LR 0.000063    Time 0.529586    
2023-06-16 01:32:27,591 - Epoch: [58][  140/  142]    Overall Loss 0.595238    Objective Loss 0.595238                                        LR 0.000063    Time 0.524763    
2023-06-16 01:32:28,448 - Epoch: [58][  142/  142]    Overall Loss 0.598303    Objective Loss 0.598303    Top1 73.437500    LR 0.000063    Time 0.523409    
2023-06-16 01:32:29,105 - --- validate (epoch=58)-----------
2023-06-16 01:32:29,106 - 1422 samples (32 per mini-batch)
2023-06-16 01:32:37,012 - Epoch: [58][   10/   45]    Loss 0.992572    Top1 66.875000    
2023-06-16 01:32:41,712 - Epoch: [58][   20/   45]    Loss 0.905067    Top1 70.156250    
2023-06-16 01:32:46,203 - Epoch: [58][   30/   45]    Loss 0.876616    Top1 71.354167    
2023-06-16 01:32:50,670 - Epoch: [58][   40/   45]    Loss 0.844824    Top1 72.109375    
2023-06-16 01:32:52,141 - Epoch: [58][   45/   45]    Loss 0.846493    Top1 72.503516    
2023-06-16 01:32:52,794 - ==> Top1: 72.504    Loss: 0.846

2023-06-16 01:32:52,796 - ==> Best [Top1: 73.629   Sparsity:0.00   Params: 375264 on epoch: 51]
2023-06-16 01:32:52,796 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:32:52,809 - 

2023-06-16 01:32:52,810 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:33:02,141 - Epoch: [59][   10/  142]    Overall Loss 0.572574    Objective Loss 0.572574                                        LR 0.000063    Time 0.932982    
2023-06-16 01:33:07,226 - Epoch: [59][   20/  142]    Overall Loss 0.599509    Objective Loss 0.599509                                        LR 0.000063    Time 0.720718    
2023-06-16 01:33:12,093 - Epoch: [59][   30/  142]    Overall Loss 0.589458    Objective Loss 0.589458                                        LR 0.000063    Time 0.642711    
2023-06-16 01:33:17,045 - Epoch: [59][   40/  142]    Overall Loss 0.582899    Objective Loss 0.582899                                        LR 0.000063    Time 0.605804    
2023-06-16 01:33:21,987 - Epoch: [59][   50/  142]    Overall Loss 0.571603    Objective Loss 0.571603                                        LR 0.000063    Time 0.583470    
2023-06-16 01:33:27,015 - Epoch: [59][   60/  142]    Overall Loss 0.597007    Objective Loss 0.597007                                        LR 0.000063    Time 0.570021    
2023-06-16 01:33:31,883 - Epoch: [59][   70/  142]    Overall Loss 0.591494    Objective Loss 0.591494                                        LR 0.000063    Time 0.558117    
2023-06-16 01:33:36,784 - Epoch: [59][   80/  142]    Overall Loss 0.590189    Objective Loss 0.590189                                        LR 0.000063    Time 0.549615    
2023-06-16 01:33:41,779 - Epoch: [59][   90/  142]    Overall Loss 0.591478    Objective Loss 0.591478                                        LR 0.000063    Time 0.544038    
2023-06-16 01:33:46,710 - Epoch: [59][  100/  142]    Overall Loss 0.595403    Objective Loss 0.595403                                        LR 0.000063    Time 0.538932    
2023-06-16 01:33:51,740 - Epoch: [59][  110/  142]    Overall Loss 0.602640    Objective Loss 0.602640                                        LR 0.000063    Time 0.535662    
2023-06-16 01:33:56,742 - Epoch: [59][  120/  142]    Overall Loss 0.603758    Objective Loss 0.603758                                        LR 0.000063    Time 0.532701    
2023-06-16 01:34:01,587 - Epoch: [59][  130/  142]    Overall Loss 0.602714    Objective Loss 0.602714                                        LR 0.000063    Time 0.528985    
2023-06-16 01:34:06,297 - Epoch: [59][  140/  142]    Overall Loss 0.598293    Objective Loss 0.598293                                        LR 0.000063    Time 0.524840    
2023-06-16 01:34:07,151 - Epoch: [59][  142/  142]    Overall Loss 0.597029    Objective Loss 0.597029    Top1 81.250000    LR 0.000063    Time 0.523466    
2023-06-16 01:34:07,765 - --- validate (epoch=59)-----------
2023-06-16 01:34:07,766 - 1422 samples (32 per mini-batch)
2023-06-16 01:34:15,962 - Epoch: [59][   10/   45]    Loss 0.860396    Top1 72.187500    
2023-06-16 01:34:20,001 - Epoch: [59][   20/   45]    Loss 0.877675    Top1 71.875000    
2023-06-16 01:34:24,578 - Epoch: [59][   30/   45]    Loss 0.864219    Top1 71.875000    
2023-06-16 01:34:29,148 - Epoch: [59][   40/   45]    Loss 0.837327    Top1 72.734375    
2023-06-16 01:34:30,532 - Epoch: [59][   45/   45]    Loss 0.839451    Top1 73.066104    
2023-06-16 01:34:31,193 - ==> Top1: 73.066    Loss: 0.839

2023-06-16 01:34:31,195 - ==> Best [Top1: 73.629   Sparsity:0.00   Params: 375264 on epoch: 51]
2023-06-16 01:34:31,195 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:34:31,216 - 

2023-06-16 01:34:31,216 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:34:40,684 - Epoch: [60][   10/  142]    Overall Loss 0.611723    Objective Loss 0.611723                                        LR 0.000063    Time 0.946630    
2023-06-16 01:34:45,653 - Epoch: [60][   20/  142]    Overall Loss 0.636424    Objective Loss 0.636424                                        LR 0.000063    Time 0.721750    
2023-06-16 01:34:50,722 - Epoch: [60][   30/  142]    Overall Loss 0.616506    Objective Loss 0.616506                                        LR 0.000063    Time 0.650129    
2023-06-16 01:34:55,815 - Epoch: [60][   40/  142]    Overall Loss 0.618407    Objective Loss 0.618407                                        LR 0.000063    Time 0.614899    
2023-06-16 01:35:00,843 - Epoch: [60][   50/  142]    Overall Loss 0.617868    Objective Loss 0.617868                                        LR 0.000063    Time 0.592469    
2023-06-16 01:35:05,741 - Epoch: [60][   60/  142]    Overall Loss 0.623139    Objective Loss 0.623139                                        LR 0.000063    Time 0.575350    
2023-06-16 01:35:10,788 - Epoch: [60][   70/  142]    Overall Loss 0.610105    Objective Loss 0.610105                                        LR 0.000063    Time 0.565242    
2023-06-16 01:35:15,840 - Epoch: [60][   80/  142]    Overall Loss 0.610929    Objective Loss 0.610929                                        LR 0.000063    Time 0.557737    
2023-06-16 01:35:20,885 - Epoch: [60][   90/  142]    Overall Loss 0.608032    Objective Loss 0.608032                                        LR 0.000063    Time 0.551815    
2023-06-16 01:35:25,969 - Epoch: [60][  100/  142]    Overall Loss 0.614296    Objective Loss 0.614296                                        LR 0.000063    Time 0.547470    
2023-06-16 01:35:30,964 - Epoch: [60][  110/  142]    Overall Loss 0.605489    Objective Loss 0.605489                                        LR 0.000063    Time 0.543100    
2023-06-16 01:35:35,955 - Epoch: [60][  120/  142]    Overall Loss 0.602888    Objective Loss 0.602888                                        LR 0.000063    Time 0.539424    
2023-06-16 01:35:40,986 - Epoch: [60][  130/  142]    Overall Loss 0.605402    Objective Loss 0.605402                                        LR 0.000063    Time 0.536628    
2023-06-16 01:35:45,773 - Epoch: [60][  140/  142]    Overall Loss 0.599755    Objective Loss 0.599755                                        LR 0.000063    Time 0.532484    
2023-06-16 01:35:46,628 - Epoch: [60][  142/  142]    Overall Loss 0.598090    Objective Loss 0.598090    Top1 87.500000    LR 0.000063    Time 0.531008    
2023-06-16 01:35:47,273 - --- validate (epoch=60)-----------
2023-06-16 01:35:47,274 - 1422 samples (32 per mini-batch)
2023-06-16 01:35:55,505 - Epoch: [60][   10/   45]    Loss 0.915281    Top1 70.625000    
2023-06-16 01:35:59,611 - Epoch: [60][   20/   45]    Loss 0.891721    Top1 71.406250    
2023-06-16 01:36:04,497 - Epoch: [60][   30/   45]    Loss 0.883206    Top1 72.083333    
2023-06-16 01:36:08,667 - Epoch: [60][   40/   45]    Loss 0.846046    Top1 74.140625    
2023-06-16 01:36:10,192 - Epoch: [60][   45/   45]    Loss 0.840360    Top1 73.699015    
2023-06-16 01:36:10,849 - ==> Top1: 73.699    Loss: 0.840

2023-06-16 01:36:10,851 - ==> Best [Top1: 73.699   Sparsity:0.00   Params: 375264 on epoch: 60]
2023-06-16 01:36:10,851 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:36:10,877 - 

2023-06-16 01:36:10,877 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:36:19,934 - Epoch: [61][   10/  142]    Overall Loss 0.539377    Objective Loss 0.539377                                        LR 0.000063    Time 0.905605    
2023-06-16 01:36:25,049 - Epoch: [61][   20/  142]    Overall Loss 0.567527    Objective Loss 0.567527                                        LR 0.000063    Time 0.708533    
2023-06-16 01:36:30,201 - Epoch: [61][   30/  142]    Overall Loss 0.591542    Objective Loss 0.591542                                        LR 0.000063    Time 0.644055    
2023-06-16 01:36:35,187 - Epoch: [61][   40/  142]    Overall Loss 0.601701    Objective Loss 0.601701                                        LR 0.000063    Time 0.607674    
2023-06-16 01:36:40,256 - Epoch: [61][   50/  142]    Overall Loss 0.601741    Objective Loss 0.601741                                        LR 0.000063    Time 0.587525    
2023-06-16 01:36:45,289 - Epoch: [61][   60/  142]    Overall Loss 0.604342    Objective Loss 0.604342                                        LR 0.000063    Time 0.573466    
2023-06-16 01:36:50,135 - Epoch: [61][   70/  142]    Overall Loss 0.593279    Objective Loss 0.593279                                        LR 0.000063    Time 0.560764    
2023-06-16 01:36:55,086 - Epoch: [61][   80/  142]    Overall Loss 0.588178    Objective Loss 0.588178                                        LR 0.000063    Time 0.552559    
2023-06-16 01:37:00,093 - Epoch: [61][   90/  142]    Overall Loss 0.592775    Objective Loss 0.592775                                        LR 0.000063    Time 0.546790    
2023-06-16 01:37:05,056 - Epoch: [61][  100/  142]    Overall Loss 0.587427    Objective Loss 0.587427                                        LR 0.000063    Time 0.541733    
2023-06-16 01:37:10,089 - Epoch: [61][  110/  142]    Overall Loss 0.593382    Objective Loss 0.593382                                        LR 0.000063    Time 0.538228    
2023-06-16 01:37:14,978 - Epoch: [61][  120/  142]    Overall Loss 0.590251    Objective Loss 0.590251                                        LR 0.000063    Time 0.534118    
2023-06-16 01:37:19,962 - Epoch: [61][  130/  142]    Overall Loss 0.598343    Objective Loss 0.598343                                        LR 0.000063    Time 0.531365    
2023-06-16 01:37:24,664 - Epoch: [61][  140/  142]    Overall Loss 0.599555    Objective Loss 0.599555                                        LR 0.000063    Time 0.526990    
2023-06-16 01:37:25,518 - Epoch: [61][  142/  142]    Overall Loss 0.598288    Objective Loss 0.598288    Top1 79.687500    LR 0.000063    Time 0.525578    
2023-06-16 01:37:26,141 - --- validate (epoch=61)-----------
2023-06-16 01:37:26,142 - 1422 samples (32 per mini-batch)
2023-06-16 01:37:33,988 - Epoch: [61][   10/   45]    Loss 0.743412    Top1 77.187500    
2023-06-16 01:37:39,177 - Epoch: [61][   20/   45]    Loss 0.838652    Top1 74.062500    
2023-06-16 01:37:43,535 - Epoch: [61][   30/   45]    Loss 0.871849    Top1 72.916667    
2023-06-16 01:37:47,776 - Epoch: [61][   40/   45]    Loss 0.865948    Top1 72.890625    
2023-06-16 01:37:49,086 - Epoch: [61][   45/   45]    Loss 0.868053    Top1 73.558368    
2023-06-16 01:37:49,695 - ==> Top1: 73.558    Loss: 0.868

2023-06-16 01:37:49,697 - ==> Best [Top1: 73.699   Sparsity:0.00   Params: 375264 on epoch: 60]
2023-06-16 01:37:49,697 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:37:49,711 - 

2023-06-16 01:37:49,711 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:37:58,872 - Epoch: [62][   10/  142]    Overall Loss 0.531823    Objective Loss 0.531823                                        LR 0.000063    Time 0.916013    
2023-06-16 01:38:03,819 - Epoch: [62][   20/  142]    Overall Loss 0.502898    Objective Loss 0.502898                                        LR 0.000063    Time 0.705312    
2023-06-16 01:38:08,729 - Epoch: [62][   30/  142]    Overall Loss 0.542873    Objective Loss 0.542873                                        LR 0.000063    Time 0.633834    
2023-06-16 01:38:13,784 - Epoch: [62][   40/  142]    Overall Loss 0.542320    Objective Loss 0.542320                                        LR 0.000063    Time 0.601753    
2023-06-16 01:38:18,610 - Epoch: [62][   50/  142]    Overall Loss 0.542137    Objective Loss 0.542137                                        LR 0.000063    Time 0.577901    
2023-06-16 01:38:23,630 - Epoch: [62][   60/  142]    Overall Loss 0.561546    Objective Loss 0.561546                                        LR 0.000063    Time 0.565237    
2023-06-16 01:38:28,487 - Epoch: [62][   70/  142]    Overall Loss 0.560592    Objective Loss 0.560592                                        LR 0.000063    Time 0.553862    
2023-06-16 01:38:33,505 - Epoch: [62][   80/  142]    Overall Loss 0.563866    Objective Loss 0.563866                                        LR 0.000063    Time 0.547355    
2023-06-16 01:38:38,518 - Epoch: [62][   90/  142]    Overall Loss 0.568496    Objective Loss 0.568496                                        LR 0.000063    Time 0.542230    
2023-06-16 01:38:43,482 - Epoch: [62][  100/  142]    Overall Loss 0.578367    Objective Loss 0.578367                                        LR 0.000063    Time 0.537643    
2023-06-16 01:38:48,405 - Epoch: [62][  110/  142]    Overall Loss 0.589636    Objective Loss 0.589636                                        LR 0.000063    Time 0.533514    
2023-06-16 01:38:53,409 - Epoch: [62][  120/  142]    Overall Loss 0.589075    Objective Loss 0.589075                                        LR 0.000063    Time 0.530754    
2023-06-16 01:38:58,310 - Epoch: [62][  130/  142]    Overall Loss 0.595037    Objective Loss 0.595037                                        LR 0.000063    Time 0.527623    
2023-06-16 01:39:02,940 - Epoch: [62][  140/  142]    Overall Loss 0.595065    Objective Loss 0.595065                                        LR 0.000063    Time 0.523004    
2023-06-16 01:39:03,781 - Epoch: [62][  142/  142]    Overall Loss 0.596135    Objective Loss 0.596135    Top1 81.250000    LR 0.000063    Time 0.521559    
2023-06-16 01:39:04,417 - --- validate (epoch=62)-----------
2023-06-16 01:39:04,417 - 1422 samples (32 per mini-batch)
2023-06-16 01:39:12,684 - Epoch: [62][   10/   45]    Loss 0.831711    Top1 72.500000    
2023-06-16 01:39:16,697 - Epoch: [62][   20/   45]    Loss 0.874370    Top1 73.281250    
2023-06-16 01:39:21,179 - Epoch: [62][   30/   45]    Loss 0.828787    Top1 74.583333    
2023-06-16 01:39:25,240 - Epoch: [62][   40/   45]    Loss 0.872426    Top1 73.828125    
2023-06-16 01:39:26,931 - Epoch: [62][   45/   45]    Loss 0.875876    Top1 73.558368    
2023-06-16 01:39:27,583 - ==> Top1: 73.558    Loss: 0.876

2023-06-16 01:39:27,585 - ==> Best [Top1: 73.699   Sparsity:0.00   Params: 375264 on epoch: 60]
2023-06-16 01:39:27,585 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:39:27,606 - 

2023-06-16 01:39:27,606 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:39:36,805 - Epoch: [63][   10/  142]    Overall Loss 0.696908    Objective Loss 0.696908                                        LR 0.000063    Time 0.919782    
2023-06-16 01:39:41,860 - Epoch: [63][   20/  142]    Overall Loss 0.625260    Objective Loss 0.625260                                        LR 0.000063    Time 0.712636    
2023-06-16 01:39:46,786 - Epoch: [63][   30/  142]    Overall Loss 0.623802    Objective Loss 0.623802                                        LR 0.000063    Time 0.639263    
2023-06-16 01:39:51,839 - Epoch: [63][   40/  142]    Overall Loss 0.617774    Objective Loss 0.617774                                        LR 0.000063    Time 0.605755    
2023-06-16 01:39:56,795 - Epoch: [63][   50/  142]    Overall Loss 0.630670    Objective Loss 0.630670                                        LR 0.000063    Time 0.583709    
2023-06-16 01:40:01,791 - Epoch: [63][   60/  142]    Overall Loss 0.643017    Objective Loss 0.643017                                        LR 0.000063    Time 0.569679    
2023-06-16 01:40:06,816 - Epoch: [63][   70/  142]    Overall Loss 0.633751    Objective Loss 0.633751                                        LR 0.000063    Time 0.560066    
2023-06-16 01:40:11,686 - Epoch: [63][   80/  142]    Overall Loss 0.631069    Objective Loss 0.631069                                        LR 0.000063    Time 0.550932    
2023-06-16 01:40:16,738 - Epoch: [63][   90/  142]    Overall Loss 0.623454    Objective Loss 0.623454                                        LR 0.000063    Time 0.545840    
2023-06-16 01:40:21,748 - Epoch: [63][  100/  142]    Overall Loss 0.618915    Objective Loss 0.618915                                        LR 0.000063    Time 0.541352    
2023-06-16 01:40:26,686 - Epoch: [63][  110/  142]    Overall Loss 0.611056    Objective Loss 0.611056                                        LR 0.000063    Time 0.537031    
2023-06-16 01:40:31,727 - Epoch: [63][  120/  142]    Overall Loss 0.610697    Objective Loss 0.610697                                        LR 0.000063    Time 0.534282    
2023-06-16 01:40:36,873 - Epoch: [63][  130/  142]    Overall Loss 0.604492    Objective Loss 0.604492                                        LR 0.000063    Time 0.532761    
2023-06-16 01:40:41,450 - Epoch: [63][  140/  142]    Overall Loss 0.609542    Objective Loss 0.609542                                        LR 0.000063    Time 0.527394    
2023-06-16 01:40:42,293 - Epoch: [63][  142/  142]    Overall Loss 0.613435    Objective Loss 0.613435    Top1 67.187500    LR 0.000063    Time 0.525905    
2023-06-16 01:40:42,932 - --- validate (epoch=63)-----------
2023-06-16 01:40:42,933 - 1422 samples (32 per mini-batch)
2023-06-16 01:40:51,294 - Epoch: [63][   10/   45]    Loss 0.891763    Top1 71.875000    
2023-06-16 01:40:55,491 - Epoch: [63][   20/   45]    Loss 0.872845    Top1 72.656250    
2023-06-16 01:40:59,969 - Epoch: [63][   30/   45]    Loss 0.857987    Top1 72.708333    
2023-06-16 01:41:05,217 - Epoch: [63][   40/   45]    Loss 0.880384    Top1 71.562500    
2023-06-16 01:41:06,558 - Epoch: [63][   45/   45]    Loss 0.901863    Top1 71.237693    
2023-06-16 01:41:07,203 - ==> Top1: 71.238    Loss: 0.902

2023-06-16 01:41:07,205 - ==> Best [Top1: 73.699   Sparsity:0.00   Params: 375264 on epoch: 60]
2023-06-16 01:41:07,205 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:41:07,226 - 

2023-06-16 01:41:07,226 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:41:16,573 - Epoch: [64][   10/  142]    Overall Loss 0.657152    Objective Loss 0.657152                                        LR 0.000063    Time 0.934536    
2023-06-16 01:41:21,550 - Epoch: [64][   20/  142]    Overall Loss 0.557758    Objective Loss 0.557758                                        LR 0.000063    Time 0.716073    
2023-06-16 01:41:26,570 - Epoch: [64][   30/  142]    Overall Loss 0.560402    Objective Loss 0.560402                                        LR 0.000063    Time 0.644686    
2023-06-16 01:41:31,697 - Epoch: [64][   40/  142]    Overall Loss 0.563774    Objective Loss 0.563774                                        LR 0.000063    Time 0.611692    
2023-06-16 01:41:36,718 - Epoch: [64][   50/  142]    Overall Loss 0.581167    Objective Loss 0.581167                                        LR 0.000063    Time 0.589761    
2023-06-16 01:41:41,820 - Epoch: [64][   60/  142]    Overall Loss 0.582909    Objective Loss 0.582909                                        LR 0.000063    Time 0.576496    
2023-06-16 01:41:46,961 - Epoch: [64][   70/  142]    Overall Loss 0.576343    Objective Loss 0.576343                                        LR 0.000063    Time 0.567564    
2023-06-16 01:41:51,898 - Epoch: [64][   80/  142]    Overall Loss 0.589855    Objective Loss 0.589855                                        LR 0.000063    Time 0.558332    
2023-06-16 01:41:56,886 - Epoch: [64][   90/  142]    Overall Loss 0.594356    Objective Loss 0.594356                                        LR 0.000063    Time 0.551704    
2023-06-16 01:42:02,008 - Epoch: [64][  100/  142]    Overall Loss 0.593101    Objective Loss 0.593101                                        LR 0.000063    Time 0.547750    
2023-06-16 01:42:07,049 - Epoch: [64][  110/  142]    Overall Loss 0.595560    Objective Loss 0.595560                                        LR 0.000063    Time 0.543783    
2023-06-16 01:42:12,115 - Epoch: [64][  120/  142]    Overall Loss 0.587984    Objective Loss 0.587984                                        LR 0.000063    Time 0.540677    
2023-06-16 01:42:17,199 - Epoch: [64][  130/  142]    Overall Loss 0.588421    Objective Loss 0.588421                                        LR 0.000063    Time 0.538186    
2023-06-16 01:42:21,944 - Epoch: [64][  140/  142]    Overall Loss 0.593625    Objective Loss 0.593625                                        LR 0.000063    Time 0.533635    
2023-06-16 01:42:22,787 - Epoch: [64][  142/  142]    Overall Loss 0.593448    Objective Loss 0.593448    Top1 84.375000    LR 0.000063    Time 0.532054    
2023-06-16 01:42:23,438 - --- validate (epoch=64)-----------
2023-06-16 01:42:23,439 - 1422 samples (32 per mini-batch)
2023-06-16 01:42:31,336 - Epoch: [64][   10/   45]    Loss 0.983892    Top1 69.375000    
2023-06-16 01:42:35,602 - Epoch: [64][   20/   45]    Loss 0.873858    Top1 72.343750    
2023-06-16 01:42:40,039 - Epoch: [64][   30/   45]    Loss 0.871755    Top1 72.708333    
2023-06-16 01:42:44,711 - Epoch: [64][   40/   45]    Loss 0.860935    Top1 73.046875    
2023-06-16 01:42:46,053 - Epoch: [64][   45/   45]    Loss 0.872469    Top1 72.995781    
2023-06-16 01:42:46,706 - ==> Top1: 72.996    Loss: 0.872

2023-06-16 01:42:46,708 - ==> Best [Top1: 73.699   Sparsity:0.00   Params: 375264 on epoch: 60]
2023-06-16 01:42:46,708 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:42:46,729 - 

2023-06-16 01:42:46,729 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:42:56,144 - Epoch: [65][   10/  142]    Overall Loss 0.592064    Objective Loss 0.592064                                        LR 0.000063    Time 0.941407    
2023-06-16 01:43:01,100 - Epoch: [65][   20/  142]    Overall Loss 0.600330    Objective Loss 0.600330                                        LR 0.000063    Time 0.718430    
2023-06-16 01:43:06,070 - Epoch: [65][   30/  142]    Overall Loss 0.591318    Objective Loss 0.591318                                        LR 0.000063    Time 0.644611    
2023-06-16 01:43:11,078 - Epoch: [65][   40/  142]    Overall Loss 0.603901    Objective Loss 0.603901                                        LR 0.000063    Time 0.608634    
2023-06-16 01:43:16,005 - Epoch: [65][   50/  142]    Overall Loss 0.604913    Objective Loss 0.604913                                        LR 0.000063    Time 0.585447    
2023-06-16 01:43:21,163 - Epoch: [65][   60/  142]    Overall Loss 0.607950    Objective Loss 0.607950                                        LR 0.000063    Time 0.573824    
2023-06-16 01:43:26,096 - Epoch: [65][   70/  142]    Overall Loss 0.609647    Objective Loss 0.609647                                        LR 0.000063    Time 0.562315    
2023-06-16 01:43:31,115 - Epoch: [65][   80/  142]    Overall Loss 0.599872    Objective Loss 0.599872                                        LR 0.000063    Time 0.554754    
2023-06-16 01:43:36,162 - Epoch: [65][   90/  142]    Overall Loss 0.592651    Objective Loss 0.592651                                        LR 0.000063    Time 0.549188    
2023-06-16 01:43:41,143 - Epoch: [65][  100/  142]    Overall Loss 0.598415    Objective Loss 0.598415                                        LR 0.000063    Time 0.544077    
2023-06-16 01:43:46,235 - Epoch: [65][  110/  142]    Overall Loss 0.601929    Objective Loss 0.601929                                        LR 0.000063    Time 0.540902    
2023-06-16 01:43:51,272 - Epoch: [65][  120/  142]    Overall Loss 0.606869    Objective Loss 0.606869                                        LR 0.000063    Time 0.537790    
2023-06-16 01:43:56,226 - Epoch: [65][  130/  142]    Overall Loss 0.605465    Objective Loss 0.605465                                        LR 0.000063    Time 0.534528    
2023-06-16 01:44:01,054 - Epoch: [65][  140/  142]    Overall Loss 0.600795    Objective Loss 0.600795                                        LR 0.000063    Time 0.530833    
2023-06-16 01:44:01,908 - Epoch: [65][  142/  142]    Overall Loss 0.597071    Objective Loss 0.597071    Top1 84.375000    LR 0.000063    Time 0.529366    
2023-06-16 01:44:02,562 - --- validate (epoch=65)-----------
2023-06-16 01:44:02,562 - 1422 samples (32 per mini-batch)
2023-06-16 01:44:10,478 - Epoch: [65][   10/   45]    Loss 0.996306    Top1 74.062500    
2023-06-16 01:44:15,060 - Epoch: [65][   20/   45]    Loss 0.856490    Top1 76.093750    
2023-06-16 01:44:19,816 - Epoch: [65][   30/   45]    Loss 0.840891    Top1 75.000000    
2023-06-16 01:44:24,330 - Epoch: [65][   40/   45]    Loss 0.858212    Top1 74.218750    
2023-06-16 01:44:25,672 - Epoch: [65][   45/   45]    Loss 0.858622    Top1 73.909986    
2023-06-16 01:44:26,323 - ==> Top1: 73.910    Loss: 0.859

2023-06-16 01:44:26,325 - ==> Best [Top1: 73.910   Sparsity:0.00   Params: 375264 on epoch: 65]
2023-06-16 01:44:26,325 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:44:26,350 - 

2023-06-16 01:44:26,351 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:44:35,531 - Epoch: [66][   10/  142]    Overall Loss 0.532606    Objective Loss 0.532606                                        LR 0.000063    Time 0.917919    
2023-06-16 01:44:40,443 - Epoch: [66][   20/  142]    Overall Loss 0.569054    Objective Loss 0.569054                                        LR 0.000063    Time 0.704547    
2023-06-16 01:44:45,419 - Epoch: [66][   30/  142]    Overall Loss 0.550832    Objective Loss 0.550832                                        LR 0.000063    Time 0.635544    
2023-06-16 01:44:50,342 - Epoch: [66][   40/  142]    Overall Loss 0.557968    Objective Loss 0.557968                                        LR 0.000063    Time 0.599700    
2023-06-16 01:44:55,220 - Epoch: [66][   50/  142]    Overall Loss 0.551017    Objective Loss 0.551017                                        LR 0.000063    Time 0.577322    
2023-06-16 01:45:00,135 - Epoch: [66][   60/  142]    Overall Loss 0.566355    Objective Loss 0.566355                                        LR 0.000063    Time 0.563007    
2023-06-16 01:45:05,114 - Epoch: [66][   70/  142]    Overall Loss 0.573886    Objective Loss 0.573886                                        LR 0.000063    Time 0.553697    
2023-06-16 01:45:10,050 - Epoch: [66][   80/  142]    Overall Loss 0.582470    Objective Loss 0.582470                                        LR 0.000063    Time 0.546177    
2023-06-16 01:45:14,872 - Epoch: [66][   90/  142]    Overall Loss 0.571902    Objective Loss 0.571902                                        LR 0.000063    Time 0.539064    
2023-06-16 01:45:19,793 - Epoch: [66][  100/  142]    Overall Loss 0.570363    Objective Loss 0.570363                                        LR 0.000063    Time 0.534361    
2023-06-16 01:45:24,694 - Epoch: [66][  110/  142]    Overall Loss 0.573497    Objective Loss 0.573497                                        LR 0.000063    Time 0.530338    
2023-06-16 01:45:29,601 - Epoch: [66][  120/  142]    Overall Loss 0.580941    Objective Loss 0.580941                                        LR 0.000063    Time 0.527031    
2023-06-16 01:45:34,531 - Epoch: [66][  130/  142]    Overall Loss 0.587151    Objective Loss 0.587151                                        LR 0.000063    Time 0.524403    
2023-06-16 01:45:39,147 - Epoch: [66][  140/  142]    Overall Loss 0.588944    Objective Loss 0.588944                                        LR 0.000063    Time 0.519916    
2023-06-16 01:45:39,990 - Epoch: [66][  142/  142]    Overall Loss 0.589905    Objective Loss 0.589905    Top1 78.125000    LR 0.000063    Time 0.518527    
2023-06-16 01:45:40,624 - --- validate (epoch=66)-----------
2023-06-16 01:45:40,624 - 1422 samples (32 per mini-batch)
2023-06-16 01:45:48,525 - Epoch: [66][   10/   45]    Loss 1.103389    Top1 67.500000    
2023-06-16 01:45:53,348 - Epoch: [66][   20/   45]    Loss 0.960444    Top1 69.687500    
2023-06-16 01:45:58,026 - Epoch: [66][   30/   45]    Loss 0.891386    Top1 71.875000    
2023-06-16 01:46:02,095 - Epoch: [66][   40/   45]    Loss 0.896096    Top1 72.343750    
2023-06-16 01:46:03,636 - Epoch: [66][   45/   45]    Loss 0.886940    Top1 72.433193    
2023-06-16 01:46:04,276 - ==> Top1: 72.433    Loss: 0.887

2023-06-16 01:46:04,278 - ==> Best [Top1: 73.910   Sparsity:0.00   Params: 375264 on epoch: 65]
2023-06-16 01:46:04,278 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:46:04,295 - 

2023-06-16 01:46:04,295 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:46:13,702 - Epoch: [67][   10/  142]    Overall Loss 0.616884    Objective Loss 0.616884                                        LR 0.000063    Time 0.940560    
2023-06-16 01:46:18,628 - Epoch: [67][   20/  142]    Overall Loss 0.602878    Objective Loss 0.602878                                        LR 0.000063    Time 0.716586    
2023-06-16 01:46:23,592 - Epoch: [67][   30/  142]    Overall Loss 0.628553    Objective Loss 0.628553                                        LR 0.000063    Time 0.643162    
2023-06-16 01:46:28,713 - Epoch: [67][   40/  142]    Overall Loss 0.624431    Objective Loss 0.624431                                        LR 0.000063    Time 0.610387    
2023-06-16 01:46:33,623 - Epoch: [67][   50/  142]    Overall Loss 0.610487    Objective Loss 0.610487                                        LR 0.000063    Time 0.586498    
2023-06-16 01:46:38,667 - Epoch: [67][   60/  142]    Overall Loss 0.610509    Objective Loss 0.610509                                        LR 0.000063    Time 0.572793    
2023-06-16 01:46:43,736 - Epoch: [67][   70/  142]    Overall Loss 0.605669    Objective Loss 0.605669                                        LR 0.000063    Time 0.563374    
2023-06-16 01:46:48,739 - Epoch: [67][   80/  142]    Overall Loss 0.608312    Objective Loss 0.608312                                        LR 0.000063    Time 0.555476    
2023-06-16 01:46:53,799 - Epoch: [67][   90/  142]    Overall Loss 0.611724    Objective Loss 0.611724                                        LR 0.000063    Time 0.549974    
2023-06-16 01:46:58,664 - Epoch: [67][  100/  142]    Overall Loss 0.608181    Objective Loss 0.608181                                        LR 0.000063    Time 0.543620    
2023-06-16 01:47:03,734 - Epoch: [67][  110/  142]    Overall Loss 0.604719    Objective Loss 0.604719                                        LR 0.000063    Time 0.540290    
2023-06-16 01:47:08,715 - Epoch: [67][  120/  142]    Overall Loss 0.601498    Objective Loss 0.601498                                        LR 0.000063    Time 0.536767    
2023-06-16 01:47:13,656 - Epoch: [67][  130/  142]    Overall Loss 0.609247    Objective Loss 0.609247                                        LR 0.000063    Time 0.533480    
2023-06-16 01:47:18,314 - Epoch: [67][  140/  142]    Overall Loss 0.603019    Objective Loss 0.603019                                        LR 0.000063    Time 0.528638    
2023-06-16 01:47:19,158 - Epoch: [67][  142/  142]    Overall Loss 0.603211    Objective Loss 0.603211    Top1 76.562500    LR 0.000063    Time 0.527139    
2023-06-16 01:47:19,796 - --- validate (epoch=67)-----------
2023-06-16 01:47:19,797 - 1422 samples (32 per mini-batch)
2023-06-16 01:47:27,586 - Epoch: [67][   10/   45]    Loss 0.833735    Top1 73.437500    
2023-06-16 01:47:31,752 - Epoch: [67][   20/   45]    Loss 0.840776    Top1 74.375000    
2023-06-16 01:47:36,685 - Epoch: [67][   30/   45]    Loss 0.877088    Top1 72.812500    
2023-06-16 01:47:40,986 - Epoch: [67][   40/   45]    Loss 0.859630    Top1 73.281250    
2023-06-16 01:47:42,311 - Epoch: [67][   45/   45]    Loss 0.837735    Top1 73.699015    
2023-06-16 01:47:42,951 - ==> Top1: 73.699    Loss: 0.838

2023-06-16 01:47:42,953 - ==> Best [Top1: 73.910   Sparsity:0.00   Params: 375264 on epoch: 65]
2023-06-16 01:47:42,953 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:47:42,975 - 

2023-06-16 01:47:42,975 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:47:52,314 - Epoch: [68][   10/  142]    Overall Loss 0.513808    Objective Loss 0.513808                                        LR 0.000063    Time 0.933846    
2023-06-16 01:47:57,306 - Epoch: [68][   20/  142]    Overall Loss 0.524110    Objective Loss 0.524110                                        LR 0.000063    Time 0.716435    
2023-06-16 01:48:02,320 - Epoch: [68][   30/  142]    Overall Loss 0.568213    Objective Loss 0.568213                                        LR 0.000063    Time 0.644747    
2023-06-16 01:48:07,392 - Epoch: [68][   40/  142]    Overall Loss 0.580901    Objective Loss 0.580901                                        LR 0.000063    Time 0.610332    
2023-06-16 01:48:12,295 - Epoch: [68][   50/  142]    Overall Loss 0.570939    Objective Loss 0.570939                                        LR 0.000063    Time 0.586320    
2023-06-16 01:48:17,237 - Epoch: [68][   60/  142]    Overall Loss 0.555445    Objective Loss 0.555445                                        LR 0.000063    Time 0.570960    
2023-06-16 01:48:22,255 - Epoch: [68][   70/  142]    Overall Loss 0.573779    Objective Loss 0.573779                                        LR 0.000063    Time 0.561070    
2023-06-16 01:48:27,265 - Epoch: [68][   80/  142]    Overall Loss 0.579016    Objective Loss 0.579016                                        LR 0.000063    Time 0.553549    
2023-06-16 01:48:32,260 - Epoch: [68][   90/  142]    Overall Loss 0.586154    Objective Loss 0.586154                                        LR 0.000063    Time 0.547536    
2023-06-16 01:48:37,348 - Epoch: [68][  100/  142]    Overall Loss 0.585619    Objective Loss 0.585619                                        LR 0.000063    Time 0.543658    
2023-06-16 01:48:42,290 - Epoch: [68][  110/  142]    Overall Loss 0.590393    Objective Loss 0.590393                                        LR 0.000063    Time 0.539158    
2023-06-16 01:48:47,342 - Epoch: [68][  120/  142]    Overall Loss 0.590527    Objective Loss 0.590527                                        LR 0.000063    Time 0.536322    
2023-06-16 01:48:52,226 - Epoch: [68][  130/  142]    Overall Loss 0.595047    Objective Loss 0.595047                                        LR 0.000063    Time 0.532636    
2023-06-16 01:48:56,958 - Epoch: [68][  140/  142]    Overall Loss 0.590725    Objective Loss 0.590725                                        LR 0.000063    Time 0.528381    
2023-06-16 01:48:57,801 - Epoch: [68][  142/  142]    Overall Loss 0.590740    Objective Loss 0.590740    Top1 81.250000    LR 0.000063    Time 0.526874    
2023-06-16 01:48:58,449 - --- validate (epoch=68)-----------
2023-06-16 01:48:58,450 - 1422 samples (32 per mini-batch)
2023-06-16 01:49:06,487 - Epoch: [68][   10/   45]    Loss 0.858030    Top1 70.937500    
2023-06-16 01:49:10,662 - Epoch: [68][   20/   45]    Loss 0.906714    Top1 70.781250    
2023-06-16 01:49:15,355 - Epoch: [68][   30/   45]    Loss 0.864527    Top1 71.770833    
2023-06-16 01:49:19,707 - Epoch: [68][   40/   45]    Loss 0.844872    Top1 73.046875    
2023-06-16 01:49:21,054 - Epoch: [68][   45/   45]    Loss 0.837225    Top1 73.136428    
2023-06-16 01:49:21,716 - ==> Top1: 73.136    Loss: 0.837

2023-06-16 01:49:21,718 - ==> Best [Top1: 73.910   Sparsity:0.00   Params: 375264 on epoch: 65]
2023-06-16 01:49:21,718 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:49:21,732 - 

2023-06-16 01:49:21,732 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:49:31,405 - Epoch: [69][   10/  142]    Overall Loss 0.624913    Objective Loss 0.624913                                        LR 0.000063    Time 0.967112    
2023-06-16 01:49:36,421 - Epoch: [69][   20/  142]    Overall Loss 0.530250    Objective Loss 0.530250                                        LR 0.000063    Time 0.734335    
2023-06-16 01:49:41,352 - Epoch: [69][   30/  142]    Overall Loss 0.540270    Objective Loss 0.540270                                        LR 0.000063    Time 0.653916    
2023-06-16 01:49:46,488 - Epoch: [69][   40/  142]    Overall Loss 0.568650    Objective Loss 0.568650                                        LR 0.000063    Time 0.618827    
2023-06-16 01:49:51,357 - Epoch: [69][   50/  142]    Overall Loss 0.577119    Objective Loss 0.577119                                        LR 0.000063    Time 0.592413    
2023-06-16 01:49:56,363 - Epoch: [69][   60/  142]    Overall Loss 0.564305    Objective Loss 0.564305                                        LR 0.000063    Time 0.577112    
2023-06-16 01:50:01,393 - Epoch: [69][   70/  142]    Overall Loss 0.572133    Objective Loss 0.572133                                        LR 0.000063    Time 0.566514    
2023-06-16 01:50:06,352 - Epoch: [69][   80/  142]    Overall Loss 0.572934    Objective Loss 0.572934                                        LR 0.000063    Time 0.557678    
2023-06-16 01:50:11,396 - Epoch: [69][   90/  142]    Overall Loss 0.571606    Objective Loss 0.571606                                        LR 0.000063    Time 0.551755    
2023-06-16 01:50:16,391 - Epoch: [69][  100/  142]    Overall Loss 0.570543    Objective Loss 0.570543                                        LR 0.000063    Time 0.546521    
2023-06-16 01:50:21,299 - Epoch: [69][  110/  142]    Overall Loss 0.576743    Objective Loss 0.576743                                        LR 0.000063    Time 0.541447    
2023-06-16 01:50:26,249 - Epoch: [69][  120/  142]    Overall Loss 0.576994    Objective Loss 0.576994                                        LR 0.000063    Time 0.537576    
2023-06-16 01:50:31,206 - Epoch: [69][  130/  142]    Overall Loss 0.579754    Objective Loss 0.579754                                        LR 0.000063    Time 0.534348    
2023-06-16 01:50:35,902 - Epoch: [69][  140/  142]    Overall Loss 0.583460    Objective Loss 0.583460                                        LR 0.000063    Time 0.529719    
2023-06-16 01:50:36,741 - Epoch: [69][  142/  142]    Overall Loss 0.584052    Objective Loss 0.584052    Top1 79.687500    LR 0.000063    Time 0.528171    
2023-06-16 01:50:37,375 - --- validate (epoch=69)-----------
2023-06-16 01:50:37,375 - 1422 samples (32 per mini-batch)
2023-06-16 01:50:45,379 - Epoch: [69][   10/   45]    Loss 0.683187    Top1 76.562500    
2023-06-16 01:50:49,713 - Epoch: [69][   20/   45]    Loss 0.791265    Top1 74.218750    
2023-06-16 01:50:54,369 - Epoch: [69][   30/   45]    Loss 0.765590    Top1 75.312500    
2023-06-16 01:50:58,981 - Epoch: [69][   40/   45]    Loss 0.782381    Top1 74.453125    
2023-06-16 01:51:00,302 - Epoch: [69][   45/   45]    Loss 0.791508    Top1 73.909986    
2023-06-16 01:51:00,931 - ==> Top1: 73.910    Loss: 0.792

2023-06-16 01:51:00,934 - ==> Best [Top1: 73.910   Sparsity:0.00   Params: 375264 on epoch: 69]
2023-06-16 01:51:00,934 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:51:00,953 - 

2023-06-16 01:51:00,953 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:51:10,262 - Epoch: [70][   10/  142]    Overall Loss 0.595441    Objective Loss 0.595441                                        LR 0.000063    Time 0.930773    
2023-06-16 01:51:15,176 - Epoch: [70][   20/  142]    Overall Loss 0.582599    Objective Loss 0.582599                                        LR 0.000063    Time 0.711046    
2023-06-16 01:51:20,130 - Epoch: [70][   30/  142]    Overall Loss 0.565917    Objective Loss 0.565917                                        LR 0.000063    Time 0.639157    
2023-06-16 01:51:25,171 - Epoch: [70][   40/  142]    Overall Loss 0.580991    Objective Loss 0.580991                                        LR 0.000063    Time 0.605382    
2023-06-16 01:51:30,135 - Epoch: [70][   50/  142]    Overall Loss 0.579285    Objective Loss 0.579285                                        LR 0.000063    Time 0.583552    
2023-06-16 01:51:35,041 - Epoch: [70][   60/  142]    Overall Loss 0.562997    Objective Loss 0.562997                                        LR 0.000063    Time 0.568054    
2023-06-16 01:51:39,971 - Epoch: [70][   70/  142]    Overall Loss 0.557741    Objective Loss 0.557741                                        LR 0.000063    Time 0.557328    
2023-06-16 01:51:44,958 - Epoch: [70][   80/  142]    Overall Loss 0.572318    Objective Loss 0.572318                                        LR 0.000063    Time 0.549988    
2023-06-16 01:51:50,050 - Epoch: [70][   90/  142]    Overall Loss 0.565529    Objective Loss 0.565529                                        LR 0.000063    Time 0.545456    
2023-06-16 01:51:55,023 - Epoch: [70][  100/  142]    Overall Loss 0.567381    Objective Loss 0.567381                                        LR 0.000063    Time 0.540636    
2023-06-16 01:51:59,984 - Epoch: [70][  110/  142]    Overall Loss 0.574896    Objective Loss 0.574896                                        LR 0.000063    Time 0.536576    
2023-06-16 01:52:04,930 - Epoch: [70][  120/  142]    Overall Loss 0.573468    Objective Loss 0.573468                                        LR 0.000063    Time 0.533076    
2023-06-16 01:52:09,845 - Epoch: [70][  130/  142]    Overall Loss 0.573111    Objective Loss 0.573111                                        LR 0.000063    Time 0.529870    
2023-06-16 01:52:14,567 - Epoch: [70][  140/  142]    Overall Loss 0.573054    Objective Loss 0.573054                                        LR 0.000063    Time 0.525745    
2023-06-16 01:52:15,421 - Epoch: [70][  142/  142]    Overall Loss 0.572012    Objective Loss 0.572012    Top1 81.250000    LR 0.000063    Time 0.524359    
2023-06-16 01:52:16,045 - --- validate (epoch=70)-----------
2023-06-16 01:52:16,046 - 1422 samples (32 per mini-batch)
2023-06-16 01:52:24,124 - Epoch: [70][   10/   45]    Loss 0.953893    Top1 73.437500    
2023-06-16 01:52:28,463 - Epoch: [70][   20/   45]    Loss 0.977098    Top1 74.062500    
2023-06-16 01:52:33,108 - Epoch: [70][   30/   45]    Loss 0.951353    Top1 74.270833    
2023-06-16 01:52:37,729 - Epoch: [70][   40/   45]    Loss 0.945721    Top1 73.750000    
2023-06-16 01:52:39,128 - Epoch: [70][   45/   45]    Loss 0.937013    Top1 73.909986    
2023-06-16 01:52:39,716 - ==> Top1: 73.910    Loss: 0.937

2023-06-16 01:52:39,718 - ==> Best [Top1: 73.910   Sparsity:0.00   Params: 375264 on epoch: 70]
2023-06-16 01:52:39,718 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:52:39,737 - 

2023-06-16 01:52:39,737 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:52:48,895 - Epoch: [71][   10/  142]    Overall Loss 0.625069    Objective Loss 0.625069                                        LR 0.000063    Time 0.915635    
2023-06-16 01:52:53,901 - Epoch: [71][   20/  142]    Overall Loss 0.678408    Objective Loss 0.678408                                        LR 0.000063    Time 0.708084    
2023-06-16 01:52:58,767 - Epoch: [71][   30/  142]    Overall Loss 0.636342    Objective Loss 0.636342                                        LR 0.000063    Time 0.634241    
2023-06-16 01:53:03,803 - Epoch: [71][   40/  142]    Overall Loss 0.628777    Objective Loss 0.628777                                        LR 0.000063    Time 0.601565    
2023-06-16 01:53:08,723 - Epoch: [71][   50/  142]    Overall Loss 0.604434    Objective Loss 0.604434                                        LR 0.000063    Time 0.579641    
2023-06-16 01:53:13,677 - Epoch: [71][   60/  142]    Overall Loss 0.610506    Objective Loss 0.610506                                        LR 0.000063    Time 0.565601    
2023-06-16 01:53:18,635 - Epoch: [71][   70/  142]    Overall Loss 0.603970    Objective Loss 0.603970                                        LR 0.000063    Time 0.555614    
2023-06-16 01:53:23,695 - Epoch: [71][   80/  142]    Overall Loss 0.600550    Objective Loss 0.600550                                        LR 0.000063    Time 0.549400    
2023-06-16 01:53:28,559 - Epoch: [71][   90/  142]    Overall Loss 0.595825    Objective Loss 0.595825                                        LR 0.000063    Time 0.542403    
2023-06-16 01:53:33,612 - Epoch: [71][  100/  142]    Overall Loss 0.597188    Objective Loss 0.597188                                        LR 0.000063    Time 0.538687    
2023-06-16 01:53:38,415 - Epoch: [71][  110/  142]    Overall Loss 0.588637    Objective Loss 0.588637                                        LR 0.000063    Time 0.533371    
2023-06-16 01:53:43,497 - Epoch: [71][  120/  142]    Overall Loss 0.593792    Objective Loss 0.593792                                        LR 0.000063    Time 0.531266    
2023-06-16 01:53:48,518 - Epoch: [71][  130/  142]    Overall Loss 0.594479    Objective Loss 0.594479                                        LR 0.000063    Time 0.529025    
2023-06-16 01:53:53,086 - Epoch: [71][  140/  142]    Overall Loss 0.596158    Objective Loss 0.596158                                        LR 0.000063    Time 0.523863    
2023-06-16 01:53:53,944 - Epoch: [71][  142/  142]    Overall Loss 0.595944    Objective Loss 0.595944    Top1 82.812500    LR 0.000063    Time 0.522522    
2023-06-16 01:53:54,591 - --- validate (epoch=71)-----------
2023-06-16 01:53:54,591 - 1422 samples (32 per mini-batch)
2023-06-16 01:54:02,755 - Epoch: [71][   10/   45]    Loss 0.972858    Top1 73.125000    
2023-06-16 01:54:06,750 - Epoch: [71][   20/   45]    Loss 0.894091    Top1 73.906250    
2023-06-16 01:54:10,936 - Epoch: [71][   30/   45]    Loss 0.861831    Top1 74.791667    
2023-06-16 01:54:15,137 - Epoch: [71][   40/   45]    Loss 0.849215    Top1 73.906250    
2023-06-16 01:54:16,834 - Epoch: [71][   45/   45]    Loss 0.897861    Top1 73.347398    
2023-06-16 01:54:17,459 - ==> Top1: 73.347    Loss: 0.898

2023-06-16 01:54:17,461 - ==> Best [Top1: 73.910   Sparsity:0.00   Params: 375264 on epoch: 70]
2023-06-16 01:54:17,462 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:54:17,483 - 

2023-06-16 01:54:17,483 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:54:26,883 - Epoch: [72][   10/  142]    Overall Loss 0.605378    Objective Loss 0.605378                                        LR 0.000063    Time 0.939859    
2023-06-16 01:54:31,838 - Epoch: [72][   20/  142]    Overall Loss 0.605865    Objective Loss 0.605865                                        LR 0.000063    Time 0.717673    
2023-06-16 01:54:36,680 - Epoch: [72][   30/  142]    Overall Loss 0.589171    Objective Loss 0.589171                                        LR 0.000063    Time 0.639794    
2023-06-16 01:54:41,659 - Epoch: [72][   40/  142]    Overall Loss 0.591219    Objective Loss 0.591219                                        LR 0.000063    Time 0.604316    
2023-06-16 01:54:46,615 - Epoch: [72][   50/  142]    Overall Loss 0.575504    Objective Loss 0.575504                                        LR 0.000063    Time 0.582561    
2023-06-16 01:54:51,489 - Epoch: [72][   60/  142]    Overall Loss 0.584241    Objective Loss 0.584241                                        LR 0.000063    Time 0.566685    
2023-06-16 01:54:56,451 - Epoch: [72][   70/  142]    Overall Loss 0.579808    Objective Loss 0.579808                                        LR 0.000063    Time 0.556608    
2023-06-16 01:55:01,413 - Epoch: [72][   80/  142]    Overall Loss 0.576942    Objective Loss 0.576942                                        LR 0.000063    Time 0.549053    
2023-06-16 01:55:06,425 - Epoch: [72][   90/  142]    Overall Loss 0.566994    Objective Loss 0.566994                                        LR 0.000063    Time 0.543727    
2023-06-16 01:55:11,385 - Epoch: [72][  100/  142]    Overall Loss 0.565259    Objective Loss 0.565259                                        LR 0.000063    Time 0.538950    
2023-06-16 01:55:16,339 - Epoch: [72][  110/  142]    Overall Loss 0.561994    Objective Loss 0.561994                                        LR 0.000063    Time 0.534988    
2023-06-16 01:55:21,379 - Epoch: [72][  120/  142]    Overall Loss 0.565856    Objective Loss 0.565856                                        LR 0.000063    Time 0.532399    
2023-06-16 01:55:26,355 - Epoch: [72][  130/  142]    Overall Loss 0.573508    Objective Loss 0.573508                                        LR 0.000063    Time 0.529715    
2023-06-16 01:55:30,928 - Epoch: [72][  140/  142]    Overall Loss 0.577201    Objective Loss 0.577201                                        LR 0.000063    Time 0.524539    
2023-06-16 01:55:31,775 - Epoch: [72][  142/  142]    Overall Loss 0.578170    Objective Loss 0.578170    Top1 78.125000    LR 0.000063    Time 0.523114    
2023-06-16 01:55:32,387 - --- validate (epoch=72)-----------
2023-06-16 01:55:32,387 - 1422 samples (32 per mini-batch)
2023-06-16 01:55:40,561 - Epoch: [72][   10/   45]    Loss 0.853993    Top1 73.437500    
2023-06-16 01:55:44,541 - Epoch: [72][   20/   45]    Loss 0.862068    Top1 72.968750    
2023-06-16 01:55:49,478 - Epoch: [72][   30/   45]    Loss 0.791421    Top1 74.583333    
2023-06-16 01:55:53,901 - Epoch: [72][   40/   45]    Loss 0.826555    Top1 73.515625    
2023-06-16 01:55:55,224 - Epoch: [72][   45/   45]    Loss 0.816809    Top1 73.980309    
2023-06-16 01:55:55,877 - ==> Top1: 73.980    Loss: 0.817

2023-06-16 01:55:55,879 - ==> Best [Top1: 73.980   Sparsity:0.00   Params: 375264 on epoch: 72]
2023-06-16 01:55:55,879 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:55:55,904 - 

2023-06-16 01:55:55,904 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:56:05,261 - Epoch: [73][   10/  142]    Overall Loss 0.481428    Objective Loss 0.481428                                        LR 0.000063    Time 0.935606    
2023-06-16 01:56:10,106 - Epoch: [73][   20/  142]    Overall Loss 0.496287    Objective Loss 0.496287                                        LR 0.000063    Time 0.710035    
2023-06-16 01:56:15,133 - Epoch: [73][   30/  142]    Overall Loss 0.552789    Objective Loss 0.552789                                        LR 0.000063    Time 0.640903    
2023-06-16 01:56:20,089 - Epoch: [73][   40/  142]    Overall Loss 0.554567    Objective Loss 0.554567                                        LR 0.000063    Time 0.604554    
2023-06-16 01:56:24,981 - Epoch: [73][   50/  142]    Overall Loss 0.550545    Objective Loss 0.550545                                        LR 0.000063    Time 0.581477    
2023-06-16 01:56:29,789 - Epoch: [73][   60/  142]    Overall Loss 0.567590    Objective Loss 0.567590                                        LR 0.000063    Time 0.564682    
2023-06-16 01:56:34,752 - Epoch: [73][   70/  142]    Overall Loss 0.569529    Objective Loss 0.569529                                        LR 0.000063    Time 0.554906    
2023-06-16 01:56:39,802 - Epoch: [73][   80/  142]    Overall Loss 0.567023    Objective Loss 0.567023                                        LR 0.000063    Time 0.548668    
2023-06-16 01:56:44,597 - Epoch: [73][   90/  142]    Overall Loss 0.565698    Objective Loss 0.565698                                        LR 0.000063    Time 0.540974    
2023-06-16 01:56:49,646 - Epoch: [73][  100/  142]    Overall Loss 0.569824    Objective Loss 0.569824                                        LR 0.000063    Time 0.537357    
2023-06-16 01:56:54,501 - Epoch: [73][  110/  142]    Overall Loss 0.568951    Objective Loss 0.568951                                        LR 0.000063    Time 0.532639    
2023-06-16 01:56:59,375 - Epoch: [73][  120/  142]    Overall Loss 0.572107    Objective Loss 0.572107                                        LR 0.000063    Time 0.528867    
2023-06-16 01:57:04,366 - Epoch: [73][  130/  142]    Overall Loss 0.579009    Objective Loss 0.579009                                        LR 0.000063    Time 0.526569    
2023-06-16 01:57:08,949 - Epoch: [73][  140/  142]    Overall Loss 0.583535    Objective Loss 0.583535                                        LR 0.000063    Time 0.521690    
2023-06-16 01:57:09,802 - Epoch: [73][  142/  142]    Overall Loss 0.584996    Objective Loss 0.584996    Top1 78.125000    LR 0.000063    Time 0.520347    
2023-06-16 01:57:10,452 - --- validate (epoch=73)-----------
2023-06-16 01:57:10,452 - 1422 samples (32 per mini-batch)
2023-06-16 01:57:18,621 - Epoch: [73][   10/   45]    Loss 0.940150    Top1 72.812500    
2023-06-16 01:57:22,765 - Epoch: [73][   20/   45]    Loss 0.886187    Top1 73.437500    
2023-06-16 01:57:26,911 - Epoch: [73][   30/   45]    Loss 0.882830    Top1 73.020833    
2023-06-16 01:57:31,138 - Epoch: [73][   40/   45]    Loss 0.874879    Top1 72.578125    
2023-06-16 01:57:32,551 - Epoch: [73][   45/   45]    Loss 0.871661    Top1 72.573840    
2023-06-16 01:57:33,209 - ==> Top1: 72.574    Loss: 0.872

2023-06-16 01:57:33,212 - ==> Best [Top1: 73.980   Sparsity:0.00   Params: 375264 on epoch: 72]
2023-06-16 01:57:33,212 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:57:33,233 - 

2023-06-16 01:57:33,233 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:57:42,660 - Epoch: [74][   10/  142]    Overall Loss 0.585394    Objective Loss 0.585394                                        LR 0.000063    Time 0.942632    
2023-06-16 01:57:47,837 - Epoch: [74][   20/  142]    Overall Loss 0.587358    Objective Loss 0.587358                                        LR 0.000063    Time 0.730100    
2023-06-16 01:57:52,732 - Epoch: [74][   30/  142]    Overall Loss 0.584415    Objective Loss 0.584415                                        LR 0.000063    Time 0.649884    
2023-06-16 01:57:57,792 - Epoch: [74][   40/  142]    Overall Loss 0.587767    Objective Loss 0.587767                                        LR 0.000063    Time 0.613894    
2023-06-16 01:58:02,791 - Epoch: [74][   50/  142]    Overall Loss 0.582288    Objective Loss 0.582288                                        LR 0.000063    Time 0.591074    
2023-06-16 01:58:07,810 - Epoch: [74][   60/  142]    Overall Loss 0.565438    Objective Loss 0.565438                                        LR 0.000063    Time 0.576201    
2023-06-16 01:58:12,838 - Epoch: [74][   70/  142]    Overall Loss 0.554185    Objective Loss 0.554185                                        LR 0.000063    Time 0.565716    
2023-06-16 01:58:17,945 - Epoch: [74][   80/  142]    Overall Loss 0.564037    Objective Loss 0.564037                                        LR 0.000063    Time 0.558839    
2023-06-16 01:58:23,055 - Epoch: [74][   90/  142]    Overall Loss 0.574326    Objective Loss 0.574326                                        LR 0.000063    Time 0.553513    
2023-06-16 01:58:27,898 - Epoch: [74][  100/  142]    Overall Loss 0.566334    Objective Loss 0.566334                                        LR 0.000063    Time 0.546591    
2023-06-16 01:58:32,926 - Epoch: [74][  110/  142]    Overall Loss 0.565723    Objective Loss 0.565723                                        LR 0.000063    Time 0.542599    
2023-06-16 01:58:37,944 - Epoch: [74][  120/  142]    Overall Loss 0.567019    Objective Loss 0.567019                                        LR 0.000063    Time 0.539199    
2023-06-16 01:58:43,094 - Epoch: [74][  130/  142]    Overall Loss 0.571572    Objective Loss 0.571572                                        LR 0.000063    Time 0.537332    
2023-06-16 01:58:47,632 - Epoch: [74][  140/  142]    Overall Loss 0.572222    Objective Loss 0.572222                                        LR 0.000063    Time 0.531363    
2023-06-16 01:58:48,489 - Epoch: [74][  142/  142]    Overall Loss 0.573135    Objective Loss 0.573135    Top1 73.437500    LR 0.000063    Time 0.529909    
2023-06-16 01:58:49,127 - --- validate (epoch=74)-----------
2023-06-16 01:58:49,127 - 1422 samples (32 per mini-batch)
2023-06-16 01:58:57,103 - Epoch: [74][   10/   45]    Loss 0.853159    Top1 73.125000    
2023-06-16 01:59:01,273 - Epoch: [74][   20/   45]    Loss 0.835825    Top1 74.687500    
2023-06-16 01:59:06,143 - Epoch: [74][   30/   45]    Loss 0.847881    Top1 73.854167    
2023-06-16 01:59:10,285 - Epoch: [74][   40/   45]    Loss 0.854340    Top1 73.906250    
2023-06-16 01:59:11,756 - Epoch: [74][   45/   45]    Loss 0.889399    Top1 73.347398    
2023-06-16 01:59:12,419 - ==> Top1: 73.347    Loss: 0.889

2023-06-16 01:59:12,421 - ==> Best [Top1: 73.980   Sparsity:0.00   Params: 375264 on epoch: 72]
2023-06-16 01:59:12,421 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 01:59:12,443 - 

2023-06-16 01:59:12,443 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 01:59:21,745 - Epoch: [75][   10/  142]    Overall Loss 0.566368    Objective Loss 0.566368                                        LR 0.000063    Time 0.930146    
2023-06-16 01:59:26,685 - Epoch: [75][   20/  142]    Overall Loss 0.563034    Objective Loss 0.563034                                        LR 0.000063    Time 0.712006    
2023-06-16 01:59:31,632 - Epoch: [75][   30/  142]    Overall Loss 0.599467    Objective Loss 0.599467                                        LR 0.000063    Time 0.639572    
2023-06-16 01:59:36,534 - Epoch: [75][   40/  142]    Overall Loss 0.604033    Objective Loss 0.604033                                        LR 0.000063    Time 0.602211    
2023-06-16 01:59:41,494 - Epoch: [75][   50/  142]    Overall Loss 0.611352    Objective Loss 0.611352                                        LR 0.000063    Time 0.580949    
2023-06-16 01:59:46,446 - Epoch: [75][   60/  142]    Overall Loss 0.623861    Objective Loss 0.623861                                        LR 0.000063    Time 0.566654    
2023-06-16 01:59:51,331 - Epoch: [75][   70/  142]    Overall Loss 0.614823    Objective Loss 0.614823                                        LR 0.000063    Time 0.555486    
2023-06-16 01:59:56,263 - Epoch: [75][   80/  142]    Overall Loss 0.600359    Objective Loss 0.600359                                        LR 0.000063    Time 0.547684    
2023-06-16 02:00:01,258 - Epoch: [75][   90/  142]    Overall Loss 0.594055    Objective Loss 0.594055                                        LR 0.000063    Time 0.542326    
2023-06-16 02:00:06,246 - Epoch: [75][  100/  142]    Overall Loss 0.582039    Objective Loss 0.582039                                        LR 0.000063    Time 0.537971    
2023-06-16 02:00:11,108 - Epoch: [75][  110/  142]    Overall Loss 0.579261    Objective Loss 0.579261                                        LR 0.000063    Time 0.533257    
2023-06-16 02:00:16,050 - Epoch: [75][  120/  142]    Overall Loss 0.576692    Objective Loss 0.576692                                        LR 0.000063    Time 0.530000    
2023-06-16 02:00:21,068 - Epoch: [75][  130/  142]    Overall Loss 0.573368    Objective Loss 0.573368                                        LR 0.000063    Time 0.527824    
2023-06-16 02:00:25,772 - Epoch: [75][  140/  142]    Overall Loss 0.574460    Objective Loss 0.574460                                        LR 0.000063    Time 0.523715    
2023-06-16 02:00:26,628 - Epoch: [75][  142/  142]    Overall Loss 0.573461    Objective Loss 0.573461    Top1 79.687500    LR 0.000063    Time 0.522367    
2023-06-16 02:00:27,266 - --- validate (epoch=75)-----------
2023-06-16 02:00:27,267 - 1422 samples (32 per mini-batch)
2023-06-16 02:00:35,354 - Epoch: [75][   10/   45]    Loss 0.923601    Top1 71.875000    
2023-06-16 02:00:39,597 - Epoch: [75][   20/   45]    Loss 0.966987    Top1 71.718750    
2023-06-16 02:00:44,268 - Epoch: [75][   30/   45]    Loss 0.993166    Top1 70.416667    
2023-06-16 02:00:48,473 - Epoch: [75][   40/   45]    Loss 0.949624    Top1 71.015625    
2023-06-16 02:00:49,876 - Epoch: [75][   45/   45]    Loss 0.945988    Top1 70.886076    
2023-06-16 02:00:50,517 - ==> Top1: 70.886    Loss: 0.946

2023-06-16 02:00:50,519 - ==> Best [Top1: 73.980   Sparsity:0.00   Params: 375264 on epoch: 72]
2023-06-16 02:00:50,519 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:00:50,541 - 

2023-06-16 02:00:50,541 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:00:59,615 - Epoch: [76][   10/  142]    Overall Loss 0.640403    Objective Loss 0.640403                                        LR 0.000063    Time 0.907236    
2023-06-16 02:01:04,564 - Epoch: [76][   20/  142]    Overall Loss 0.614740    Objective Loss 0.614740                                        LR 0.000063    Time 0.701039    
2023-06-16 02:01:09,560 - Epoch: [76][   30/  142]    Overall Loss 0.595691    Objective Loss 0.595691                                        LR 0.000063    Time 0.633870    
2023-06-16 02:01:14,496 - Epoch: [76][   40/  142]    Overall Loss 0.587920    Objective Loss 0.587920                                        LR 0.000063    Time 0.598803    
2023-06-16 02:01:19,485 - Epoch: [76][   50/  142]    Overall Loss 0.575501    Objective Loss 0.575501                                        LR 0.000063    Time 0.578807    
2023-06-16 02:01:24,467 - Epoch: [76][   60/  142]    Overall Loss 0.564923    Objective Loss 0.564923                                        LR 0.000063    Time 0.565362    
2023-06-16 02:01:29,299 - Epoch: [76][   70/  142]    Overall Loss 0.563928    Objective Loss 0.563928                                        LR 0.000063    Time 0.553616    
2023-06-16 02:01:34,280 - Epoch: [76][   80/  142]    Overall Loss 0.573387    Objective Loss 0.573387                                        LR 0.000063    Time 0.546672    
2023-06-16 02:01:39,301 - Epoch: [76][   90/  142]    Overall Loss 0.580671    Objective Loss 0.580671                                        LR 0.000063    Time 0.541718    
2023-06-16 02:01:44,298 - Epoch: [76][  100/  142]    Overall Loss 0.580515    Objective Loss 0.580515                                        LR 0.000063    Time 0.537506    
2023-06-16 02:01:49,145 - Epoch: [76][  110/  142]    Overall Loss 0.583989    Objective Loss 0.583989                                        LR 0.000063    Time 0.532696    
2023-06-16 02:01:54,093 - Epoch: [76][  120/  142]    Overall Loss 0.584235    Objective Loss 0.584235                                        LR 0.000063    Time 0.529538    
2023-06-16 02:01:59,055 - Epoch: [76][  130/  142]    Overall Loss 0.581449    Objective Loss 0.581449                                        LR 0.000063    Time 0.526969    
2023-06-16 02:02:03,737 - Epoch: [76][  140/  142]    Overall Loss 0.575907    Objective Loss 0.575907                                        LR 0.000063    Time 0.522769    
2023-06-16 02:02:04,591 - Epoch: [76][  142/  142]    Overall Loss 0.575529    Objective Loss 0.575529    Top1 78.125000    LR 0.000063    Time 0.521415    
2023-06-16 02:02:05,241 - --- validate (epoch=76)-----------
2023-06-16 02:02:05,242 - 1422 samples (32 per mini-batch)
2023-06-16 02:02:13,088 - Epoch: [76][   10/   45]    Loss 0.880661    Top1 75.000000    
2023-06-16 02:02:17,082 - Epoch: [76][   20/   45]    Loss 0.867624    Top1 75.468750    
2023-06-16 02:02:22,111 - Epoch: [76][   30/   45]    Loss 0.877912    Top1 74.895833    
2023-06-16 02:02:26,082 - Epoch: [76][   40/   45]    Loss 0.891077    Top1 74.140625    
2023-06-16 02:02:27,524 - Epoch: [76][   45/   45]    Loss 0.902446    Top1 73.769339    
2023-06-16 02:02:28,154 - ==> Top1: 73.769    Loss: 0.902

2023-06-16 02:02:28,156 - ==> Best [Top1: 73.980   Sparsity:0.00   Params: 375264 on epoch: 72]
2023-06-16 02:02:28,156 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:02:28,178 - 

2023-06-16 02:02:28,178 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:02:37,762 - Epoch: [77][   10/  142]    Overall Loss 0.575272    Objective Loss 0.575272                                        LR 0.000063    Time 0.958361    
2023-06-16 02:02:42,824 - Epoch: [77][   20/  142]    Overall Loss 0.511317    Objective Loss 0.511317                                        LR 0.000063    Time 0.732202    
2023-06-16 02:02:47,864 - Epoch: [77][   30/  142]    Overall Loss 0.548588    Objective Loss 0.548588                                        LR 0.000063    Time 0.656139    
2023-06-16 02:02:52,851 - Epoch: [77][   40/  142]    Overall Loss 0.535871    Objective Loss 0.535871                                        LR 0.000063    Time 0.616762    
2023-06-16 02:02:58,037 - Epoch: [77][   50/  142]    Overall Loss 0.543384    Objective Loss 0.543384                                        LR 0.000063    Time 0.597107    
2023-06-16 02:03:03,063 - Epoch: [77][   60/  142]    Overall Loss 0.542593    Objective Loss 0.542593                                        LR 0.000063    Time 0.581347    
2023-06-16 02:03:08,199 - Epoch: [77][   70/  142]    Overall Loss 0.553585    Objective Loss 0.553585                                        LR 0.000063    Time 0.571673    
2023-06-16 02:03:13,253 - Epoch: [77][   80/  142]    Overall Loss 0.552962    Objective Loss 0.552962                                        LR 0.000063    Time 0.563373    
2023-06-16 02:03:18,456 - Epoch: [77][   90/  142]    Overall Loss 0.552141    Objective Loss 0.552141                                        LR 0.000063    Time 0.558588    
2023-06-16 02:03:23,503 - Epoch: [77][  100/  142]    Overall Loss 0.550806    Objective Loss 0.550806                                        LR 0.000063    Time 0.553192    
2023-06-16 02:03:28,606 - Epoch: [77][  110/  142]    Overall Loss 0.546596    Objective Loss 0.546596                                        LR 0.000063    Time 0.549286    
2023-06-16 02:03:33,526 - Epoch: [77][  120/  142]    Overall Loss 0.543832    Objective Loss 0.543832                                        LR 0.000063    Time 0.544510    
2023-06-16 02:03:38,603 - Epoch: [77][  130/  142]    Overall Loss 0.551063    Objective Loss 0.551063                                        LR 0.000063    Time 0.541672    
2023-06-16 02:03:43,341 - Epoch: [77][  140/  142]    Overall Loss 0.553846    Objective Loss 0.553846                                        LR 0.000063    Time 0.536824    
2023-06-16 02:03:44,184 - Epoch: [77][  142/  142]    Overall Loss 0.557137    Objective Loss 0.557137    Top1 71.875000    LR 0.000063    Time 0.535194    
2023-06-16 02:03:44,829 - --- validate (epoch=77)-----------
2023-06-16 02:03:44,830 - 1422 samples (32 per mini-batch)
2023-06-16 02:03:52,503 - Epoch: [77][   10/   45]    Loss 0.849104    Top1 75.312500    
2023-06-16 02:03:57,277 - Epoch: [77][   20/   45]    Loss 0.837424    Top1 75.468750    
2023-06-16 02:04:01,744 - Epoch: [77][   30/   45]    Loss 0.843157    Top1 73.645833    
2023-06-16 02:04:05,989 - Epoch: [77][   40/   45]    Loss 0.855261    Top1 73.750000    
2023-06-16 02:04:07,406 - Epoch: [77][   45/   45]    Loss 0.869026    Top1 73.277075    
2023-06-16 02:04:08,047 - ==> Top1: 73.277    Loss: 0.869

2023-06-16 02:04:08,049 - ==> Best [Top1: 73.980   Sparsity:0.00   Params: 375264 on epoch: 72]
2023-06-16 02:04:08,050 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:04:08,071 - 

2023-06-16 02:04:08,071 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:04:17,459 - Epoch: [78][   10/  142]    Overall Loss 0.543415    Objective Loss 0.543415                                        LR 0.000063    Time 0.938677    
2023-06-16 02:04:22,517 - Epoch: [78][   20/  142]    Overall Loss 0.579072    Objective Loss 0.579072                                        LR 0.000063    Time 0.722179    
2023-06-16 02:04:27,446 - Epoch: [78][   30/  142]    Overall Loss 0.587104    Objective Loss 0.587104                                        LR 0.000063    Time 0.645759    
2023-06-16 02:04:32,505 - Epoch: [78][   40/  142]    Overall Loss 0.597972    Objective Loss 0.597972                                        LR 0.000063    Time 0.610764    
2023-06-16 02:04:37,426 - Epoch: [78][   50/  142]    Overall Loss 0.595017    Objective Loss 0.595017                                        LR 0.000063    Time 0.587020    
2023-06-16 02:04:42,352 - Epoch: [78][   60/  142]    Overall Loss 0.590916    Objective Loss 0.590916                                        LR 0.000063    Time 0.571271    
2023-06-16 02:04:47,378 - Epoch: [78][   70/  142]    Overall Loss 0.596174    Objective Loss 0.596174                                        LR 0.000063    Time 0.561462    
2023-06-16 02:04:52,464 - Epoch: [78][   80/  142]    Overall Loss 0.593077    Objective Loss 0.593077                                        LR 0.000063    Time 0.554847    
2023-06-16 02:04:57,354 - Epoch: [78][   90/  142]    Overall Loss 0.588902    Objective Loss 0.588902                                        LR 0.000063    Time 0.547524    
2023-06-16 02:05:02,408 - Epoch: [78][  100/  142]    Overall Loss 0.591139    Objective Loss 0.591139                                        LR 0.000063    Time 0.543305    
2023-06-16 02:05:07,385 - Epoch: [78][  110/  142]    Overall Loss 0.596305    Objective Loss 0.596305                                        LR 0.000063    Time 0.539153    
2023-06-16 02:05:12,409 - Epoch: [78][  120/  142]    Overall Loss 0.593784    Objective Loss 0.593784                                        LR 0.000063    Time 0.536086    
2023-06-16 02:05:17,414 - Epoch: [78][  130/  142]    Overall Loss 0.591101    Objective Loss 0.591101                                        LR 0.000063    Time 0.533343    
2023-06-16 02:05:22,130 - Epoch: [78][  140/  142]    Overall Loss 0.591694    Objective Loss 0.591694                                        LR 0.000063    Time 0.528929    
2023-06-16 02:05:22,970 - Epoch: [78][  142/  142]    Overall Loss 0.591330    Objective Loss 0.591330    Top1 76.562500    LR 0.000063    Time 0.527394    
2023-06-16 02:05:23,620 - --- validate (epoch=78)-----------
2023-06-16 02:05:23,620 - 1422 samples (32 per mini-batch)
2023-06-16 02:05:31,869 - Epoch: [78][   10/   45]    Loss 0.839374    Top1 73.125000    
2023-06-16 02:05:35,911 - Epoch: [78][   20/   45]    Loss 0.807919    Top1 72.187500    
2023-06-16 02:05:39,808 - Epoch: [78][   30/   45]    Loss 0.832439    Top1 70.937500    
2023-06-16 02:05:44,645 - Epoch: [78][   40/   45]    Loss 0.835178    Top1 71.562500    
2023-06-16 02:05:46,192 - Epoch: [78][   45/   45]    Loss 0.822273    Top1 71.940928    
2023-06-16 02:05:46,829 - ==> Top1: 71.941    Loss: 0.822

2023-06-16 02:05:46,831 - ==> Best [Top1: 73.980   Sparsity:0.00   Params: 375264 on epoch: 72]
2023-06-16 02:05:46,831 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:05:46,852 - 

2023-06-16 02:05:46,852 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:05:56,234 - Epoch: [79][   10/  142]    Overall Loss 0.574895    Objective Loss 0.574895                                        LR 0.000063    Time 0.938051    
2023-06-16 02:06:01,255 - Epoch: [79][   20/  142]    Overall Loss 0.554604    Objective Loss 0.554604                                        LR 0.000063    Time 0.720065    
2023-06-16 02:06:06,300 - Epoch: [79][   30/  142]    Overall Loss 0.562240    Objective Loss 0.562240                                        LR 0.000063    Time 0.648183    
2023-06-16 02:06:11,202 - Epoch: [79][   40/  142]    Overall Loss 0.562008    Objective Loss 0.562008                                        LR 0.000063    Time 0.608691    
2023-06-16 02:06:16,158 - Epoch: [79][   50/  142]    Overall Loss 0.553296    Objective Loss 0.553296                                        LR 0.000063    Time 0.586063    
2023-06-16 02:06:21,164 - Epoch: [79][   60/  142]    Overall Loss 0.541822    Objective Loss 0.541822                                        LR 0.000063    Time 0.571807    
2023-06-16 02:06:26,214 - Epoch: [79][   70/  142]    Overall Loss 0.545131    Objective Loss 0.545131                                        LR 0.000063    Time 0.562248    
2023-06-16 02:06:31,225 - Epoch: [79][   80/  142]    Overall Loss 0.549352    Objective Loss 0.549352                                        LR 0.000063    Time 0.554597    
2023-06-16 02:06:36,046 - Epoch: [79][   90/  142]    Overall Loss 0.551926    Objective Loss 0.551926                                        LR 0.000063    Time 0.546534    
2023-06-16 02:06:41,069 - Epoch: [79][  100/  142]    Overall Loss 0.551601    Objective Loss 0.551601                                        LR 0.000063    Time 0.542103    
2023-06-16 02:06:46,067 - Epoch: [79][  110/  142]    Overall Loss 0.558380    Objective Loss 0.558380                                        LR 0.000063    Time 0.538256    
2023-06-16 02:06:51,016 - Epoch: [79][  120/  142]    Overall Loss 0.560040    Objective Loss 0.560040                                        LR 0.000063    Time 0.534640    
2023-06-16 02:06:55,993 - Epoch: [79][  130/  142]    Overall Loss 0.561041    Objective Loss 0.561041                                        LR 0.000063    Time 0.531792    
2023-06-16 02:07:00,669 - Epoch: [79][  140/  142]    Overall Loss 0.559794    Objective Loss 0.559794                                        LR 0.000063    Time 0.527205    
2023-06-16 02:07:01,525 - Epoch: [79][  142/  142]    Overall Loss 0.560971    Objective Loss 0.560971    Top1 73.437500    LR 0.000063    Time 0.525801    
2023-06-16 02:07:02,172 - --- validate (epoch=79)-----------
2023-06-16 02:07:02,172 - 1422 samples (32 per mini-batch)
2023-06-16 02:07:10,124 - Epoch: [79][   10/   45]    Loss 1.106152    Top1 68.125000    
2023-06-16 02:07:14,238 - Epoch: [79][   20/   45]    Loss 1.060867    Top1 68.906250    
2023-06-16 02:07:18,272 - Epoch: [79][   30/   45]    Loss 0.971120    Top1 71.145833    
2023-06-16 02:07:22,657 - Epoch: [79][   40/   45]    Loss 0.933307    Top1 72.500000    
2023-06-16 02:07:24,036 - Epoch: [79][   45/   45]    Loss 0.911506    Top1 72.925457    
2023-06-16 02:07:24,699 - ==> Top1: 72.925    Loss: 0.912

2023-06-16 02:07:24,701 - ==> Best [Top1: 73.980   Sparsity:0.00   Params: 375264 on epoch: 72]
2023-06-16 02:07:24,701 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:07:24,714 - 

2023-06-16 02:07:24,714 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:07:34,176 - Epoch: [80][   10/  142]    Overall Loss 0.547951    Objective Loss 0.547951                                        LR 0.000031    Time 0.946081    
2023-06-16 02:07:39,281 - Epoch: [80][   20/  142]    Overall Loss 0.585007    Objective Loss 0.585007                                        LR 0.000031    Time 0.728250    
2023-06-16 02:07:44,290 - Epoch: [80][   30/  142]    Overall Loss 0.584559    Objective Loss 0.584559                                        LR 0.000031    Time 0.652438    
2023-06-16 02:07:49,225 - Epoch: [80][   40/  142]    Overall Loss 0.582320    Objective Loss 0.582320                                        LR 0.000031    Time 0.612712    
2023-06-16 02:07:54,312 - Epoch: [80][   50/  142]    Overall Loss 0.564854    Objective Loss 0.564854                                        LR 0.000031    Time 0.591891    
2023-06-16 02:07:59,305 - Epoch: [80][   60/  142]    Overall Loss 0.556964    Objective Loss 0.556964                                        LR 0.000031    Time 0.576456    
2023-06-16 02:08:04,383 - Epoch: [80][   70/  142]    Overall Loss 0.553684    Objective Loss 0.553684                                        LR 0.000031    Time 0.566632    
2023-06-16 02:08:09,395 - Epoch: [80][   80/  142]    Overall Loss 0.558234    Objective Loss 0.558234                                        LR 0.000031    Time 0.558455    
2023-06-16 02:08:14,515 - Epoch: [80][   90/  142]    Overall Loss 0.556893    Objective Loss 0.556893                                        LR 0.000031    Time 0.553286    
2023-06-16 02:08:19,429 - Epoch: [80][  100/  142]    Overall Loss 0.550998    Objective Loss 0.550998                                        LR 0.000031    Time 0.547091    
2023-06-16 02:08:24,530 - Epoch: [80][  110/  142]    Overall Loss 0.544591    Objective Loss 0.544591                                        LR 0.000031    Time 0.543719    
2023-06-16 02:08:29,622 - Epoch: [80][  120/  142]    Overall Loss 0.542468    Objective Loss 0.542468                                        LR 0.000031    Time 0.540841    
2023-06-16 02:08:34,554 - Epoch: [80][  130/  142]    Overall Loss 0.538905    Objective Loss 0.538905                                        LR 0.000031    Time 0.537175    
2023-06-16 02:08:39,275 - Epoch: [80][  140/  142]    Overall Loss 0.544647    Objective Loss 0.544647                                        LR 0.000031    Time 0.532518    
2023-06-16 02:08:40,116 - Epoch: [80][  142/  142]    Overall Loss 0.543792    Objective Loss 0.543792    Top1 79.687500    LR 0.000031    Time 0.530943    
2023-06-16 02:08:40,724 - --- validate (epoch=80)-----------
2023-06-16 02:08:40,725 - 1422 samples (32 per mini-batch)
2023-06-16 02:08:48,809 - Epoch: [80][   10/   45]    Loss 1.023174    Top1 69.062500    
2023-06-16 02:08:52,950 - Epoch: [80][   20/   45]    Loss 0.881566    Top1 73.750000    
2023-06-16 02:08:57,181 - Epoch: [80][   30/   45]    Loss 0.865016    Top1 73.229167    
2023-06-16 02:09:01,631 - Epoch: [80][   40/   45]    Loss 0.890077    Top1 72.734375    
2023-06-16 02:09:03,173 - Epoch: [80][   45/   45]    Loss 0.899400    Top1 72.784810    
2023-06-16 02:09:03,819 - ==> Top1: 72.785    Loss: 0.899

2023-06-16 02:09:03,822 - ==> Best [Top1: 73.980   Sparsity:0.00   Params: 375264 on epoch: 72]
2023-06-16 02:09:03,822 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:09:03,843 - 

2023-06-16 02:09:03,843 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:09:13,129 - Epoch: [81][   10/  142]    Overall Loss 0.541443    Objective Loss 0.541443                                        LR 0.000031    Time 0.928503    
2023-06-16 02:09:18,009 - Epoch: [81][   20/  142]    Overall Loss 0.522069    Objective Loss 0.522069                                        LR 0.000031    Time 0.708247    
2023-06-16 02:09:22,994 - Epoch: [81][   30/  142]    Overall Loss 0.552868    Objective Loss 0.552868                                        LR 0.000031    Time 0.638302    
2023-06-16 02:09:27,957 - Epoch: [81][   40/  142]    Overall Loss 0.544810    Objective Loss 0.544810                                        LR 0.000031    Time 0.602772    
2023-06-16 02:09:32,893 - Epoch: [81][   50/  142]    Overall Loss 0.555983    Objective Loss 0.555983                                        LR 0.000031    Time 0.580925    
2023-06-16 02:09:37,874 - Epoch: [81][   60/  142]    Overall Loss 0.538760    Objective Loss 0.538760                                        LR 0.000031    Time 0.567125    
2023-06-16 02:09:42,735 - Epoch: [81][   70/  142]    Overall Loss 0.539729    Objective Loss 0.539729                                        LR 0.000031    Time 0.555536    
2023-06-16 02:09:47,686 - Epoch: [81][   80/  142]    Overall Loss 0.546130    Objective Loss 0.546130                                        LR 0.000031    Time 0.547981    
2023-06-16 02:09:52,613 - Epoch: [81][   90/  142]    Overall Loss 0.536499    Objective Loss 0.536499                                        LR 0.000031    Time 0.541824    
2023-06-16 02:09:57,626 - Epoch: [81][  100/  142]    Overall Loss 0.538231    Objective Loss 0.538231                                        LR 0.000031    Time 0.537769    
2023-06-16 02:10:02,577 - Epoch: [81][  110/  142]    Overall Loss 0.532597    Objective Loss 0.532597                                        LR 0.000031    Time 0.533888    
2023-06-16 02:10:07,474 - Epoch: [81][  120/  142]    Overall Loss 0.534228    Objective Loss 0.534228                                        LR 0.000031    Time 0.530194    
2023-06-16 02:10:12,416 - Epoch: [81][  130/  142]    Overall Loss 0.532438    Objective Loss 0.532438                                        LR 0.000031    Time 0.527427    
2023-06-16 02:10:16,993 - Epoch: [81][  140/  142]    Overall Loss 0.535411    Objective Loss 0.535411                                        LR 0.000031    Time 0.522443    
2023-06-16 02:10:17,840 - Epoch: [81][  142/  142]    Overall Loss 0.535984    Objective Loss 0.535984    Top1 79.687500    LR 0.000031    Time 0.521048    
2023-06-16 02:10:18,500 - --- validate (epoch=81)-----------
2023-06-16 02:10:18,500 - 1422 samples (32 per mini-batch)
2023-06-16 02:10:26,705 - Epoch: [81][   10/   45]    Loss 0.921517    Top1 74.062500    
2023-06-16 02:10:31,167 - Epoch: [81][   20/   45]    Loss 0.900135    Top1 74.375000    
2023-06-16 02:10:35,921 - Epoch: [81][   30/   45]    Loss 0.913319    Top1 73.854167    
2023-06-16 02:10:39,889 - Epoch: [81][   40/   45]    Loss 0.920456    Top1 73.281250    
2023-06-16 02:10:41,620 - Epoch: [81][   45/   45]    Loss 0.897149    Top1 74.191280    
2023-06-16 02:10:42,277 - ==> Top1: 74.191    Loss: 0.897

2023-06-16 02:10:42,279 - ==> Best [Top1: 74.191   Sparsity:0.00   Params: 375264 on epoch: 81]
2023-06-16 02:10:42,279 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:10:42,304 - 

2023-06-16 02:10:42,304 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:10:51,462 - Epoch: [82][   10/  142]    Overall Loss 0.533740    Objective Loss 0.533740                                        LR 0.000031    Time 0.915678    
2023-06-16 02:10:56,368 - Epoch: [82][   20/  142]    Overall Loss 0.524068    Objective Loss 0.524068                                        LR 0.000031    Time 0.703135    
2023-06-16 02:11:01,261 - Epoch: [82][   30/  142]    Overall Loss 0.511202    Objective Loss 0.511202                                        LR 0.000031    Time 0.631820    
2023-06-16 02:11:06,182 - Epoch: [82][   40/  142]    Overall Loss 0.537686    Objective Loss 0.537686                                        LR 0.000031    Time 0.596870    
2023-06-16 02:11:11,097 - Epoch: [82][   50/  142]    Overall Loss 0.530686    Objective Loss 0.530686                                        LR 0.000031    Time 0.575794    
2023-06-16 02:11:16,131 - Epoch: [82][   60/  142]    Overall Loss 0.544733    Objective Loss 0.544733                                        LR 0.000031    Time 0.563717    
2023-06-16 02:11:21,050 - Epoch: [82][   70/  142]    Overall Loss 0.542603    Objective Loss 0.542603                                        LR 0.000031    Time 0.553457    
2023-06-16 02:11:26,013 - Epoch: [82][   80/  142]    Overall Loss 0.544270    Objective Loss 0.544270                                        LR 0.000031    Time 0.546305    
2023-06-16 02:11:30,993 - Epoch: [82][   90/  142]    Overall Loss 0.537466    Objective Loss 0.537466                                        LR 0.000031    Time 0.540933    
2023-06-16 02:11:35,929 - Epoch: [82][  100/  142]    Overall Loss 0.532929    Objective Loss 0.532929                                        LR 0.000031    Time 0.536189    
2023-06-16 02:11:40,780 - Epoch: [82][  110/  142]    Overall Loss 0.527717    Objective Loss 0.527717                                        LR 0.000031    Time 0.531542    
2023-06-16 02:11:45,781 - Epoch: [82][  120/  142]    Overall Loss 0.527190    Objective Loss 0.527190                                        LR 0.000031    Time 0.528916    
2023-06-16 02:11:50,785 - Epoch: [82][  130/  142]    Overall Loss 0.527609    Objective Loss 0.527609                                        LR 0.000031    Time 0.526720    
2023-06-16 02:11:55,347 - Epoch: [82][  140/  142]    Overall Loss 0.532314    Objective Loss 0.532314                                        LR 0.000031    Time 0.521680    
2023-06-16 02:11:56,194 - Epoch: [82][  142/  142]    Overall Loss 0.532142    Objective Loss 0.532142    Top1 81.250000    LR 0.000031    Time 0.520298    
2023-06-16 02:11:56,837 - --- validate (epoch=82)-----------
2023-06-16 02:11:56,838 - 1422 samples (32 per mini-batch)
2023-06-16 02:12:04,935 - Epoch: [82][   10/   45]    Loss 0.912318    Top1 75.000000    
2023-06-16 02:12:09,241 - Epoch: [82][   20/   45]    Loss 0.881820    Top1 74.687500    
2023-06-16 02:12:13,632 - Epoch: [82][   30/   45]    Loss 0.850830    Top1 75.208333    
2023-06-16 02:12:18,532 - Epoch: [82][   40/   45]    Loss 0.877219    Top1 73.515625    
2023-06-16 02:12:19,826 - Epoch: [82][   45/   45]    Loss 0.868586    Top1 73.699015    
2023-06-16 02:12:20,478 - ==> Top1: 73.699    Loss: 0.869

2023-06-16 02:12:20,480 - ==> Best [Top1: 74.191   Sparsity:0.00   Params: 375264 on epoch: 81]
2023-06-16 02:12:20,480 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:12:20,494 - 

2023-06-16 02:12:20,494 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:12:29,354 - Epoch: [83][   10/  142]    Overall Loss 0.466792    Objective Loss 0.466792                                        LR 0.000031    Time 0.885885    
2023-06-16 02:12:34,580 - Epoch: [83][   20/  142]    Overall Loss 0.527549    Objective Loss 0.527549                                        LR 0.000031    Time 0.704197    
2023-06-16 02:12:39,700 - Epoch: [83][   30/  142]    Overall Loss 0.529791    Objective Loss 0.529791                                        LR 0.000031    Time 0.640108    
2023-06-16 02:12:44,613 - Epoch: [83][   40/  142]    Overall Loss 0.522862    Objective Loss 0.522862                                        LR 0.000031    Time 0.602898    
2023-06-16 02:12:49,667 - Epoch: [83][   50/  142]    Overall Loss 0.514506    Objective Loss 0.514506                                        LR 0.000031    Time 0.583373    
2023-06-16 02:12:54,559 - Epoch: [83][   60/  142]    Overall Loss 0.518011    Objective Loss 0.518011                                        LR 0.000031    Time 0.567671    
2023-06-16 02:12:59,648 - Epoch: [83][   70/  142]    Overall Loss 0.531302    Objective Loss 0.531302                                        LR 0.000031    Time 0.559273    
2023-06-16 02:13:04,532 - Epoch: [83][   80/  142]    Overall Loss 0.544278    Objective Loss 0.544278                                        LR 0.000031    Time 0.550411    
2023-06-16 02:13:09,686 - Epoch: [83][   90/  142]    Overall Loss 0.550493    Objective Loss 0.550493                                        LR 0.000031    Time 0.546505    
2023-06-16 02:13:14,546 - Epoch: [83][  100/  142]    Overall Loss 0.542117    Objective Loss 0.542117                                        LR 0.000031    Time 0.540451    
2023-06-16 02:13:19,646 - Epoch: [83][  110/  142]    Overall Loss 0.538376    Objective Loss 0.538376                                        LR 0.000031    Time 0.537678    
2023-06-16 02:13:24,519 - Epoch: [83][  120/  142]    Overall Loss 0.539425    Objective Loss 0.539425                                        LR 0.000031    Time 0.533478    
2023-06-16 02:13:29,571 - Epoch: [83][  130/  142]    Overall Loss 0.542439    Objective Loss 0.542439                                        LR 0.000031    Time 0.531297    
2023-06-16 02:13:34,178 - Epoch: [83][  140/  142]    Overall Loss 0.542631    Objective Loss 0.542631                                        LR 0.000031    Time 0.526253    
2023-06-16 02:13:35,036 - Epoch: [83][  142/  142]    Overall Loss 0.542667    Objective Loss 0.542667    Top1 81.250000    LR 0.000031    Time 0.524881    
2023-06-16 02:13:35,630 - --- validate (epoch=83)-----------
2023-06-16 02:13:35,631 - 1422 samples (32 per mini-batch)
2023-06-16 02:13:43,862 - Epoch: [83][   10/   45]    Loss 0.838115    Top1 75.625000    
2023-06-16 02:13:47,838 - Epoch: [83][   20/   45]    Loss 0.830757    Top1 75.937500    
2023-06-16 02:13:52,974 - Epoch: [83][   30/   45]    Loss 0.843864    Top1 75.312500    
2023-06-16 02:13:57,649 - Epoch: [83][   40/   45]    Loss 0.844631    Top1 74.687500    
2023-06-16 02:13:58,947 - Epoch: [83][   45/   45]    Loss 0.844188    Top1 74.753868    
2023-06-16 02:13:59,561 - ==> Top1: 74.754    Loss: 0.844

2023-06-16 02:13:59,563 - ==> Best [Top1: 74.754   Sparsity:0.00   Params: 375264 on epoch: 83]
2023-06-16 02:13:59,563 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:13:59,581 - 

2023-06-16 02:13:59,582 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:14:08,914 - Epoch: [84][   10/  142]    Overall Loss 0.489027    Objective Loss 0.489027                                        LR 0.000031    Time 0.933148    
2023-06-16 02:14:13,839 - Epoch: [84][   20/  142]    Overall Loss 0.514251    Objective Loss 0.514251                                        LR 0.000031    Time 0.712786    
2023-06-16 02:14:18,924 - Epoch: [84][   30/  142]    Overall Loss 0.542664    Objective Loss 0.542664                                        LR 0.000031    Time 0.644666    
2023-06-16 02:14:23,974 - Epoch: [84][   40/  142]    Overall Loss 0.539633    Objective Loss 0.539633                                        LR 0.000031    Time 0.609746    
2023-06-16 02:14:28,847 - Epoch: [84][   50/  142]    Overall Loss 0.529417    Objective Loss 0.529417                                        LR 0.000031    Time 0.585248    
2023-06-16 02:14:33,819 - Epoch: [84][   60/  142]    Overall Loss 0.526250    Objective Loss 0.526250                                        LR 0.000031    Time 0.570551    
2023-06-16 02:14:38,824 - Epoch: [84][   70/  142]    Overall Loss 0.532832    Objective Loss 0.532832                                        LR 0.000031    Time 0.560541    
2023-06-16 02:14:43,726 - Epoch: [84][   80/  142]    Overall Loss 0.536960    Objective Loss 0.536960                                        LR 0.000031    Time 0.551743    
2023-06-16 02:14:48,610 - Epoch: [84][   90/  142]    Overall Loss 0.527904    Objective Loss 0.527904                                        LR 0.000031    Time 0.544697    
2023-06-16 02:14:53,562 - Epoch: [84][  100/  142]    Overall Loss 0.530504    Objective Loss 0.530504                                        LR 0.000031    Time 0.539746    
2023-06-16 02:14:58,519 - Epoch: [84][  110/  142]    Overall Loss 0.528544    Objective Loss 0.528544                                        LR 0.000031    Time 0.535735    
2023-06-16 02:15:03,446 - Epoch: [84][  120/  142]    Overall Loss 0.532747    Objective Loss 0.532747                                        LR 0.000031    Time 0.532149    
2023-06-16 02:15:08,353 - Epoch: [84][  130/  142]    Overall Loss 0.528860    Objective Loss 0.528860                                        LR 0.000031    Time 0.528959    
2023-06-16 02:15:13,027 - Epoch: [84][  140/  142]    Overall Loss 0.532766    Objective Loss 0.532766                                        LR 0.000031    Time 0.524558    
2023-06-16 02:15:13,882 - Epoch: [84][  142/  142]    Overall Loss 0.532862    Objective Loss 0.532862    Top1 79.687500    LR 0.000031    Time 0.523190    
2023-06-16 02:15:14,523 - --- validate (epoch=84)-----------
2023-06-16 02:15:14,523 - 1422 samples (32 per mini-batch)
2023-06-16 02:15:22,501 - Epoch: [84][   10/   45]    Loss 0.929062    Top1 74.375000    
2023-06-16 02:15:26,591 - Epoch: [84][   20/   45]    Loss 0.897510    Top1 74.218750    
2023-06-16 02:15:30,828 - Epoch: [84][   30/   45]    Loss 0.901674    Top1 74.062500    
2023-06-16 02:15:35,248 - Epoch: [84][   40/   45]    Loss 0.922382    Top1 73.046875    
2023-06-16 02:15:36,647 - Epoch: [84][   45/   45]    Loss 0.918244    Top1 73.206751    
2023-06-16 02:15:37,310 - ==> Top1: 73.207    Loss: 0.918

2023-06-16 02:15:37,312 - ==> Best [Top1: 74.754   Sparsity:0.00   Params: 375264 on epoch: 83]
2023-06-16 02:15:37,312 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:15:37,334 - 

2023-06-16 02:15:37,334 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:15:46,667 - Epoch: [85][   10/  142]    Overall Loss 0.612626    Objective Loss 0.612626                                        LR 0.000031    Time 0.933163    
2023-06-16 02:15:51,668 - Epoch: [85][   20/  142]    Overall Loss 0.550137    Objective Loss 0.550137                                        LR 0.000031    Time 0.716597    
2023-06-16 02:15:56,408 - Epoch: [85][   30/  142]    Overall Loss 0.550952    Objective Loss 0.550952                                        LR 0.000031    Time 0.635723    
2023-06-16 02:16:01,467 - Epoch: [85][   40/  142]    Overall Loss 0.559519    Objective Loss 0.559519                                        LR 0.000031    Time 0.603253    
2023-06-16 02:16:06,365 - Epoch: [85][   50/  142]    Overall Loss 0.557015    Objective Loss 0.557015                                        LR 0.000031    Time 0.580540    
2023-06-16 02:16:11,340 - Epoch: [85][   60/  142]    Overall Loss 0.549036    Objective Loss 0.549036                                        LR 0.000031    Time 0.566702    
2023-06-16 02:16:16,402 - Epoch: [85][   70/  142]    Overall Loss 0.557275    Objective Loss 0.557275                                        LR 0.000031    Time 0.558045    
2023-06-16 02:16:21,300 - Epoch: [85][   80/  142]    Overall Loss 0.561528    Objective Loss 0.561528                                        LR 0.000031    Time 0.549502    
2023-06-16 02:16:26,251 - Epoch: [85][   90/  142]    Overall Loss 0.556921    Objective Loss 0.556921                                        LR 0.000031    Time 0.543452    
2023-06-16 02:16:31,174 - Epoch: [85][  100/  142]    Overall Loss 0.550009    Objective Loss 0.550009                                        LR 0.000031    Time 0.538330    
2023-06-16 02:16:36,116 - Epoch: [85][  110/  142]    Overall Loss 0.543757    Objective Loss 0.543757                                        LR 0.000031    Time 0.534316    
2023-06-16 02:16:41,139 - Epoch: [85][  120/  142]    Overall Loss 0.539836    Objective Loss 0.539836                                        LR 0.000031    Time 0.531643    
2023-06-16 02:16:46,016 - Epoch: [85][  130/  142]    Overall Loss 0.535726    Objective Loss 0.535726                                        LR 0.000031    Time 0.528262    
2023-06-16 02:16:50,584 - Epoch: [85][  140/  142]    Overall Loss 0.527698    Objective Loss 0.527698                                        LR 0.000031    Time 0.523154    
2023-06-16 02:16:51,425 - Epoch: [85][  142/  142]    Overall Loss 0.528190    Objective Loss 0.528190    Top1 81.250000    LR 0.000031    Time 0.521708    
2023-06-16 02:16:52,078 - --- validate (epoch=85)-----------
2023-06-16 02:16:52,079 - 1422 samples (32 per mini-batch)
2023-06-16 02:17:00,056 - Epoch: [85][   10/   45]    Loss 0.914994    Top1 74.687500    
2023-06-16 02:17:04,229 - Epoch: [85][   20/   45]    Loss 0.905654    Top1 74.218750    
2023-06-16 02:17:09,236 - Epoch: [85][   30/   45]    Loss 0.926816    Top1 72.916667    
2023-06-16 02:17:13,397 - Epoch: [85][   40/   45]    Loss 0.923366    Top1 73.750000    
2023-06-16 02:17:14,767 - Epoch: [85][   45/   45]    Loss 0.900178    Top1 74.050633    
2023-06-16 02:17:15,418 - ==> Top1: 74.051    Loss: 0.900

2023-06-16 02:17:15,420 - ==> Best [Top1: 74.754   Sparsity:0.00   Params: 375264 on epoch: 83]
2023-06-16 02:17:15,420 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:17:15,441 - 

2023-06-16 02:17:15,441 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:17:24,638 - Epoch: [86][   10/  142]    Overall Loss 0.567253    Objective Loss 0.567253                                        LR 0.000031    Time 0.919576    
2023-06-16 02:17:29,665 - Epoch: [86][   20/  142]    Overall Loss 0.538902    Objective Loss 0.538902                                        LR 0.000031    Time 0.711089    
2023-06-16 02:17:34,678 - Epoch: [86][   30/  142]    Overall Loss 0.532538    Objective Loss 0.532538                                        LR 0.000031    Time 0.641134    
2023-06-16 02:17:39,659 - Epoch: [86][   40/  142]    Overall Loss 0.534163    Objective Loss 0.534163                                        LR 0.000031    Time 0.605365    
2023-06-16 02:17:44,669 - Epoch: [86][   50/  142]    Overall Loss 0.520252    Objective Loss 0.520252                                        LR 0.000031    Time 0.584487    
2023-06-16 02:17:49,618 - Epoch: [86][   60/  142]    Overall Loss 0.534314    Objective Loss 0.534314                                        LR 0.000031    Time 0.569539    
2023-06-16 02:17:54,587 - Epoch: [86][   70/  142]    Overall Loss 0.539025    Objective Loss 0.539025                                        LR 0.000031    Time 0.559159    
2023-06-16 02:17:59,535 - Epoch: [86][   80/  142]    Overall Loss 0.539883    Objective Loss 0.539883                                        LR 0.000031    Time 0.551103    
2023-06-16 02:18:04,575 - Epoch: [86][   90/  142]    Overall Loss 0.545916    Objective Loss 0.545916                                        LR 0.000031    Time 0.545853    
2023-06-16 02:18:09,575 - Epoch: [86][  100/  142]    Overall Loss 0.544612    Objective Loss 0.544612                                        LR 0.000031    Time 0.541264    
2023-06-16 02:18:14,495 - Epoch: [86][  110/  142]    Overall Loss 0.552790    Objective Loss 0.552790                                        LR 0.000031    Time 0.536774    
2023-06-16 02:18:19,432 - Epoch: [86][  120/  142]    Overall Loss 0.550624    Objective Loss 0.550624                                        LR 0.000031    Time 0.533181    
2023-06-16 02:18:24,395 - Epoch: [86][  130/  142]    Overall Loss 0.542782    Objective Loss 0.542782                                        LR 0.000031    Time 0.530337    
2023-06-16 02:18:28,984 - Epoch: [86][  140/  142]    Overall Loss 0.540192    Objective Loss 0.540192                                        LR 0.000031    Time 0.525233    
2023-06-16 02:18:29,824 - Epoch: [86][  142/  142]    Overall Loss 0.539225    Objective Loss 0.539225    Top1 79.687500    LR 0.000031    Time 0.523750    
2023-06-16 02:18:30,468 - --- validate (epoch=86)-----------
2023-06-16 02:18:30,468 - 1422 samples (32 per mini-batch)
2023-06-16 02:18:38,173 - Epoch: [86][   10/   45]    Loss 0.741727    Top1 75.312500    
2023-06-16 02:18:43,028 - Epoch: [86][   20/   45]    Loss 0.803913    Top1 75.937500    
2023-06-16 02:18:47,461 - Epoch: [86][   30/   45]    Loss 0.884589    Top1 74.791667    
2023-06-16 02:18:52,219 - Epoch: [86][   40/   45]    Loss 0.878550    Top1 74.453125    
2023-06-16 02:18:53,565 - Epoch: [86][   45/   45]    Loss 0.898977    Top1 73.769339    
2023-06-16 02:18:54,208 - ==> Top1: 73.769    Loss: 0.899

2023-06-16 02:18:54,210 - ==> Best [Top1: 74.754   Sparsity:0.00   Params: 375264 on epoch: 83]
2023-06-16 02:18:54,210 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:18:54,231 - 

2023-06-16 02:18:54,231 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:19:03,628 - Epoch: [87][   10/  142]    Overall Loss 0.483880    Objective Loss 0.483880                                        LR 0.000031    Time 0.939582    
2023-06-16 02:19:08,402 - Epoch: [87][   20/  142]    Overall Loss 0.462335    Objective Loss 0.462335                                        LR 0.000031    Time 0.708450    
2023-06-16 02:19:13,375 - Epoch: [87][   30/  142]    Overall Loss 0.477528    Objective Loss 0.477528                                        LR 0.000031    Time 0.638061    
2023-06-16 02:19:18,287 - Epoch: [87][   40/  142]    Overall Loss 0.495867    Objective Loss 0.495867                                        LR 0.000031    Time 0.601322    
2023-06-16 02:19:23,141 - Epoch: [87][   50/  142]    Overall Loss 0.474467    Objective Loss 0.474467                                        LR 0.000031    Time 0.578134    
2023-06-16 02:19:28,148 - Epoch: [87][   60/  142]    Overall Loss 0.491353    Objective Loss 0.491353                                        LR 0.000031    Time 0.565224    
2023-06-16 02:19:33,050 - Epoch: [87][   70/  142]    Overall Loss 0.499768    Objective Loss 0.499768                                        LR 0.000031    Time 0.554494    
2023-06-16 02:19:38,029 - Epoch: [87][   80/  142]    Overall Loss 0.501215    Objective Loss 0.501215                                        LR 0.000031    Time 0.546755    
2023-06-16 02:19:42,834 - Epoch: [87][   90/  142]    Overall Loss 0.505750    Objective Loss 0.505750                                        LR 0.000031    Time 0.539388    
2023-06-16 02:19:47,832 - Epoch: [87][  100/  142]    Overall Loss 0.514145    Objective Loss 0.514145                                        LR 0.000031    Time 0.535430    
2023-06-16 02:19:52,754 - Epoch: [87][  110/  142]    Overall Loss 0.514519    Objective Loss 0.514519                                        LR 0.000031    Time 0.531488    
2023-06-16 02:19:57,723 - Epoch: [87][  120/  142]    Overall Loss 0.520194    Objective Loss 0.520194                                        LR 0.000031    Time 0.528600    
2023-06-16 02:20:02,678 - Epoch: [87][  130/  142]    Overall Loss 0.523978    Objective Loss 0.523978                                        LR 0.000031    Time 0.526052    
2023-06-16 02:20:07,313 - Epoch: [87][  140/  142]    Overall Loss 0.525609    Objective Loss 0.525609                                        LR 0.000031    Time 0.521582    
2023-06-16 02:20:08,168 - Epoch: [87][  142/  142]    Overall Loss 0.526063    Objective Loss 0.526063    Top1 79.687500    LR 0.000031    Time 0.520251    
2023-06-16 02:20:08,815 - --- validate (epoch=87)-----------
2023-06-16 02:20:08,815 - 1422 samples (32 per mini-batch)
2023-06-16 02:20:16,713 - Epoch: [87][   10/   45]    Loss 0.918892    Top1 73.125000    
2023-06-16 02:20:20,778 - Epoch: [87][   20/   45]    Loss 0.925726    Top1 72.500000    
2023-06-16 02:20:25,184 - Epoch: [87][   30/   45]    Loss 0.893077    Top1 74.270833    
2023-06-16 02:20:29,957 - Epoch: [87][   40/   45]    Loss 0.877662    Top1 73.984375    
2023-06-16 02:20:31,355 - Epoch: [87][   45/   45]    Loss 0.880694    Top1 74.050633    
2023-06-16 02:20:32,013 - ==> Top1: 74.051    Loss: 0.881

2023-06-16 02:20:32,016 - ==> Best [Top1: 74.754   Sparsity:0.00   Params: 375264 on epoch: 83]
2023-06-16 02:20:32,016 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:20:32,029 - 

2023-06-16 02:20:32,029 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:20:41,394 - Epoch: [88][   10/  142]    Overall Loss 0.582475    Objective Loss 0.582475                                        LR 0.000031    Time 0.936378    
2023-06-16 02:20:46,262 - Epoch: [88][   20/  142]    Overall Loss 0.626441    Objective Loss 0.626441                                        LR 0.000031    Time 0.711516    
2023-06-16 02:20:51,291 - Epoch: [88][   30/  142]    Overall Loss 0.623172    Objective Loss 0.623172                                        LR 0.000031    Time 0.641978    
2023-06-16 02:20:56,233 - Epoch: [88][   40/  142]    Overall Loss 0.591908    Objective Loss 0.591908                                        LR 0.000031    Time 0.605017    
2023-06-16 02:21:01,272 - Epoch: [88][   50/  142]    Overall Loss 0.588629    Objective Loss 0.588629                                        LR 0.000031    Time 0.584781    
2023-06-16 02:21:06,109 - Epoch: [88][   60/  142]    Overall Loss 0.566537    Objective Loss 0.566537                                        LR 0.000031    Time 0.567920    
2023-06-16 02:21:11,114 - Epoch: [88][   70/  142]    Overall Loss 0.547646    Objective Loss 0.547646                                        LR 0.000031    Time 0.558277    
2023-06-16 02:21:16,113 - Epoch: [88][   80/  142]    Overall Loss 0.543498    Objective Loss 0.543498                                        LR 0.000031    Time 0.550974    
2023-06-16 02:21:21,048 - Epoch: [88][   90/  142]    Overall Loss 0.538419    Objective Loss 0.538419                                        LR 0.000031    Time 0.544588    
2023-06-16 02:21:26,014 - Epoch: [88][  100/  142]    Overall Loss 0.533165    Objective Loss 0.533165                                        LR 0.000031    Time 0.539785    
2023-06-16 02:21:31,103 - Epoch: [88][  110/  142]    Overall Loss 0.529712    Objective Loss 0.529712                                        LR 0.000031    Time 0.536974    
2023-06-16 02:21:35,968 - Epoch: [88][  120/  142]    Overall Loss 0.530694    Objective Loss 0.530694                                        LR 0.000031    Time 0.532763    
2023-06-16 02:21:40,946 - Epoch: [88][  130/  142]    Overall Loss 0.533391    Objective Loss 0.533391                                        LR 0.000031    Time 0.530069    
2023-06-16 02:21:45,623 - Epoch: [88][  140/  142]    Overall Loss 0.533851    Objective Loss 0.533851                                        LR 0.000031    Time 0.525610    
2023-06-16 02:21:46,478 - Epoch: [88][  142/  142]    Overall Loss 0.535497    Objective Loss 0.535497    Top1 73.437500    LR 0.000031    Time 0.524224    
2023-06-16 02:21:47,145 - --- validate (epoch=88)-----------
2023-06-16 02:21:47,146 - 1422 samples (32 per mini-batch)
2023-06-16 02:21:55,331 - Epoch: [88][   10/   45]    Loss 0.860211    Top1 79.375000    
2023-06-16 02:21:59,338 - Epoch: [88][   20/   45]    Loss 0.936042    Top1 74.687500    
2023-06-16 02:22:03,578 - Epoch: [88][   30/   45]    Loss 0.947904    Top1 73.854167    
2023-06-16 02:22:07,705 - Epoch: [88][   40/   45]    Loss 0.903887    Top1 73.906250    
2023-06-16 02:22:09,108 - Epoch: [88][   45/   45]    Loss 0.922573    Top1 73.769339    
2023-06-16 02:22:09,709 - ==> Top1: 73.769    Loss: 0.923

2023-06-16 02:22:09,711 - ==> Best [Top1: 74.754   Sparsity:0.00   Params: 375264 on epoch: 83]
2023-06-16 02:22:09,711 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:22:09,725 - 

2023-06-16 02:22:09,725 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:22:19,134 - Epoch: [89][   10/  142]    Overall Loss 0.528521    Objective Loss 0.528521                                        LR 0.000031    Time 0.940831    
2023-06-16 02:22:24,029 - Epoch: [89][   20/  142]    Overall Loss 0.512720    Objective Loss 0.512720                                        LR 0.000031    Time 0.715119    
2023-06-16 02:22:29,065 - Epoch: [89][   30/  142]    Overall Loss 0.528238    Objective Loss 0.528238                                        LR 0.000031    Time 0.644606    
2023-06-16 02:22:33,984 - Epoch: [89][   40/  142]    Overall Loss 0.534876    Objective Loss 0.534876                                        LR 0.000031    Time 0.606403    
2023-06-16 02:22:38,834 - Epoch: [89][   50/  142]    Overall Loss 0.527709    Objective Loss 0.527709                                        LR 0.000031    Time 0.582125    
2023-06-16 02:22:43,809 - Epoch: [89][   60/  142]    Overall Loss 0.533220    Objective Loss 0.533220                                        LR 0.000031    Time 0.568009    
2023-06-16 02:22:48,766 - Epoch: [89][   70/  142]    Overall Loss 0.526932    Objective Loss 0.526932                                        LR 0.000031    Time 0.557669    
2023-06-16 02:22:53,605 - Epoch: [89][   80/  142]    Overall Loss 0.528083    Objective Loss 0.528083                                        LR 0.000031    Time 0.548433    
2023-06-16 02:22:58,541 - Epoch: [89][   90/  142]    Overall Loss 0.529843    Objective Loss 0.529843                                        LR 0.000031    Time 0.542339    
2023-06-16 02:23:03,481 - Epoch: [89][  100/  142]    Overall Loss 0.522121    Objective Loss 0.522121                                        LR 0.000031    Time 0.537500    
2023-06-16 02:23:08,415 - Epoch: [89][  110/  142]    Overall Loss 0.524532    Objective Loss 0.524532                                        LR 0.000031    Time 0.533481    
2023-06-16 02:23:13,425 - Epoch: [89][  120/  142]    Overall Loss 0.525320    Objective Loss 0.525320                                        LR 0.000031    Time 0.530769    
2023-06-16 02:23:18,380 - Epoch: [89][  130/  142]    Overall Loss 0.531876    Objective Loss 0.531876                                        LR 0.000031    Time 0.528053    
2023-06-16 02:23:22,949 - Epoch: [89][  140/  142]    Overall Loss 0.534142    Objective Loss 0.534142                                        LR 0.000031    Time 0.522972    
2023-06-16 02:23:23,790 - Epoch: [89][  142/  142]    Overall Loss 0.533714    Objective Loss 0.533714    Top1 78.125000    LR 0.000031    Time 0.521523    
2023-06-16 02:23:24,447 - --- validate (epoch=89)-----------
2023-06-16 02:23:24,447 - 1422 samples (32 per mini-batch)
2023-06-16 02:23:32,524 - Epoch: [89][   10/   45]    Loss 0.978068    Top1 73.750000    
2023-06-16 02:23:36,589 - Epoch: [89][   20/   45]    Loss 0.911941    Top1 73.281250    
2023-06-16 02:23:41,236 - Epoch: [89][   30/   45]    Loss 0.938561    Top1 72.500000    
2023-06-16 02:23:45,264 - Epoch: [89][   40/   45]    Loss 0.909536    Top1 73.750000    
2023-06-16 02:23:46,848 - Epoch: [89][   45/   45]    Loss 0.903234    Top1 74.120956    
2023-06-16 02:23:47,481 - ==> Top1: 74.121    Loss: 0.903

2023-06-16 02:23:47,483 - ==> Best [Top1: 74.754   Sparsity:0.00   Params: 375264 on epoch: 83]
2023-06-16 02:23:47,483 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:23:47,504 - 

2023-06-16 02:23:47,504 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:23:56,683 - Epoch: [90][   10/  142]    Overall Loss 0.560319    Objective Loss 0.560319                                        LR 0.000031    Time 0.917781    
2023-06-16 02:24:01,850 - Epoch: [90][   20/  142]    Overall Loss 0.550330    Objective Loss 0.550330                                        LR 0.000031    Time 0.717191    
2023-06-16 02:24:06,790 - Epoch: [90][   30/  142]    Overall Loss 0.551124    Objective Loss 0.551124                                        LR 0.000031    Time 0.642798    
2023-06-16 02:24:11,882 - Epoch: [90][   40/  142]    Overall Loss 0.540018    Objective Loss 0.540018                                        LR 0.000031    Time 0.609385    
2023-06-16 02:24:16,927 - Epoch: [90][   50/  142]    Overall Loss 0.529581    Objective Loss 0.529581                                        LR 0.000031    Time 0.588396    
2023-06-16 02:24:22,079 - Epoch: [90][   60/  142]    Overall Loss 0.527591    Objective Loss 0.527591                                        LR 0.000031    Time 0.576187    
2023-06-16 02:24:26,992 - Epoch: [90][   70/  142]    Overall Loss 0.516016    Objective Loss 0.516016                                        LR 0.000031    Time 0.564055    
2023-06-16 02:24:32,092 - Epoch: [90][   80/  142]    Overall Loss 0.526860    Objective Loss 0.526860                                        LR 0.000031    Time 0.557283    
2023-06-16 02:24:37,008 - Epoch: [90][   90/  142]    Overall Loss 0.517589    Objective Loss 0.517589                                        LR 0.000031    Time 0.549986    
2023-06-16 02:24:41,943 - Epoch: [90][  100/  142]    Overall Loss 0.522290    Objective Loss 0.522290                                        LR 0.000031    Time 0.544324    
2023-06-16 02:24:46,993 - Epoch: [90][  110/  142]    Overall Loss 0.525182    Objective Loss 0.525182                                        LR 0.000031    Time 0.540750    
2023-06-16 02:24:52,132 - Epoch: [90][  120/  142]    Overall Loss 0.522398    Objective Loss 0.522398                                        LR 0.000031    Time 0.538505    
2023-06-16 02:24:57,065 - Epoch: [90][  130/  142]    Overall Loss 0.518779    Objective Loss 0.518779                                        LR 0.000031    Time 0.535024    
2023-06-16 02:25:01,753 - Epoch: [90][  140/  142]    Overall Loss 0.521471    Objective Loss 0.521471                                        LR 0.000031    Time 0.530291    
2023-06-16 02:25:02,607 - Epoch: [90][  142/  142]    Overall Loss 0.521963    Objective Loss 0.521963    Top1 84.375000    LR 0.000031    Time 0.528837    
2023-06-16 02:25:03,262 - --- validate (epoch=90)-----------
2023-06-16 02:25:03,263 - 1422 samples (32 per mini-batch)
2023-06-16 02:25:11,477 - Epoch: [90][   10/   45]    Loss 0.705160    Top1 77.500000    
2023-06-16 02:25:15,684 - Epoch: [90][   20/   45]    Loss 0.800852    Top1 74.843750    
2023-06-16 02:25:20,307 - Epoch: [90][   30/   45]    Loss 0.812186    Top1 74.270833    
2023-06-16 02:25:24,490 - Epoch: [90][   40/   45]    Loss 0.849841    Top1 73.593750    
2023-06-16 02:25:25,887 - Epoch: [90][   45/   45]    Loss 0.828338    Top1 74.050633    
2023-06-16 02:25:26,520 - ==> Top1: 74.051    Loss: 0.828

2023-06-16 02:25:26,523 - ==> Best [Top1: 74.754   Sparsity:0.00   Params: 375264 on epoch: 83]
2023-06-16 02:25:26,523 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:25:26,544 - 

2023-06-16 02:25:26,544 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:25:35,853 - Epoch: [91][   10/  142]    Overall Loss 0.483980    Objective Loss 0.483980                                        LR 0.000031    Time 0.930755    
2023-06-16 02:25:40,793 - Epoch: [91][   20/  142]    Overall Loss 0.480493    Objective Loss 0.480493                                        LR 0.000031    Time 0.712341    
2023-06-16 02:25:45,674 - Epoch: [91][   30/  142]    Overall Loss 0.527400    Objective Loss 0.527400                                        LR 0.000031    Time 0.637578    
2023-06-16 02:25:50,635 - Epoch: [91][   40/  142]    Overall Loss 0.537775    Objective Loss 0.537775                                        LR 0.000031    Time 0.602173    
2023-06-16 02:25:55,589 - Epoch: [91][   50/  142]    Overall Loss 0.539300    Objective Loss 0.539300                                        LR 0.000031    Time 0.580806    
2023-06-16 02:26:00,582 - Epoch: [91][   60/  142]    Overall Loss 0.523903    Objective Loss 0.523903                                        LR 0.000031    Time 0.567215    
2023-06-16 02:26:05,555 - Epoch: [91][   70/  142]    Overall Loss 0.520206    Objective Loss 0.520206                                        LR 0.000031    Time 0.557228    
2023-06-16 02:26:10,578 - Epoch: [91][   80/  142]    Overall Loss 0.524136    Objective Loss 0.524136                                        LR 0.000031    Time 0.550347    
2023-06-16 02:26:15,611 - Epoch: [91][   90/  142]    Overall Loss 0.521754    Objective Loss 0.521754                                        LR 0.000031    Time 0.545115    
2023-06-16 02:26:20,617 - Epoch: [91][  100/  142]    Overall Loss 0.518386    Objective Loss 0.518386                                        LR 0.000031    Time 0.540662    
2023-06-16 02:26:25,684 - Epoch: [91][  110/  142]    Overall Loss 0.521688    Objective Loss 0.521688                                        LR 0.000031    Time 0.537566    
2023-06-16 02:26:30,561 - Epoch: [91][  120/  142]    Overall Loss 0.518104    Objective Loss 0.518104                                        LR 0.000031    Time 0.533410    
2023-06-16 02:26:35,603 - Epoch: [91][  130/  142]    Overall Loss 0.518557    Objective Loss 0.518557                                        LR 0.000031    Time 0.531152    
2023-06-16 02:26:40,327 - Epoch: [91][  140/  142]    Overall Loss 0.516911    Objective Loss 0.516911                                        LR 0.000031    Time 0.526956    
2023-06-16 02:26:41,173 - Epoch: [91][  142/  142]    Overall Loss 0.516078    Objective Loss 0.516078    Top1 84.375000    LR 0.000031    Time 0.525491    
2023-06-16 02:26:41,833 - --- validate (epoch=91)-----------
2023-06-16 02:26:41,833 - 1422 samples (32 per mini-batch)
2023-06-16 02:26:49,950 - Epoch: [91][   10/   45]    Loss 0.831520    Top1 73.750000    
2023-06-16 02:26:54,266 - Epoch: [91][   20/   45]    Loss 0.826079    Top1 75.312500    
2023-06-16 02:26:58,730 - Epoch: [91][   30/   45]    Loss 0.813648    Top1 74.791667    
2023-06-16 02:27:02,818 - Epoch: [91][   40/   45]    Loss 0.830754    Top1 74.218750    
2023-06-16 02:27:04,206 - Epoch: [91][   45/   45]    Loss 0.852429    Top1 74.050633    
2023-06-16 02:27:04,805 - ==> Top1: 74.051    Loss: 0.852

2023-06-16 02:27:04,807 - ==> Best [Top1: 74.754   Sparsity:0.00   Params: 375264 on epoch: 83]
2023-06-16 02:27:04,807 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:27:04,822 - 

2023-06-16 02:27:04,822 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:27:14,320 - Epoch: [92][   10/  142]    Overall Loss 0.519352    Objective Loss 0.519352                                        LR 0.000031    Time 0.949721    
2023-06-16 02:27:19,419 - Epoch: [92][   20/  142]    Overall Loss 0.509929    Objective Loss 0.509929                                        LR 0.000031    Time 0.729784    
2023-06-16 02:27:24,368 - Epoch: [92][   30/  142]    Overall Loss 0.514559    Objective Loss 0.514559                                        LR 0.000031    Time 0.651450    
2023-06-16 02:27:29,395 - Epoch: [92][   40/  142]    Overall Loss 0.505155    Objective Loss 0.505155                                        LR 0.000031    Time 0.614255    
2023-06-16 02:27:34,495 - Epoch: [92][   50/  142]    Overall Loss 0.503863    Objective Loss 0.503863                                        LR 0.000031    Time 0.593381    
2023-06-16 02:27:39,509 - Epoch: [92][   60/  142]    Overall Loss 0.520909    Objective Loss 0.520909                                        LR 0.000031    Time 0.578048    
2023-06-16 02:27:44,410 - Epoch: [92][   70/  142]    Overall Loss 0.529682    Objective Loss 0.529682                                        LR 0.000031    Time 0.565485    
2023-06-16 02:27:49,509 - Epoch: [92][   80/  142]    Overall Loss 0.529217    Objective Loss 0.529217                                        LR 0.000031    Time 0.558524    
2023-06-16 02:27:54,479 - Epoch: [92][   90/  142]    Overall Loss 0.525175    Objective Loss 0.525175                                        LR 0.000031    Time 0.551679    
2023-06-16 02:27:59,554 - Epoch: [92][  100/  142]    Overall Loss 0.531392    Objective Loss 0.531392                                        LR 0.000031    Time 0.547261    
2023-06-16 02:28:04,736 - Epoch: [92][  110/  142]    Overall Loss 0.525265    Objective Loss 0.525265                                        LR 0.000031    Time 0.544616    
2023-06-16 02:28:09,669 - Epoch: [92][  120/  142]    Overall Loss 0.520139    Objective Loss 0.520139                                        LR 0.000031    Time 0.540335    
2023-06-16 02:28:14,846 - Epoch: [92][  130/  142]    Overall Loss 0.511926    Objective Loss 0.511926                                        LR 0.000031    Time 0.538584    
2023-06-16 02:28:19,421 - Epoch: [92][  140/  142]    Overall Loss 0.514507    Objective Loss 0.514507                                        LR 0.000031    Time 0.532795    
2023-06-16 02:28:20,260 - Epoch: [92][  142/  142]    Overall Loss 0.512865    Objective Loss 0.512865    Top1 84.375000    LR 0.000031    Time 0.531198    
2023-06-16 02:28:20,845 - --- validate (epoch=92)-----------
2023-06-16 02:28:20,846 - 1422 samples (32 per mini-batch)
2023-06-16 02:28:29,104 - Epoch: [92][   10/   45]    Loss 0.906208    Top1 73.437500    
2023-06-16 02:28:33,176 - Epoch: [92][   20/   45]    Loss 0.856705    Top1 73.125000    
2023-06-16 02:28:37,176 - Epoch: [92][   30/   45]    Loss 0.858640    Top1 73.958333    
2023-06-16 02:28:42,417 - Epoch: [92][   40/   45]    Loss 0.844400    Top1 74.062500    
2023-06-16 02:28:43,744 - Epoch: [92][   45/   45]    Loss 0.872035    Top1 73.699015    
2023-06-16 02:28:44,334 - ==> Top1: 73.699    Loss: 0.872

2023-06-16 02:28:44,336 - ==> Best [Top1: 74.754   Sparsity:0.00   Params: 375264 on epoch: 83]
2023-06-16 02:28:44,336 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:28:44,357 - 

2023-06-16 02:28:44,358 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:28:53,642 - Epoch: [93][   10/  142]    Overall Loss 0.596577    Objective Loss 0.596577                                        LR 0.000031    Time 0.928354    
2023-06-16 02:28:58,559 - Epoch: [93][   20/  142]    Overall Loss 0.575009    Objective Loss 0.575009                                        LR 0.000031    Time 0.709978    
2023-06-16 02:29:03,426 - Epoch: [93][   30/  142]    Overall Loss 0.551513    Objective Loss 0.551513                                        LR 0.000031    Time 0.635533    
2023-06-16 02:29:08,237 - Epoch: [93][   40/  142]    Overall Loss 0.556830    Objective Loss 0.556830                                        LR 0.000031    Time 0.596919    
2023-06-16 02:29:13,188 - Epoch: [93][   50/  142]    Overall Loss 0.554442    Objective Loss 0.554442                                        LR 0.000031    Time 0.576534    
2023-06-16 02:29:18,074 - Epoch: [93][   60/  142]    Overall Loss 0.558555    Objective Loss 0.558555                                        LR 0.000031    Time 0.561865    
2023-06-16 02:29:23,100 - Epoch: [93][   70/  142]    Overall Loss 0.538176    Objective Loss 0.538176                                        LR 0.000031    Time 0.553400    
2023-06-16 02:29:27,971 - Epoch: [93][   80/  142]    Overall Loss 0.544010    Objective Loss 0.544010                                        LR 0.000031    Time 0.545107    
2023-06-16 02:29:32,933 - Epoch: [93][   90/  142]    Overall Loss 0.536952    Objective Loss 0.536952                                        LR 0.000031    Time 0.539658    
2023-06-16 02:29:37,732 - Epoch: [93][  100/  142]    Overall Loss 0.538036    Objective Loss 0.538036                                        LR 0.000031    Time 0.533683    
2023-06-16 02:29:42,765 - Epoch: [93][  110/  142]    Overall Loss 0.533526    Objective Loss 0.533526                                        LR 0.000031    Time 0.530908    
2023-06-16 02:29:47,676 - Epoch: [93][  120/  142]    Overall Loss 0.532268    Objective Loss 0.532268                                        LR 0.000031    Time 0.527592    
2023-06-16 02:29:52,537 - Epoch: [93][  130/  142]    Overall Loss 0.526889    Objective Loss 0.526889                                        LR 0.000031    Time 0.524392    
2023-06-16 02:29:57,199 - Epoch: [93][  140/  142]    Overall Loss 0.525027    Objective Loss 0.525027                                        LR 0.000031    Time 0.520232    
2023-06-16 02:29:58,041 - Epoch: [93][  142/  142]    Overall Loss 0.523507    Objective Loss 0.523507    Top1 82.812500    LR 0.000031    Time 0.518833    
2023-06-16 02:29:58,704 - --- validate (epoch=93)-----------
2023-06-16 02:29:58,705 - 1422 samples (32 per mini-batch)
2023-06-16 02:30:06,741 - Epoch: [93][   10/   45]    Loss 0.781039    Top1 73.750000    
2023-06-16 02:30:11,179 - Epoch: [93][   20/   45]    Loss 0.837449    Top1 73.437500    
2023-06-16 02:30:16,335 - Epoch: [93][   30/   45]    Loss 0.892220    Top1 72.291667    
2023-06-16 02:30:20,513 - Epoch: [93][   40/   45]    Loss 0.925253    Top1 72.109375    
2023-06-16 02:30:21,925 - Epoch: [93][   45/   45]    Loss 0.919640    Top1 72.081575    
2023-06-16 02:30:22,550 - ==> Top1: 72.082    Loss: 0.920

2023-06-16 02:30:22,552 - ==> Best [Top1: 74.754   Sparsity:0.00   Params: 375264 on epoch: 83]
2023-06-16 02:30:22,552 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:30:22,573 - 

2023-06-16 02:30:22,573 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:30:31,860 - Epoch: [94][   10/  142]    Overall Loss 0.467824    Objective Loss 0.467824                                        LR 0.000031    Time 0.928643    
2023-06-16 02:30:36,928 - Epoch: [94][   20/  142]    Overall Loss 0.459993    Objective Loss 0.459993                                        LR 0.000031    Time 0.717641    
2023-06-16 02:30:41,747 - Epoch: [94][   30/  142]    Overall Loss 0.471839    Objective Loss 0.471839                                        LR 0.000031    Time 0.639042    
2023-06-16 02:30:46,667 - Epoch: [94][   40/  142]    Overall Loss 0.491570    Objective Loss 0.491570                                        LR 0.000031    Time 0.602281    
2023-06-16 02:30:51,572 - Epoch: [94][   50/  142]    Overall Loss 0.496511    Objective Loss 0.496511                                        LR 0.000031    Time 0.579906    
2023-06-16 02:30:56,499 - Epoch: [94][   60/  142]    Overall Loss 0.497508    Objective Loss 0.497508                                        LR 0.000031    Time 0.565364    
2023-06-16 02:31:01,423 - Epoch: [94][   70/  142]    Overall Loss 0.493232    Objective Loss 0.493232                                        LR 0.000031    Time 0.554931    
2023-06-16 02:31:06,420 - Epoch: [94][   80/  142]    Overall Loss 0.488539    Objective Loss 0.488539                                        LR 0.000031    Time 0.548013    
2023-06-16 02:31:11,306 - Epoch: [94][   90/  142]    Overall Loss 0.491630    Objective Loss 0.491630                                        LR 0.000031    Time 0.541404    
2023-06-16 02:31:16,189 - Epoch: [94][  100/  142]    Overall Loss 0.497152    Objective Loss 0.497152                                        LR 0.000031    Time 0.536089    
2023-06-16 02:31:21,189 - Epoch: [94][  110/  142]    Overall Loss 0.502170    Objective Loss 0.502170                                        LR 0.000031    Time 0.532800    
2023-06-16 02:31:26,157 - Epoch: [94][  120/  142]    Overall Loss 0.501540    Objective Loss 0.501540                                        LR 0.000031    Time 0.529794    
2023-06-16 02:31:31,103 - Epoch: [94][  130/  142]    Overall Loss 0.508822    Objective Loss 0.508822                                        LR 0.000031    Time 0.527082    
2023-06-16 02:31:35,721 - Epoch: [94][  140/  142]    Overall Loss 0.515632    Objective Loss 0.515632                                        LR 0.000031    Time 0.522418    
2023-06-16 02:31:36,564 - Epoch: [94][  142/  142]    Overall Loss 0.516024    Objective Loss 0.516024    Top1 78.125000    LR 0.000031    Time 0.520995    
2023-06-16 02:31:37,211 - --- validate (epoch=94)-----------
2023-06-16 02:31:37,211 - 1422 samples (32 per mini-batch)
2023-06-16 02:31:45,107 - Epoch: [94][   10/   45]    Loss 0.873413    Top1 71.875000    
2023-06-16 02:31:49,199 - Epoch: [94][   20/   45]    Loss 0.821289    Top1 73.750000    
2023-06-16 02:31:53,745 - Epoch: [94][   30/   45]    Loss 0.845401    Top1 73.645833    
2023-06-16 02:31:58,014 - Epoch: [94][   40/   45]    Loss 0.864655    Top1 73.593750    
2023-06-16 02:31:59,664 - Epoch: [94][   45/   45]    Loss 0.857786    Top1 73.769339    
2023-06-16 02:32:00,278 - ==> Top1: 73.769    Loss: 0.858

2023-06-16 02:32:00,280 - ==> Best [Top1: 74.754   Sparsity:0.00   Params: 375264 on epoch: 83]
2023-06-16 02:32:00,280 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:32:00,294 - 

2023-06-16 02:32:00,294 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:32:09,500 - Epoch: [95][   10/  142]    Overall Loss 0.545960    Objective Loss 0.545960                                        LR 0.000031    Time 0.920456    
2023-06-16 02:32:14,531 - Epoch: [95][   20/  142]    Overall Loss 0.502811    Objective Loss 0.502811                                        LR 0.000031    Time 0.711746    
2023-06-16 02:32:19,446 - Epoch: [95][   30/  142]    Overall Loss 0.518231    Objective Loss 0.518231                                        LR 0.000031    Time 0.638326    
2023-06-16 02:32:24,379 - Epoch: [95][   40/  142]    Overall Loss 0.526878    Objective Loss 0.526878                                        LR 0.000031    Time 0.602048    
2023-06-16 02:32:29,403 - Epoch: [95][   50/  142]    Overall Loss 0.535483    Objective Loss 0.535483                                        LR 0.000031    Time 0.582104    
2023-06-16 02:32:34,248 - Epoch: [95][   60/  142]    Overall Loss 0.536748    Objective Loss 0.536748                                        LR 0.000031    Time 0.565817    
2023-06-16 02:32:39,195 - Epoch: [95][   70/  142]    Overall Loss 0.532685    Objective Loss 0.532685                                        LR 0.000031    Time 0.555651    
2023-06-16 02:32:44,121 - Epoch: [95][   80/  142]    Overall Loss 0.514583    Objective Loss 0.514583                                        LR 0.000031    Time 0.547761    
2023-06-16 02:32:49,037 - Epoch: [95][   90/  142]    Overall Loss 0.506389    Objective Loss 0.506389                                        LR 0.000031    Time 0.541518    
2023-06-16 02:32:53,989 - Epoch: [95][  100/  142]    Overall Loss 0.509044    Objective Loss 0.509044                                        LR 0.000031    Time 0.536882    
2023-06-16 02:32:59,049 - Epoch: [95][  110/  142]    Overall Loss 0.504097    Objective Loss 0.504097                                        LR 0.000031    Time 0.534073    
2023-06-16 02:33:03,978 - Epoch: [95][  120/  142]    Overall Loss 0.499343    Objective Loss 0.499343                                        LR 0.000031    Time 0.530637    
2023-06-16 02:33:08,778 - Epoch: [95][  130/  142]    Overall Loss 0.496314    Objective Loss 0.496314                                        LR 0.000031    Time 0.526733    
2023-06-16 02:33:13,501 - Epoch: [95][  140/  142]    Overall Loss 0.499967    Objective Loss 0.499967                                        LR 0.000031    Time 0.522847    
2023-06-16 02:33:14,345 - Epoch: [95][  142/  142]    Overall Loss 0.503479    Objective Loss 0.503479    Top1 81.250000    LR 0.000031    Time 0.521422    
2023-06-16 02:33:15,003 - --- validate (epoch=95)-----------
2023-06-16 02:33:15,003 - 1422 samples (32 per mini-batch)
2023-06-16 02:33:23,234 - Epoch: [95][   10/   45]    Loss 0.944080    Top1 73.750000    
2023-06-16 02:33:27,142 - Epoch: [95][   20/   45]    Loss 0.906331    Top1 74.375000    
2023-06-16 02:33:32,222 - Epoch: [95][   30/   45]    Loss 0.922480    Top1 74.166667    
2023-06-16 02:33:37,128 - Epoch: [95][   40/   45]    Loss 0.897406    Top1 74.921875    
2023-06-16 02:33:38,421 - Epoch: [95][   45/   45]    Loss 0.902598    Top1 74.683544    
2023-06-16 02:33:39,076 - ==> Top1: 74.684    Loss: 0.903

2023-06-16 02:33:39,078 - ==> Best [Top1: 74.754   Sparsity:0.00   Params: 375264 on epoch: 83]
2023-06-16 02:33:39,078 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:33:39,093 - 

2023-06-16 02:33:39,093 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:33:48,547 - Epoch: [96][   10/  142]    Overall Loss 0.503521    Objective Loss 0.503521                                        LR 0.000031    Time 0.945353    
2023-06-16 02:33:53,537 - Epoch: [96][   20/  142]    Overall Loss 0.493344    Objective Loss 0.493344                                        LR 0.000031    Time 0.722107    
2023-06-16 02:33:58,586 - Epoch: [96][   30/  142]    Overall Loss 0.515343    Objective Loss 0.515343                                        LR 0.000031    Time 0.649705    
2023-06-16 02:34:03,643 - Epoch: [96][   40/  142]    Overall Loss 0.527834    Objective Loss 0.527834                                        LR 0.000031    Time 0.613673    
2023-06-16 02:34:08,579 - Epoch: [96][   50/  142]    Overall Loss 0.525456    Objective Loss 0.525456                                        LR 0.000031    Time 0.589656    
2023-06-16 02:34:13,682 - Epoch: [96][   60/  142]    Overall Loss 0.517443    Objective Loss 0.517443                                        LR 0.000031    Time 0.576413    
2023-06-16 02:34:18,574 - Epoch: [96][   70/  142]    Overall Loss 0.513265    Objective Loss 0.513265                                        LR 0.000031    Time 0.563953    
2023-06-16 02:34:23,646 - Epoch: [96][   80/  142]    Overall Loss 0.499494    Objective Loss 0.499494                                        LR 0.000031    Time 0.556848    
2023-06-16 02:34:28,560 - Epoch: [96][   90/  142]    Overall Loss 0.495729    Objective Loss 0.495729                                        LR 0.000031    Time 0.549571    
2023-06-16 02:34:33,555 - Epoch: [96][  100/  142]    Overall Loss 0.498588    Objective Loss 0.498588                                        LR 0.000031    Time 0.544554    
2023-06-16 02:34:38,584 - Epoch: [96][  110/  142]    Overall Loss 0.503481    Objective Loss 0.503481                                        LR 0.000031    Time 0.540762    
2023-06-16 02:34:43,398 - Epoch: [96][  120/  142]    Overall Loss 0.506824    Objective Loss 0.506824                                        LR 0.000031    Time 0.535808    
2023-06-16 02:34:48,509 - Epoch: [96][  130/  142]    Overall Loss 0.512227    Objective Loss 0.512227                                        LR 0.000031    Time 0.533904    
2023-06-16 02:34:53,093 - Epoch: [96][  140/  142]    Overall Loss 0.514557    Objective Loss 0.514557                                        LR 0.000031    Time 0.528512    
2023-06-16 02:34:53,946 - Epoch: [96][  142/  142]    Overall Loss 0.513475    Objective Loss 0.513475    Top1 87.500000    LR 0.000031    Time 0.527073    
2023-06-16 02:34:54,608 - --- validate (epoch=96)-----------
2023-06-16 02:34:54,608 - 1422 samples (32 per mini-batch)
2023-06-16 02:35:02,533 - Epoch: [96][   10/   45]    Loss 0.898949    Top1 76.875000    
2023-06-16 02:35:06,888 - Epoch: [96][   20/   45]    Loss 0.859710    Top1 76.718750    
2023-06-16 02:35:11,834 - Epoch: [96][   30/   45]    Loss 0.933586    Top1 73.645833    
2023-06-16 02:35:15,882 - Epoch: [96][   40/   45]    Loss 0.931475    Top1 74.218750    
2023-06-16 02:35:17,368 - Epoch: [96][   45/   45]    Loss 0.915631    Top1 74.402250    
2023-06-16 02:35:18,019 - ==> Top1: 74.402    Loss: 0.916

2023-06-16 02:35:18,021 - ==> Best [Top1: 74.754   Sparsity:0.00   Params: 375264 on epoch: 83]
2023-06-16 02:35:18,021 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:35:18,042 - 

2023-06-16 02:35:18,043 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:35:27,538 - Epoch: [97][   10/  142]    Overall Loss 0.484082    Objective Loss 0.484082                                        LR 0.000031    Time 0.949460    
2023-06-16 02:35:32,521 - Epoch: [97][   20/  142]    Overall Loss 0.504363    Objective Loss 0.504363                                        LR 0.000031    Time 0.723820    
2023-06-16 02:35:37,395 - Epoch: [97][   30/  142]    Overall Loss 0.504296    Objective Loss 0.504296                                        LR 0.000031    Time 0.645016    
2023-06-16 02:35:42,243 - Epoch: [97][   40/  142]    Overall Loss 0.507037    Objective Loss 0.507037                                        LR 0.000031    Time 0.604949    
2023-06-16 02:35:47,212 - Epoch: [97][   50/  142]    Overall Loss 0.496913    Objective Loss 0.496913                                        LR 0.000031    Time 0.583324    
2023-06-16 02:35:52,132 - Epoch: [97][   60/  142]    Overall Loss 0.509042    Objective Loss 0.509042                                        LR 0.000031    Time 0.568100    
2023-06-16 02:35:57,085 - Epoch: [97][   70/  142]    Overall Loss 0.506143    Objective Loss 0.506143                                        LR 0.000031    Time 0.557682    
2023-06-16 02:36:02,037 - Epoch: [97][   80/  142]    Overall Loss 0.507942    Objective Loss 0.507942                                        LR 0.000031    Time 0.549864    
2023-06-16 02:36:07,121 - Epoch: [97][   90/  142]    Overall Loss 0.501077    Objective Loss 0.501077                                        LR 0.000031    Time 0.545252    
2023-06-16 02:36:11,986 - Epoch: [97][  100/  142]    Overall Loss 0.499766    Objective Loss 0.499766                                        LR 0.000031    Time 0.539372    
2023-06-16 02:36:16,812 - Epoch: [97][  110/  142]    Overall Loss 0.505595    Objective Loss 0.505595                                        LR 0.000031    Time 0.534213    
2023-06-16 02:36:21,793 - Epoch: [97][  120/  142]    Overall Loss 0.500109    Objective Loss 0.500109                                        LR 0.000031    Time 0.531197    
2023-06-16 02:36:26,759 - Epoch: [97][  130/  142]    Overall Loss 0.500799    Objective Loss 0.500799                                        LR 0.000031    Time 0.528528    
2023-06-16 02:36:31,342 - Epoch: [97][  140/  142]    Overall Loss 0.510266    Objective Loss 0.510266                                        LR 0.000031    Time 0.523511    
2023-06-16 02:36:32,197 - Epoch: [97][  142/  142]    Overall Loss 0.510012    Objective Loss 0.510012    Top1 84.375000    LR 0.000031    Time 0.522159    
2023-06-16 02:36:32,861 - --- validate (epoch=97)-----------
2023-06-16 02:36:32,861 - 1422 samples (32 per mini-batch)
2023-06-16 02:36:41,212 - Epoch: [97][   10/   45]    Loss 0.813904    Top1 72.187500    
2023-06-16 02:36:45,211 - Epoch: [97][   20/   45]    Loss 0.848948    Top1 72.812500    
2023-06-16 02:36:49,278 - Epoch: [97][   30/   45]    Loss 0.858577    Top1 72.500000    
2023-06-16 02:36:53,806 - Epoch: [97][   40/   45]    Loss 0.865404    Top1 73.437500    
2023-06-16 02:36:55,154 - Epoch: [97][   45/   45]    Loss 0.848247    Top1 73.699015    
2023-06-16 02:36:55,784 - ==> Top1: 73.699    Loss: 0.848

2023-06-16 02:36:55,786 - ==> Best [Top1: 74.754   Sparsity:0.00   Params: 375264 on epoch: 83]
2023-06-16 02:36:55,786 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:36:55,807 - 

2023-06-16 02:36:55,807 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:37:05,132 - Epoch: [98][   10/  142]    Overall Loss 0.530789    Objective Loss 0.530789                                        LR 0.000031    Time 0.932388    
2023-06-16 02:37:10,196 - Epoch: [98][   20/  142]    Overall Loss 0.550440    Objective Loss 0.550440                                        LR 0.000031    Time 0.719343    
2023-06-16 02:37:15,048 - Epoch: [98][   30/  142]    Overall Loss 0.535767    Objective Loss 0.535767                                        LR 0.000031    Time 0.641274    
2023-06-16 02:37:20,008 - Epoch: [98][   40/  142]    Overall Loss 0.525251    Objective Loss 0.525251                                        LR 0.000031    Time 0.604937    
2023-06-16 02:37:24,912 - Epoch: [98][   50/  142]    Overall Loss 0.539646    Objective Loss 0.539646                                        LR 0.000031    Time 0.582015    
2023-06-16 02:37:29,845 - Epoch: [98][   60/  142]    Overall Loss 0.542214    Objective Loss 0.542214                                        LR 0.000031    Time 0.567231    
2023-06-16 02:37:34,881 - Epoch: [98][   70/  142]    Overall Loss 0.529550    Objective Loss 0.529550                                        LR 0.000031    Time 0.558133    
2023-06-16 02:37:39,713 - Epoch: [98][   80/  142]    Overall Loss 0.530811    Objective Loss 0.530811                                        LR 0.000031    Time 0.548751    
2023-06-16 02:37:44,611 - Epoch: [98][   90/  142]    Overall Loss 0.524766    Objective Loss 0.524766                                        LR 0.000031    Time 0.542198    
2023-06-16 02:37:49,443 - Epoch: [98][  100/  142]    Overall Loss 0.524860    Objective Loss 0.524860                                        LR 0.000031    Time 0.536294    
2023-06-16 02:37:54,391 - Epoch: [98][  110/  142]    Overall Loss 0.526281    Objective Loss 0.526281                                        LR 0.000031    Time 0.532514    
2023-06-16 02:37:59,413 - Epoch: [98][  120/  142]    Overall Loss 0.524148    Objective Loss 0.524148                                        LR 0.000031    Time 0.529978    
2023-06-16 02:38:04,217 - Epoch: [98][  130/  142]    Overall Loss 0.523798    Objective Loss 0.523798                                        LR 0.000031    Time 0.526161    
2023-06-16 02:38:08,882 - Epoch: [98][  140/  142]    Overall Loss 0.526067    Objective Loss 0.526067                                        LR 0.000031    Time 0.521900    
2023-06-16 02:38:09,724 - Epoch: [98][  142/  142]    Overall Loss 0.525981    Objective Loss 0.525981    Top1 81.250000    LR 0.000031    Time 0.520475    
2023-06-16 02:38:10,363 - --- validate (epoch=98)-----------
2023-06-16 02:38:10,364 - 1422 samples (32 per mini-batch)
2023-06-16 02:38:18,530 - Epoch: [98][   10/   45]    Loss 0.908168    Top1 73.125000    
2023-06-16 02:38:22,531 - Epoch: [98][   20/   45]    Loss 0.909752    Top1 74.062500    
2023-06-16 02:38:27,278 - Epoch: [98][   30/   45]    Loss 0.857596    Top1 74.583333    
2023-06-16 02:38:31,478 - Epoch: [98][   40/   45]    Loss 0.906426    Top1 73.515625    
2023-06-16 02:38:32,869 - Epoch: [98][   45/   45]    Loss 0.890740    Top1 74.191280    
2023-06-16 02:38:33,531 - ==> Top1: 74.191    Loss: 0.891

2023-06-16 02:38:33,533 - ==> Best [Top1: 74.754   Sparsity:0.00   Params: 375264 on epoch: 83]
2023-06-16 02:38:33,533 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:38:33,554 - 

2023-06-16 02:38:33,555 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:38:42,907 - Epoch: [99][   10/  142]    Overall Loss 0.555715    Objective Loss 0.555715                                        LR 0.000031    Time 0.935084    
2023-06-16 02:38:47,831 - Epoch: [99][   20/  142]    Overall Loss 0.495566    Objective Loss 0.495566                                        LR 0.000031    Time 0.713714    
2023-06-16 02:38:52,835 - Epoch: [99][   30/  142]    Overall Loss 0.499024    Objective Loss 0.499024                                        LR 0.000031    Time 0.642617    
2023-06-16 02:38:57,880 - Epoch: [99][   40/  142]    Overall Loss 0.521464    Objective Loss 0.521464                                        LR 0.000031    Time 0.608064    
2023-06-16 02:39:02,873 - Epoch: [99][   50/  142]    Overall Loss 0.521945    Objective Loss 0.521945                                        LR 0.000031    Time 0.586308    
2023-06-16 02:39:07,837 - Epoch: [99][   60/  142]    Overall Loss 0.525018    Objective Loss 0.525018                                        LR 0.000031    Time 0.571309    
2023-06-16 02:39:12,842 - Epoch: [99][   70/  142]    Overall Loss 0.523290    Objective Loss 0.523290                                        LR 0.000031    Time 0.561191    
2023-06-16 02:39:17,773 - Epoch: [99][   80/  142]    Overall Loss 0.534800    Objective Loss 0.534800                                        LR 0.000031    Time 0.552669    
2023-06-16 02:39:22,684 - Epoch: [99][   90/  142]    Overall Loss 0.537451    Objective Loss 0.537451                                        LR 0.000031    Time 0.545813    
2023-06-16 02:39:27,722 - Epoch: [99][  100/  142]    Overall Loss 0.534579    Objective Loss 0.534579                                        LR 0.000031    Time 0.541610    
2023-06-16 02:39:32,650 - Epoch: [99][  110/  142]    Overall Loss 0.538818    Objective Loss 0.538818                                        LR 0.000031    Time 0.537173    
2023-06-16 02:39:37,617 - Epoch: [99][  120/  142]    Overall Loss 0.534521    Objective Loss 0.534521                                        LR 0.000031    Time 0.533794    
2023-06-16 02:39:42,713 - Epoch: [99][  130/  142]    Overall Loss 0.533270    Objective Loss 0.533270                                        LR 0.000031    Time 0.531926    
2023-06-16 02:39:47,320 - Epoch: [99][  140/  142]    Overall Loss 0.529273    Objective Loss 0.529273                                        LR 0.000031    Time 0.526837    
2023-06-16 02:39:48,175 - Epoch: [99][  142/  142]    Overall Loss 0.529096    Objective Loss 0.529096    Top1 85.937500    LR 0.000031    Time 0.525437    
2023-06-16 02:39:48,819 - --- validate (epoch=99)-----------
2023-06-16 02:39:48,820 - 1422 samples (32 per mini-batch)
2023-06-16 02:39:56,802 - Epoch: [99][   10/   45]    Loss 0.872338    Top1 70.625000    
2023-06-16 02:40:01,081 - Epoch: [99][   20/   45]    Loss 0.800417    Top1 73.593750    
2023-06-16 02:40:05,608 - Epoch: [99][   30/   45]    Loss 0.805339    Top1 74.479167    
2023-06-16 02:40:09,881 - Epoch: [99][   40/   45]    Loss 0.809567    Top1 73.906250    
2023-06-16 02:40:11,378 - Epoch: [99][   45/   45]    Loss 0.826110    Top1 73.277075    
2023-06-16 02:40:12,032 - ==> Top1: 73.277    Loss: 0.826

2023-06-16 02:40:12,034 - ==> Best [Top1: 74.754   Sparsity:0.00   Params: 375264 on epoch: 83]
2023-06-16 02:40:12,034 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:40:12,055 - 

2023-06-16 02:40:12,056 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:40:21,462 - Epoch: [100][   10/  142]    Overall Loss 0.478711    Objective Loss 0.478711                                        LR 0.000031    Time 0.940549    
2023-06-16 02:40:26,533 - Epoch: [100][   20/  142]    Overall Loss 0.459751    Objective Loss 0.459751                                        LR 0.000031    Time 0.723743    
2023-06-16 02:40:31,562 - Epoch: [100][   30/  142]    Overall Loss 0.533102    Objective Loss 0.533102                                        LR 0.000031    Time 0.650133    
2023-06-16 02:40:36,651 - Epoch: [100][   40/  142]    Overall Loss 0.522972    Objective Loss 0.522972                                        LR 0.000031    Time 0.614814    
2023-06-16 02:40:41,567 - Epoch: [100][   50/  142]    Overall Loss 0.505302    Objective Loss 0.505302                                        LR 0.000031    Time 0.590156    
2023-06-16 02:40:46,675 - Epoch: [100][   60/  142]    Overall Loss 0.514759    Objective Loss 0.514759                                        LR 0.000031    Time 0.576921    
2023-06-16 02:40:51,777 - Epoch: [100][   70/  142]    Overall Loss 0.510663    Objective Loss 0.510663                                        LR 0.000031    Time 0.567373    
2023-06-16 02:40:56,713 - Epoch: [100][   80/  142]    Overall Loss 0.514485    Objective Loss 0.514485                                        LR 0.000031    Time 0.558149    
2023-06-16 02:41:01,785 - Epoch: [100][   90/  142]    Overall Loss 0.505088    Objective Loss 0.505088                                        LR 0.000031    Time 0.552477    
2023-06-16 02:41:06,831 - Epoch: [100][  100/  142]    Overall Loss 0.520934    Objective Loss 0.520934                                        LR 0.000031    Time 0.547687    
2023-06-16 02:41:11,827 - Epoch: [100][  110/  142]    Overall Loss 0.524405    Objective Loss 0.524405                                        LR 0.000031    Time 0.543311    
2023-06-16 02:41:16,856 - Epoch: [100][  120/  142]    Overall Loss 0.524851    Objective Loss 0.524851                                        LR 0.000031    Time 0.539939    
2023-06-16 02:41:22,027 - Epoch: [100][  130/  142]    Overall Loss 0.525284    Objective Loss 0.525284                                        LR 0.000031    Time 0.538175    
2023-06-16 02:41:26,656 - Epoch: [100][  140/  142]    Overall Loss 0.521618    Objective Loss 0.521618                                        LR 0.000031    Time 0.532793    
2023-06-16 02:41:27,500 - Epoch: [100][  142/  142]    Overall Loss 0.522511    Objective Loss 0.522511    Top1 81.250000    LR 0.000031    Time 0.531231    
2023-06-16 02:41:28,156 - --- validate (epoch=100)-----------
2023-06-16 02:41:28,157 - 1422 samples (32 per mini-batch)
2023-06-16 02:41:36,314 - Epoch: [100][   10/   45]    Loss 0.872839    Top1 77.187500    
2023-06-16 02:41:40,274 - Epoch: [100][   20/   45]    Loss 0.857226    Top1 76.250000    
2023-06-16 02:41:44,882 - Epoch: [100][   30/   45]    Loss 0.867767    Top1 75.104167    
2023-06-16 02:41:49,728 - Epoch: [100][   40/   45]    Loss 0.901018    Top1 74.218750    
2023-06-16 02:41:51,108 - Epoch: [100][   45/   45]    Loss 0.882866    Top1 74.824191    
2023-06-16 02:41:51,768 - ==> Top1: 74.824    Loss: 0.883

2023-06-16 02:41:51,770 - ==> Best [Top1: 74.824   Sparsity:0.00   Params: 375264 on epoch: 100]
2023-06-16 02:41:51,770 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:41:51,795 - 

2023-06-16 02:41:51,795 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:42:01,064 - Epoch: [101][   10/  142]    Overall Loss 0.432093    Objective Loss 0.432093                                        LR 0.000031    Time 0.926763    
2023-06-16 02:42:06,086 - Epoch: [101][   20/  142]    Overall Loss 0.485029    Objective Loss 0.485029                                        LR 0.000031    Time 0.714438    
2023-06-16 02:42:11,245 - Epoch: [101][   30/  142]    Overall Loss 0.485770    Objective Loss 0.485770                                        LR 0.000031    Time 0.648258    
2023-06-16 02:42:16,303 - Epoch: [101][   40/  142]    Overall Loss 0.476587    Objective Loss 0.476587                                        LR 0.000031    Time 0.612614    
2023-06-16 02:42:21,340 - Epoch: [101][   50/  142]    Overall Loss 0.474543    Objective Loss 0.474543                                        LR 0.000031    Time 0.590830    
2023-06-16 02:42:26,379 - Epoch: [101][   60/  142]    Overall Loss 0.473800    Objective Loss 0.473800                                        LR 0.000031    Time 0.576336    
2023-06-16 02:42:31,398 - Epoch: [101][   70/  142]    Overall Loss 0.477115    Objective Loss 0.477115                                        LR 0.000031    Time 0.565699    
2023-06-16 02:42:36,499 - Epoch: [101][   80/  142]    Overall Loss 0.486517    Objective Loss 0.486517                                        LR 0.000031    Time 0.558739    
2023-06-16 02:42:41,498 - Epoch: [101][   90/  142]    Overall Loss 0.498296    Objective Loss 0.498296                                        LR 0.000031    Time 0.552201    
2023-06-16 02:42:46,548 - Epoch: [101][  100/  142]    Overall Loss 0.502130    Objective Loss 0.502130                                        LR 0.000031    Time 0.547469    
2023-06-16 02:42:51,646 - Epoch: [101][  110/  142]    Overall Loss 0.501965    Objective Loss 0.501965                                        LR 0.000031    Time 0.544040    
2023-06-16 02:42:56,664 - Epoch: [101][  120/  142]    Overall Loss 0.503151    Objective Loss 0.503151                                        LR 0.000031    Time 0.540519    
2023-06-16 02:43:01,660 - Epoch: [101][  130/  142]    Overall Loss 0.503058    Objective Loss 0.503058                                        LR 0.000031    Time 0.537369    
2023-06-16 02:43:06,336 - Epoch: [101][  140/  142]    Overall Loss 0.505817    Objective Loss 0.505817                                        LR 0.000031    Time 0.532379    
2023-06-16 02:43:07,176 - Epoch: [101][  142/  142]    Overall Loss 0.506725    Objective Loss 0.506725    Top1 71.875000    LR 0.000031    Time 0.530796    
2023-06-16 02:43:07,831 - --- validate (epoch=101)-----------
2023-06-16 02:43:07,832 - 1422 samples (32 per mini-batch)
2023-06-16 02:43:15,917 - Epoch: [101][   10/   45]    Loss 0.887761    Top1 72.500000    
2023-06-16 02:43:19,963 - Epoch: [101][   20/   45]    Loss 0.877406    Top1 73.437500    
2023-06-16 02:43:25,032 - Epoch: [101][   30/   45]    Loss 0.828324    Top1 74.375000    
2023-06-16 02:43:29,410 - Epoch: [101][   40/   45]    Loss 0.853069    Top1 74.140625    
2023-06-16 02:43:30,775 - Epoch: [101][   45/   45]    Loss 0.854668    Top1 73.909986    
2023-06-16 02:43:31,397 - ==> Top1: 73.910    Loss: 0.855

2023-06-16 02:43:31,399 - ==> Best [Top1: 74.824   Sparsity:0.00   Params: 375264 on epoch: 100]
2023-06-16 02:43:31,399 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:43:31,420 - 

2023-06-16 02:43:31,420 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:43:40,685 - Epoch: [102][   10/  142]    Overall Loss 0.496932    Objective Loss 0.496932                                        LR 0.000031    Time 0.926361    
2023-06-16 02:43:45,614 - Epoch: [102][   20/  142]    Overall Loss 0.506792    Objective Loss 0.506792                                        LR 0.000031    Time 0.709615    
2023-06-16 02:43:50,610 - Epoch: [102][   30/  142]    Overall Loss 0.519957    Objective Loss 0.519957                                        LR 0.000031    Time 0.639593    
2023-06-16 02:43:55,708 - Epoch: [102][   40/  142]    Overall Loss 0.504783    Objective Loss 0.504783                                        LR 0.000031    Time 0.607146    
2023-06-16 02:44:00,536 - Epoch: [102][   50/  142]    Overall Loss 0.513474    Objective Loss 0.513474                                        LR 0.000031    Time 0.582248    
2023-06-16 02:44:05,611 - Epoch: [102][   60/  142]    Overall Loss 0.500301    Objective Loss 0.500301                                        LR 0.000031    Time 0.569774    
2023-06-16 02:44:10,550 - Epoch: [102][   70/  142]    Overall Loss 0.504146    Objective Loss 0.504146                                        LR 0.000031    Time 0.558932    
2023-06-16 02:44:15,653 - Epoch: [102][   80/  142]    Overall Loss 0.505255    Objective Loss 0.505255                                        LR 0.000031    Time 0.552849    
2023-06-16 02:44:20,580 - Epoch: [102][   90/  142]    Overall Loss 0.501067    Objective Loss 0.501067                                        LR 0.000031    Time 0.546160    
2023-06-16 02:44:25,578 - Epoch: [102][  100/  142]    Overall Loss 0.505585    Objective Loss 0.505585                                        LR 0.000031    Time 0.541514    
2023-06-16 02:44:30,558 - Epoch: [102][  110/  142]    Overall Loss 0.510135    Objective Loss 0.510135                                        LR 0.000031    Time 0.537548    
2023-06-16 02:44:35,515 - Epoch: [102][  120/  142]    Overall Loss 0.511327    Objective Loss 0.511327                                        LR 0.000031    Time 0.534055    
2023-06-16 02:44:40,550 - Epoch: [102][  130/  142]    Overall Loss 0.509205    Objective Loss 0.509205                                        LR 0.000031    Time 0.531702    
2023-06-16 02:44:45,166 - Epoch: [102][  140/  142]    Overall Loss 0.508347    Objective Loss 0.508347                                        LR 0.000031    Time 0.526691    
2023-06-16 02:44:46,020 - Epoch: [102][  142/  142]    Overall Loss 0.506974    Objective Loss 0.506974    Top1 87.500000    LR 0.000031    Time 0.525285    
2023-06-16 02:44:46,661 - --- validate (epoch=102)-----------
2023-06-16 02:44:46,661 - 1422 samples (32 per mini-batch)
2023-06-16 02:44:54,912 - Epoch: [102][   10/   45]    Loss 0.894319    Top1 73.125000    
2023-06-16 02:44:58,936 - Epoch: [102][   20/   45]    Loss 0.846580    Top1 75.156250    
2023-06-16 02:45:03,617 - Epoch: [102][   30/   45]    Loss 0.859437    Top1 74.791667    
2023-06-16 02:45:07,931 - Epoch: [102][   40/   45]    Loss 0.866407    Top1 73.750000    
2023-06-16 02:45:09,316 - Epoch: [102][   45/   45]    Loss 0.879551    Top1 73.980309    
2023-06-16 02:45:09,967 - ==> Top1: 73.980    Loss: 0.880

2023-06-16 02:45:09,969 - ==> Best [Top1: 74.824   Sparsity:0.00   Params: 375264 on epoch: 100]
2023-06-16 02:45:09,969 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:45:09,990 - 

2023-06-16 02:45:09,991 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:45:19,290 - Epoch: [103][   10/  142]    Overall Loss 0.486005    Objective Loss 0.486005                                        LR 0.000031    Time 0.929843    
2023-06-16 02:45:24,310 - Epoch: [103][   20/  142]    Overall Loss 0.499247    Objective Loss 0.499247                                        LR 0.000031    Time 0.715890    
2023-06-16 02:45:29,348 - Epoch: [103][   30/  142]    Overall Loss 0.489395    Objective Loss 0.489395                                        LR 0.000031    Time 0.645161    
2023-06-16 02:45:34,393 - Epoch: [103][   40/  142]    Overall Loss 0.488102    Objective Loss 0.488102                                        LR 0.000031    Time 0.609992    
2023-06-16 02:45:39,264 - Epoch: [103][   50/  142]    Overall Loss 0.488047    Objective Loss 0.488047                                        LR 0.000031    Time 0.585395    
2023-06-16 02:45:44,153 - Epoch: [103][   60/  142]    Overall Loss 0.495629    Objective Loss 0.495629                                        LR 0.000031    Time 0.569305    
2023-06-16 02:45:49,189 - Epoch: [103][   70/  142]    Overall Loss 0.509941    Objective Loss 0.509941                                        LR 0.000031    Time 0.559905    
2023-06-16 02:45:54,092 - Epoch: [103][   80/  142]    Overall Loss 0.505839    Objective Loss 0.505839                                        LR 0.000031    Time 0.551200    
2023-06-16 02:45:59,069 - Epoch: [103][   90/  142]    Overall Loss 0.501496    Objective Loss 0.501496                                        LR 0.000031    Time 0.545253    
2023-06-16 02:46:04,034 - Epoch: [103][  100/  142]    Overall Loss 0.498903    Objective Loss 0.498903                                        LR 0.000031    Time 0.540363    
2023-06-16 02:46:09,063 - Epoch: [103][  110/  142]    Overall Loss 0.494923    Objective Loss 0.494923                                        LR 0.000031    Time 0.536954    
2023-06-16 02:46:13,966 - Epoch: [103][  120/  142]    Overall Loss 0.497242    Objective Loss 0.497242                                        LR 0.000031    Time 0.533060    
2023-06-16 02:46:19,051 - Epoch: [103][  130/  142]    Overall Loss 0.502033    Objective Loss 0.502033                                        LR 0.000031    Time 0.531168    
2023-06-16 02:46:23,582 - Epoch: [103][  140/  142]    Overall Loss 0.503519    Objective Loss 0.503519                                        LR 0.000031    Time 0.525585    
2023-06-16 02:46:24,422 - Epoch: [103][  142/  142]    Overall Loss 0.503245    Objective Loss 0.503245    Top1 82.812500    LR 0.000031    Time 0.524098    
2023-06-16 02:46:25,070 - --- validate (epoch=103)-----------
2023-06-16 02:46:25,071 - 1422 samples (32 per mini-batch)
2023-06-16 02:46:33,059 - Epoch: [103][   10/   45]    Loss 0.777054    Top1 75.312500    
2023-06-16 02:46:37,555 - Epoch: [103][   20/   45]    Loss 0.802085    Top1 75.468750    
2023-06-16 02:46:42,339 - Epoch: [103][   30/   45]    Loss 0.857440    Top1 74.479167    
2023-06-16 02:46:46,607 - Epoch: [103][   40/   45]    Loss 0.880549    Top1 74.453125    
2023-06-16 02:46:47,958 - Epoch: [103][   45/   45]    Loss 0.880089    Top1 74.683544    
2023-06-16 02:46:48,599 - ==> Top1: 74.684    Loss: 0.880

2023-06-16 02:46:48,601 - ==> Best [Top1: 74.824   Sparsity:0.00   Params: 375264 on epoch: 100]
2023-06-16 02:46:48,601 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:46:48,622 - 

2023-06-16 02:46:48,622 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:46:58,040 - Epoch: [104][   10/  142]    Overall Loss 0.462094    Objective Loss 0.462094                                        LR 0.000031    Time 0.941621    
2023-06-16 02:47:02,975 - Epoch: [104][   20/  142]    Overall Loss 0.463898    Objective Loss 0.463898                                        LR 0.000031    Time 0.717552    
2023-06-16 02:47:07,908 - Epoch: [104][   30/  142]    Overall Loss 0.472002    Objective Loss 0.472002                                        LR 0.000031    Time 0.642762    
2023-06-16 02:47:12,924 - Epoch: [104][   40/  142]    Overall Loss 0.466009    Objective Loss 0.466009                                        LR 0.000031    Time 0.607465    
2023-06-16 02:47:17,816 - Epoch: [104][   50/  142]    Overall Loss 0.460022    Objective Loss 0.460022                                        LR 0.000031    Time 0.583783    
2023-06-16 02:47:22,791 - Epoch: [104][   60/  142]    Overall Loss 0.468262    Objective Loss 0.468262                                        LR 0.000031    Time 0.569396    
2023-06-16 02:47:27,763 - Epoch: [104][   70/  142]    Overall Loss 0.478983    Objective Loss 0.478983                                        LR 0.000031    Time 0.559072    
2023-06-16 02:47:32,787 - Epoch: [104][   80/  142]    Overall Loss 0.485826    Objective Loss 0.485826                                        LR 0.000031    Time 0.551982    
2023-06-16 02:47:37,739 - Epoch: [104][   90/  142]    Overall Loss 0.487282    Objective Loss 0.487282                                        LR 0.000031    Time 0.545664    
2023-06-16 02:47:42,796 - Epoch: [104][  100/  142]    Overall Loss 0.491870    Objective Loss 0.491870                                        LR 0.000031    Time 0.541665    
2023-06-16 02:47:47,693 - Epoch: [104][  110/  142]    Overall Loss 0.492793    Objective Loss 0.492793                                        LR 0.000031    Time 0.536936    
2023-06-16 02:47:52,732 - Epoch: [104][  120/  142]    Overall Loss 0.494911    Objective Loss 0.494911                                        LR 0.000031    Time 0.534177    
2023-06-16 02:47:57,650 - Epoch: [104][  130/  142]    Overall Loss 0.491361    Objective Loss 0.491361                                        LR 0.000031    Time 0.530909    
2023-06-16 02:48:02,358 - Epoch: [104][  140/  142]    Overall Loss 0.486525    Objective Loss 0.486525                                        LR 0.000031    Time 0.526614    
2023-06-16 02:48:03,201 - Epoch: [104][  142/  142]    Overall Loss 0.487873    Objective Loss 0.487873    Top1 81.250000    LR 0.000031    Time 0.525131    
2023-06-16 02:48:03,821 - --- validate (epoch=104)-----------
2023-06-16 02:48:03,822 - 1422 samples (32 per mini-batch)
2023-06-16 02:48:11,638 - Epoch: [104][   10/   45]    Loss 0.918871    Top1 73.125000    
2023-06-16 02:48:15,679 - Epoch: [104][   20/   45]    Loss 0.912118    Top1 73.281250    
2023-06-16 02:48:20,294 - Epoch: [104][   30/   45]    Loss 0.947026    Top1 73.125000    
2023-06-16 02:48:24,749 - Epoch: [104][   40/   45]    Loss 0.947637    Top1 73.203125    
2023-06-16 02:48:26,041 - Epoch: [104][   45/   45]    Loss 0.929780    Top1 73.699015    
2023-06-16 02:48:26,645 - ==> Top1: 73.699    Loss: 0.930

2023-06-16 02:48:26,648 - ==> Best [Top1: 74.824   Sparsity:0.00   Params: 375264 on epoch: 100]
2023-06-16 02:48:26,648 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:48:26,669 - 

2023-06-16 02:48:26,669 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:48:35,906 - Epoch: [105][   10/  142]    Overall Loss 0.438237    Objective Loss 0.438237                                        LR 0.000031    Time 0.923577    
2023-06-16 02:48:40,845 - Epoch: [105][   20/  142]    Overall Loss 0.460771    Objective Loss 0.460771                                        LR 0.000031    Time 0.708751    
2023-06-16 02:48:45,872 - Epoch: [105][   30/  142]    Overall Loss 0.490390    Objective Loss 0.490390                                        LR 0.000031    Time 0.640017    
2023-06-16 02:48:50,770 - Epoch: [105][   40/  142]    Overall Loss 0.485359    Objective Loss 0.485359                                        LR 0.000031    Time 0.602455    
2023-06-16 02:48:55,814 - Epoch: [105][   50/  142]    Overall Loss 0.510241    Objective Loss 0.510241                                        LR 0.000031    Time 0.582822    
2023-06-16 02:49:00,759 - Epoch: [105][   60/  142]    Overall Loss 0.505816    Objective Loss 0.505816                                        LR 0.000031    Time 0.568094    
2023-06-16 02:49:05,676 - Epoch: [105][   70/  142]    Overall Loss 0.516667    Objective Loss 0.516667                                        LR 0.000031    Time 0.557178    
2023-06-16 02:49:10,627 - Epoch: [105][   80/  142]    Overall Loss 0.517123    Objective Loss 0.517123                                        LR 0.000031    Time 0.549404    
2023-06-16 02:49:15,740 - Epoch: [105][   90/  142]    Overall Loss 0.515991    Objective Loss 0.515991                                        LR 0.000031    Time 0.545172    
2023-06-16 02:49:20,597 - Epoch: [105][  100/  142]    Overall Loss 0.519005    Objective Loss 0.519005                                        LR 0.000031    Time 0.539219    
2023-06-16 02:49:25,555 - Epoch: [105][  110/  142]    Overall Loss 0.519833    Objective Loss 0.519833                                        LR 0.000031    Time 0.535260    
2023-06-16 02:49:30,477 - Epoch: [105][  120/  142]    Overall Loss 0.523718    Objective Loss 0.523718                                        LR 0.000031    Time 0.531672    
2023-06-16 02:49:35,362 - Epoch: [105][  130/  142]    Overall Loss 0.525808    Objective Loss 0.525808                                        LR 0.000031    Time 0.527950    
2023-06-16 02:49:40,022 - Epoch: [105][  140/  142]    Overall Loss 0.526251    Objective Loss 0.526251                                        LR 0.000031    Time 0.523523    
2023-06-16 02:49:40,878 - Epoch: [105][  142/  142]    Overall Loss 0.524957    Objective Loss 0.524957    Top1 85.937500    LR 0.000031    Time 0.522177    
2023-06-16 02:49:41,537 - --- validate (epoch=105)-----------
2023-06-16 02:49:41,538 - 1422 samples (32 per mini-batch)
2023-06-16 02:49:49,429 - Epoch: [105][   10/   45]    Loss 0.799130    Top1 74.375000    
2023-06-16 02:49:53,472 - Epoch: [105][   20/   45]    Loss 0.827699    Top1 73.906250    
2023-06-16 02:49:57,769 - Epoch: [105][   30/   45]    Loss 0.814040    Top1 74.895833    
2023-06-16 02:50:02,769 - Epoch: [105][   40/   45]    Loss 0.840076    Top1 74.531250    
2023-06-16 02:50:04,062 - Epoch: [105][   45/   45]    Loss 0.834567    Top1 74.824191    
2023-06-16 02:50:04,687 - ==> Top1: 74.824    Loss: 0.835

2023-06-16 02:50:04,689 - ==> Best [Top1: 74.824   Sparsity:0.00   Params: 375264 on epoch: 105]
2023-06-16 02:50:04,689 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:50:04,714 - 

2023-06-16 02:50:04,714 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:50:13,997 - Epoch: [106][   10/  142]    Overall Loss 0.535815    Objective Loss 0.535815                                        LR 0.000031    Time 0.928215    
2023-06-16 02:50:19,001 - Epoch: [106][   20/  142]    Overall Loss 0.514732    Objective Loss 0.514732                                        LR 0.000031    Time 0.714254    
2023-06-16 02:50:24,005 - Epoch: [106][   30/  142]    Overall Loss 0.521528    Objective Loss 0.521528                                        LR 0.000031    Time 0.642974    
2023-06-16 02:50:29,041 - Epoch: [106][   40/  142]    Overall Loss 0.510812    Objective Loss 0.510812                                        LR 0.000031    Time 0.608112    
2023-06-16 02:50:33,988 - Epoch: [106][   50/  142]    Overall Loss 0.488897    Objective Loss 0.488897                                        LR 0.000031    Time 0.585413    
2023-06-16 02:50:38,972 - Epoch: [106][   60/  142]    Overall Loss 0.484952    Objective Loss 0.484952                                        LR 0.000031    Time 0.570886    
2023-06-16 02:50:43,992 - Epoch: [106][   70/  142]    Overall Loss 0.489099    Objective Loss 0.489099                                        LR 0.000031    Time 0.561037    
2023-06-16 02:50:49,031 - Epoch: [106][   80/  142]    Overall Loss 0.488928    Objective Loss 0.488928                                        LR 0.000031    Time 0.553885    
2023-06-16 02:50:54,029 - Epoch: [106][   90/  142]    Overall Loss 0.490841    Objective Loss 0.490841                                        LR 0.000031    Time 0.547874    
2023-06-16 02:50:59,012 - Epoch: [106][  100/  142]    Overall Loss 0.489911    Objective Loss 0.489911                                        LR 0.000031    Time 0.542911    
2023-06-16 02:51:03,976 - Epoch: [106][  110/  142]    Overall Loss 0.486243    Objective Loss 0.486243                                        LR 0.000031    Time 0.538671    
2023-06-16 02:51:09,025 - Epoch: [106][  120/  142]    Overall Loss 0.480763    Objective Loss 0.480763                                        LR 0.000031    Time 0.535857    
2023-06-16 02:51:13,949 - Epoch: [106][  130/  142]    Overall Loss 0.484710    Objective Loss 0.484710                                        LR 0.000031    Time 0.532505    
2023-06-16 02:51:18,621 - Epoch: [106][  140/  142]    Overall Loss 0.492499    Objective Loss 0.492499                                        LR 0.000031    Time 0.527841    
2023-06-16 02:51:19,464 - Epoch: [106][  142/  142]    Overall Loss 0.492369    Objective Loss 0.492369    Top1 90.625000    LR 0.000031    Time 0.526337    
2023-06-16 02:51:20,114 - --- validate (epoch=106)-----------
2023-06-16 02:51:20,115 - 1422 samples (32 per mini-batch)
2023-06-16 02:51:28,565 - Epoch: [106][   10/   45]    Loss 0.789974    Top1 71.875000    
2023-06-16 02:51:32,732 - Epoch: [106][   20/   45]    Loss 0.826369    Top1 71.875000    
2023-06-16 02:51:37,206 - Epoch: [106][   30/   45]    Loss 0.832969    Top1 73.437500    
2023-06-16 02:51:41,489 - Epoch: [106][   40/   45]    Loss 0.869592    Top1 72.812500    
2023-06-16 02:51:42,934 - Epoch: [106][   45/   45]    Loss 0.871619    Top1 72.503516    
2023-06-16 02:51:43,573 - ==> Top1: 72.504    Loss: 0.872

2023-06-16 02:51:43,575 - ==> Best [Top1: 74.824   Sparsity:0.00   Params: 375264 on epoch: 105]
2023-06-16 02:51:43,575 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:51:43,596 - 

2023-06-16 02:51:43,596 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:51:53,104 - Epoch: [107][   10/  142]    Overall Loss 0.459551    Objective Loss 0.459551                                        LR 0.000031    Time 0.950700    
2023-06-16 02:51:58,000 - Epoch: [107][   20/  142]    Overall Loss 0.468392    Objective Loss 0.468392                                        LR 0.000031    Time 0.720070    
2023-06-16 02:52:02,946 - Epoch: [107][   30/  142]    Overall Loss 0.509884    Objective Loss 0.509884                                        LR 0.000031    Time 0.644897    
2023-06-16 02:52:07,928 - Epoch: [107][   40/  142]    Overall Loss 0.501469    Objective Loss 0.501469                                        LR 0.000031    Time 0.608208    
2023-06-16 02:52:12,995 - Epoch: [107][   50/  142]    Overall Loss 0.505685    Objective Loss 0.505685                                        LR 0.000031    Time 0.587908    
2023-06-16 02:52:17,880 - Epoch: [107][   60/  142]    Overall Loss 0.514165    Objective Loss 0.514165                                        LR 0.000031    Time 0.571318    
2023-06-16 02:52:22,690 - Epoch: [107][   70/  142]    Overall Loss 0.502727    Objective Loss 0.502727                                        LR 0.000031    Time 0.558416    
2023-06-16 02:52:27,750 - Epoch: [107][   80/  142]    Overall Loss 0.503424    Objective Loss 0.503424                                        LR 0.000031    Time 0.551849    
2023-06-16 02:52:32,638 - Epoch: [107][   90/  142]    Overall Loss 0.512709    Objective Loss 0.512709                                        LR 0.000031    Time 0.544841    
2023-06-16 02:52:37,557 - Epoch: [107][  100/  142]    Overall Loss 0.504815    Objective Loss 0.504815                                        LR 0.000031    Time 0.539538    
2023-06-16 02:52:42,498 - Epoch: [107][  110/  142]    Overall Loss 0.506536    Objective Loss 0.506536                                        LR 0.000031    Time 0.535404    
2023-06-16 02:52:47,522 - Epoch: [107][  120/  142]    Overall Loss 0.508488    Objective Loss 0.508488                                        LR 0.000031    Time 0.532652    
2023-06-16 02:52:52,373 - Epoch: [107][  130/  142]    Overall Loss 0.510308    Objective Loss 0.510308                                        LR 0.000031    Time 0.528984    
2023-06-16 02:52:57,020 - Epoch: [107][  140/  142]    Overall Loss 0.506598    Objective Loss 0.506598                                        LR 0.000031    Time 0.524391    
2023-06-16 02:52:57,862 - Epoch: [107][  142/  142]    Overall Loss 0.507636    Objective Loss 0.507636    Top1 78.125000    LR 0.000031    Time 0.522938    
2023-06-16 02:52:58,471 - --- validate (epoch=107)-----------
2023-06-16 02:52:58,471 - 1422 samples (32 per mini-batch)
2023-06-16 02:53:06,187 - Epoch: [107][   10/   45]    Loss 0.763373    Top1 77.500000    
2023-06-16 02:53:10,592 - Epoch: [107][   20/   45]    Loss 0.906623    Top1 75.312500    
2023-06-16 02:53:15,676 - Epoch: [107][   30/   45]    Loss 0.895174    Top1 74.375000    
2023-06-16 02:53:20,124 - Epoch: [107][   40/   45]    Loss 0.890186    Top1 74.921875    
2023-06-16 02:53:21,427 - Epoch: [107][   45/   45]    Loss 0.903756    Top1 74.613221    
2023-06-16 02:53:22,091 - ==> Top1: 74.613    Loss: 0.904

2023-06-16 02:53:22,093 - ==> Best [Top1: 74.824   Sparsity:0.00   Params: 375264 on epoch: 105]
2023-06-16 02:53:22,093 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:53:22,114 - 

2023-06-16 02:53:22,114 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:53:31,564 - Epoch: [108][   10/  142]    Overall Loss 0.480018    Objective Loss 0.480018                                        LR 0.000031    Time 0.944838    
2023-06-16 02:53:36,668 - Epoch: [108][   20/  142]    Overall Loss 0.485565    Objective Loss 0.485565                                        LR 0.000031    Time 0.727606    
2023-06-16 02:53:41,700 - Epoch: [108][   30/  142]    Overall Loss 0.495148    Objective Loss 0.495148                                        LR 0.000031    Time 0.652798    
2023-06-16 02:53:46,696 - Epoch: [108][   40/  142]    Overall Loss 0.523118    Objective Loss 0.523118                                        LR 0.000031    Time 0.613141    
2023-06-16 02:53:51,862 - Epoch: [108][   50/  142]    Overall Loss 0.523016    Objective Loss 0.523016                                        LR 0.000031    Time 0.593832    
2023-06-16 02:53:56,828 - Epoch: [108][   60/  142]    Overall Loss 0.510709    Objective Loss 0.510709                                        LR 0.000031    Time 0.577616    
2023-06-16 02:54:01,981 - Epoch: [108][   70/  142]    Overall Loss 0.499980    Objective Loss 0.499980                                        LR 0.000031    Time 0.568709    
2023-06-16 02:54:06,980 - Epoch: [108][   80/  142]    Overall Loss 0.508677    Objective Loss 0.508677                                        LR 0.000031    Time 0.560100    
2023-06-16 02:54:12,153 - Epoch: [108][   90/  142]    Overall Loss 0.500398    Objective Loss 0.500398                                        LR 0.000031    Time 0.555331    
2023-06-16 02:54:17,117 - Epoch: [108][  100/  142]    Overall Loss 0.503586    Objective Loss 0.503586                                        LR 0.000031    Time 0.549434    
2023-06-16 02:54:22,046 - Epoch: [108][  110/  142]    Overall Loss 0.499935    Objective Loss 0.499935                                        LR 0.000031    Time 0.544295    
2023-06-16 02:54:27,064 - Epoch: [108][  120/  142]    Overall Loss 0.500989    Objective Loss 0.500989                                        LR 0.000031    Time 0.540747    
2023-06-16 02:54:31,943 - Epoch: [108][  130/  142]    Overall Loss 0.497706    Objective Loss 0.497706                                        LR 0.000031    Time 0.536683    
2023-06-16 02:54:36,544 - Epoch: [108][  140/  142]    Overall Loss 0.497356    Objective Loss 0.497356                                        LR 0.000031    Time 0.531205    
2023-06-16 02:54:37,387 - Epoch: [108][  142/  142]    Overall Loss 0.498493    Objective Loss 0.498493    Top1 79.687500    LR 0.000031    Time 0.529662    
2023-06-16 02:54:37,996 - --- validate (epoch=108)-----------
2023-06-16 02:54:37,997 - 1422 samples (32 per mini-batch)
2023-06-16 02:54:45,717 - Epoch: [108][   10/   45]    Loss 0.915382    Top1 74.375000    
2023-06-16 02:54:50,414 - Epoch: [108][   20/   45]    Loss 0.854739    Top1 74.375000    
2023-06-16 02:54:54,843 - Epoch: [108][   30/   45]    Loss 0.824963    Top1 75.312500    
2023-06-16 02:54:59,030 - Epoch: [108][   40/   45]    Loss 0.824238    Top1 75.546875    
2023-06-16 02:55:00,407 - Epoch: [108][   45/   45]    Loss 0.851625    Top1 74.894515    
2023-06-16 02:55:01,063 - ==> Top1: 74.895    Loss: 0.852

2023-06-16 02:55:01,066 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 108]
2023-06-16 02:55:01,066 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:55:01,091 - 

2023-06-16 02:55:01,091 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:55:10,446 - Epoch: [109][   10/  142]    Overall Loss 0.548711    Objective Loss 0.548711                                        LR 0.000031    Time 0.935376    
2023-06-16 02:55:15,475 - Epoch: [109][   20/  142]    Overall Loss 0.534507    Objective Loss 0.534507                                        LR 0.000031    Time 0.719083    
2023-06-16 02:55:20,445 - Epoch: [109][   30/  142]    Overall Loss 0.510500    Objective Loss 0.510500                                        LR 0.000031    Time 0.645043    
2023-06-16 02:55:25,307 - Epoch: [109][   40/  142]    Overall Loss 0.520144    Objective Loss 0.520144                                        LR 0.000031    Time 0.605303    
2023-06-16 02:55:30,320 - Epoch: [109][   50/  142]    Overall Loss 0.504824    Objective Loss 0.504824                                        LR 0.000031    Time 0.584500    
2023-06-16 02:55:35,283 - Epoch: [109][   60/  142]    Overall Loss 0.500022    Objective Loss 0.500022                                        LR 0.000031    Time 0.569800    
2023-06-16 02:55:40,120 - Epoch: [109][   70/  142]    Overall Loss 0.503730    Objective Loss 0.503730                                        LR 0.000031    Time 0.557484    
2023-06-16 02:55:45,026 - Epoch: [109][   80/  142]    Overall Loss 0.503787    Objective Loss 0.503787                                        LR 0.000031    Time 0.549119    
2023-06-16 02:55:50,047 - Epoch: [109][   90/  142]    Overall Loss 0.502467    Objective Loss 0.502467                                        LR 0.000031    Time 0.543888    
2023-06-16 02:55:55,000 - Epoch: [109][  100/  142]    Overall Loss 0.504908    Objective Loss 0.504908                                        LR 0.000031    Time 0.539024    
2023-06-16 02:55:59,912 - Epoch: [109][  110/  142]    Overall Loss 0.503796    Objective Loss 0.503796                                        LR 0.000031    Time 0.534675    
2023-06-16 02:56:04,885 - Epoch: [109][  120/  142]    Overall Loss 0.500449    Objective Loss 0.500449                                        LR 0.000031    Time 0.531552    
2023-06-16 02:56:09,858 - Epoch: [109][  130/  142]    Overall Loss 0.497138    Objective Loss 0.497138                                        LR 0.000031    Time 0.528910    
2023-06-16 02:56:14,470 - Epoch: [109][  140/  142]    Overall Loss 0.491127    Objective Loss 0.491127                                        LR 0.000031    Time 0.524072    
2023-06-16 02:56:15,312 - Epoch: [109][  142/  142]    Overall Loss 0.491078    Objective Loss 0.491078    Top1 81.250000    LR 0.000031    Time 0.522623    
2023-06-16 02:56:15,954 - --- validate (epoch=109)-----------
2023-06-16 02:56:15,954 - 1422 samples (32 per mini-batch)
2023-06-16 02:56:24,042 - Epoch: [109][   10/   45]    Loss 0.942109    Top1 74.375000    
2023-06-16 02:56:28,184 - Epoch: [109][   20/   45]    Loss 0.960841    Top1 74.531250    
2023-06-16 02:56:32,409 - Epoch: [109][   30/   45]    Loss 0.933248    Top1 74.270833    
2023-06-16 02:56:36,920 - Epoch: [109][   40/   45]    Loss 0.933072    Top1 74.375000    
2023-06-16 02:56:38,360 - Epoch: [109][   45/   45]    Loss 0.968364    Top1 74.191280    
2023-06-16 02:56:38,995 - ==> Top1: 74.191    Loss: 0.968

2023-06-16 02:56:38,997 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 108]
2023-06-16 02:56:38,997 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:56:39,018 - 

2023-06-16 02:56:39,018 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:56:48,453 - Epoch: [110][   10/  142]    Overall Loss 0.467002    Objective Loss 0.467002                                        LR 0.000031    Time 0.943346    
2023-06-16 02:56:53,492 - Epoch: [110][   20/  142]    Overall Loss 0.458799    Objective Loss 0.458799                                        LR 0.000031    Time 0.723592    
2023-06-16 02:56:58,511 - Epoch: [110][   30/  142]    Overall Loss 0.489340    Objective Loss 0.489340                                        LR 0.000031    Time 0.649682    
2023-06-16 02:57:03,452 - Epoch: [110][   40/  142]    Overall Loss 0.489711    Objective Loss 0.489711                                        LR 0.000031    Time 0.610776    
2023-06-16 02:57:08,393 - Epoch: [110][   50/  142]    Overall Loss 0.494606    Objective Loss 0.494606                                        LR 0.000031    Time 0.587429    
2023-06-16 02:57:13,375 - Epoch: [110][   60/  142]    Overall Loss 0.495986    Objective Loss 0.495986                                        LR 0.000031    Time 0.572539    
2023-06-16 02:57:18,384 - Epoch: [110][   70/  142]    Overall Loss 0.489986    Objective Loss 0.489986                                        LR 0.000031    Time 0.562306    
2023-06-16 02:57:23,270 - Epoch: [110][   80/  142]    Overall Loss 0.488880    Objective Loss 0.488880                                        LR 0.000031    Time 0.553078    
2023-06-16 02:57:28,364 - Epoch: [110][   90/  142]    Overall Loss 0.486967    Objective Loss 0.486967                                        LR 0.000031    Time 0.548219    
2023-06-16 02:57:33,368 - Epoch: [110][  100/  142]    Overall Loss 0.484027    Objective Loss 0.484027                                        LR 0.000031    Time 0.543437    
2023-06-16 02:57:38,335 - Epoch: [110][  110/  142]    Overall Loss 0.485776    Objective Loss 0.485776                                        LR 0.000031    Time 0.539176    
2023-06-16 02:57:43,482 - Epoch: [110][  120/  142]    Overall Loss 0.485795    Objective Loss 0.485795                                        LR 0.000031    Time 0.537138    
2023-06-16 02:57:48,470 - Epoch: [110][  130/  142]    Overall Loss 0.490018    Objective Loss 0.490018                                        LR 0.000031    Time 0.534179    
2023-06-16 02:57:53,017 - Epoch: [110][  140/  142]    Overall Loss 0.491973    Objective Loss 0.491973                                        LR 0.000031    Time 0.528504    
2023-06-16 02:57:53,871 - Epoch: [110][  142/  142]    Overall Loss 0.491771    Objective Loss 0.491771    Top1 84.375000    LR 0.000031    Time 0.527069    
2023-06-16 02:57:54,482 - --- validate (epoch=110)-----------
2023-06-16 02:57:54,483 - 1422 samples (32 per mini-batch)
2023-06-16 02:58:02,388 - Epoch: [110][   10/   45]    Loss 0.971023    Top1 70.937500    
2023-06-16 02:58:06,456 - Epoch: [110][   20/   45]    Loss 0.913254    Top1 73.281250    
2023-06-16 02:58:11,078 - Epoch: [110][   30/   45]    Loss 0.870454    Top1 73.020833    
2023-06-16 02:58:15,473 - Epoch: [110][   40/   45]    Loss 0.841511    Top1 74.140625    
2023-06-16 02:58:16,868 - Epoch: [110][   45/   45]    Loss 0.825801    Top1 74.050633    
2023-06-16 02:58:17,505 - ==> Top1: 74.051    Loss: 0.826

2023-06-16 02:58:17,507 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 108]
2023-06-16 02:58:17,507 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:58:17,528 - 

2023-06-16 02:58:17,528 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 02:58:26,840 - Epoch: [111][   10/  142]    Overall Loss 0.539667    Objective Loss 0.539667                                        LR 0.000031    Time 0.931109    
2023-06-16 02:58:31,741 - Epoch: [111][   20/  142]    Overall Loss 0.501076    Objective Loss 0.501076                                        LR 0.000031    Time 0.710559    
2023-06-16 02:58:36,755 - Epoch: [111][   30/  142]    Overall Loss 0.483554    Objective Loss 0.483554                                        LR 0.000031    Time 0.640817    
2023-06-16 02:58:41,687 - Epoch: [111][   40/  142]    Overall Loss 0.490501    Objective Loss 0.490501                                        LR 0.000031    Time 0.603896    
2023-06-16 02:58:46,645 - Epoch: [111][   50/  142]    Overall Loss 0.487342    Objective Loss 0.487342                                        LR 0.000031    Time 0.582273    
2023-06-16 02:58:51,613 - Epoch: [111][   60/  142]    Overall Loss 0.484470    Objective Loss 0.484470                                        LR 0.000031    Time 0.568007    
2023-06-16 02:58:56,500 - Epoch: [111][   70/  142]    Overall Loss 0.480168    Objective Loss 0.480168                                        LR 0.000031    Time 0.556669    
2023-06-16 02:59:01,480 - Epoch: [111][   80/  142]    Overall Loss 0.476671    Objective Loss 0.476671                                        LR 0.000031    Time 0.549330    
2023-06-16 02:59:06,409 - Epoch: [111][   90/  142]    Overall Loss 0.470023    Objective Loss 0.470023                                        LR 0.000031    Time 0.543054    
2023-06-16 02:59:11,352 - Epoch: [111][  100/  142]    Overall Loss 0.474279    Objective Loss 0.474279                                        LR 0.000031    Time 0.538166    
2023-06-16 02:59:16,318 - Epoch: [111][  110/  142]    Overall Loss 0.476801    Objective Loss 0.476801                                        LR 0.000031    Time 0.534386    
2023-06-16 02:59:21,372 - Epoch: [111][  120/  142]    Overall Loss 0.477960    Objective Loss 0.477960                                        LR 0.000031    Time 0.531962    
2023-06-16 02:59:26,380 - Epoch: [111][  130/  142]    Overall Loss 0.483218    Objective Loss 0.483218                                        LR 0.000031    Time 0.529565    
2023-06-16 02:59:30,982 - Epoch: [111][  140/  142]    Overall Loss 0.480498    Objective Loss 0.480498                                        LR 0.000031    Time 0.524602    
2023-06-16 02:59:31,836 - Epoch: [111][  142/  142]    Overall Loss 0.483955    Objective Loss 0.483955    Top1 75.000000    LR 0.000031    Time 0.523233    
2023-06-16 02:59:32,494 - --- validate (epoch=111)-----------
2023-06-16 02:59:32,495 - 1422 samples (32 per mini-batch)
2023-06-16 02:59:40,657 - Epoch: [111][   10/   45]    Loss 0.879612    Top1 69.687500    
2023-06-16 02:59:44,997 - Epoch: [111][   20/   45]    Loss 0.975378    Top1 70.781250    
2023-06-16 02:59:49,011 - Epoch: [111][   30/   45]    Loss 0.931730    Top1 72.395833    
2023-06-16 02:59:53,175 - Epoch: [111][   40/   45]    Loss 0.904467    Top1 73.671875    
2023-06-16 02:59:54,717 - Epoch: [111][   45/   45]    Loss 0.899636    Top1 74.050633    
2023-06-16 02:59:55,375 - ==> Top1: 74.051    Loss: 0.900

2023-06-16 02:59:55,377 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 108]
2023-06-16 02:59:55,377 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 02:59:55,399 - 

2023-06-16 02:59:55,399 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:00:04,844 - Epoch: [112][   10/  142]    Overall Loss 0.492776    Objective Loss 0.492776                                        LR 0.000031    Time 0.944346    
2023-06-16 03:00:09,716 - Epoch: [112][   20/  142]    Overall Loss 0.472421    Objective Loss 0.472421                                        LR 0.000031    Time 0.715744    
2023-06-16 03:00:14,773 - Epoch: [112][   30/  142]    Overall Loss 0.507023    Objective Loss 0.507023                                        LR 0.000031    Time 0.645719    
2023-06-16 03:00:19,626 - Epoch: [112][   40/  142]    Overall Loss 0.511437    Objective Loss 0.511437                                        LR 0.000031    Time 0.604296    
2023-06-16 03:00:24,464 - Epoch: [112][   50/  142]    Overall Loss 0.506515    Objective Loss 0.506515                                        LR 0.000031    Time 0.580193    
2023-06-16 03:00:29,567 - Epoch: [112][   60/  142]    Overall Loss 0.504993    Objective Loss 0.504993                                        LR 0.000031    Time 0.568530    
2023-06-16 03:00:34,484 - Epoch: [112][   70/  142]    Overall Loss 0.501329    Objective Loss 0.501329                                        LR 0.000031    Time 0.557548    
2023-06-16 03:00:39,391 - Epoch: [112][   80/  142]    Overall Loss 0.497200    Objective Loss 0.497200                                        LR 0.000031    Time 0.549173    
2023-06-16 03:00:44,340 - Epoch: [112][   90/  142]    Overall Loss 0.489079    Objective Loss 0.489079                                        LR 0.000031    Time 0.543139    
2023-06-16 03:00:49,331 - Epoch: [112][  100/  142]    Overall Loss 0.480702    Objective Loss 0.480702                                        LR 0.000031    Time 0.538727    
2023-06-16 03:00:54,251 - Epoch: [112][  110/  142]    Overall Loss 0.481681    Objective Loss 0.481681                                        LR 0.000031    Time 0.534477    
2023-06-16 03:00:59,249 - Epoch: [112][  120/  142]    Overall Loss 0.484010    Objective Loss 0.484010                                        LR 0.000031    Time 0.531582    
2023-06-16 03:01:04,207 - Epoch: [112][  130/  142]    Overall Loss 0.490444    Objective Loss 0.490444                                        LR 0.000031    Time 0.528821    
2023-06-16 03:01:08,814 - Epoch: [112][  140/  142]    Overall Loss 0.500896    Objective Loss 0.500896                                        LR 0.000031    Time 0.523955    
2023-06-16 03:01:09,661 - Epoch: [112][  142/  142]    Overall Loss 0.501524    Objective Loss 0.501524    Top1 84.375000    LR 0.000031    Time 0.522536    
2023-06-16 03:01:10,279 - --- validate (epoch=112)-----------
2023-06-16 03:01:10,280 - 1422 samples (32 per mini-batch)
2023-06-16 03:01:17,925 - Epoch: [112][   10/   45]    Loss 0.813095    Top1 75.937500    
2023-06-16 03:01:22,535 - Epoch: [112][   20/   45]    Loss 0.874869    Top1 73.906250    
2023-06-16 03:01:26,626 - Epoch: [112][   30/   45]    Loss 0.851388    Top1 74.583333    
2023-06-16 03:01:31,216 - Epoch: [112][   40/   45]    Loss 0.866071    Top1 74.218750    
2023-06-16 03:01:32,633 - Epoch: [112][   45/   45]    Loss 0.884995    Top1 74.050633    
2023-06-16 03:01:33,292 - ==> Top1: 74.051    Loss: 0.885

2023-06-16 03:01:33,294 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 108]
2023-06-16 03:01:33,294 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:01:33,316 - 

2023-06-16 03:01:33,316 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:01:42,734 - Epoch: [113][   10/  142]    Overall Loss 0.561398    Objective Loss 0.561398                                        LR 0.000031    Time 0.941688    
2023-06-16 03:01:47,700 - Epoch: [113][   20/  142]    Overall Loss 0.501206    Objective Loss 0.501206                                        LR 0.000031    Time 0.719136    
2023-06-16 03:01:52,690 - Epoch: [113][   30/  142]    Overall Loss 0.509532    Objective Loss 0.509532                                        LR 0.000031    Time 0.645724    
2023-06-16 03:01:57,681 - Epoch: [113][   40/  142]    Overall Loss 0.502897    Objective Loss 0.502897                                        LR 0.000031    Time 0.609053    
2023-06-16 03:02:02,720 - Epoch: [113][   50/  142]    Overall Loss 0.500820    Objective Loss 0.500820                                        LR 0.000031    Time 0.588026    
2023-06-16 03:02:07,754 - Epoch: [113][   60/  142]    Overall Loss 0.496945    Objective Loss 0.496945                                        LR 0.000031    Time 0.573900    
2023-06-16 03:02:12,862 - Epoch: [113][   70/  142]    Overall Loss 0.495175    Objective Loss 0.495175                                        LR 0.000031    Time 0.564878    
2023-06-16 03:02:17,897 - Epoch: [113][   80/  142]    Overall Loss 0.489039    Objective Loss 0.489039                                        LR 0.000031    Time 0.557201    
2023-06-16 03:02:22,763 - Epoch: [113][   90/  142]    Overall Loss 0.494354    Objective Loss 0.494354                                        LR 0.000031    Time 0.549358    
2023-06-16 03:02:27,787 - Epoch: [113][  100/  142]    Overall Loss 0.488407    Objective Loss 0.488407                                        LR 0.000031    Time 0.544649    
2023-06-16 03:02:32,643 - Epoch: [113][  110/  142]    Overall Loss 0.491743    Objective Loss 0.491743                                        LR 0.000031    Time 0.539274    
2023-06-16 03:02:37,735 - Epoch: [113][  120/  142]    Overall Loss 0.492095    Objective Loss 0.492095                                        LR 0.000031    Time 0.536768    
2023-06-16 03:02:42,666 - Epoch: [113][  130/  142]    Overall Loss 0.493589    Objective Loss 0.493589                                        LR 0.000031    Time 0.533401    
2023-06-16 03:02:47,336 - Epoch: [113][  140/  142]    Overall Loss 0.494077    Objective Loss 0.494077                                        LR 0.000031    Time 0.528654    
2023-06-16 03:02:48,193 - Epoch: [113][  142/  142]    Overall Loss 0.495217    Objective Loss 0.495217    Top1 79.687500    LR 0.000031    Time 0.527244    
2023-06-16 03:02:48,851 - --- validate (epoch=113)-----------
2023-06-16 03:02:48,852 - 1422 samples (32 per mini-batch)
2023-06-16 03:02:57,129 - Epoch: [113][   10/   45]    Loss 0.953697    Top1 74.687500    
2023-06-16 03:03:01,006 - Epoch: [113][   20/   45]    Loss 0.913924    Top1 75.781250    
2023-06-16 03:03:05,945 - Epoch: [113][   30/   45]    Loss 0.876197    Top1 75.625000    
2023-06-16 03:03:10,767 - Epoch: [113][   40/   45]    Loss 0.889331    Top1 74.765625    
2023-06-16 03:03:12,130 - Epoch: [113][   45/   45]    Loss 0.912485    Top1 74.542897    
2023-06-16 03:03:12,788 - ==> Top1: 74.543    Loss: 0.912

2023-06-16 03:03:12,790 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 108]
2023-06-16 03:03:12,790 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:03:12,811 - 

2023-06-16 03:03:12,811 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:03:22,176 - Epoch: [114][   10/  142]    Overall Loss 0.449761    Objective Loss 0.449761                                        LR 0.000031    Time 0.936376    
2023-06-16 03:03:27,222 - Epoch: [114][   20/  142]    Overall Loss 0.447812    Objective Loss 0.447812                                        LR 0.000031    Time 0.720424    
2023-06-16 03:03:32,239 - Epoch: [114][   30/  142]    Overall Loss 0.477181    Objective Loss 0.477181                                        LR 0.000031    Time 0.647498    
2023-06-16 03:03:37,401 - Epoch: [114][   40/  142]    Overall Loss 0.488915    Objective Loss 0.488915                                        LR 0.000031    Time 0.614664    
2023-06-16 03:03:42,475 - Epoch: [114][   50/  142]    Overall Loss 0.493174    Objective Loss 0.493174                                        LR 0.000031    Time 0.593201    
2023-06-16 03:03:47,444 - Epoch: [114][   60/  142]    Overall Loss 0.491787    Objective Loss 0.491787                                        LR 0.000031    Time 0.577140    
2023-06-16 03:03:52,475 - Epoch: [114][   70/  142]    Overall Loss 0.504054    Objective Loss 0.504054                                        LR 0.000031    Time 0.566550    
2023-06-16 03:03:57,436 - Epoch: [114][   80/  142]    Overall Loss 0.501824    Objective Loss 0.501824                                        LR 0.000031    Time 0.557741    
2023-06-16 03:04:02,527 - Epoch: [114][   90/  142]    Overall Loss 0.504316    Objective Loss 0.504316                                        LR 0.000031    Time 0.552334    
2023-06-16 03:04:07,655 - Epoch: [114][  100/  142]    Overall Loss 0.498973    Objective Loss 0.498973                                        LR 0.000031    Time 0.548367    
2023-06-16 03:04:12,747 - Epoch: [114][  110/  142]    Overall Loss 0.501196    Objective Loss 0.501196                                        LR 0.000031    Time 0.544803    
2023-06-16 03:04:17,781 - Epoch: [114][  120/  142]    Overall Loss 0.497739    Objective Loss 0.497739                                        LR 0.000031    Time 0.541344    
2023-06-16 03:04:22,891 - Epoch: [114][  130/  142]    Overall Loss 0.498805    Objective Loss 0.498805                                        LR 0.000031    Time 0.539010    
2023-06-16 03:04:27,585 - Epoch: [114][  140/  142]    Overall Loss 0.501726    Objective Loss 0.501726                                        LR 0.000031    Time 0.534032    
2023-06-16 03:04:28,430 - Epoch: [114][  142/  142]    Overall Loss 0.501340    Objective Loss 0.501340    Top1 81.250000    LR 0.000031    Time 0.532460    
2023-06-16 03:04:29,061 - --- validate (epoch=114)-----------
2023-06-16 03:04:29,062 - 1422 samples (32 per mini-batch)
2023-06-16 03:04:37,107 - Epoch: [114][   10/   45]    Loss 1.034481    Top1 71.562500    
2023-06-16 03:04:41,229 - Epoch: [114][   20/   45]    Loss 0.903993    Top1 73.593750    
2023-06-16 03:04:45,296 - Epoch: [114][   30/   45]    Loss 0.894402    Top1 74.687500    
2023-06-16 03:04:49,638 - Epoch: [114][   40/   45]    Loss 0.906892    Top1 75.234375    
2023-06-16 03:04:51,074 - Epoch: [114][   45/   45]    Loss 0.926534    Top1 74.753868    
2023-06-16 03:04:51,731 - ==> Top1: 74.754    Loss: 0.927

2023-06-16 03:04:51,733 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 108]
2023-06-16 03:04:51,733 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:04:51,754 - 

2023-06-16 03:04:51,754 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:05:01,048 - Epoch: [115][   10/  142]    Overall Loss 0.500751    Objective Loss 0.500751                                        LR 0.000031    Time 0.929246    
2023-06-16 03:05:05,991 - Epoch: [115][   20/  142]    Overall Loss 0.525308    Objective Loss 0.525308                                        LR 0.000031    Time 0.711731    
2023-06-16 03:05:10,871 - Epoch: [115][   30/  142]    Overall Loss 0.526645    Objective Loss 0.526645                                        LR 0.000031    Time 0.637132    
2023-06-16 03:05:15,878 - Epoch: [115][   40/  142]    Overall Loss 0.492624    Objective Loss 0.492624                                        LR 0.000031    Time 0.603008    
2023-06-16 03:05:20,681 - Epoch: [115][   50/  142]    Overall Loss 0.496615    Objective Loss 0.496615                                        LR 0.000031    Time 0.578464    
2023-06-16 03:05:25,521 - Epoch: [115][   60/  142]    Overall Loss 0.470950    Objective Loss 0.470950                                        LR 0.000031    Time 0.562709    
2023-06-16 03:05:30,544 - Epoch: [115][   70/  142]    Overall Loss 0.473431    Objective Loss 0.473431                                        LR 0.000031    Time 0.554076    
2023-06-16 03:05:35,405 - Epoch: [115][   80/  142]    Overall Loss 0.485980    Objective Loss 0.485980                                        LR 0.000031    Time 0.545562    
2023-06-16 03:05:40,492 - Epoch: [115][   90/  142]    Overall Loss 0.495568    Objective Loss 0.495568                                        LR 0.000031    Time 0.541465    
2023-06-16 03:05:45,355 - Epoch: [115][  100/  142]    Overall Loss 0.496079    Objective Loss 0.496079                                        LR 0.000031    Time 0.535947    
2023-06-16 03:05:50,271 - Epoch: [115][  110/  142]    Overall Loss 0.500937    Objective Loss 0.500937                                        LR 0.000031    Time 0.531908    
2023-06-16 03:05:55,229 - Epoch: [115][  120/  142]    Overall Loss 0.494969    Objective Loss 0.494969                                        LR 0.000031    Time 0.528895    
2023-06-16 03:06:00,100 - Epoch: [115][  130/  142]    Overall Loss 0.493522    Objective Loss 0.493522                                        LR 0.000031    Time 0.525676    
2023-06-16 03:06:04,724 - Epoch: [115][  140/  142]    Overall Loss 0.495389    Objective Loss 0.495389                                        LR 0.000031    Time 0.521154    
2023-06-16 03:06:05,566 - Epoch: [115][  142/  142]    Overall Loss 0.498135    Objective Loss 0.498135    Top1 79.687500    LR 0.000031    Time 0.519740    
2023-06-16 03:06:06,194 - --- validate (epoch=115)-----------
2023-06-16 03:06:06,195 - 1422 samples (32 per mini-batch)
2023-06-16 03:06:14,434 - Epoch: [115][   10/   45]    Loss 1.004561    Top1 75.937500    
2023-06-16 03:06:18,461 - Epoch: [115][   20/   45]    Loss 0.994352    Top1 73.906250    
2023-06-16 03:06:22,583 - Epoch: [115][   30/   45]    Loss 0.999530    Top1 72.916667    
2023-06-16 03:06:27,381 - Epoch: [115][   40/   45]    Loss 0.992543    Top1 73.515625    
2023-06-16 03:06:28,762 - Epoch: [115][   45/   45]    Loss 0.965148    Top1 73.909986    
2023-06-16 03:06:29,400 - ==> Top1: 73.910    Loss: 0.965

2023-06-16 03:06:29,403 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 108]
2023-06-16 03:06:29,403 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:06:29,416 - 

2023-06-16 03:06:29,416 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:06:38,688 - Epoch: [116][   10/  142]    Overall Loss 0.540999    Objective Loss 0.540999                                        LR 0.000031    Time 0.927021    
2023-06-16 03:06:43,662 - Epoch: [116][   20/  142]    Overall Loss 0.512020    Objective Loss 0.512020                                        LR 0.000031    Time 0.712171    
2023-06-16 03:06:48,556 - Epoch: [116][   30/  142]    Overall Loss 0.477218    Objective Loss 0.477218                                        LR 0.000031    Time 0.637909    
2023-06-16 03:06:53,714 - Epoch: [116][   40/  142]    Overall Loss 0.486247    Objective Loss 0.486247                                        LR 0.000031    Time 0.607369    
2023-06-16 03:06:58,643 - Epoch: [116][   50/  142]    Overall Loss 0.481173    Objective Loss 0.481173                                        LR 0.000031    Time 0.584465    
2023-06-16 03:07:03,642 - Epoch: [116][   60/  142]    Overall Loss 0.485228    Objective Loss 0.485228                                        LR 0.000031    Time 0.570353    
2023-06-16 03:07:08,657 - Epoch: [116][   70/  142]    Overall Loss 0.489473    Objective Loss 0.489473                                        LR 0.000031    Time 0.560515    
2023-06-16 03:07:13,706 - Epoch: [116][   80/  142]    Overall Loss 0.492748    Objective Loss 0.492748                                        LR 0.000031    Time 0.553560    
2023-06-16 03:07:18,696 - Epoch: [116][   90/  142]    Overall Loss 0.487900    Objective Loss 0.487900                                        LR 0.000031    Time 0.547481    
2023-06-16 03:07:23,753 - Epoch: [116][  100/  142]    Overall Loss 0.484823    Objective Loss 0.484823                                        LR 0.000031    Time 0.543306    
2023-06-16 03:07:28,781 - Epoch: [116][  110/  142]    Overall Loss 0.488593    Objective Loss 0.488593                                        LR 0.000031    Time 0.539616    
2023-06-16 03:07:33,927 - Epoch: [116][  120/  142]    Overall Loss 0.492010    Objective Loss 0.492010                                        LR 0.000031    Time 0.537528    
2023-06-16 03:07:38,929 - Epoch: [116][  130/  142]    Overall Loss 0.490142    Objective Loss 0.490142                                        LR 0.000031    Time 0.534652    
2023-06-16 03:07:43,563 - Epoch: [116][  140/  142]    Overall Loss 0.487865    Objective Loss 0.487865                                        LR 0.000031    Time 0.529562    
2023-06-16 03:07:44,418 - Epoch: [116][  142/  142]    Overall Loss 0.487236    Objective Loss 0.487236    Top1 85.937500    LR 0.000031    Time 0.528119    
2023-06-16 03:07:45,059 - --- validate (epoch=116)-----------
2023-06-16 03:07:45,060 - 1422 samples (32 per mini-batch)
2023-06-16 03:07:53,154 - Epoch: [116][   10/   45]    Loss 0.798084    Top1 73.125000    
2023-06-16 03:07:57,328 - Epoch: [116][   20/   45]    Loss 0.812445    Top1 75.625000    
2023-06-16 03:08:02,375 - Epoch: [116][   30/   45]    Loss 0.842013    Top1 74.791667    
2023-06-16 03:08:06,548 - Epoch: [116][   40/   45]    Loss 0.861935    Top1 74.375000    
2023-06-16 03:08:07,930 - Epoch: [116][   45/   45]    Loss 0.863880    Top1 74.191280    
2023-06-16 03:08:08,593 - ==> Top1: 74.191    Loss: 0.864

2023-06-16 03:08:08,595 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 108]
2023-06-16 03:08:08,595 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:08:08,617 - 

2023-06-16 03:08:08,617 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:08:17,993 - Epoch: [117][   10/  142]    Overall Loss 0.471503    Objective Loss 0.471503                                        LR 0.000031    Time 0.937468    
2023-06-16 03:08:23,048 - Epoch: [117][   20/  142]    Overall Loss 0.446872    Objective Loss 0.446872                                        LR 0.000031    Time 0.721480    
2023-06-16 03:08:28,019 - Epoch: [117][   30/  142]    Overall Loss 0.470678    Objective Loss 0.470678                                        LR 0.000031    Time 0.646676    
2023-06-16 03:08:32,982 - Epoch: [117][   40/  142]    Overall Loss 0.504710    Objective Loss 0.504710                                        LR 0.000031    Time 0.609062    
2023-06-16 03:08:37,882 - Epoch: [117][   50/  142]    Overall Loss 0.494622    Objective Loss 0.494622                                        LR 0.000031    Time 0.585231    
2023-06-16 03:08:42,955 - Epoch: [117][   60/  142]    Overall Loss 0.502142    Objective Loss 0.502142                                        LR 0.000031    Time 0.572228    
2023-06-16 03:08:47,931 - Epoch: [117][   70/  142]    Overall Loss 0.509187    Objective Loss 0.509187                                        LR 0.000031    Time 0.561568    
2023-06-16 03:08:52,952 - Epoch: [117][   80/  142]    Overall Loss 0.494515    Objective Loss 0.494515                                        LR 0.000031    Time 0.554127    
2023-06-16 03:08:57,930 - Epoch: [117][   90/  142]    Overall Loss 0.487613    Objective Loss 0.487613                                        LR 0.000031    Time 0.547856    
2023-06-16 03:09:02,822 - Epoch: [117][  100/  142]    Overall Loss 0.488166    Objective Loss 0.488166                                        LR 0.000031    Time 0.541989    
2023-06-16 03:09:07,954 - Epoch: [117][  110/  142]    Overall Loss 0.485058    Objective Loss 0.485058                                        LR 0.000031    Time 0.539368    
2023-06-16 03:09:12,885 - Epoch: [117][  120/  142]    Overall Loss 0.484744    Objective Loss 0.484744                                        LR 0.000031    Time 0.535504    
2023-06-16 03:09:17,839 - Epoch: [117][  130/  142]    Overall Loss 0.488547    Objective Loss 0.488547                                        LR 0.000031    Time 0.532420    
2023-06-16 03:09:22,547 - Epoch: [117][  140/  142]    Overall Loss 0.492867    Objective Loss 0.492867                                        LR 0.000031    Time 0.528010    
2023-06-16 03:09:23,402 - Epoch: [117][  142/  142]    Overall Loss 0.492857    Objective Loss 0.492857    Top1 84.375000    LR 0.000031    Time 0.526594    
2023-06-16 03:09:24,009 - --- validate (epoch=117)-----------
2023-06-16 03:09:24,010 - 1422 samples (32 per mini-batch)
2023-06-16 03:09:31,981 - Epoch: [117][   10/   45]    Loss 0.920792    Top1 73.437500    
2023-06-16 03:09:36,218 - Epoch: [117][   20/   45]    Loss 0.982574    Top1 72.343750    
2023-06-16 03:09:40,423 - Epoch: [117][   30/   45]    Loss 0.945157    Top1 72.812500    
2023-06-16 03:09:45,069 - Epoch: [117][   40/   45]    Loss 0.915910    Top1 74.218750    
2023-06-16 03:09:46,574 - Epoch: [117][   45/   45]    Loss 0.909196    Top1 74.191280    
2023-06-16 03:09:47,199 - ==> Top1: 74.191    Loss: 0.909

2023-06-16 03:09:47,202 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 108]
2023-06-16 03:09:47,202 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:09:47,223 - 

2023-06-16 03:09:47,223 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:09:56,700 - Epoch: [118][   10/  142]    Overall Loss 0.579291    Objective Loss 0.579291                                        LR 0.000031    Time 0.947600    
2023-06-16 03:10:01,677 - Epoch: [118][   20/  142]    Overall Loss 0.508746    Objective Loss 0.508746                                        LR 0.000031    Time 0.722638    
2023-06-16 03:10:06,634 - Epoch: [118][   30/  142]    Overall Loss 0.533310    Objective Loss 0.533310                                        LR 0.000031    Time 0.646972    
2023-06-16 03:10:11,605 - Epoch: [118][   40/  142]    Overall Loss 0.528125    Objective Loss 0.528125                                        LR 0.000031    Time 0.609485    
2023-06-16 03:10:16,619 - Epoch: [118][   50/  142]    Overall Loss 0.537446    Objective Loss 0.537446                                        LR 0.000031    Time 0.587851    
2023-06-16 03:10:21,602 - Epoch: [118][   60/  142]    Overall Loss 0.523206    Objective Loss 0.523206                                        LR 0.000031    Time 0.572917    
2023-06-16 03:10:26,547 - Epoch: [118][   70/  142]    Overall Loss 0.512189    Objective Loss 0.512189                                        LR 0.000031    Time 0.561708    
2023-06-16 03:10:31,491 - Epoch: [118][   80/  142]    Overall Loss 0.509408    Objective Loss 0.509408                                        LR 0.000031    Time 0.553289    
2023-06-16 03:10:36,547 - Epoch: [118][   90/  142]    Overall Loss 0.502835    Objective Loss 0.502835                                        LR 0.000031    Time 0.547983    
2023-06-16 03:10:41,475 - Epoch: [118][  100/  142]    Overall Loss 0.496748    Objective Loss 0.496748                                        LR 0.000031    Time 0.542458    
2023-06-16 03:10:46,400 - Epoch: [118][  110/  142]    Overall Loss 0.495077    Objective Loss 0.495077                                        LR 0.000031    Time 0.537913    
2023-06-16 03:10:51,442 - Epoch: [118][  120/  142]    Overall Loss 0.486596    Objective Loss 0.486596                                        LR 0.000031    Time 0.535096    
2023-06-16 03:10:56,440 - Epoch: [118][  130/  142]    Overall Loss 0.491427    Objective Loss 0.491427                                        LR 0.000031    Time 0.532375    
2023-06-16 03:11:01,104 - Epoch: [118][  140/  142]    Overall Loss 0.491533    Objective Loss 0.491533                                        LR 0.000031    Time 0.527662    
2023-06-16 03:11:01,959 - Epoch: [118][  142/  142]    Overall Loss 0.489984    Objective Loss 0.489984    Top1 87.500000    LR 0.000031    Time 0.526250    
2023-06-16 03:11:02,592 - --- validate (epoch=118)-----------
2023-06-16 03:11:02,592 - 1422 samples (32 per mini-batch)
2023-06-16 03:11:10,681 - Epoch: [118][   10/   45]    Loss 0.801235    Top1 75.625000    
2023-06-16 03:11:14,912 - Epoch: [118][   20/   45]    Loss 0.830820    Top1 76.250000    
2023-06-16 03:11:19,265 - Epoch: [118][   30/   45]    Loss 0.839540    Top1 75.520833    
2023-06-16 03:11:24,002 - Epoch: [118][   40/   45]    Loss 0.897403    Top1 74.687500    
2023-06-16 03:11:25,364 - Epoch: [118][   45/   45]    Loss 0.895290    Top1 74.542897    
2023-06-16 03:11:25,987 - ==> Top1: 74.543    Loss: 0.895

2023-06-16 03:11:25,989 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 108]
2023-06-16 03:11:25,989 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:11:26,010 - 

2023-06-16 03:11:26,010 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:11:35,336 - Epoch: [119][   10/  142]    Overall Loss 0.518744    Objective Loss 0.518744                                        LR 0.000031    Time 0.932478    
2023-06-16 03:11:40,317 - Epoch: [119][   20/  142]    Overall Loss 0.484769    Objective Loss 0.484769                                        LR 0.000031    Time 0.715269    
2023-06-16 03:11:45,167 - Epoch: [119][   30/  142]    Overall Loss 0.458336    Objective Loss 0.458336                                        LR 0.000031    Time 0.638468    
2023-06-16 03:11:50,128 - Epoch: [119][   40/  142]    Overall Loss 0.460644    Objective Loss 0.460644                                        LR 0.000031    Time 0.602862    
2023-06-16 03:11:55,048 - Epoch: [119][   50/  142]    Overall Loss 0.471907    Objective Loss 0.471907                                        LR 0.000031    Time 0.580683    
2023-06-16 03:11:59,889 - Epoch: [119][   60/  142]    Overall Loss 0.475605    Objective Loss 0.475605                                        LR 0.000031    Time 0.564581    
2023-06-16 03:12:04,799 - Epoch: [119][   70/  142]    Overall Loss 0.482302    Objective Loss 0.482302                                        LR 0.000031    Time 0.554053    
2023-06-16 03:12:09,767 - Epoch: [119][   80/  142]    Overall Loss 0.488125    Objective Loss 0.488125                                        LR 0.000031    Time 0.546896    
2023-06-16 03:12:14,584 - Epoch: [119][   90/  142]    Overall Loss 0.477775    Objective Loss 0.477775                                        LR 0.000031    Time 0.539647    
2023-06-16 03:12:19,581 - Epoch: [119][  100/  142]    Overall Loss 0.473187    Objective Loss 0.473187                                        LR 0.000031    Time 0.535645    
2023-06-16 03:12:24,434 - Epoch: [119][  110/  142]    Overall Loss 0.477930    Objective Loss 0.477930                                        LR 0.000031    Time 0.531059    
2023-06-16 03:12:29,329 - Epoch: [119][  120/  142]    Overall Loss 0.478894    Objective Loss 0.478894                                        LR 0.000031    Time 0.527591    
2023-06-16 03:12:34,164 - Epoch: [119][  130/  142]    Overall Loss 0.481180    Objective Loss 0.481180                                        LR 0.000031    Time 0.524199    
2023-06-16 03:12:38,798 - Epoch: [119][  140/  142]    Overall Loss 0.483092    Objective Loss 0.483092                                        LR 0.000031    Time 0.519850    
2023-06-16 03:12:39,641 - Epoch: [119][  142/  142]    Overall Loss 0.485498    Objective Loss 0.485498    Top1 79.687500    LR 0.000031    Time 0.518462    
2023-06-16 03:12:40,300 - --- validate (epoch=119)-----------
2023-06-16 03:12:40,300 - 1422 samples (32 per mini-batch)
2023-06-16 03:12:48,531 - Epoch: [119][   10/   45]    Loss 0.877433    Top1 77.187500    
2023-06-16 03:12:52,622 - Epoch: [119][   20/   45]    Loss 0.920116    Top1 74.687500    
2023-06-16 03:12:56,956 - Epoch: [119][   30/   45]    Loss 0.884913    Top1 74.583333    
2023-06-16 03:13:01,193 - Epoch: [119][   40/   45]    Loss 0.907970    Top1 73.593750    
2023-06-16 03:13:02,652 - Epoch: [119][   45/   45]    Loss 0.914255    Top1 73.488045    
2023-06-16 03:13:03,302 - ==> Top1: 73.488    Loss: 0.914

2023-06-16 03:13:03,304 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 108]
2023-06-16 03:13:03,304 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:13:03,326 - 

2023-06-16 03:13:03,326 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:13:12,725 - Epoch: [120][   10/  142]    Overall Loss 0.507428    Objective Loss 0.507428                                        LR 0.000016    Time 0.939792    
2023-06-16 03:13:17,715 - Epoch: [120][   20/  142]    Overall Loss 0.489601    Objective Loss 0.489601                                        LR 0.000016    Time 0.719385    
2023-06-16 03:13:22,649 - Epoch: [120][   30/  142]    Overall Loss 0.480782    Objective Loss 0.480782                                        LR 0.000016    Time 0.644016    
2023-06-16 03:13:27,719 - Epoch: [120][   40/  142]    Overall Loss 0.463804    Objective Loss 0.463804                                        LR 0.000016    Time 0.609760    
2023-06-16 03:13:32,592 - Epoch: [120][   50/  142]    Overall Loss 0.474885    Objective Loss 0.474885                                        LR 0.000016    Time 0.585256    
2023-06-16 03:13:37,558 - Epoch: [120][   60/  142]    Overall Loss 0.487167    Objective Loss 0.487167                                        LR 0.000016    Time 0.570479    
2023-06-16 03:13:42,623 - Epoch: [120][   70/  142]    Overall Loss 0.478984    Objective Loss 0.478984                                        LR 0.000016    Time 0.561329    
2023-06-16 03:13:47,523 - Epoch: [120][   80/  142]    Overall Loss 0.468359    Objective Loss 0.468359                                        LR 0.000016    Time 0.552403    
2023-06-16 03:13:52,545 - Epoch: [120][   90/  142]    Overall Loss 0.466995    Objective Loss 0.466995                                        LR 0.000016    Time 0.546812    
2023-06-16 03:13:57,454 - Epoch: [120][  100/  142]    Overall Loss 0.471389    Objective Loss 0.471389                                        LR 0.000016    Time 0.541214    
2023-06-16 03:14:02,446 - Epoch: [120][  110/  142]    Overall Loss 0.474162    Objective Loss 0.474162                                        LR 0.000016    Time 0.537389    
2023-06-16 03:14:07,291 - Epoch: [120][  120/  142]    Overall Loss 0.469178    Objective Loss 0.469178                                        LR 0.000016    Time 0.532975    
2023-06-16 03:14:12,286 - Epoch: [120][  130/  142]    Overall Loss 0.473926    Objective Loss 0.473926                                        LR 0.000016    Time 0.530399    
2023-06-16 03:14:16,934 - Epoch: [120][  140/  142]    Overall Loss 0.480248    Objective Loss 0.480248                                        LR 0.000016    Time 0.525712    
2023-06-16 03:14:17,775 - Epoch: [120][  142/  142]    Overall Loss 0.478623    Objective Loss 0.478623    Top1 89.062500    LR 0.000016    Time 0.524226    
2023-06-16 03:14:18,434 - --- validate (epoch=120)-----------
2023-06-16 03:14:18,435 - 1422 samples (32 per mini-batch)
2023-06-16 03:14:26,678 - Epoch: [120][   10/   45]    Loss 0.838640    Top1 75.312500    
2023-06-16 03:14:31,532 - Epoch: [120][   20/   45]    Loss 0.838045    Top1 75.312500    
2023-06-16 03:14:35,608 - Epoch: [120][   30/   45]    Loss 0.925749    Top1 74.062500    
2023-06-16 03:14:39,751 - Epoch: [120][   40/   45]    Loss 0.911767    Top1 74.375000    
2023-06-16 03:14:41,194 - Epoch: [120][   45/   45]    Loss 0.902583    Top1 74.613221    
2023-06-16 03:14:41,841 - ==> Top1: 74.613    Loss: 0.903

2023-06-16 03:14:41,843 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 108]
2023-06-16 03:14:41,843 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:14:41,864 - 

2023-06-16 03:14:41,865 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:14:51,202 - Epoch: [121][   10/  142]    Overall Loss 0.530717    Objective Loss 0.530717                                        LR 0.000016    Time 0.933627    
2023-06-16 03:14:56,166 - Epoch: [121][   20/  142]    Overall Loss 0.520213    Objective Loss 0.520213                                        LR 0.000016    Time 0.714938    
2023-06-16 03:15:01,130 - Epoch: [121][   30/  142]    Overall Loss 0.491449    Objective Loss 0.491449                                        LR 0.000016    Time 0.642081    
2023-06-16 03:15:06,102 - Epoch: [121][   40/  142]    Overall Loss 0.494110    Objective Loss 0.494110                                        LR 0.000016    Time 0.605851    
2023-06-16 03:15:11,051 - Epoch: [121][   50/  142]    Overall Loss 0.472915    Objective Loss 0.472915                                        LR 0.000016    Time 0.583642    
2023-06-16 03:15:16,102 - Epoch: [121][   60/  142]    Overall Loss 0.470515    Objective Loss 0.470515                                        LR 0.000016    Time 0.570552    
2023-06-16 03:15:21,095 - Epoch: [121][   70/  142]    Overall Loss 0.473645    Objective Loss 0.473645                                        LR 0.000016    Time 0.560357    
2023-06-16 03:15:26,116 - Epoch: [121][   80/  142]    Overall Loss 0.465845    Objective Loss 0.465845                                        LR 0.000016    Time 0.553076    
2023-06-16 03:15:31,040 - Epoch: [121][   90/  142]    Overall Loss 0.469305    Objective Loss 0.469305                                        LR 0.000016    Time 0.546322    
2023-06-16 03:15:36,123 - Epoch: [121][  100/  142]    Overall Loss 0.464417    Objective Loss 0.464417                                        LR 0.000016    Time 0.542519    
2023-06-16 03:15:40,991 - Epoch: [121][  110/  142]    Overall Loss 0.469031    Objective Loss 0.469031                                        LR 0.000016    Time 0.537450    
2023-06-16 03:15:45,999 - Epoch: [121][  120/  142]    Overall Loss 0.477331    Objective Loss 0.477331                                        LR 0.000016    Time 0.534388    
2023-06-16 03:15:50,959 - Epoch: [121][  130/  142]    Overall Loss 0.473982    Objective Loss 0.473982                                        LR 0.000016    Time 0.531428    
2023-06-16 03:15:55,597 - Epoch: [121][  140/  142]    Overall Loss 0.477196    Objective Loss 0.477196                                        LR 0.000016    Time 0.526596    
2023-06-16 03:15:56,451 - Epoch: [121][  142/  142]    Overall Loss 0.475662    Objective Loss 0.475662    Top1 87.500000    LR 0.000016    Time 0.525194    
2023-06-16 03:15:57,082 - --- validate (epoch=121)-----------
2023-06-16 03:15:57,082 - 1422 samples (32 per mini-batch)
2023-06-16 03:16:05,072 - Epoch: [121][   10/   45]    Loss 0.951956    Top1 75.312500    
2023-06-16 03:16:09,240 - Epoch: [121][   20/   45]    Loss 0.982496    Top1 73.750000    
2023-06-16 03:16:13,682 - Epoch: [121][   30/   45]    Loss 0.955779    Top1 73.333333    
2023-06-16 03:16:17,958 - Epoch: [121][   40/   45]    Loss 0.925318    Top1 74.296875    
2023-06-16 03:16:19,295 - Epoch: [121][   45/   45]    Loss 0.901700    Top1 74.613221    
2023-06-16 03:16:19,903 - ==> Top1: 74.613    Loss: 0.902

2023-06-16 03:16:19,905 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 108]
2023-06-16 03:16:19,905 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:16:19,926 - 

2023-06-16 03:16:19,926 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:16:29,462 - Epoch: [122][   10/  142]    Overall Loss 0.465737    Objective Loss 0.465737                                        LR 0.000016    Time 0.953502    
2023-06-16 03:16:34,524 - Epoch: [122][   20/  142]    Overall Loss 0.454876    Objective Loss 0.454876                                        LR 0.000016    Time 0.729793    
2023-06-16 03:16:39,564 - Epoch: [122][   30/  142]    Overall Loss 0.446790    Objective Loss 0.446790                                        LR 0.000016    Time 0.654522    
2023-06-16 03:16:44,606 - Epoch: [122][   40/  142]    Overall Loss 0.473783    Objective Loss 0.473783                                        LR 0.000016    Time 0.616925    
2023-06-16 03:16:49,641 - Epoch: [122][   50/  142]    Overall Loss 0.469953    Objective Loss 0.469953                                        LR 0.000016    Time 0.594230    
2023-06-16 03:16:54,694 - Epoch: [122][   60/  142]    Overall Loss 0.465612    Objective Loss 0.465612                                        LR 0.000016    Time 0.579394    
2023-06-16 03:16:59,714 - Epoch: [122][   70/  142]    Overall Loss 0.467959    Objective Loss 0.467959                                        LR 0.000016    Time 0.568330    
2023-06-16 03:17:04,782 - Epoch: [122][   80/  142]    Overall Loss 0.462474    Objective Loss 0.462474                                        LR 0.000016    Time 0.560641    
2023-06-16 03:17:09,740 - Epoch: [122][   90/  142]    Overall Loss 0.461724    Objective Loss 0.461724                                        LR 0.000016    Time 0.553426    
2023-06-16 03:17:14,731 - Epoch: [122][  100/  142]    Overall Loss 0.451835    Objective Loss 0.451835                                        LR 0.000016    Time 0.547988    
2023-06-16 03:17:19,848 - Epoch: [122][  110/  142]    Overall Loss 0.449913    Objective Loss 0.449913                                        LR 0.000016    Time 0.544687    
2023-06-16 03:17:24,894 - Epoch: [122][  120/  142]    Overall Loss 0.453648    Objective Loss 0.453648                                        LR 0.000016    Time 0.541341    
2023-06-16 03:17:29,872 - Epoch: [122][  130/  142]    Overall Loss 0.454687    Objective Loss 0.454687                                        LR 0.000016    Time 0.537986    
2023-06-16 03:17:34,602 - Epoch: [122][  140/  142]    Overall Loss 0.458203    Objective Loss 0.458203                                        LR 0.000016    Time 0.533342    
2023-06-16 03:17:35,461 - Epoch: [122][  142/  142]    Overall Loss 0.461764    Objective Loss 0.461764    Top1 76.562500    LR 0.000016    Time 0.531875    
2023-06-16 03:17:36,096 - --- validate (epoch=122)-----------
2023-06-16 03:17:36,096 - 1422 samples (32 per mini-batch)
2023-06-16 03:17:44,032 - Epoch: [122][   10/   45]    Loss 0.723968    Top1 78.750000    
2023-06-16 03:17:48,255 - Epoch: [122][   20/   45]    Loss 0.811043    Top1 75.000000    
2023-06-16 03:17:52,864 - Epoch: [122][   30/   45]    Loss 0.804671    Top1 75.416667    
2023-06-16 03:17:57,051 - Epoch: [122][   40/   45]    Loss 0.832055    Top1 74.843750    
2023-06-16 03:17:58,659 - Epoch: [122][   45/   45]    Loss 0.838751    Top1 74.894515    
2023-06-16 03:17:59,307 - ==> Top1: 74.895    Loss: 0.839

2023-06-16 03:17:59,310 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 122]
2023-06-16 03:17:59,310 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:17:59,335 - 

2023-06-16 03:17:59,335 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:18:08,494 - Epoch: [123][   10/  142]    Overall Loss 0.473863    Objective Loss 0.473863                                        LR 0.000016    Time 0.915753    
2023-06-16 03:18:13,453 - Epoch: [123][   20/  142]    Overall Loss 0.517036    Objective Loss 0.517036                                        LR 0.000016    Time 0.705803    
2023-06-16 03:18:18,534 - Epoch: [123][   30/  142]    Overall Loss 0.508598    Objective Loss 0.508598                                        LR 0.000016    Time 0.639881    
2023-06-16 03:18:23,514 - Epoch: [123][   40/  142]    Overall Loss 0.510354    Objective Loss 0.510354                                        LR 0.000016    Time 0.604391    
2023-06-16 03:18:28,543 - Epoch: [123][   50/  142]    Overall Loss 0.492311    Objective Loss 0.492311                                        LR 0.000016    Time 0.584085    
2023-06-16 03:18:33,522 - Epoch: [123][   60/  142]    Overall Loss 0.484887    Objective Loss 0.484887                                        LR 0.000016    Time 0.569709    
2023-06-16 03:18:38,566 - Epoch: [123][   70/  142]    Overall Loss 0.474722    Objective Loss 0.474722                                        LR 0.000016    Time 0.560373    
2023-06-16 03:18:43,552 - Epoch: [123][   80/  142]    Overall Loss 0.477892    Objective Loss 0.477892                                        LR 0.000016    Time 0.552649    
2023-06-16 03:18:48,601 - Epoch: [123][   90/  142]    Overall Loss 0.476306    Objective Loss 0.476306                                        LR 0.000016    Time 0.547331    
2023-06-16 03:18:53,638 - Epoch: [123][  100/  142]    Overall Loss 0.474596    Objective Loss 0.474596                                        LR 0.000016    Time 0.542964    
2023-06-16 03:18:58,595 - Epoch: [123][  110/  142]    Overall Loss 0.481890    Objective Loss 0.481890                                        LR 0.000016    Time 0.538665    
2023-06-16 03:19:03,609 - Epoch: [123][  120/  142]    Overall Loss 0.480109    Objective Loss 0.480109                                        LR 0.000016    Time 0.535557    
2023-06-16 03:19:08,550 - Epoch: [123][  130/  142]    Overall Loss 0.473750    Objective Loss 0.473750                                        LR 0.000016    Time 0.532361    
2023-06-16 03:19:13,253 - Epoch: [123][  140/  142]    Overall Loss 0.474498    Objective Loss 0.474498                                        LR 0.000016    Time 0.527921    
2023-06-16 03:19:14,111 - Epoch: [123][  142/  142]    Overall Loss 0.472488    Objective Loss 0.472488    Top1 90.625000    LR 0.000016    Time 0.526525    
2023-06-16 03:19:14,761 - --- validate (epoch=123)-----------
2023-06-16 03:19:14,761 - 1422 samples (32 per mini-batch)
2023-06-16 03:19:22,798 - Epoch: [123][   10/   45]    Loss 0.811320    Top1 76.562500    
2023-06-16 03:19:27,208 - Epoch: [123][   20/   45]    Loss 0.874430    Top1 73.906250    
2023-06-16 03:19:32,009 - Epoch: [123][   30/   45]    Loss 0.856864    Top1 74.479167    
2023-06-16 03:19:36,090 - Epoch: [123][   40/   45]    Loss 0.880019    Top1 75.000000    
2023-06-16 03:19:37,384 - Epoch: [123][   45/   45]    Loss 0.909649    Top1 74.402250    
2023-06-16 03:19:38,041 - ==> Top1: 74.402    Loss: 0.910

2023-06-16 03:19:38,043 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 122]
2023-06-16 03:19:38,044 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:19:38,057 - 

2023-06-16 03:19:38,057 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:19:47,128 - Epoch: [124][   10/  142]    Overall Loss 0.361556    Objective Loss 0.361556                                        LR 0.000016    Time 0.906923    
2023-06-16 03:19:52,194 - Epoch: [124][   20/  142]    Overall Loss 0.447907    Objective Loss 0.447907                                        LR 0.000016    Time 0.706706    
2023-06-16 03:19:57,053 - Epoch: [124][   30/  142]    Overall Loss 0.462356    Objective Loss 0.462356                                        LR 0.000016    Time 0.633102    
2023-06-16 03:20:02,096 - Epoch: [124][   40/  142]    Overall Loss 0.454067    Objective Loss 0.454067                                        LR 0.000016    Time 0.600883    
2023-06-16 03:20:07,068 - Epoch: [124][   50/  142]    Overall Loss 0.469947    Objective Loss 0.469947                                        LR 0.000016    Time 0.580142    
2023-06-16 03:20:12,007 - Epoch: [124][   60/  142]    Overall Loss 0.484461    Objective Loss 0.484461                                        LR 0.000016    Time 0.565752    
2023-06-16 03:20:16,953 - Epoch: [124][   70/  142]    Overall Loss 0.470741    Objective Loss 0.470741                                        LR 0.000016    Time 0.555588    
2023-06-16 03:20:21,808 - Epoch: [124][   80/  142]    Overall Loss 0.470808    Objective Loss 0.470808                                        LR 0.000016    Time 0.546818    
2023-06-16 03:20:26,813 - Epoch: [124][   90/  142]    Overall Loss 0.458448    Objective Loss 0.458448                                        LR 0.000016    Time 0.541658    
2023-06-16 03:20:31,780 - Epoch: [124][  100/  142]    Overall Loss 0.455192    Objective Loss 0.455192                                        LR 0.000016    Time 0.537156    
2023-06-16 03:20:36,817 - Epoch: [124][  110/  142]    Overall Loss 0.463074    Objective Loss 0.463074                                        LR 0.000016    Time 0.533643    
2023-06-16 03:20:41,608 - Epoch: [124][  120/  142]    Overall Loss 0.458574    Objective Loss 0.458574                                        LR 0.000016    Time 0.529098    
2023-06-16 03:20:46,593 - Epoch: [124][  130/  142]    Overall Loss 0.464539    Objective Loss 0.464539                                        LR 0.000016    Time 0.526738    
2023-06-16 03:20:51,224 - Epoch: [124][  140/  142]    Overall Loss 0.470673    Objective Loss 0.470673                                        LR 0.000016    Time 0.522189    
2023-06-16 03:20:52,071 - Epoch: [124][  142/  142]    Overall Loss 0.471783    Objective Loss 0.471783    Top1 73.437500    LR 0.000016    Time 0.520795    
2023-06-16 03:20:52,703 - --- validate (epoch=124)-----------
2023-06-16 03:20:52,703 - 1422 samples (32 per mini-batch)
2023-06-16 03:21:00,279 - Epoch: [124][   10/   45]    Loss 1.088394    Top1 66.875000    
2023-06-16 03:21:04,970 - Epoch: [124][   20/   45]    Loss 0.975769    Top1 71.406250    
2023-06-16 03:21:09,637 - Epoch: [124][   30/   45]    Loss 0.892386    Top1 73.437500    
2023-06-16 03:21:14,175 - Epoch: [124][   40/   45]    Loss 0.887158    Top1 74.062500    
2023-06-16 03:21:15,488 - Epoch: [124][   45/   45]    Loss 0.878181    Top1 73.980309    
2023-06-16 03:21:16,141 - ==> Top1: 73.980    Loss: 0.878

2023-06-16 03:21:16,143 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 122]
2023-06-16 03:21:16,143 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:21:16,164 - 

2023-06-16 03:21:16,165 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:21:25,265 - Epoch: [125][   10/  142]    Overall Loss 0.462663    Objective Loss 0.462663                                        LR 0.000016    Time 0.909958    
2023-06-16 03:21:30,216 - Epoch: [125][   20/  142]    Overall Loss 0.454064    Objective Loss 0.454064                                        LR 0.000016    Time 0.702456    
2023-06-16 03:21:35,081 - Epoch: [125][   30/  142]    Overall Loss 0.447059    Objective Loss 0.447059                                        LR 0.000016    Time 0.630455    
2023-06-16 03:21:39,970 - Epoch: [125][   40/  142]    Overall Loss 0.453612    Objective Loss 0.453612                                        LR 0.000016    Time 0.595060    
2023-06-16 03:21:44,818 - Epoch: [125][   50/  142]    Overall Loss 0.439732    Objective Loss 0.439732                                        LR 0.000016    Time 0.572996    
2023-06-16 03:21:49,771 - Epoch: [125][   60/  142]    Overall Loss 0.444267    Objective Loss 0.444267                                        LR 0.000016    Time 0.560040    
2023-06-16 03:21:54,705 - Epoch: [125][   70/  142]    Overall Loss 0.434494    Objective Loss 0.434494                                        LR 0.000016    Time 0.550503    
2023-06-16 03:21:59,470 - Epoch: [125][   80/  142]    Overall Loss 0.444580    Objective Loss 0.444580                                        LR 0.000016    Time 0.541249    
2023-06-16 03:22:04,431 - Epoch: [125][   90/  142]    Overall Loss 0.449207    Objective Loss 0.449207                                        LR 0.000016    Time 0.536223    
2023-06-16 03:22:09,369 - Epoch: [125][  100/  142]    Overall Loss 0.446120    Objective Loss 0.446120                                        LR 0.000016    Time 0.531982    
2023-06-16 03:22:14,094 - Epoch: [125][  110/  142]    Overall Loss 0.455944    Objective Loss 0.455944                                        LR 0.000016    Time 0.526564    
2023-06-16 03:22:19,065 - Epoch: [125][  120/  142]    Overall Loss 0.448198    Objective Loss 0.448198                                        LR 0.000016    Time 0.524105    
2023-06-16 03:22:23,904 - Epoch: [125][  130/  142]    Overall Loss 0.453488    Objective Loss 0.453488                                        LR 0.000016    Time 0.521009    
2023-06-16 03:22:28,514 - Epoch: [125][  140/  142]    Overall Loss 0.454276    Objective Loss 0.454276                                        LR 0.000016    Time 0.516720    
2023-06-16 03:22:29,359 - Epoch: [125][  142/  142]    Overall Loss 0.455550    Objective Loss 0.455550    Top1 81.250000    LR 0.000016    Time 0.515396    
2023-06-16 03:22:29,979 - --- validate (epoch=125)-----------
2023-06-16 03:22:29,979 - 1422 samples (32 per mini-batch)
2023-06-16 03:22:38,142 - Epoch: [125][   10/   45]    Loss 0.951593    Top1 74.062500    
2023-06-16 03:22:42,155 - Epoch: [125][   20/   45]    Loss 0.904698    Top1 74.218750    
2023-06-16 03:22:46,153 - Epoch: [125][   30/   45]    Loss 0.934080    Top1 73.958333    
2023-06-16 03:22:50,690 - Epoch: [125][   40/   45]    Loss 0.953040    Top1 73.593750    
2023-06-16 03:22:52,037 - Epoch: [125][   45/   45]    Loss 0.941775    Top1 74.050633    
2023-06-16 03:22:52,696 - ==> Top1: 74.051    Loss: 0.942

2023-06-16 03:22:52,698 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 122]
2023-06-16 03:22:52,698 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:22:52,719 - 

2023-06-16 03:22:52,719 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:23:02,017 - Epoch: [126][   10/  142]    Overall Loss 0.398575    Objective Loss 0.398575                                        LR 0.000016    Time 0.929697    
2023-06-16 03:23:06,899 - Epoch: [126][   20/  142]    Overall Loss 0.393868    Objective Loss 0.393868                                        LR 0.000016    Time 0.708915    
2023-06-16 03:23:11,788 - Epoch: [126][   30/  142]    Overall Loss 0.400876    Objective Loss 0.400876                                        LR 0.000016    Time 0.635561    
2023-06-16 03:23:16,670 - Epoch: [126][   40/  142]    Overall Loss 0.418496    Objective Loss 0.418496                                        LR 0.000016    Time 0.598710    
2023-06-16 03:23:21,584 - Epoch: [126][   50/  142]    Overall Loss 0.434069    Objective Loss 0.434069                                        LR 0.000016    Time 0.577232    
2023-06-16 03:23:26,533 - Epoch: [126][   60/  142]    Overall Loss 0.455260    Objective Loss 0.455260                                        LR 0.000016    Time 0.563508    
2023-06-16 03:23:31,514 - Epoch: [126][   70/  142]    Overall Loss 0.463526    Objective Loss 0.463526                                        LR 0.000016    Time 0.554155    
2023-06-16 03:23:36,342 - Epoch: [126][   80/  142]    Overall Loss 0.467730    Objective Loss 0.467730                                        LR 0.000016    Time 0.545223    
2023-06-16 03:23:41,222 - Epoch: [126][   90/  142]    Overall Loss 0.463905    Objective Loss 0.463905                                        LR 0.000016    Time 0.538858    
2023-06-16 03:23:46,104 - Epoch: [126][  100/  142]    Overall Loss 0.467612    Objective Loss 0.467612                                        LR 0.000016    Time 0.533789    
2023-06-16 03:23:50,981 - Epoch: [126][  110/  142]    Overall Loss 0.461152    Objective Loss 0.461152                                        LR 0.000016    Time 0.529595    
2023-06-16 03:23:55,968 - Epoch: [126][  120/  142]    Overall Loss 0.465547    Objective Loss 0.465547                                        LR 0.000016    Time 0.527015    
2023-06-16 03:24:00,859 - Epoch: [126][  130/  142]    Overall Loss 0.466953    Objective Loss 0.466953                                        LR 0.000016    Time 0.524087    
2023-06-16 03:24:05,464 - Epoch: [126][  140/  142]    Overall Loss 0.464764    Objective Loss 0.464764                                        LR 0.000016    Time 0.519541    
2023-06-16 03:24:06,318 - Epoch: [126][  142/  142]    Overall Loss 0.464695    Objective Loss 0.464695    Top1 84.375000    LR 0.000016    Time 0.518242    
2023-06-16 03:24:06,959 - --- validate (epoch=126)-----------
2023-06-16 03:24:06,960 - 1422 samples (32 per mini-batch)
2023-06-16 03:24:14,824 - Epoch: [126][   10/   45]    Loss 0.880757    Top1 75.625000    
2023-06-16 03:24:18,893 - Epoch: [126][   20/   45]    Loss 0.883303    Top1 75.312500    
2023-06-16 03:24:23,740 - Epoch: [126][   30/   45]    Loss 0.896059    Top1 75.312500    
2023-06-16 03:24:28,185 - Epoch: [126][   40/   45]    Loss 0.920992    Top1 74.453125    
2023-06-16 03:24:29,515 - Epoch: [126][   45/   45]    Loss 0.915073    Top1 74.191280    
2023-06-16 03:24:30,175 - ==> Top1: 74.191    Loss: 0.915

2023-06-16 03:24:30,177 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 122]
2023-06-16 03:24:30,178 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:24:30,199 - 

2023-06-16 03:24:30,199 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:24:39,551 - Epoch: [127][   10/  142]    Overall Loss 0.515521    Objective Loss 0.515521                                        LR 0.000016    Time 0.935076    
2023-06-16 03:24:44,715 - Epoch: [127][   20/  142]    Overall Loss 0.484828    Objective Loss 0.484828                                        LR 0.000016    Time 0.725729    
2023-06-16 03:24:49,679 - Epoch: [127][   30/  142]    Overall Loss 0.471516    Objective Loss 0.471516                                        LR 0.000016    Time 0.649258    
2023-06-16 03:24:54,529 - Epoch: [127][   40/  142]    Overall Loss 0.474058    Objective Loss 0.474058                                        LR 0.000016    Time 0.608174    
2023-06-16 03:24:59,498 - Epoch: [127][   50/  142]    Overall Loss 0.466623    Objective Loss 0.466623                                        LR 0.000016    Time 0.585914    
2023-06-16 03:25:04,435 - Epoch: [127][   60/  142]    Overall Loss 0.459626    Objective Loss 0.459626                                        LR 0.000016    Time 0.570535    
2023-06-16 03:25:09,449 - Epoch: [127][   70/  142]    Overall Loss 0.454285    Objective Loss 0.454285                                        LR 0.000016    Time 0.560650    
2023-06-16 03:25:14,438 - Epoch: [127][   80/  142]    Overall Loss 0.455000    Objective Loss 0.455000                                        LR 0.000016    Time 0.552927    
2023-06-16 03:25:19,471 - Epoch: [127][   90/  142]    Overall Loss 0.459971    Objective Loss 0.459971                                        LR 0.000016    Time 0.547408    
2023-06-16 03:25:24,394 - Epoch: [127][  100/  142]    Overall Loss 0.459587    Objective Loss 0.459587                                        LR 0.000016    Time 0.541893    
2023-06-16 03:25:29,423 - Epoch: [127][  110/  142]    Overall Loss 0.456880    Objective Loss 0.456880                                        LR 0.000016    Time 0.538335    
2023-06-16 03:25:34,334 - Epoch: [127][  120/  142]    Overall Loss 0.455719    Objective Loss 0.455719                                        LR 0.000016    Time 0.534395    
2023-06-16 03:25:39,242 - Epoch: [127][  130/  142]    Overall Loss 0.457425    Objective Loss 0.457425                                        LR 0.000016    Time 0.531039    
2023-06-16 03:25:43,852 - Epoch: [127][  140/  142]    Overall Loss 0.464168    Objective Loss 0.464168                                        LR 0.000016    Time 0.526035    
2023-06-16 03:25:44,711 - Epoch: [127][  142/  142]    Overall Loss 0.467049    Objective Loss 0.467049    Top1 81.250000    LR 0.000016    Time 0.524670    
2023-06-16 03:25:45,339 - --- validate (epoch=127)-----------
2023-06-16 03:25:45,340 - 1422 samples (32 per mini-batch)
2023-06-16 03:25:53,527 - Epoch: [127][   10/   45]    Loss 0.934762    Top1 74.062500    
2023-06-16 03:25:57,507 - Epoch: [127][   20/   45]    Loss 0.919638    Top1 72.187500    
2023-06-16 03:26:02,290 - Epoch: [127][   30/   45]    Loss 0.919053    Top1 73.125000    
2023-06-16 03:26:06,310 - Epoch: [127][   40/   45]    Loss 0.873564    Top1 73.984375    
2023-06-16 03:26:08,146 - Epoch: [127][   45/   45]    Loss 0.864423    Top1 74.120956    
2023-06-16 03:26:08,749 - ==> Top1: 74.121    Loss: 0.864

2023-06-16 03:26:08,751 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 122]
2023-06-16 03:26:08,751 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:26:08,772 - 

2023-06-16 03:26:08,772 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:26:18,232 - Epoch: [128][   10/  142]    Overall Loss 0.509919    Objective Loss 0.509919                                        LR 0.000016    Time 0.945823    
2023-06-16 03:26:23,118 - Epoch: [128][   20/  142]    Overall Loss 0.528303    Objective Loss 0.528303                                        LR 0.000016    Time 0.717224    
2023-06-16 03:26:28,106 - Epoch: [128][   30/  142]    Overall Loss 0.501654    Objective Loss 0.501654                                        LR 0.000016    Time 0.644384    
2023-06-16 03:26:33,287 - Epoch: [128][   40/  142]    Overall Loss 0.506241    Objective Loss 0.506241                                        LR 0.000016    Time 0.612788    
2023-06-16 03:26:38,257 - Epoch: [128][   50/  142]    Overall Loss 0.511531    Objective Loss 0.511531                                        LR 0.000016    Time 0.589621    
2023-06-16 03:26:43,281 - Epoch: [128][   60/  142]    Overall Loss 0.500976    Objective Loss 0.500976                                        LR 0.000016    Time 0.575080    
2023-06-16 03:26:48,356 - Epoch: [128][   70/  142]    Overall Loss 0.504824    Objective Loss 0.504824                                        LR 0.000016    Time 0.565419    
2023-06-16 03:26:53,504 - Epoch: [128][   80/  142]    Overall Loss 0.497834    Objective Loss 0.497834                                        LR 0.000016    Time 0.559071    
2023-06-16 03:26:58,442 - Epoch: [128][   90/  142]    Overall Loss 0.490787    Objective Loss 0.490787                                        LR 0.000016    Time 0.551814    
2023-06-16 03:27:03,394 - Epoch: [128][  100/  142]    Overall Loss 0.481736    Objective Loss 0.481736                                        LR 0.000016    Time 0.546150    
2023-06-16 03:27:08,327 - Epoch: [128][  110/  142]    Overall Loss 0.480069    Objective Loss 0.480069                                        LR 0.000016    Time 0.540866    
2023-06-16 03:27:13,479 - Epoch: [128][  120/  142]    Overall Loss 0.475689    Objective Loss 0.475689                                        LR 0.000016    Time 0.538720    
2023-06-16 03:27:18,448 - Epoch: [128][  130/  142]    Overall Loss 0.475499    Objective Loss 0.475499                                        LR 0.000016    Time 0.535496    
2023-06-16 03:27:23,064 - Epoch: [128][  140/  142]    Overall Loss 0.476999    Objective Loss 0.476999                                        LR 0.000016    Time 0.530219    
2023-06-16 03:27:23,919 - Epoch: [128][  142/  142]    Overall Loss 0.477864    Objective Loss 0.477864    Top1 76.562500    LR 0.000016    Time 0.528773    
2023-06-16 03:27:24,571 - --- validate (epoch=128)-----------
2023-06-16 03:27:24,571 - 1422 samples (32 per mini-batch)
2023-06-16 03:27:32,555 - Epoch: [128][   10/   45]    Loss 0.838695    Top1 73.437500    
2023-06-16 03:27:37,096 - Epoch: [128][   20/   45]    Loss 0.866894    Top1 73.750000    
2023-06-16 03:27:41,864 - Epoch: [128][   30/   45]    Loss 0.856161    Top1 74.687500    
2023-06-16 03:27:46,158 - Epoch: [128][   40/   45]    Loss 0.899706    Top1 73.671875    
2023-06-16 03:27:47,740 - Epoch: [128][   45/   45]    Loss 0.871622    Top1 74.331927    
2023-06-16 03:27:48,389 - ==> Top1: 74.332    Loss: 0.872

2023-06-16 03:27:48,391 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 122]
2023-06-16 03:27:48,391 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:27:48,412 - 

2023-06-16 03:27:48,412 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:27:57,869 - Epoch: [129][   10/  142]    Overall Loss 0.561861    Objective Loss 0.561861                                        LR 0.000016    Time 0.945527    
2023-06-16 03:28:02,775 - Epoch: [129][   20/  142]    Overall Loss 0.517997    Objective Loss 0.517997                                        LR 0.000016    Time 0.718068    
2023-06-16 03:28:07,729 - Epoch: [129][   30/  142]    Overall Loss 0.493405    Objective Loss 0.493405                                        LR 0.000016    Time 0.643819    
2023-06-16 03:28:12,690 - Epoch: [129][   40/  142]    Overall Loss 0.487579    Objective Loss 0.487579                                        LR 0.000016    Time 0.606875    
2023-06-16 03:28:17,662 - Epoch: [129][   50/  142]    Overall Loss 0.497645    Objective Loss 0.497645                                        LR 0.000016    Time 0.584912    
2023-06-16 03:28:22,596 - Epoch: [129][   60/  142]    Overall Loss 0.477759    Objective Loss 0.477759                                        LR 0.000016    Time 0.569653    
2023-06-16 03:28:27,656 - Epoch: [129][   70/  142]    Overall Loss 0.468542    Objective Loss 0.468542                                        LR 0.000016    Time 0.560555    
2023-06-16 03:28:32,636 - Epoch: [129][   80/  142]    Overall Loss 0.465664    Objective Loss 0.465664                                        LR 0.000016    Time 0.552730    
2023-06-16 03:28:37,585 - Epoch: [129][   90/  142]    Overall Loss 0.461034    Objective Loss 0.461034                                        LR 0.000016    Time 0.546295    
2023-06-16 03:28:42,595 - Epoch: [129][  100/  142]    Overall Loss 0.456531    Objective Loss 0.456531                                        LR 0.000016    Time 0.541761    
2023-06-16 03:28:47,492 - Epoch: [129][  110/  142]    Overall Loss 0.459051    Objective Loss 0.459051                                        LR 0.000016    Time 0.537025    
2023-06-16 03:28:52,451 - Epoch: [129][  120/  142]    Overall Loss 0.461637    Objective Loss 0.461637                                        LR 0.000016    Time 0.533600    
2023-06-16 03:28:57,441 - Epoch: [129][  130/  142]    Overall Loss 0.465437    Objective Loss 0.465437                                        LR 0.000016    Time 0.530933    
2023-06-16 03:29:01,999 - Epoch: [129][  140/  142]    Overall Loss 0.465870    Objective Loss 0.465870                                        LR 0.000016    Time 0.525557    
2023-06-16 03:29:02,852 - Epoch: [129][  142/  142]    Overall Loss 0.466363    Objective Loss 0.466363    Top1 79.687500    LR 0.000016    Time 0.524165    
2023-06-16 03:29:03,482 - --- validate (epoch=129)-----------
2023-06-16 03:29:03,482 - 1422 samples (32 per mini-batch)
2023-06-16 03:29:11,566 - Epoch: [129][   10/   45]    Loss 0.834316    Top1 73.437500    
2023-06-16 03:29:15,616 - Epoch: [129][   20/   45]    Loss 0.908822    Top1 72.812500    
2023-06-16 03:29:20,450 - Epoch: [129][   30/   45]    Loss 0.846684    Top1 74.895833    
2023-06-16 03:29:24,498 - Epoch: [129][   40/   45]    Loss 0.859463    Top1 74.453125    
2023-06-16 03:29:25,987 - Epoch: [129][   45/   45]    Loss 0.869208    Top1 74.472574    
2023-06-16 03:29:26,622 - ==> Top1: 74.473    Loss: 0.869

2023-06-16 03:29:26,624 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 122]
2023-06-16 03:29:26,624 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:29:26,645 - 

2023-06-16 03:29:26,645 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:29:36,006 - Epoch: [130][   10/  142]    Overall Loss 0.514882    Objective Loss 0.514882                                        LR 0.000016    Time 0.936017    
2023-06-16 03:29:40,909 - Epoch: [130][   20/  142]    Overall Loss 0.464350    Objective Loss 0.464350                                        LR 0.000016    Time 0.713084    
2023-06-16 03:29:45,953 - Epoch: [130][   30/  142]    Overall Loss 0.464857    Objective Loss 0.464857                                        LR 0.000016    Time 0.643529    
2023-06-16 03:29:50,844 - Epoch: [130][   40/  142]    Overall Loss 0.466012    Objective Loss 0.466012                                        LR 0.000016    Time 0.604908    
2023-06-16 03:29:55,853 - Epoch: [130][   50/  142]    Overall Loss 0.482939    Objective Loss 0.482939                                        LR 0.000016    Time 0.584077    
2023-06-16 03:30:00,854 - Epoch: [130][   60/  142]    Overall Loss 0.476227    Objective Loss 0.476227                                        LR 0.000016    Time 0.570079    
2023-06-16 03:30:05,823 - Epoch: [130][   70/  142]    Overall Loss 0.476313    Objective Loss 0.476313                                        LR 0.000016    Time 0.559618    
2023-06-16 03:30:10,750 - Epoch: [130][   80/  142]    Overall Loss 0.474268    Objective Loss 0.474268                                        LR 0.000016    Time 0.551242    
2023-06-16 03:30:15,728 - Epoch: [130][   90/  142]    Overall Loss 0.476431    Objective Loss 0.476431                                        LR 0.000016    Time 0.545297    
2023-06-16 03:30:20,609 - Epoch: [130][  100/  142]    Overall Loss 0.474703    Objective Loss 0.474703                                        LR 0.000016    Time 0.539569    
2023-06-16 03:30:25,575 - Epoch: [130][  110/  142]    Overall Loss 0.476379    Objective Loss 0.476379                                        LR 0.000016    Time 0.535657    
2023-06-16 03:30:30,479 - Epoch: [130][  120/  142]    Overall Loss 0.481716    Objective Loss 0.481716                                        LR 0.000016    Time 0.531880    
2023-06-16 03:30:35,434 - Epoch: [130][  130/  142]    Overall Loss 0.483682    Objective Loss 0.483682                                        LR 0.000016    Time 0.529082    
2023-06-16 03:30:40,071 - Epoch: [130][  140/  142]    Overall Loss 0.481273    Objective Loss 0.481273                                        LR 0.000016    Time 0.524403    
2023-06-16 03:30:40,925 - Epoch: [130][  142/  142]    Overall Loss 0.480747    Objective Loss 0.480747    Top1 84.375000    LR 0.000016    Time 0.523034    
2023-06-16 03:30:41,571 - --- validate (epoch=130)-----------
2023-06-16 03:30:41,572 - 1422 samples (32 per mini-batch)
2023-06-16 03:30:49,492 - Epoch: [130][   10/   45]    Loss 0.805140    Top1 76.562500    
2023-06-16 03:30:53,706 - Epoch: [130][   20/   45]    Loss 0.753561    Top1 78.125000    
2023-06-16 03:30:58,903 - Epoch: [130][   30/   45]    Loss 0.827358    Top1 76.041667    
2023-06-16 03:31:02,918 - Epoch: [130][   40/   45]    Loss 0.842331    Top1 75.312500    
2023-06-16 03:31:04,320 - Epoch: [130][   45/   45]    Loss 0.857303    Top1 74.753868    
2023-06-16 03:31:04,909 - ==> Top1: 74.754    Loss: 0.857

2023-06-16 03:31:04,911 - ==> Best [Top1: 74.895   Sparsity:0.00   Params: 375264 on epoch: 122]
2023-06-16 03:31:04,911 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:31:04,932 - 

2023-06-16 03:31:04,932 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:31:14,168 - Epoch: [131][   10/  142]    Overall Loss 0.446808    Objective Loss 0.446808                                        LR 0.000016    Time 0.923402    
2023-06-16 03:31:19,022 - Epoch: [131][   20/  142]    Overall Loss 0.456702    Objective Loss 0.456702                                        LR 0.000016    Time 0.704383    
2023-06-16 03:31:23,947 - Epoch: [131][   30/  142]    Overall Loss 0.467472    Objective Loss 0.467472                                        LR 0.000016    Time 0.633750    
2023-06-16 03:31:28,983 - Epoch: [131][   40/  142]    Overall Loss 0.475238    Objective Loss 0.475238                                        LR 0.000016    Time 0.601181    
2023-06-16 03:31:33,948 - Epoch: [131][   50/  142]    Overall Loss 0.485233    Objective Loss 0.485233                                        LR 0.000016    Time 0.580247    
2023-06-16 03:31:38,845 - Epoch: [131][   60/  142]    Overall Loss 0.485196    Objective Loss 0.485196                                        LR 0.000016    Time 0.565148    
2023-06-16 03:31:43,733 - Epoch: [131][   70/  142]    Overall Loss 0.477396    Objective Loss 0.477396                                        LR 0.000016    Time 0.554234    
2023-06-16 03:31:48,613 - Epoch: [131][   80/  142]    Overall Loss 0.471931    Objective Loss 0.471931                                        LR 0.000016    Time 0.545949    
2023-06-16 03:31:53,608 - Epoch: [131][   90/  142]    Overall Loss 0.468944    Objective Loss 0.468944                                        LR 0.000016    Time 0.540780    
2023-06-16 03:31:58,636 - Epoch: [131][  100/  142]    Overall Loss 0.471877    Objective Loss 0.471877                                        LR 0.000016    Time 0.536976    
2023-06-16 03:32:03,569 - Epoch: [131][  110/  142]    Overall Loss 0.475334    Objective Loss 0.475334                                        LR 0.000016    Time 0.532996    
2023-06-16 03:32:08,546 - Epoch: [131][  120/  142]    Overall Loss 0.473395    Objective Loss 0.473395                                        LR 0.000016    Time 0.530052    
2023-06-16 03:32:13,422 - Epoch: [131][  130/  142]    Overall Loss 0.472537    Objective Loss 0.472537                                        LR 0.000016    Time 0.526789    
2023-06-16 03:32:18,037 - Epoch: [131][  140/  142]    Overall Loss 0.471591    Objective Loss 0.471591                                        LR 0.000016    Time 0.522122    
2023-06-16 03:32:18,880 - Epoch: [131][  142/  142]    Overall Loss 0.469275    Objective Loss 0.469275    Top1 87.500000    LR 0.000016    Time 0.520703    
2023-06-16 03:32:19,505 - --- validate (epoch=131)-----------
2023-06-16 03:32:19,505 - 1422 samples (32 per mini-batch)
2023-06-16 03:32:27,812 - Epoch: [131][   10/   45]    Loss 0.958358    Top1 71.562500    
2023-06-16 03:32:31,844 - Epoch: [131][   20/   45]    Loss 0.989922    Top1 72.031250    
2023-06-16 03:32:36,662 - Epoch: [131][   30/   45]    Loss 0.954509    Top1 73.229167    
2023-06-16 03:32:40,710 - Epoch: [131][   40/   45]    Loss 0.904493    Top1 74.609375    
2023-06-16 03:32:42,146 - Epoch: [131][   45/   45]    Loss 0.877788    Top1 75.105485    
2023-06-16 03:32:42,799 - ==> Top1: 75.105    Loss: 0.878

2023-06-16 03:32:42,801 - ==> Best [Top1: 75.105   Sparsity:0.00   Params: 375264 on epoch: 131]
2023-06-16 03:32:42,801 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:32:42,826 - 

2023-06-16 03:32:42,826 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:32:52,414 - Epoch: [132][   10/  142]    Overall Loss 0.427001    Objective Loss 0.427001                                        LR 0.000016    Time 0.958726    
2023-06-16 03:32:57,385 - Epoch: [132][   20/  142]    Overall Loss 0.438511    Objective Loss 0.438511                                        LR 0.000016    Time 0.727891    
2023-06-16 03:33:02,347 - Epoch: [132][   30/  142]    Overall Loss 0.479862    Objective Loss 0.479862                                        LR 0.000016    Time 0.650629    
2023-06-16 03:33:07,339 - Epoch: [132][   40/  142]    Overall Loss 0.485989    Objective Loss 0.485989                                        LR 0.000016    Time 0.612767    
2023-06-16 03:33:12,417 - Epoch: [132][   50/  142]    Overall Loss 0.483507    Objective Loss 0.483507                                        LR 0.000016    Time 0.591744    
2023-06-16 03:33:17,506 - Epoch: [132][   60/  142]    Overall Loss 0.484322    Objective Loss 0.484322                                        LR 0.000016    Time 0.577942    
2023-06-16 03:33:22,504 - Epoch: [132][   70/  142]    Overall Loss 0.474615    Objective Loss 0.474615                                        LR 0.000016    Time 0.566768    
2023-06-16 03:33:27,551 - Epoch: [132][   80/  142]    Overall Loss 0.472336    Objective Loss 0.472336                                        LR 0.000016    Time 0.558994    
2023-06-16 03:33:32,637 - Epoch: [132][   90/  142]    Overall Loss 0.470930    Objective Loss 0.470930                                        LR 0.000016    Time 0.553394    
2023-06-16 03:33:37,712 - Epoch: [132][  100/  142]    Overall Loss 0.472675    Objective Loss 0.472675                                        LR 0.000016    Time 0.548796    
2023-06-16 03:33:42,564 - Epoch: [132][  110/  142]    Overall Loss 0.476674    Objective Loss 0.476674                                        LR 0.000016    Time 0.543009    
2023-06-16 03:33:47,605 - Epoch: [132][  120/  142]    Overall Loss 0.476075    Objective Loss 0.476075                                        LR 0.000016    Time 0.539762    
2023-06-16 03:33:52,649 - Epoch: [132][  130/  142]    Overall Loss 0.479272    Objective Loss 0.479272                                        LR 0.000016    Time 0.537038    
2023-06-16 03:33:57,207 - Epoch: [132][  140/  142]    Overall Loss 0.473841    Objective Loss 0.473841                                        LR 0.000016    Time 0.531236    
2023-06-16 03:33:58,048 - Epoch: [132][  142/  142]    Overall Loss 0.473739    Objective Loss 0.473739    Top1 87.500000    LR 0.000016    Time 0.529673    
2023-06-16 03:33:58,693 - --- validate (epoch=132)-----------
2023-06-16 03:33:58,694 - 1422 samples (32 per mini-batch)
2023-06-16 03:34:06,977 - Epoch: [132][   10/   45]    Loss 0.873489    Top1 74.375000    
2023-06-16 03:34:11,111 - Epoch: [132][   20/   45]    Loss 0.891008    Top1 75.468750    
2023-06-16 03:34:15,123 - Epoch: [132][   30/   45]    Loss 0.893499    Top1 74.687500    
2023-06-16 03:34:19,111 - Epoch: [132][   40/   45]    Loss 0.859745    Top1 75.000000    
2023-06-16 03:34:20,827 - Epoch: [132][   45/   45]    Loss 0.867497    Top1 74.894515    
2023-06-16 03:34:21,487 - ==> Top1: 74.895    Loss: 0.867

2023-06-16 03:34:21,489 - ==> Best [Top1: 75.105   Sparsity:0.00   Params: 375264 on epoch: 131]
2023-06-16 03:34:21,489 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:34:21,502 - 

2023-06-16 03:34:21,503 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:34:30,771 - Epoch: [133][   10/  142]    Overall Loss 0.458539    Objective Loss 0.458539                                        LR 0.000016    Time 0.926739    
2023-06-16 03:34:35,739 - Epoch: [133][   20/  142]    Overall Loss 0.455863    Objective Loss 0.455863                                        LR 0.000016    Time 0.711754    
2023-06-16 03:34:40,689 - Epoch: [133][   30/  142]    Overall Loss 0.467773    Objective Loss 0.467773                                        LR 0.000016    Time 0.639466    
2023-06-16 03:34:45,624 - Epoch: [133][   40/  142]    Overall Loss 0.466066    Objective Loss 0.466066                                        LR 0.000016    Time 0.602953    
2023-06-16 03:34:50,645 - Epoch: [133][   50/  142]    Overall Loss 0.483006    Objective Loss 0.483006                                        LR 0.000016    Time 0.582783    
2023-06-16 03:34:55,601 - Epoch: [133][   60/  142]    Overall Loss 0.493732    Objective Loss 0.493732                                        LR 0.000016    Time 0.568241    
2023-06-16 03:35:00,592 - Epoch: [133][   70/  142]    Overall Loss 0.481621    Objective Loss 0.481621                                        LR 0.000016    Time 0.558352    
2023-06-16 03:35:05,648 - Epoch: [133][   80/  142]    Overall Loss 0.478177    Objective Loss 0.478177                                        LR 0.000016    Time 0.551754    
2023-06-16 03:35:10,717 - Epoch: [133][   90/  142]    Overall Loss 0.478091    Objective Loss 0.478091                                        LR 0.000016    Time 0.546768    
2023-06-16 03:35:15,652 - Epoch: [133][  100/  142]    Overall Loss 0.465140    Objective Loss 0.465140                                        LR 0.000016    Time 0.541433    
2023-06-16 03:35:20,683 - Epoch: [133][  110/  142]    Overall Loss 0.465483    Objective Loss 0.465483                                        LR 0.000016    Time 0.537946    
2023-06-16 03:35:25,628 - Epoch: [133][  120/  142]    Overall Loss 0.469734    Objective Loss 0.469734                                        LR 0.000016    Time 0.534316    
2023-06-16 03:35:30,687 - Epoch: [133][  130/  142]    Overall Loss 0.471976    Objective Loss 0.471976                                        LR 0.000016    Time 0.532129    
2023-06-16 03:35:35,332 - Epoch: [133][  140/  142]    Overall Loss 0.469376    Objective Loss 0.469376                                        LR 0.000016    Time 0.527295    
2023-06-16 03:35:36,190 - Epoch: [133][  142/  142]    Overall Loss 0.470466    Objective Loss 0.470466    Top1 73.437500    LR 0.000016    Time 0.525907    
2023-06-16 03:35:36,783 - --- validate (epoch=133)-----------
2023-06-16 03:35:36,783 - 1422 samples (32 per mini-batch)
2023-06-16 03:35:45,280 - Epoch: [133][   10/   45]    Loss 0.815327    Top1 77.187500    
2023-06-16 03:35:48,996 - Epoch: [133][   20/   45]    Loss 0.858702    Top1 76.093750    
2023-06-16 03:35:52,987 - Epoch: [133][   30/   45]    Loss 0.861907    Top1 75.937500    
2023-06-16 03:35:57,633 - Epoch: [133][   40/   45]    Loss 0.880841    Top1 75.156250    
2023-06-16 03:35:59,052 - Epoch: [133][   45/   45]    Loss 0.898456    Top1 74.753868    
2023-06-16 03:35:59,712 - ==> Top1: 74.754    Loss: 0.898

2023-06-16 03:35:59,715 - ==> Best [Top1: 75.105   Sparsity:0.00   Params: 375264 on epoch: 131]
2023-06-16 03:35:59,715 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:35:59,736 - 

2023-06-16 03:35:59,736 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:36:09,110 - Epoch: [134][   10/  142]    Overall Loss 0.504526    Objective Loss 0.504526                                        LR 0.000016    Time 0.937274    
2023-06-16 03:36:14,092 - Epoch: [134][   20/  142]    Overall Loss 0.482684    Objective Loss 0.482684                                        LR 0.000016    Time 0.717690    
2023-06-16 03:36:19,064 - Epoch: [134][   30/  142]    Overall Loss 0.483050    Objective Loss 0.483050                                        LR 0.000016    Time 0.644179    
2023-06-16 03:36:23,988 - Epoch: [134][   40/  142]    Overall Loss 0.483595    Objective Loss 0.483595                                        LR 0.000016    Time 0.606219    
2023-06-16 03:36:29,002 - Epoch: [134][   50/  142]    Overall Loss 0.481097    Objective Loss 0.481097                                        LR 0.000016    Time 0.585252    
2023-06-16 03:36:34,036 - Epoch: [134][   60/  142]    Overall Loss 0.477274    Objective Loss 0.477274                                        LR 0.000016    Time 0.571606    
2023-06-16 03:36:38,964 - Epoch: [134][   70/  142]    Overall Loss 0.476567    Objective Loss 0.476567                                        LR 0.000016    Time 0.560336    
2023-06-16 03:36:43,979 - Epoch: [134][   80/  142]    Overall Loss 0.473846    Objective Loss 0.473846                                        LR 0.000016    Time 0.552980    
2023-06-16 03:36:49,069 - Epoch: [134][   90/  142]    Overall Loss 0.465425    Objective Loss 0.465425                                        LR 0.000016    Time 0.548082    
2023-06-16 03:36:54,032 - Epoch: [134][  100/  142]    Overall Loss 0.464079    Objective Loss 0.464079                                        LR 0.000016    Time 0.542902    
2023-06-16 03:36:59,052 - Epoch: [134][  110/  142]    Overall Loss 0.465785    Objective Loss 0.465785                                        LR 0.000016    Time 0.539179    
2023-06-16 03:37:04,176 - Epoch: [134][  120/  142]    Overall Loss 0.467868    Objective Loss 0.467868                                        LR 0.000016    Time 0.536943    
2023-06-16 03:37:09,116 - Epoch: [134][  130/  142]    Overall Loss 0.465106    Objective Loss 0.465106                                        LR 0.000016    Time 0.533637    
2023-06-16 03:37:13,776 - Epoch: [134][  140/  142]    Overall Loss 0.467290    Objective Loss 0.467290                                        LR 0.000016    Time 0.528803    
2023-06-16 03:37:14,617 - Epoch: [134][  142/  142]    Overall Loss 0.465301    Objective Loss 0.465301    Top1 87.500000    LR 0.000016    Time 0.527277    
2023-06-16 03:37:15,258 - --- validate (epoch=134)-----------
2023-06-16 03:37:15,258 - 1422 samples (32 per mini-batch)
2023-06-16 03:37:22,973 - Epoch: [134][   10/   45]    Loss 1.103226    Top1 73.437500    
2023-06-16 03:37:27,629 - Epoch: [134][   20/   45]    Loss 1.025067    Top1 73.593750    
2023-06-16 03:37:31,813 - Epoch: [134][   30/   45]    Loss 0.965009    Top1 74.895833    
2023-06-16 03:37:36,428 - Epoch: [134][   40/   45]    Loss 0.942225    Top1 75.156250    
2023-06-16 03:37:37,936 - Epoch: [134][   45/   45]    Loss 0.979579    Top1 74.542897    
2023-06-16 03:37:38,582 - ==> Top1: 74.543    Loss: 0.980

2023-06-16 03:37:38,585 - ==> Best [Top1: 75.105   Sparsity:0.00   Params: 375264 on epoch: 131]
2023-06-16 03:37:38,585 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:37:38,606 - 

2023-06-16 03:37:38,606 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:37:48,087 - Epoch: [135][   10/  142]    Overall Loss 0.423027    Objective Loss 0.423027                                        LR 0.000016    Time 0.947945    
2023-06-16 03:37:53,085 - Epoch: [135][   20/  142]    Overall Loss 0.434617    Objective Loss 0.434617                                        LR 0.000016    Time 0.723832    
2023-06-16 03:37:57,930 - Epoch: [135][   30/  142]    Overall Loss 0.453587    Objective Loss 0.453587                                        LR 0.000016    Time 0.644051    
2023-06-16 03:38:02,918 - Epoch: [135][   40/  142]    Overall Loss 0.459041    Objective Loss 0.459041                                        LR 0.000016    Time 0.607723    
2023-06-16 03:38:07,890 - Epoch: [135][   50/  142]    Overall Loss 0.457745    Objective Loss 0.457745                                        LR 0.000016    Time 0.585601    
2023-06-16 03:38:12,853 - Epoch: [135][   60/  142]    Overall Loss 0.473475    Objective Loss 0.473475                                        LR 0.000016    Time 0.570714    
2023-06-16 03:38:17,825 - Epoch: [135][   70/  142]    Overall Loss 0.467863    Objective Loss 0.467863                                        LR 0.000016    Time 0.560204    
2023-06-16 03:38:22,878 - Epoch: [135][   80/  142]    Overall Loss 0.466120    Objective Loss 0.466120                                        LR 0.000016    Time 0.553331    
2023-06-16 03:38:27,806 - Epoch: [135][   90/  142]    Overall Loss 0.458667    Objective Loss 0.458667                                        LR 0.000016    Time 0.546602    
2023-06-16 03:38:32,850 - Epoch: [135][  100/  142]    Overall Loss 0.447094    Objective Loss 0.447094                                        LR 0.000016    Time 0.542378    
2023-06-16 03:38:37,689 - Epoch: [135][  110/  142]    Overall Loss 0.454942    Objective Loss 0.454942                                        LR 0.000016    Time 0.537054    
2023-06-16 03:38:42,680 - Epoch: [135][  120/  142]    Overall Loss 0.450143    Objective Loss 0.450143                                        LR 0.000016    Time 0.533881    
2023-06-16 03:38:47,523 - Epoch: [135][  130/  142]    Overall Loss 0.447220    Objective Loss 0.447220                                        LR 0.000016    Time 0.530062    
2023-06-16 03:38:52,132 - Epoch: [135][  140/  142]    Overall Loss 0.450751    Objective Loss 0.450751                                        LR 0.000016    Time 0.525123    
2023-06-16 03:38:52,975 - Epoch: [135][  142/  142]    Overall Loss 0.449525    Objective Loss 0.449525    Top1 89.062500    LR 0.000016    Time 0.523664    
2023-06-16 03:38:53,601 - --- validate (epoch=135)-----------
2023-06-16 03:38:53,602 - 1422 samples (32 per mini-batch)
2023-06-16 03:39:01,797 - Epoch: [135][   10/   45]    Loss 0.966422    Top1 74.687500    
2023-06-16 03:39:06,092 - Epoch: [135][   20/   45]    Loss 0.889830    Top1 74.843750    
2023-06-16 03:39:10,533 - Epoch: [135][   30/   45]    Loss 0.882559    Top1 75.312500    
2023-06-16 03:39:15,109 - Epoch: [135][   40/   45]    Loss 0.899342    Top1 75.703125    
2023-06-16 03:39:16,470 - Epoch: [135][   45/   45]    Loss 0.915210    Top1 75.457103    
2023-06-16 03:39:17,115 - ==> Top1: 75.457    Loss: 0.915

2023-06-16 03:39:17,117 - ==> Best [Top1: 75.457   Sparsity:0.00   Params: 375264 on epoch: 135]
2023-06-16 03:39:17,117 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:39:17,142 - 

2023-06-16 03:39:17,142 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:39:26,421 - Epoch: [136][   10/  142]    Overall Loss 0.485488    Objective Loss 0.485488                                        LR 0.000016    Time 0.927715    
2023-06-16 03:39:31,354 - Epoch: [136][   20/  142]    Overall Loss 0.439319    Objective Loss 0.439319                                        LR 0.000016    Time 0.710474    
2023-06-16 03:39:36,200 - Epoch: [136][   30/  142]    Overall Loss 0.423388    Objective Loss 0.423388                                        LR 0.000016    Time 0.635162    
2023-06-16 03:39:41,190 - Epoch: [136][   40/  142]    Overall Loss 0.454323    Objective Loss 0.454323                                        LR 0.000016    Time 0.601122    
2023-06-16 03:39:46,109 - Epoch: [136][   50/  142]    Overall Loss 0.451655    Objective Loss 0.451655                                        LR 0.000016    Time 0.579258    
2023-06-16 03:39:51,018 - Epoch: [136][   60/  142]    Overall Loss 0.447647    Objective Loss 0.447647                                        LR 0.000016    Time 0.564523    
2023-06-16 03:39:55,877 - Epoch: [136][   70/  142]    Overall Loss 0.451435    Objective Loss 0.451435                                        LR 0.000016    Time 0.553290    
2023-06-16 03:40:00,874 - Epoch: [136][   80/  142]    Overall Loss 0.456128    Objective Loss 0.456128                                        LR 0.000016    Time 0.546583    
2023-06-16 03:40:05,795 - Epoch: [136][   90/  142]    Overall Loss 0.454309    Objective Loss 0.454309                                        LR 0.000016    Time 0.540516    
2023-06-16 03:40:10,817 - Epoch: [136][  100/  142]    Overall Loss 0.452436    Objective Loss 0.452436                                        LR 0.000016    Time 0.536686    
2023-06-16 03:40:15,743 - Epoch: [136][  110/  142]    Overall Loss 0.457146    Objective Loss 0.457146                                        LR 0.000016    Time 0.532671    
2023-06-16 03:40:20,666 - Epoch: [136][  120/  142]    Overall Loss 0.458247    Objective Loss 0.458247                                        LR 0.000016    Time 0.529303    
2023-06-16 03:40:25,603 - Epoch: [136][  130/  142]    Overall Loss 0.455047    Objective Loss 0.455047                                        LR 0.000016    Time 0.526557    
2023-06-16 03:40:30,147 - Epoch: [136][  140/  142]    Overall Loss 0.459986    Objective Loss 0.459986                                        LR 0.000016    Time 0.521401    
2023-06-16 03:40:30,992 - Epoch: [136][  142/  142]    Overall Loss 0.459391    Objective Loss 0.459391    Top1 85.937500    LR 0.000016    Time 0.520006    
2023-06-16 03:40:31,653 - --- validate (epoch=136)-----------
2023-06-16 03:40:31,654 - 1422 samples (32 per mini-batch)
2023-06-16 03:40:39,834 - Epoch: [136][   10/   45]    Loss 0.703778    Top1 76.875000    
2023-06-16 03:40:43,826 - Epoch: [136][   20/   45]    Loss 0.826859    Top1 75.625000    
2023-06-16 03:40:47,941 - Epoch: [136][   30/   45]    Loss 0.829213    Top1 75.625000    
2023-06-16 03:40:52,426 - Epoch: [136][   40/   45]    Loss 0.866387    Top1 75.000000    
2023-06-16 03:40:53,788 - Epoch: [136][   45/   45]    Loss 0.881696    Top1 75.105485    
2023-06-16 03:40:54,416 - ==> Top1: 75.105    Loss: 0.882

2023-06-16 03:40:54,418 - ==> Best [Top1: 75.457   Sparsity:0.00   Params: 375264 on epoch: 135]
2023-06-16 03:40:54,418 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:40:54,439 - 

2023-06-16 03:40:54,439 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:41:03,840 - Epoch: [137][   10/  142]    Overall Loss 0.440220    Objective Loss 0.440220                                        LR 0.000016    Time 0.939990    
2023-06-16 03:41:08,779 - Epoch: [137][   20/  142]    Overall Loss 0.467199    Objective Loss 0.467199                                        LR 0.000016    Time 0.716905    
2023-06-16 03:41:13,788 - Epoch: [137][   30/  142]    Overall Loss 0.450467    Objective Loss 0.450467                                        LR 0.000016    Time 0.644865    
2023-06-16 03:41:18,773 - Epoch: [137][   40/  142]    Overall Loss 0.455955    Objective Loss 0.455955                                        LR 0.000016    Time 0.608273    
2023-06-16 03:41:23,685 - Epoch: [137][   50/  142]    Overall Loss 0.458314    Objective Loss 0.458314                                        LR 0.000016    Time 0.584853    
2023-06-16 03:41:28,724 - Epoch: [137][   60/  142]    Overall Loss 0.464928    Objective Loss 0.464928                                        LR 0.000016    Time 0.571342    
2023-06-16 03:41:33,880 - Epoch: [137][   70/  142]    Overall Loss 0.478493    Objective Loss 0.478493                                        LR 0.000016    Time 0.563370    
2023-06-16 03:41:38,892 - Epoch: [137][   80/  142]    Overall Loss 0.469672    Objective Loss 0.469672                                        LR 0.000016    Time 0.555588    
2023-06-16 03:41:43,967 - Epoch: [137][   90/  142]    Overall Loss 0.460412    Objective Loss 0.460412                                        LR 0.000016    Time 0.550249    
2023-06-16 03:41:49,021 - Epoch: [137][  100/  142]    Overall Loss 0.452136    Objective Loss 0.452136                                        LR 0.000016    Time 0.545755    
2023-06-16 03:41:53,978 - Epoch: [137][  110/  142]    Overall Loss 0.453052    Objective Loss 0.453052                                        LR 0.000016    Time 0.541201    
2023-06-16 03:41:59,089 - Epoch: [137][  120/  142]    Overall Loss 0.448419    Objective Loss 0.448419                                        LR 0.000016    Time 0.538688    
2023-06-16 03:42:04,038 - Epoch: [137][  130/  142]    Overall Loss 0.454026    Objective Loss 0.454026                                        LR 0.000016    Time 0.535310    
2023-06-16 03:42:08,798 - Epoch: [137][  140/  142]    Overall Loss 0.454366    Objective Loss 0.454366                                        LR 0.000016    Time 0.531072    
2023-06-16 03:42:09,652 - Epoch: [137][  142/  142]    Overall Loss 0.452863    Objective Loss 0.452863    Top1 87.500000    LR 0.000016    Time 0.529604    
2023-06-16 03:42:10,292 - --- validate (epoch=137)-----------
2023-06-16 03:42:10,293 - 1422 samples (32 per mini-batch)
2023-06-16 03:42:18,350 - Epoch: [137][   10/   45]    Loss 0.895480    Top1 75.312500    
2023-06-16 03:42:23,083 - Epoch: [137][   20/   45]    Loss 0.860824    Top1 75.625000    
2023-06-16 03:42:27,413 - Epoch: [137][   30/   45]    Loss 0.885329    Top1 75.520833    
2023-06-16 03:42:31,560 - Epoch: [137][   40/   45]    Loss 0.878784    Top1 75.156250    
2023-06-16 03:42:32,920 - Epoch: [137][   45/   45]    Loss 0.902857    Top1 75.035162    
2023-06-16 03:42:33,525 - ==> Top1: 75.035    Loss: 0.903

2023-06-16 03:42:33,527 - ==> Best [Top1: 75.457   Sparsity:0.00   Params: 375264 on epoch: 135]
2023-06-16 03:42:33,527 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:42:33,548 - 

2023-06-16 03:42:33,548 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:42:42,794 - Epoch: [138][   10/  142]    Overall Loss 0.458386    Objective Loss 0.458386                                        LR 0.000016    Time 0.924487    
2023-06-16 03:42:47,762 - Epoch: [138][   20/  142]    Overall Loss 0.445683    Objective Loss 0.445683                                        LR 0.000016    Time 0.710595    
2023-06-16 03:42:52,732 - Epoch: [138][   30/  142]    Overall Loss 0.471091    Objective Loss 0.471091                                        LR 0.000016    Time 0.639351    
2023-06-16 03:42:57,660 - Epoch: [138][   40/  142]    Overall Loss 0.484752    Objective Loss 0.484752                                        LR 0.000016    Time 0.602719    
2023-06-16 03:43:02,719 - Epoch: [138][   50/  142]    Overall Loss 0.468752    Objective Loss 0.468752                                        LR 0.000016    Time 0.583342    
2023-06-16 03:43:07,709 - Epoch: [138][   60/  142]    Overall Loss 0.471660    Objective Loss 0.471660                                        LR 0.000016    Time 0.569267    
2023-06-16 03:43:12,587 - Epoch: [138][   70/  142]    Overall Loss 0.467233    Objective Loss 0.467233                                        LR 0.000016    Time 0.557623    
2023-06-16 03:43:17,565 - Epoch: [138][   80/  142]    Overall Loss 0.459918    Objective Loss 0.459918                                        LR 0.000016    Time 0.550138    
2023-06-16 03:43:22,594 - Epoch: [138][   90/  142]    Overall Loss 0.458601    Objective Loss 0.458601                                        LR 0.000016    Time 0.544882    
2023-06-16 03:43:27,533 - Epoch: [138][  100/  142]    Overall Loss 0.456401    Objective Loss 0.456401                                        LR 0.000016    Time 0.539770    
2023-06-16 03:43:32,470 - Epoch: [138][  110/  142]    Overall Loss 0.448990    Objective Loss 0.448990                                        LR 0.000016    Time 0.535580    
2023-06-16 03:43:37,512 - Epoch: [138][  120/  142]    Overall Loss 0.446148    Objective Loss 0.446148                                        LR 0.000016    Time 0.532957    
2023-06-16 03:43:42,530 - Epoch: [138][  130/  142]    Overall Loss 0.447227    Objective Loss 0.447227                                        LR 0.000016    Time 0.530559    
2023-06-16 03:43:47,214 - Epoch: [138][  140/  142]    Overall Loss 0.451873    Objective Loss 0.451873                                        LR 0.000016    Time 0.526118    
2023-06-16 03:43:48,070 - Epoch: [138][  142/  142]    Overall Loss 0.454028    Objective Loss 0.454028    Top1 78.125000    LR 0.000016    Time 0.524731    
2023-06-16 03:43:48,721 - --- validate (epoch=138)-----------
2023-06-16 03:43:48,721 - 1422 samples (32 per mini-batch)
2023-06-16 03:43:57,023 - Epoch: [138][   10/   45]    Loss 0.820495    Top1 75.937500    
2023-06-16 03:44:01,327 - Epoch: [138][   20/   45]    Loss 0.891700    Top1 74.687500    
2023-06-16 03:44:05,958 - Epoch: [138][   30/   45]    Loss 0.889393    Top1 74.270833    
2023-06-16 03:44:10,559 - Epoch: [138][   40/   45]    Loss 0.844223    Top1 75.234375    
2023-06-16 03:44:11,853 - Epoch: [138][   45/   45]    Loss 0.880403    Top1 74.472574    
2023-06-16 03:44:12,515 - ==> Top1: 74.473    Loss: 0.880

2023-06-16 03:44:12,517 - ==> Best [Top1: 75.457   Sparsity:0.00   Params: 375264 on epoch: 135]
2023-06-16 03:44:12,517 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:44:12,538 - 

2023-06-16 03:44:12,538 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:44:21,788 - Epoch: [139][   10/  142]    Overall Loss 0.432930    Objective Loss 0.432930                                        LR 0.000016    Time 0.924929    
2023-06-16 03:44:26,733 - Epoch: [139][   20/  142]    Overall Loss 0.444092    Objective Loss 0.444092                                        LR 0.000016    Time 0.709650    
2023-06-16 03:44:31,845 - Epoch: [139][   30/  142]    Overall Loss 0.453594    Objective Loss 0.453594                                        LR 0.000016    Time 0.643501    
2023-06-16 03:44:36,912 - Epoch: [139][   40/  142]    Overall Loss 0.455326    Objective Loss 0.455326                                        LR 0.000016    Time 0.609279    
2023-06-16 03:44:41,799 - Epoch: [139][   50/  142]    Overall Loss 0.473363    Objective Loss 0.473363                                        LR 0.000016    Time 0.585155    
2023-06-16 03:44:46,863 - Epoch: [139][   60/  142]    Overall Loss 0.465877    Objective Loss 0.465877                                        LR 0.000016    Time 0.572021    
2023-06-16 03:44:51,684 - Epoch: [139][   70/  142]    Overall Loss 0.472746    Objective Loss 0.472746                                        LR 0.000016    Time 0.559160    
2023-06-16 03:44:56,714 - Epoch: [139][   80/  142]    Overall Loss 0.470506    Objective Loss 0.470506                                        LR 0.000016    Time 0.552129    
2023-06-16 03:45:01,744 - Epoch: [139][   90/  142]    Overall Loss 0.460618    Objective Loss 0.460618                                        LR 0.000016    Time 0.546662    
2023-06-16 03:45:06,729 - Epoch: [139][  100/  142]    Overall Loss 0.460177    Objective Loss 0.460177                                        LR 0.000016    Time 0.541844    
2023-06-16 03:45:11,691 - Epoch: [139][  110/  142]    Overall Loss 0.468662    Objective Loss 0.468662                                        LR 0.000016    Time 0.537687    
2023-06-16 03:45:16,693 - Epoch: [139][  120/  142]    Overall Loss 0.476208    Objective Loss 0.476208                                        LR 0.000016    Time 0.534558    
2023-06-16 03:45:21,668 - Epoch: [139][  130/  142]    Overall Loss 0.472318    Objective Loss 0.472318                                        LR 0.000016    Time 0.531700    
2023-06-16 03:45:26,305 - Epoch: [139][  140/  142]    Overall Loss 0.470334    Objective Loss 0.470334                                        LR 0.000016    Time 0.526842    
2023-06-16 03:45:27,145 - Epoch: [139][  142/  142]    Overall Loss 0.470147    Objective Loss 0.470147    Top1 82.812500    LR 0.000016    Time 0.525334    
2023-06-16 03:45:27,753 - --- validate (epoch=139)-----------
2023-06-16 03:45:27,753 - 1422 samples (32 per mini-batch)
2023-06-16 03:45:35,901 - Epoch: [139][   10/   45]    Loss 0.967221    Top1 75.937500    
2023-06-16 03:45:40,149 - Epoch: [139][   20/   45]    Loss 0.880475    Top1 77.656250    
2023-06-16 03:45:44,598 - Epoch: [139][   30/   45]    Loss 0.891089    Top1 76.145833    
2023-06-16 03:45:49,324 - Epoch: [139][   40/   45]    Loss 0.869552    Top1 76.328125    
2023-06-16 03:45:50,719 - Epoch: [139][   45/   45]    Loss 0.875014    Top1 75.527426    
2023-06-16 03:45:51,353 - ==> Top1: 75.527    Loss: 0.875

2023-06-16 03:45:51,355 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 03:45:51,355 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:45:51,381 - 

2023-06-16 03:45:51,381 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:46:00,645 - Epoch: [140][   10/  142]    Overall Loss 0.524681    Objective Loss 0.524681                                        LR 0.000016    Time 0.926344    
2023-06-16 03:46:05,644 - Epoch: [140][   20/  142]    Overall Loss 0.426467    Objective Loss 0.426467                                        LR 0.000016    Time 0.713080    
2023-06-16 03:46:10,749 - Epoch: [140][   30/  142]    Overall Loss 0.424882    Objective Loss 0.424882                                        LR 0.000016    Time 0.645519    
2023-06-16 03:46:15,619 - Epoch: [140][   40/  142]    Overall Loss 0.445298    Objective Loss 0.445298                                        LR 0.000016    Time 0.605867    
2023-06-16 03:46:20,598 - Epoch: [140][   50/  142]    Overall Loss 0.440982    Objective Loss 0.440982                                        LR 0.000016    Time 0.584276    
2023-06-16 03:46:25,609 - Epoch: [140][   60/  142]    Overall Loss 0.444493    Objective Loss 0.444493                                        LR 0.000016    Time 0.570401    
2023-06-16 03:46:30,625 - Epoch: [140][   70/  142]    Overall Loss 0.453989    Objective Loss 0.453989                                        LR 0.000016    Time 0.560571    
2023-06-16 03:46:35,681 - Epoch: [140][   80/  142]    Overall Loss 0.459819    Objective Loss 0.459819                                        LR 0.000016    Time 0.553686    
2023-06-16 03:46:40,553 - Epoch: [140][   90/  142]    Overall Loss 0.462344    Objective Loss 0.462344                                        LR 0.000016    Time 0.546294    
2023-06-16 03:46:45,595 - Epoch: [140][  100/  142]    Overall Loss 0.458850    Objective Loss 0.458850                                        LR 0.000016    Time 0.542082    
2023-06-16 03:46:50,665 - Epoch: [140][  110/  142]    Overall Loss 0.457505    Objective Loss 0.457505                                        LR 0.000016    Time 0.538883    
2023-06-16 03:46:55,681 - Epoch: [140][  120/  142]    Overall Loss 0.456194    Objective Loss 0.456194                                        LR 0.000016    Time 0.535774    
2023-06-16 03:47:00,594 - Epoch: [140][  130/  142]    Overall Loss 0.454259    Objective Loss 0.454259                                        LR 0.000016    Time 0.532352    
2023-06-16 03:47:05,289 - Epoch: [140][  140/  142]    Overall Loss 0.457745    Objective Loss 0.457745                                        LR 0.000016    Time 0.527856    
2023-06-16 03:47:06,145 - Epoch: [140][  142/  142]    Overall Loss 0.457272    Objective Loss 0.457272    Top1 84.375000    LR 0.000016    Time 0.526450    
2023-06-16 03:47:06,788 - --- validate (epoch=140)-----------
2023-06-16 03:47:06,789 - 1422 samples (32 per mini-batch)
2023-06-16 03:47:14,927 - Epoch: [140][   10/   45]    Loss 0.877474    Top1 75.937500    
2023-06-16 03:47:19,042 - Epoch: [140][   20/   45]    Loss 0.864484    Top1 76.406250    
2023-06-16 03:47:23,561 - Epoch: [140][   30/   45]    Loss 0.879137    Top1 75.104167    
2023-06-16 03:47:28,091 - Epoch: [140][   40/   45]    Loss 0.889286    Top1 74.218750    
2023-06-16 03:47:29,584 - Epoch: [140][   45/   45]    Loss 0.863076    Top1 75.035162    
2023-06-16 03:47:30,243 - ==> Top1: 75.035    Loss: 0.863

2023-06-16 03:47:30,245 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 03:47:30,245 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:47:30,260 - 

2023-06-16 03:47:30,260 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:47:39,524 - Epoch: [141][   10/  142]    Overall Loss 0.351304    Objective Loss 0.351304                                        LR 0.000016    Time 0.926268    
2023-06-16 03:47:44,657 - Epoch: [141][   20/  142]    Overall Loss 0.396826    Objective Loss 0.396826                                        LR 0.000016    Time 0.719744    
2023-06-16 03:47:49,724 - Epoch: [141][   30/  142]    Overall Loss 0.441418    Objective Loss 0.441418                                        LR 0.000016    Time 0.648727    
2023-06-16 03:47:54,719 - Epoch: [141][   40/  142]    Overall Loss 0.464870    Objective Loss 0.464870                                        LR 0.000016    Time 0.611405    
2023-06-16 03:47:59,852 - Epoch: [141][   50/  142]    Overall Loss 0.465712    Objective Loss 0.465712                                        LR 0.000016    Time 0.591760    
2023-06-16 03:48:04,821 - Epoch: [141][   60/  142]    Overall Loss 0.463140    Objective Loss 0.463140                                        LR 0.000016    Time 0.575947    
2023-06-16 03:48:09,803 - Epoch: [141][   70/  142]    Overall Loss 0.473163    Objective Loss 0.473163                                        LR 0.000016    Time 0.564839    
2023-06-16 03:48:14,806 - Epoch: [141][   80/  142]    Overall Loss 0.464638    Objective Loss 0.464638                                        LR 0.000016    Time 0.556758    
2023-06-16 03:48:19,795 - Epoch: [141][   90/  142]    Overall Loss 0.457758    Objective Loss 0.457758                                        LR 0.000016    Time 0.550324    
2023-06-16 03:48:24,775 - Epoch: [141][  100/  142]    Overall Loss 0.460162    Objective Loss 0.460162                                        LR 0.000016    Time 0.545083    
2023-06-16 03:48:29,825 - Epoch: [141][  110/  142]    Overall Loss 0.456317    Objective Loss 0.456317                                        LR 0.000016    Time 0.541441    
2023-06-16 03:48:34,904 - Epoch: [141][  120/  142]    Overall Loss 0.456927    Objective Loss 0.456927                                        LR 0.000016    Time 0.538635    
2023-06-16 03:48:39,961 - Epoch: [141][  130/  142]    Overall Loss 0.459634    Objective Loss 0.459634                                        LR 0.000016    Time 0.536096    
2023-06-16 03:48:44,466 - Epoch: [141][  140/  142]    Overall Loss 0.456931    Objective Loss 0.456931                                        LR 0.000016    Time 0.529979    
2023-06-16 03:48:45,307 - Epoch: [141][  142/  142]    Overall Loss 0.457737    Objective Loss 0.457737    Top1 82.812500    LR 0.000016    Time 0.528436    
2023-06-16 03:48:45,969 - --- validate (epoch=141)-----------
2023-06-16 03:48:45,969 - 1422 samples (32 per mini-batch)
2023-06-16 03:48:54,107 - Epoch: [141][   10/   45]    Loss 0.934152    Top1 74.687500    
2023-06-16 03:48:58,111 - Epoch: [141][   20/   45]    Loss 0.966409    Top1 75.312500    
2023-06-16 03:49:02,629 - Epoch: [141][   30/   45]    Loss 0.896333    Top1 76.875000    
2023-06-16 03:49:07,257 - Epoch: [141][   40/   45]    Loss 0.921775    Top1 75.312500    
2023-06-16 03:49:08,624 - Epoch: [141][   45/   45]    Loss 0.921904    Top1 75.246132    
2023-06-16 03:49:09,275 - ==> Top1: 75.246    Loss: 0.922

2023-06-16 03:49:09,277 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 03:49:09,277 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:49:09,298 - 

2023-06-16 03:49:09,299 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:49:18,569 - Epoch: [142][   10/  142]    Overall Loss 0.373271    Objective Loss 0.373271                                        LR 0.000016    Time 0.926889    
2023-06-16 03:49:23,377 - Epoch: [142][   20/  142]    Overall Loss 0.431565    Objective Loss 0.431565                                        LR 0.000016    Time 0.703847    
2023-06-16 03:49:28,282 - Epoch: [142][   30/  142]    Overall Loss 0.441372    Objective Loss 0.441372                                        LR 0.000016    Time 0.632707    
2023-06-16 03:49:33,211 - Epoch: [142][   40/  142]    Overall Loss 0.437539    Objective Loss 0.437539                                        LR 0.000016    Time 0.597743    
2023-06-16 03:49:38,083 - Epoch: [142][   50/  142]    Overall Loss 0.429028    Objective Loss 0.429028                                        LR 0.000016    Time 0.575619    
2023-06-16 03:49:43,110 - Epoch: [142][   60/  142]    Overall Loss 0.440589    Objective Loss 0.440589                                        LR 0.000016    Time 0.563454    
2023-06-16 03:49:47,935 - Epoch: [142][   70/  142]    Overall Loss 0.443754    Objective Loss 0.443754                                        LR 0.000016    Time 0.551878    
2023-06-16 03:49:52,931 - Epoch: [142][   80/  142]    Overall Loss 0.440711    Objective Loss 0.440711                                        LR 0.000016    Time 0.545338    
2023-06-16 03:49:57,822 - Epoch: [142][   90/  142]    Overall Loss 0.441472    Objective Loss 0.441472                                        LR 0.000016    Time 0.539089    
2023-06-16 03:50:02,769 - Epoch: [142][  100/  142]    Overall Loss 0.445595    Objective Loss 0.445595                                        LR 0.000016    Time 0.534635    
2023-06-16 03:50:07,696 - Epoch: [142][  110/  142]    Overall Loss 0.444219    Objective Loss 0.444219                                        LR 0.000016    Time 0.530822    
2023-06-16 03:50:12,594 - Epoch: [142][  120/  142]    Overall Loss 0.449842    Objective Loss 0.449842                                        LR 0.000016    Time 0.527398    
2023-06-16 03:50:17,546 - Epoch: [142][  130/  142]    Overall Loss 0.451254    Objective Loss 0.451254                                        LR 0.000016    Time 0.524918    
2023-06-16 03:50:22,205 - Epoch: [142][  140/  142]    Overall Loss 0.448139    Objective Loss 0.448139                                        LR 0.000016    Time 0.520698    
2023-06-16 03:50:23,048 - Epoch: [142][  142/  142]    Overall Loss 0.450441    Objective Loss 0.450441    Top1 78.125000    LR 0.000016    Time 0.519299    
2023-06-16 03:50:23,707 - --- validate (epoch=142)-----------
2023-06-16 03:50:23,708 - 1422 samples (32 per mini-batch)
2023-06-16 03:50:31,983 - Epoch: [142][   10/   45]    Loss 0.948296    Top1 74.062500    
2023-06-16 03:50:35,700 - Epoch: [142][   20/   45]    Loss 0.904462    Top1 76.406250    
2023-06-16 03:50:40,480 - Epoch: [142][   30/   45]    Loss 0.896565    Top1 75.208333    
2023-06-16 03:50:44,550 - Epoch: [142][   40/   45]    Loss 0.929079    Top1 74.687500    
2023-06-16 03:50:46,266 - Epoch: [142][   45/   45]    Loss 0.945235    Top1 74.683544    
2023-06-16 03:50:46,905 - ==> Top1: 74.684    Loss: 0.945

2023-06-16 03:50:46,907 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 03:50:46,907 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:50:46,928 - 

2023-06-16 03:50:46,929 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:50:56,147 - Epoch: [143][   10/  142]    Overall Loss 0.426699    Objective Loss 0.426699                                        LR 0.000016    Time 0.921700    
2023-06-16 03:51:01,094 - Epoch: [143][   20/  142]    Overall Loss 0.449680    Objective Loss 0.449680                                        LR 0.000016    Time 0.708190    
2023-06-16 03:51:05,996 - Epoch: [143][   30/  142]    Overall Loss 0.456566    Objective Loss 0.456566                                        LR 0.000016    Time 0.635488    
2023-06-16 03:51:10,916 - Epoch: [143][   40/  142]    Overall Loss 0.479448    Objective Loss 0.479448                                        LR 0.000016    Time 0.599595    
2023-06-16 03:51:15,883 - Epoch: [143][   50/  142]    Overall Loss 0.472172    Objective Loss 0.472172                                        LR 0.000016    Time 0.578998    
2023-06-16 03:51:20,901 - Epoch: [143][   60/  142]    Overall Loss 0.463217    Objective Loss 0.463217                                        LR 0.000016    Time 0.566132    
2023-06-16 03:51:25,836 - Epoch: [143][   70/  142]    Overall Loss 0.458413    Objective Loss 0.458413                                        LR 0.000016    Time 0.555746    
2023-06-16 03:51:30,777 - Epoch: [143][   80/  142]    Overall Loss 0.460309    Objective Loss 0.460309                                        LR 0.000016    Time 0.548035    
2023-06-16 03:51:35,760 - Epoch: [143][   90/  142]    Overall Loss 0.457572    Objective Loss 0.457572                                        LR 0.000016    Time 0.542493    
2023-06-16 03:51:40,795 - Epoch: [143][  100/  142]    Overall Loss 0.447667    Objective Loss 0.447667                                        LR 0.000016    Time 0.538596    
2023-06-16 03:51:45,667 - Epoch: [143][  110/  142]    Overall Loss 0.452216    Objective Loss 0.452216                                        LR 0.000016    Time 0.533912    
2023-06-16 03:51:50,495 - Epoch: [143][  120/  142]    Overall Loss 0.461976    Objective Loss 0.461976                                        LR 0.000016    Time 0.529650    
2023-06-16 03:51:55,529 - Epoch: [143][  130/  142]    Overall Loss 0.462701    Objective Loss 0.462701                                        LR 0.000016    Time 0.527624    
2023-06-16 03:52:00,166 - Epoch: [143][  140/  142]    Overall Loss 0.459518    Objective Loss 0.459518                                        LR 0.000016    Time 0.523054    
2023-06-16 03:52:01,009 - Epoch: [143][  142/  142]    Overall Loss 0.458933    Objective Loss 0.458933    Top1 84.375000    LR 0.000016    Time 0.521627    
2023-06-16 03:52:01,649 - --- validate (epoch=143)-----------
2023-06-16 03:52:01,650 - 1422 samples (32 per mini-batch)
2023-06-16 03:52:09,620 - Epoch: [143][   10/   45]    Loss 1.159058    Top1 69.687500    
2023-06-16 03:52:13,602 - Epoch: [143][   20/   45]    Loss 1.044503    Top1 71.250000    
2023-06-16 03:52:18,672 - Epoch: [143][   30/   45]    Loss 0.941390    Top1 73.645833    
2023-06-16 03:52:22,996 - Epoch: [143][   40/   45]    Loss 0.921448    Top1 74.062500    
2023-06-16 03:52:24,316 - Epoch: [143][   45/   45]    Loss 0.916986    Top1 73.980309    
2023-06-16 03:52:24,960 - ==> Top1: 73.980    Loss: 0.917

2023-06-16 03:52:24,963 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 03:52:24,963 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:52:24,976 - 

2023-06-16 03:52:24,977 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:52:34,326 - Epoch: [144][   10/  142]    Overall Loss 0.444192    Objective Loss 0.444192                                        LR 0.000016    Time 0.934875    
2023-06-16 03:52:39,211 - Epoch: [144][   20/  142]    Overall Loss 0.444893    Objective Loss 0.444893                                        LR 0.000016    Time 0.711642    
2023-06-16 03:52:44,162 - Epoch: [144][   30/  142]    Overall Loss 0.453618    Objective Loss 0.453618                                        LR 0.000016    Time 0.639433    
2023-06-16 03:52:49,090 - Epoch: [144][   40/  142]    Overall Loss 0.447371    Objective Loss 0.447371                                        LR 0.000016    Time 0.602762    
2023-06-16 03:52:54,044 - Epoch: [144][   50/  142]    Overall Loss 0.460140    Objective Loss 0.460140                                        LR 0.000016    Time 0.581281    
2023-06-16 03:52:58,945 - Epoch: [144][   60/  142]    Overall Loss 0.460945    Objective Loss 0.460945                                        LR 0.000016    Time 0.566062    
2023-06-16 03:53:03,934 - Epoch: [144][   70/  142]    Overall Loss 0.459767    Objective Loss 0.459767                                        LR 0.000016    Time 0.556463    
2023-06-16 03:53:08,806 - Epoch: [144][   80/  142]    Overall Loss 0.451930    Objective Loss 0.451930                                        LR 0.000016    Time 0.547800    
2023-06-16 03:53:13,844 - Epoch: [144][   90/  142]    Overall Loss 0.442820    Objective Loss 0.442820                                        LR 0.000016    Time 0.542903    
2023-06-16 03:53:18,877 - Epoch: [144][  100/  142]    Overall Loss 0.442490    Objective Loss 0.442490                                        LR 0.000016    Time 0.538939    
2023-06-16 03:53:23,812 - Epoch: [144][  110/  142]    Overall Loss 0.443733    Objective Loss 0.443733                                        LR 0.000016    Time 0.534801    
2023-06-16 03:53:28,823 - Epoch: [144][  120/  142]    Overall Loss 0.448330    Objective Loss 0.448330                                        LR 0.000016    Time 0.531990    
2023-06-16 03:53:33,867 - Epoch: [144][  130/  142]    Overall Loss 0.450500    Objective Loss 0.450500                                        LR 0.000016    Time 0.529858    
2023-06-16 03:53:38,556 - Epoch: [144][  140/  142]    Overall Loss 0.450908    Objective Loss 0.450908                                        LR 0.000016    Time 0.525502    
2023-06-16 03:53:39,399 - Epoch: [144][  142/  142]    Overall Loss 0.451561    Objective Loss 0.451561    Top1 78.125000    LR 0.000016    Time 0.524034    
2023-06-16 03:53:40,034 - --- validate (epoch=144)-----------
2023-06-16 03:53:40,034 - 1422 samples (32 per mini-batch)
2023-06-16 03:53:47,960 - Epoch: [144][   10/   45]    Loss 1.001176    Top1 76.250000    
2023-06-16 03:53:52,043 - Epoch: [144][   20/   45]    Loss 0.896791    Top1 76.718750    
2023-06-16 03:53:56,107 - Epoch: [144][   30/   45]    Loss 0.847943    Top1 76.562500    
2023-06-16 03:54:00,891 - Epoch: [144][   40/   45]    Loss 0.907280    Top1 75.625000    
2023-06-16 03:54:02,339 - Epoch: [144][   45/   45]    Loss 0.923598    Top1 75.316456    
2023-06-16 03:54:02,999 - ==> Top1: 75.316    Loss: 0.924

2023-06-16 03:54:03,002 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 03:54:03,002 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:54:03,023 - 

2023-06-16 03:54:03,023 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:54:12,274 - Epoch: [145][   10/  142]    Overall Loss 0.441211    Objective Loss 0.441211                                        LR 0.000016    Time 0.925010    
2023-06-16 03:54:17,287 - Epoch: [145][   20/  142]    Overall Loss 0.421379    Objective Loss 0.421379                                        LR 0.000016    Time 0.713083    
2023-06-16 03:54:22,239 - Epoch: [145][   30/  142]    Overall Loss 0.451002    Objective Loss 0.451002                                        LR 0.000016    Time 0.640454    
2023-06-16 03:54:27,165 - Epoch: [145][   40/  142]    Overall Loss 0.465604    Objective Loss 0.465604                                        LR 0.000016    Time 0.603480    
2023-06-16 03:54:32,254 - Epoch: [145][   50/  142]    Overall Loss 0.481159    Objective Loss 0.481159                                        LR 0.000016    Time 0.584541    
2023-06-16 03:54:37,159 - Epoch: [145][   60/  142]    Overall Loss 0.474451    Objective Loss 0.474451                                        LR 0.000016    Time 0.568865    
2023-06-16 03:54:42,150 - Epoch: [145][   70/  142]    Overall Loss 0.472837    Objective Loss 0.472837                                        LR 0.000016    Time 0.558880    
2023-06-16 03:54:47,155 - Epoch: [145][   80/  142]    Overall Loss 0.470592    Objective Loss 0.470592                                        LR 0.000016    Time 0.551576    
2023-06-16 03:54:52,077 - Epoch: [145][   90/  142]    Overall Loss 0.475738    Objective Loss 0.475738                                        LR 0.000016    Time 0.544979    
2023-06-16 03:54:57,139 - Epoch: [145][  100/  142]    Overall Loss 0.472295    Objective Loss 0.472295                                        LR 0.000016    Time 0.541091    
2023-06-16 03:55:02,018 - Epoch: [145][  110/  142]    Overall Loss 0.469797    Objective Loss 0.469797                                        LR 0.000016    Time 0.536255    
2023-06-16 03:55:07,011 - Epoch: [145][  120/  142]    Overall Loss 0.469742    Objective Loss 0.469742                                        LR 0.000016    Time 0.533171    
2023-06-16 03:55:12,124 - Epoch: [145][  130/  142]    Overall Loss 0.470086    Objective Loss 0.470086                                        LR 0.000016    Time 0.531480    
2023-06-16 03:55:16,693 - Epoch: [145][  140/  142]    Overall Loss 0.467881    Objective Loss 0.467881                                        LR 0.000016    Time 0.526155    
2023-06-16 03:55:17,536 - Epoch: [145][  142/  142]    Overall Loss 0.466228    Objective Loss 0.466228    Top1 90.625000    LR 0.000016    Time 0.524679    
2023-06-16 03:55:18,152 - --- validate (epoch=145)-----------
2023-06-16 03:55:18,153 - 1422 samples (32 per mini-batch)
2023-06-16 03:55:26,090 - Epoch: [145][   10/   45]    Loss 1.019649    Top1 71.250000    
2023-06-16 03:55:30,467 - Epoch: [145][   20/   45]    Loss 0.937899    Top1 72.500000    
2023-06-16 03:55:35,116 - Epoch: [145][   30/   45]    Loss 0.908328    Top1 73.437500    
2023-06-16 03:55:39,679 - Epoch: [145][   40/   45]    Loss 0.905912    Top1 73.593750    
2023-06-16 03:55:40,971 - Epoch: [145][   45/   45]    Loss 0.889003    Top1 74.191280    
2023-06-16 03:55:41,609 - ==> Top1: 74.191    Loss: 0.889

2023-06-16 03:55:41,611 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 03:55:41,611 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:55:41,632 - 

2023-06-16 03:55:41,632 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:55:50,934 - Epoch: [146][   10/  142]    Overall Loss 0.445787    Objective Loss 0.445787                                        LR 0.000016    Time 0.930074    
2023-06-16 03:55:55,805 - Epoch: [146][   20/  142]    Overall Loss 0.498455    Objective Loss 0.498455                                        LR 0.000016    Time 0.708544    
2023-06-16 03:56:00,877 - Epoch: [146][   30/  142]    Overall Loss 0.491639    Objective Loss 0.491639                                        LR 0.000016    Time 0.641422    
2023-06-16 03:56:05,873 - Epoch: [146][   40/  142]    Overall Loss 0.452122    Objective Loss 0.452122                                        LR 0.000016    Time 0.605952    
2023-06-16 03:56:10,814 - Epoch: [146][   50/  142]    Overall Loss 0.444185    Objective Loss 0.444185                                        LR 0.000016    Time 0.583569    
2023-06-16 03:56:15,807 - Epoch: [146][   60/  142]    Overall Loss 0.459789    Objective Loss 0.459789                                        LR 0.000016    Time 0.569515    
2023-06-16 03:56:20,809 - Epoch: [146][   70/  142]    Overall Loss 0.459867    Objective Loss 0.459867                                        LR 0.000016    Time 0.559609    
2023-06-16 03:56:25,757 - Epoch: [146][   80/  142]    Overall Loss 0.457142    Objective Loss 0.457142                                        LR 0.000016    Time 0.551492    
2023-06-16 03:56:30,780 - Epoch: [146][   90/  142]    Overall Loss 0.462329    Objective Loss 0.462329                                        LR 0.000016    Time 0.546018    
2023-06-16 03:56:35,750 - Epoch: [146][  100/  142]    Overall Loss 0.464343    Objective Loss 0.464343                                        LR 0.000016    Time 0.541117    
2023-06-16 03:56:40,696 - Epoch: [146][  110/  142]    Overall Loss 0.457767    Objective Loss 0.457767                                        LR 0.000016    Time 0.536880    
2023-06-16 03:56:45,569 - Epoch: [146][  120/  142]    Overall Loss 0.452980    Objective Loss 0.452980                                        LR 0.000016    Time 0.532740    
2023-06-16 03:56:50,601 - Epoch: [146][  130/  142]    Overall Loss 0.456548    Objective Loss 0.456548                                        LR 0.000016    Time 0.530464    
2023-06-16 03:56:55,167 - Epoch: [146][  140/  142]    Overall Loss 0.456986    Objective Loss 0.456986                                        LR 0.000016    Time 0.525186    
2023-06-16 03:56:56,010 - Epoch: [146][  142/  142]    Overall Loss 0.458620    Objective Loss 0.458620    Top1 75.000000    LR 0.000016    Time 0.523722    
2023-06-16 03:56:56,636 - --- validate (epoch=146)-----------
2023-06-16 03:56:56,637 - 1422 samples (32 per mini-batch)
2023-06-16 03:57:04,806 - Epoch: [146][   10/   45]    Loss 0.820657    Top1 75.625000    
2023-06-16 03:57:09,057 - Epoch: [146][   20/   45]    Loss 0.841938    Top1 76.093750    
2023-06-16 03:57:13,754 - Epoch: [146][   30/   45]    Loss 0.881377    Top1 75.208333    
2023-06-16 03:57:17,804 - Epoch: [146][   40/   45]    Loss 0.876151    Top1 75.156250    
2023-06-16 03:57:19,301 - Epoch: [146][   45/   45]    Loss 0.876492    Top1 75.316456    
2023-06-16 03:57:19,941 - ==> Top1: 75.316    Loss: 0.876

2023-06-16 03:57:19,943 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 03:57:19,943 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:57:19,964 - 

2023-06-16 03:57:19,965 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:57:29,277 - Epoch: [147][   10/  142]    Overall Loss 0.398817    Objective Loss 0.398817                                        LR 0.000016    Time 0.931097    
2023-06-16 03:57:34,305 - Epoch: [147][   20/  142]    Overall Loss 0.452397    Objective Loss 0.452397                                        LR 0.000016    Time 0.716930    
2023-06-16 03:57:39,328 - Epoch: [147][   30/  142]    Overall Loss 0.446339    Objective Loss 0.446339                                        LR 0.000016    Time 0.645377    
2023-06-16 03:57:44,259 - Epoch: [147][   40/  142]    Overall Loss 0.465721    Objective Loss 0.465721                                        LR 0.000016    Time 0.607278    
2023-06-16 03:57:49,257 - Epoch: [147][   50/  142]    Overall Loss 0.468022    Objective Loss 0.468022                                        LR 0.000016    Time 0.585776    
2023-06-16 03:57:54,302 - Epoch: [147][   60/  142]    Overall Loss 0.475430    Objective Loss 0.475430                                        LR 0.000016    Time 0.572217    
2023-06-16 03:57:59,314 - Epoch: [147][   70/  142]    Overall Loss 0.485986    Objective Loss 0.485986                                        LR 0.000016    Time 0.562070    
2023-06-16 03:58:04,387 - Epoch: [147][   80/  142]    Overall Loss 0.483936    Objective Loss 0.483936                                        LR 0.000016    Time 0.555216    
2023-06-16 03:58:09,301 - Epoch: [147][   90/  142]    Overall Loss 0.470329    Objective Loss 0.470329                                        LR 0.000016    Time 0.548114    
2023-06-16 03:58:14,458 - Epoch: [147][  100/  142]    Overall Loss 0.463593    Objective Loss 0.463593                                        LR 0.000016    Time 0.544876    
2023-06-16 03:58:19,365 - Epoch: [147][  110/  142]    Overall Loss 0.462964    Objective Loss 0.462964                                        LR 0.000016    Time 0.539945    
2023-06-16 03:58:24,430 - Epoch: [147][  120/  142]    Overall Loss 0.458973    Objective Loss 0.458973                                        LR 0.000016    Time 0.537150    
2023-06-16 03:58:29,441 - Epoch: [147][  130/  142]    Overall Loss 0.451664    Objective Loss 0.451664                                        LR 0.000016    Time 0.534377    
2023-06-16 03:58:34,094 - Epoch: [147][  140/  142]    Overall Loss 0.450781    Objective Loss 0.450781                                        LR 0.000016    Time 0.529436    
2023-06-16 03:58:34,950 - Epoch: [147][  142/  142]    Overall Loss 0.451112    Objective Loss 0.451112    Top1 82.812500    LR 0.000016    Time 0.528011    
2023-06-16 03:58:35,596 - --- validate (epoch=147)-----------
2023-06-16 03:58:35,597 - 1422 samples (32 per mini-batch)
2023-06-16 03:58:43,602 - Epoch: [147][   10/   45]    Loss 0.973259    Top1 73.750000    
2023-06-16 03:58:47,626 - Epoch: [147][   20/   45]    Loss 0.947478    Top1 73.593750    
2023-06-16 03:58:51,957 - Epoch: [147][   30/   45]    Loss 0.888789    Top1 74.687500    
2023-06-16 03:58:56,687 - Epoch: [147][   40/   45]    Loss 0.911512    Top1 74.765625    
2023-06-16 03:58:58,051 - Epoch: [147][   45/   45]    Loss 0.914641    Top1 74.613221    
2023-06-16 03:58:58,709 - ==> Top1: 74.613    Loss: 0.915

2023-06-16 03:58:58,711 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 03:58:58,711 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 03:58:58,725 - 

2023-06-16 03:58:58,725 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 03:59:07,890 - Epoch: [148][   10/  142]    Overall Loss 0.476268    Objective Loss 0.476268                                        LR 0.000016    Time 0.916383    
2023-06-16 03:59:12,984 - Epoch: [148][   20/  142]    Overall Loss 0.452975    Objective Loss 0.452975                                        LR 0.000016    Time 0.712839    
2023-06-16 03:59:17,982 - Epoch: [148][   30/  142]    Overall Loss 0.482670    Objective Loss 0.482670                                        LR 0.000016    Time 0.641814    
2023-06-16 03:59:22,960 - Epoch: [148][   40/  142]    Overall Loss 0.482816    Objective Loss 0.482816                                        LR 0.000016    Time 0.605806    
2023-06-16 03:59:28,068 - Epoch: [148][   50/  142]    Overall Loss 0.466575    Objective Loss 0.466575                                        LR 0.000016    Time 0.586788    
2023-06-16 03:59:33,026 - Epoch: [148][   60/  142]    Overall Loss 0.470110    Objective Loss 0.470110                                        LR 0.000016    Time 0.571612    
2023-06-16 03:59:37,973 - Epoch: [148][   70/  142]    Overall Loss 0.467086    Objective Loss 0.467086                                        LR 0.000016    Time 0.559872    
2023-06-16 03:59:43,092 - Epoch: [148][   80/  142]    Overall Loss 0.476689    Objective Loss 0.476689                                        LR 0.000016    Time 0.553867    
2023-06-16 03:59:48,104 - Epoch: [148][   90/  142]    Overall Loss 0.471244    Objective Loss 0.471244                                        LR 0.000016    Time 0.548002    
2023-06-16 03:59:53,114 - Epoch: [148][  100/  142]    Overall Loss 0.462564    Objective Loss 0.462564                                        LR 0.000016    Time 0.543298    
2023-06-16 03:59:58,034 - Epoch: [148][  110/  142]    Overall Loss 0.468615    Objective Loss 0.468615                                        LR 0.000016    Time 0.538627    
2023-06-16 04:00:03,057 - Epoch: [148][  120/  142]    Overall Loss 0.462045    Objective Loss 0.462045                                        LR 0.000016    Time 0.535601    
2023-06-16 04:00:07,999 - Epoch: [148][  130/  142]    Overall Loss 0.464976    Objective Loss 0.464976                                        LR 0.000016    Time 0.532411    
2023-06-16 04:00:12,678 - Epoch: [148][  140/  142]    Overall Loss 0.461358    Objective Loss 0.461358                                        LR 0.000016    Time 0.527800    
2023-06-16 04:00:13,533 - Epoch: [148][  142/  142]    Overall Loss 0.458913    Objective Loss 0.458913    Top1 89.062500    LR 0.000016    Time 0.526384    
2023-06-16 04:00:14,169 - --- validate (epoch=148)-----------
2023-06-16 04:00:14,170 - 1422 samples (32 per mini-batch)
2023-06-16 04:00:22,370 - Epoch: [148][   10/   45]    Loss 0.865081    Top1 73.437500    
2023-06-16 04:00:26,662 - Epoch: [148][   20/   45]    Loss 0.830011    Top1 74.062500    
2023-06-16 04:00:31,430 - Epoch: [148][   30/   45]    Loss 0.855410    Top1 75.312500    
2023-06-16 04:00:36,022 - Epoch: [148][   40/   45]    Loss 0.891242    Top1 74.609375    
2023-06-16 04:00:37,370 - Epoch: [148][   45/   45]    Loss 0.914998    Top1 74.753868    
2023-06-16 04:00:38,009 - ==> Top1: 74.754    Loss: 0.915

2023-06-16 04:00:38,011 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:00:38,011 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:00:38,032 - 

2023-06-16 04:00:38,032 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:00:47,501 - Epoch: [149][   10/  142]    Overall Loss 0.451653    Objective Loss 0.451653                                        LR 0.000016    Time 0.946789    
2023-06-16 04:00:52,460 - Epoch: [149][   20/  142]    Overall Loss 0.462887    Objective Loss 0.462887                                        LR 0.000016    Time 0.721268    
2023-06-16 04:00:57,443 - Epoch: [149][   30/  142]    Overall Loss 0.470436    Objective Loss 0.470436                                        LR 0.000016    Time 0.646952    
2023-06-16 04:01:02,431 - Epoch: [149][   40/  142]    Overall Loss 0.468439    Objective Loss 0.468439                                        LR 0.000016    Time 0.609896    
2023-06-16 04:01:07,492 - Epoch: [149][   50/  142]    Overall Loss 0.458078    Objective Loss 0.458078                                        LR 0.000016    Time 0.589123    
2023-06-16 04:01:12,577 - Epoch: [149][   60/  142]    Overall Loss 0.459780    Objective Loss 0.459780                                        LR 0.000016    Time 0.575676    
2023-06-16 04:01:17,414 - Epoch: [149][   70/  142]    Overall Loss 0.462856    Objective Loss 0.462856                                        LR 0.000016    Time 0.562531    
2023-06-16 04:01:22,407 - Epoch: [149][   80/  142]    Overall Loss 0.459174    Objective Loss 0.459174                                        LR 0.000016    Time 0.554617    
2023-06-16 04:01:27,393 - Epoch: [149][   90/  142]    Overall Loss 0.458907    Objective Loss 0.458907                                        LR 0.000016    Time 0.548383    
2023-06-16 04:01:32,414 - Epoch: [149][  100/  142]    Overall Loss 0.457286    Objective Loss 0.457286                                        LR 0.000016    Time 0.543756    
2023-06-16 04:01:37,477 - Epoch: [149][  110/  142]    Overall Loss 0.462680    Objective Loss 0.462680                                        LR 0.000016    Time 0.540343    
2023-06-16 04:01:42,408 - Epoch: [149][  120/  142]    Overall Loss 0.456321    Objective Loss 0.456321                                        LR 0.000016    Time 0.536403    
2023-06-16 04:01:47,457 - Epoch: [149][  130/  142]    Overall Loss 0.457836    Objective Loss 0.457836                                        LR 0.000016    Time 0.533970    
2023-06-16 04:01:52,014 - Epoch: [149][  140/  142]    Overall Loss 0.458075    Objective Loss 0.458075                                        LR 0.000016    Time 0.528376    
2023-06-16 04:01:52,854 - Epoch: [149][  142/  142]    Overall Loss 0.457694    Objective Loss 0.457694    Top1 84.375000    LR 0.000016    Time 0.526854    
2023-06-16 04:01:53,488 - --- validate (epoch=149)-----------
2023-06-16 04:01:53,489 - 1422 samples (32 per mini-batch)
2023-06-16 04:02:01,590 - Epoch: [149][   10/   45]    Loss 0.861384    Top1 74.375000    
2023-06-16 04:02:06,126 - Epoch: [149][   20/   45]    Loss 0.890095    Top1 75.312500    
2023-06-16 04:02:10,808 - Epoch: [149][   30/   45]    Loss 0.852994    Top1 75.208333    
2023-06-16 04:02:15,074 - Epoch: [149][   40/   45]    Loss 0.880380    Top1 74.531250    
2023-06-16 04:02:16,385 - Epoch: [149][   45/   45]    Loss 0.876761    Top1 74.402250    
2023-06-16 04:02:17,039 - ==> Top1: 74.402    Loss: 0.877

2023-06-16 04:02:17,041 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:02:17,041 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:02:17,062 - 

2023-06-16 04:02:17,063 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:02:26,229 - Epoch: [150][   10/  142]    Overall Loss 0.524331    Objective Loss 0.524331                                        LR 0.000016    Time 0.916544    
2023-06-16 04:02:31,262 - Epoch: [150][   20/  142]    Overall Loss 0.499284    Objective Loss 0.499284                                        LR 0.000016    Time 0.709865    
2023-06-16 04:02:36,172 - Epoch: [150][   30/  142]    Overall Loss 0.483914    Objective Loss 0.483914                                        LR 0.000016    Time 0.636903    
2023-06-16 04:02:41,149 - Epoch: [150][   40/  142]    Overall Loss 0.478292    Objective Loss 0.478292                                        LR 0.000016    Time 0.602089    
2023-06-16 04:02:46,172 - Epoch: [150][   50/  142]    Overall Loss 0.466928    Objective Loss 0.466928                                        LR 0.000016    Time 0.582125    
2023-06-16 04:02:51,002 - Epoch: [150][   60/  142]    Overall Loss 0.457659    Objective Loss 0.457659                                        LR 0.000016    Time 0.565598    
2023-06-16 04:02:55,913 - Epoch: [150][   70/  142]    Overall Loss 0.456484    Objective Loss 0.456484                                        LR 0.000016    Time 0.554947    
2023-06-16 04:03:00,769 - Epoch: [150][   80/  142]    Overall Loss 0.458459    Objective Loss 0.458459                                        LR 0.000016    Time 0.546271    
2023-06-16 04:03:05,815 - Epoch: [150][   90/  142]    Overall Loss 0.456187    Objective Loss 0.456187                                        LR 0.000016    Time 0.541641    
2023-06-16 04:03:10,644 - Epoch: [150][  100/  142]    Overall Loss 0.456171    Objective Loss 0.456171                                        LR 0.000016    Time 0.535761    
2023-06-16 04:03:15,532 - Epoch: [150][  110/  142]    Overall Loss 0.455094    Objective Loss 0.455094                                        LR 0.000016    Time 0.531481    
2023-06-16 04:03:20,492 - Epoch: [150][  120/  142]    Overall Loss 0.457950    Objective Loss 0.457950                                        LR 0.000016    Time 0.528525    
2023-06-16 04:03:25,477 - Epoch: [150][  130/  142]    Overall Loss 0.449203    Objective Loss 0.449203                                        LR 0.000016    Time 0.526212    
2023-06-16 04:03:30,003 - Epoch: [150][  140/  142]    Overall Loss 0.441952    Objective Loss 0.441952                                        LR 0.000016    Time 0.520948    
2023-06-16 04:03:30,858 - Epoch: [150][  142/  142]    Overall Loss 0.440227    Objective Loss 0.440227    Top1 89.062500    LR 0.000016    Time 0.519634    
2023-06-16 04:03:31,522 - --- validate (epoch=150)-----------
2023-06-16 04:03:31,522 - 1422 samples (32 per mini-batch)
2023-06-16 04:03:39,695 - Epoch: [150][   10/   45]    Loss 1.007277    Top1 72.812500    
2023-06-16 04:03:43,813 - Epoch: [150][   20/   45]    Loss 0.970058    Top1 73.906250    
2023-06-16 04:03:48,040 - Epoch: [150][   30/   45]    Loss 0.945535    Top1 74.166667    
2023-06-16 04:03:52,760 - Epoch: [150][   40/   45]    Loss 0.922341    Top1 74.375000    
2023-06-16 04:03:54,194 - Epoch: [150][   45/   45]    Loss 0.895108    Top1 74.261603    
2023-06-16 04:03:54,856 - ==> Top1: 74.262    Loss: 0.895

2023-06-16 04:03:54,858 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:03:54,858 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:03:54,880 - 

2023-06-16 04:03:54,880 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:04:04,236 - Epoch: [151][   10/  142]    Overall Loss 0.475914    Objective Loss 0.475914                                        LR 0.000016    Time 0.935527    
2023-06-16 04:04:09,174 - Epoch: [151][   20/  142]    Overall Loss 0.437332    Objective Loss 0.437332                                        LR 0.000016    Time 0.714624    
2023-06-16 04:04:14,009 - Epoch: [151][   30/  142]    Overall Loss 0.433160    Objective Loss 0.433160                                        LR 0.000016    Time 0.637581    
2023-06-16 04:04:19,078 - Epoch: [151][   40/  142]    Overall Loss 0.429990    Objective Loss 0.429990                                        LR 0.000016    Time 0.604899    
2023-06-16 04:04:24,069 - Epoch: [151][   50/  142]    Overall Loss 0.423945    Objective Loss 0.423945                                        LR 0.000016    Time 0.583722    
2023-06-16 04:04:28,991 - Epoch: [151][   60/  142]    Overall Loss 0.427617    Objective Loss 0.427617                                        LR 0.000016    Time 0.568460    
2023-06-16 04:04:33,902 - Epoch: [151][   70/  142]    Overall Loss 0.434148    Objective Loss 0.434148                                        LR 0.000016    Time 0.557405    
2023-06-16 04:04:38,848 - Epoch: [151][   80/  142]    Overall Loss 0.442162    Objective Loss 0.442162                                        LR 0.000016    Time 0.549548    
2023-06-16 04:04:43,788 - Epoch: [151][   90/  142]    Overall Loss 0.445581    Objective Loss 0.445581                                        LR 0.000016    Time 0.543364    
2023-06-16 04:04:48,745 - Epoch: [151][  100/  142]    Overall Loss 0.440854    Objective Loss 0.440854                                        LR 0.000016    Time 0.538591    
2023-06-16 04:04:53,719 - Epoch: [151][  110/  142]    Overall Loss 0.440898    Objective Loss 0.440898                                        LR 0.000016    Time 0.534837    
2023-06-16 04:04:58,751 - Epoch: [151][  120/  142]    Overall Loss 0.440062    Objective Loss 0.440062                                        LR 0.000016    Time 0.532198    
2023-06-16 04:05:03,669 - Epoch: [151][  130/  142]    Overall Loss 0.440348    Objective Loss 0.440348                                        LR 0.000016    Time 0.529089    
2023-06-16 04:05:08,351 - Epoch: [151][  140/  142]    Overall Loss 0.440503    Objective Loss 0.440503                                        LR 0.000016    Time 0.524733    
2023-06-16 04:05:09,209 - Epoch: [151][  142/  142]    Overall Loss 0.438986    Objective Loss 0.438986    Top1 82.812500    LR 0.000016    Time 0.523381    
2023-06-16 04:05:09,865 - --- validate (epoch=151)-----------
2023-06-16 04:05:09,866 - 1422 samples (32 per mini-batch)
2023-06-16 04:05:17,769 - Epoch: [151][   10/   45]    Loss 0.782465    Top1 79.062500    
2023-06-16 04:05:21,766 - Epoch: [151][   20/   45]    Loss 0.831377    Top1 77.031250    
2023-06-16 04:05:26,721 - Epoch: [151][   30/   45]    Loss 0.827283    Top1 76.041667    
2023-06-16 04:05:30,979 - Epoch: [151][   40/   45]    Loss 0.865868    Top1 75.156250    
2023-06-16 04:05:32,320 - Epoch: [151][   45/   45]    Loss 0.907999    Top1 74.120956    
2023-06-16 04:05:32,981 - ==> Top1: 74.121    Loss: 0.908

2023-06-16 04:05:32,983 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:05:32,983 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:05:33,004 - 

2023-06-16 04:05:33,004 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:05:42,392 - Epoch: [152][   10/  142]    Overall Loss 0.390608    Objective Loss 0.390608                                        LR 0.000016    Time 0.938697    
2023-06-16 04:05:47,298 - Epoch: [152][   20/  142]    Overall Loss 0.359206    Objective Loss 0.359206                                        LR 0.000016    Time 0.714636    
2023-06-16 04:05:52,170 - Epoch: [152][   30/  142]    Overall Loss 0.378937    Objective Loss 0.378937                                        LR 0.000016    Time 0.638801    
2023-06-16 04:05:57,125 - Epoch: [152][   40/  142]    Overall Loss 0.395038    Objective Loss 0.395038                                        LR 0.000016    Time 0.602959    
2023-06-16 04:06:02,100 - Epoch: [152][   50/  142]    Overall Loss 0.410020    Objective Loss 0.410020                                        LR 0.000016    Time 0.581855    
2023-06-16 04:06:06,918 - Epoch: [152][   60/  142]    Overall Loss 0.420632    Objective Loss 0.420632                                        LR 0.000016    Time 0.565177    
2023-06-16 04:06:11,935 - Epoch: [152][   70/  142]    Overall Loss 0.423568    Objective Loss 0.423568                                        LR 0.000016    Time 0.556100    
2023-06-16 04:06:16,882 - Epoch: [152][   80/  142]    Overall Loss 0.420199    Objective Loss 0.420199                                        LR 0.000016    Time 0.548417    
2023-06-16 04:06:21,849 - Epoch: [152][   90/  142]    Overall Loss 0.425071    Objective Loss 0.425071                                        LR 0.000016    Time 0.542667    
2023-06-16 04:06:26,829 - Epoch: [152][  100/  142]    Overall Loss 0.428935    Objective Loss 0.428935                                        LR 0.000016    Time 0.538195    
2023-06-16 04:06:31,777 - Epoch: [152][  110/  142]    Overall Loss 0.432481    Objective Loss 0.432481                                        LR 0.000016    Time 0.534244    
2023-06-16 04:06:36,781 - Epoch: [152][  120/  142]    Overall Loss 0.442059    Objective Loss 0.442059                                        LR 0.000016    Time 0.531418    
2023-06-16 04:06:41,714 - Epoch: [152][  130/  142]    Overall Loss 0.448969    Objective Loss 0.448969                                        LR 0.000016    Time 0.528481    
2023-06-16 04:06:46,334 - Epoch: [152][  140/  142]    Overall Loss 0.446805    Objective Loss 0.446805                                        LR 0.000016    Time 0.523727    
2023-06-16 04:06:47,191 - Epoch: [152][  142/  142]    Overall Loss 0.446844    Objective Loss 0.446844    Top1 89.062500    LR 0.000016    Time 0.522390    
2023-06-16 04:06:47,806 - --- validate (epoch=152)-----------
2023-06-16 04:06:47,806 - 1422 samples (32 per mini-batch)
2023-06-16 04:06:55,999 - Epoch: [152][   10/   45]    Loss 0.942054    Top1 74.062500    
2023-06-16 04:06:59,970 - Epoch: [152][   20/   45]    Loss 0.984630    Top1 73.750000    
2023-06-16 04:07:05,070 - Epoch: [152][   30/   45]    Loss 0.968772    Top1 73.645833    
2023-06-16 04:07:09,150 - Epoch: [152][   40/   45]    Loss 0.910305    Top1 74.921875    
2023-06-16 04:07:10,560 - Epoch: [152][   45/   45]    Loss 0.928617    Top1 75.105485    
2023-06-16 04:07:11,217 - ==> Top1: 75.105    Loss: 0.929

2023-06-16 04:07:11,219 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:07:11,219 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:07:11,240 - 

2023-06-16 04:07:11,240 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:07:20,759 - Epoch: [153][   10/  142]    Overall Loss 0.497540    Objective Loss 0.497540                                        LR 0.000016    Time 0.951818    
2023-06-16 04:07:25,870 - Epoch: [153][   20/  142]    Overall Loss 0.463593    Objective Loss 0.463593                                        LR 0.000016    Time 0.731399    
2023-06-16 04:07:30,732 - Epoch: [153][   30/  142]    Overall Loss 0.475819    Objective Loss 0.475819                                        LR 0.000016    Time 0.649663    
2023-06-16 04:07:35,767 - Epoch: [153][   40/  142]    Overall Loss 0.477322    Objective Loss 0.477322                                        LR 0.000016    Time 0.613091    
2023-06-16 04:07:40,695 - Epoch: [153][   50/  142]    Overall Loss 0.481606    Objective Loss 0.481606                                        LR 0.000016    Time 0.589037    
2023-06-16 04:07:45,786 - Epoch: [153][   60/  142]    Overall Loss 0.471476    Objective Loss 0.471476                                        LR 0.000016    Time 0.575698    
2023-06-16 04:07:50,804 - Epoch: [153][   70/  142]    Overall Loss 0.458487    Objective Loss 0.458487                                        LR 0.000016    Time 0.565136    
2023-06-16 04:07:55,978 - Epoch: [153][   80/  142]    Overall Loss 0.464702    Objective Loss 0.464702                                        LR 0.000016    Time 0.559160    
2023-06-16 04:08:01,066 - Epoch: [153][   90/  142]    Overall Loss 0.464575    Objective Loss 0.464575                                        LR 0.000016    Time 0.553554    
2023-06-16 04:08:06,186 - Epoch: [153][  100/  142]    Overall Loss 0.467581    Objective Loss 0.467581                                        LR 0.000016    Time 0.549398    
2023-06-16 04:08:11,173 - Epoch: [153][  110/  142]    Overall Loss 0.467902    Objective Loss 0.467902                                        LR 0.000016    Time 0.544783    
2023-06-16 04:08:16,251 - Epoch: [153][  120/  142]    Overall Loss 0.470668    Objective Loss 0.470668                                        LR 0.000016    Time 0.541697    
2023-06-16 04:08:21,291 - Epoch: [153][  130/  142]    Overall Loss 0.469495    Objective Loss 0.469495                                        LR 0.000016    Time 0.538791    
2023-06-16 04:08:25,997 - Epoch: [153][  140/  142]    Overall Loss 0.464051    Objective Loss 0.464051                                        LR 0.000016    Time 0.533917    
2023-06-16 04:08:26,843 - Epoch: [153][  142/  142]    Overall Loss 0.461718    Objective Loss 0.461718    Top1 92.187500    LR 0.000016    Time 0.532353    
2023-06-16 04:08:27,450 - --- validate (epoch=153)-----------
2023-06-16 04:08:27,450 - 1422 samples (32 per mini-batch)
2023-06-16 04:08:35,675 - Epoch: [153][   10/   45]    Loss 0.838652    Top1 75.312500    
2023-06-16 04:08:39,840 - Epoch: [153][   20/   45]    Loss 0.894561    Top1 73.593750    
2023-06-16 04:08:44,586 - Epoch: [153][   30/   45]    Loss 0.924099    Top1 73.437500    
2023-06-16 04:08:48,854 - Epoch: [153][   40/   45]    Loss 0.912906    Top1 73.984375    
2023-06-16 04:08:50,167 - Epoch: [153][   45/   45]    Loss 0.944447    Top1 74.120956    
2023-06-16 04:08:50,775 - ==> Top1: 74.121    Loss: 0.944

2023-06-16 04:08:50,777 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:08:50,777 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:08:50,791 - 

2023-06-16 04:08:50,791 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:08:59,987 - Epoch: [154][   10/  142]    Overall Loss 0.421199    Objective Loss 0.421199                                        LR 0.000016    Time 0.919530    
2023-06-16 04:09:04,935 - Epoch: [154][   20/  142]    Overall Loss 0.392710    Objective Loss 0.392710                                        LR 0.000016    Time 0.707083    
2023-06-16 04:09:09,971 - Epoch: [154][   30/  142]    Overall Loss 0.393868    Objective Loss 0.393868                                        LR 0.000016    Time 0.639236    
2023-06-16 04:09:14,923 - Epoch: [154][   40/  142]    Overall Loss 0.436586    Objective Loss 0.436586                                        LR 0.000016    Time 0.603223    
2023-06-16 04:09:19,948 - Epoch: [154][   50/  142]    Overall Loss 0.434521    Objective Loss 0.434521                                        LR 0.000016    Time 0.583078    
2023-06-16 04:09:24,908 - Epoch: [154][   60/  142]    Overall Loss 0.439134    Objective Loss 0.439134                                        LR 0.000016    Time 0.568555    
2023-06-16 04:09:29,876 - Epoch: [154][   70/  142]    Overall Loss 0.439416    Objective Loss 0.439416                                        LR 0.000016    Time 0.558287    
2023-06-16 04:09:34,859 - Epoch: [154][   80/  142]    Overall Loss 0.445349    Objective Loss 0.445349                                        LR 0.000016    Time 0.550778    
2023-06-16 04:09:39,867 - Epoch: [154][   90/  142]    Overall Loss 0.442691    Objective Loss 0.442691                                        LR 0.000016    Time 0.545221    
2023-06-16 04:09:44,831 - Epoch: [154][  100/  142]    Overall Loss 0.442506    Objective Loss 0.442506                                        LR 0.000016    Time 0.540339    
2023-06-16 04:09:49,872 - Epoch: [154][  110/  142]    Overall Loss 0.446752    Objective Loss 0.446752                                        LR 0.000016    Time 0.537040    
2023-06-16 04:09:54,847 - Epoch: [154][  120/  142]    Overall Loss 0.442557    Objective Loss 0.442557                                        LR 0.000016    Time 0.533736    
2023-06-16 04:09:59,894 - Epoch: [154][  130/  142]    Overall Loss 0.444581    Objective Loss 0.444581                                        LR 0.000016    Time 0.531500    
2023-06-16 04:10:04,469 - Epoch: [154][  140/  142]    Overall Loss 0.442869    Objective Loss 0.442869                                        LR 0.000016    Time 0.526207    
2023-06-16 04:10:05,309 - Epoch: [154][  142/  142]    Overall Loss 0.443536    Objective Loss 0.443536    Top1 81.250000    LR 0.000016    Time 0.524713    
2023-06-16 04:10:05,960 - --- validate (epoch=154)-----------
2023-06-16 04:10:05,961 - 1422 samples (32 per mini-batch)
2023-06-16 04:10:14,169 - Epoch: [154][   10/   45]    Loss 0.809741    Top1 75.312500    
2023-06-16 04:10:18,218 - Epoch: [154][   20/   45]    Loss 0.852860    Top1 76.093750    
2023-06-16 04:10:22,459 - Epoch: [154][   30/   45]    Loss 0.880409    Top1 76.145833    
2023-06-16 04:10:26,637 - Epoch: [154][   40/   45]    Loss 0.890033    Top1 75.546875    
2023-06-16 04:10:28,178 - Epoch: [154][   45/   45]    Loss 0.899361    Top1 75.035162    
2023-06-16 04:10:28,786 - ==> Top1: 75.035    Loss: 0.899

2023-06-16 04:10:28,788 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:10:28,788 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:10:28,809 - 

2023-06-16 04:10:28,809 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:10:38,245 - Epoch: [155][   10/  142]    Overall Loss 0.460390    Objective Loss 0.460390                                        LR 0.000016    Time 0.943501    
2023-06-16 04:10:43,316 - Epoch: [155][   20/  142]    Overall Loss 0.467855    Objective Loss 0.467855                                        LR 0.000016    Time 0.725232    
2023-06-16 04:10:48,373 - Epoch: [155][   30/  142]    Overall Loss 0.470034    Objective Loss 0.470034                                        LR 0.000016    Time 0.652052    
2023-06-16 04:10:53,425 - Epoch: [155][   40/  142]    Overall Loss 0.455597    Objective Loss 0.455597                                        LR 0.000016    Time 0.615320    
2023-06-16 04:10:58,304 - Epoch: [155][   50/  142]    Overall Loss 0.464372    Objective Loss 0.464372                                        LR 0.000016    Time 0.589817    
2023-06-16 04:11:03,287 - Epoch: [155][   60/  142]    Overall Loss 0.465383    Objective Loss 0.465383                                        LR 0.000016    Time 0.574563    
2023-06-16 04:11:08,316 - Epoch: [155][   70/  142]    Overall Loss 0.469874    Objective Loss 0.469874                                        LR 0.000016    Time 0.564320    
2023-06-16 04:11:13,229 - Epoch: [155][   80/  142]    Overall Loss 0.473926    Objective Loss 0.473926                                        LR 0.000016    Time 0.555176    
2023-06-16 04:11:18,320 - Epoch: [155][   90/  142]    Overall Loss 0.465296    Objective Loss 0.465296                                        LR 0.000016    Time 0.550052    
2023-06-16 04:11:23,224 - Epoch: [155][  100/  142]    Overall Loss 0.464959    Objective Loss 0.464959                                        LR 0.000016    Time 0.544088    
2023-06-16 04:11:28,207 - Epoch: [155][  110/  142]    Overall Loss 0.462233    Objective Loss 0.462233                                        LR 0.000016    Time 0.539913    
2023-06-16 04:11:33,174 - Epoch: [155][  120/  142]    Overall Loss 0.459668    Objective Loss 0.459668                                        LR 0.000016    Time 0.536310    
2023-06-16 04:11:38,203 - Epoch: [155][  130/  142]    Overall Loss 0.453516    Objective Loss 0.453516                                        LR 0.000016    Time 0.533733    
2023-06-16 04:11:42,809 - Epoch: [155][  140/  142]    Overall Loss 0.458148    Objective Loss 0.458148                                        LR 0.000016    Time 0.528508    
2023-06-16 04:11:43,649 - Epoch: [155][  142/  142]    Overall Loss 0.457765    Objective Loss 0.457765    Top1 85.937500    LR 0.000016    Time 0.526980    
2023-06-16 04:11:44,288 - --- validate (epoch=155)-----------
2023-06-16 04:11:44,288 - 1422 samples (32 per mini-batch)
2023-06-16 04:11:52,366 - Epoch: [155][   10/   45]    Loss 0.906494    Top1 79.062500    
2023-06-16 04:11:56,891 - Epoch: [155][   20/   45]    Loss 0.941537    Top1 77.031250    
2023-06-16 04:12:00,958 - Epoch: [155][   30/   45]    Loss 0.938630    Top1 75.625000    
2023-06-16 04:12:05,030 - Epoch: [155][   40/   45]    Loss 0.950926    Top1 75.000000    
2023-06-16 04:12:06,513 - Epoch: [155][   45/   45]    Loss 0.964223    Top1 74.331927    
2023-06-16 04:12:07,178 - ==> Top1: 74.332    Loss: 0.964

2023-06-16 04:12:07,180 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:12:07,180 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:12:07,201 - 

2023-06-16 04:12:07,202 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:12:16,548 - Epoch: [156][   10/  142]    Overall Loss 0.417239    Objective Loss 0.417239                                        LR 0.000016    Time 0.934494    
2023-06-16 04:12:21,475 - Epoch: [156][   20/  142]    Overall Loss 0.442516    Objective Loss 0.442516                                        LR 0.000016    Time 0.713554    
2023-06-16 04:12:26,351 - Epoch: [156][   30/  142]    Overall Loss 0.441102    Objective Loss 0.441102                                        LR 0.000016    Time 0.638231    
2023-06-16 04:12:31,421 - Epoch: [156][   40/  142]    Overall Loss 0.441250    Objective Loss 0.441250                                        LR 0.000016    Time 0.605393    
2023-06-16 04:12:36,434 - Epoch: [156][   50/  142]    Overall Loss 0.430582    Objective Loss 0.430582                                        LR 0.000016    Time 0.584562    
2023-06-16 04:12:41,393 - Epoch: [156][   60/  142]    Overall Loss 0.440614    Objective Loss 0.440614                                        LR 0.000016    Time 0.569783    
2023-06-16 04:12:46,351 - Epoch: [156][   70/  142]    Overall Loss 0.442404    Objective Loss 0.442404                                        LR 0.000016    Time 0.559204    
2023-06-16 04:12:51,325 - Epoch: [156][   80/  142]    Overall Loss 0.438224    Objective Loss 0.438224                                        LR 0.000016    Time 0.551457    
2023-06-16 04:12:56,244 - Epoch: [156][   90/  142]    Overall Loss 0.431459    Objective Loss 0.431459                                        LR 0.000016    Time 0.544835    
2023-06-16 04:13:01,205 - Epoch: [156][  100/  142]    Overall Loss 0.425571    Objective Loss 0.425571                                        LR 0.000016    Time 0.539954    
2023-06-16 04:13:06,189 - Epoch: [156][  110/  142]    Overall Loss 0.432613    Objective Loss 0.432613                                        LR 0.000016    Time 0.536172    
2023-06-16 04:13:11,149 - Epoch: [156][  120/  142]    Overall Loss 0.436685    Objective Loss 0.436685                                        LR 0.000016    Time 0.532818    
2023-06-16 04:13:16,104 - Epoch: [156][  130/  142]    Overall Loss 0.441087    Objective Loss 0.441087                                        LR 0.000016    Time 0.529946    
2023-06-16 04:13:20,777 - Epoch: [156][  140/  142]    Overall Loss 0.443272    Objective Loss 0.443272                                        LR 0.000016    Time 0.525468    
2023-06-16 04:13:21,632 - Epoch: [156][  142/  142]    Overall Loss 0.442424    Objective Loss 0.442424    Top1 81.250000    LR 0.000016    Time 0.524089    
2023-06-16 04:13:22,289 - --- validate (epoch=156)-----------
2023-06-16 04:13:22,289 - 1422 samples (32 per mini-batch)
2023-06-16 04:13:30,709 - Epoch: [156][   10/   45]    Loss 0.911403    Top1 73.437500    
2023-06-16 04:13:34,982 - Epoch: [156][   20/   45]    Loss 0.979135    Top1 72.500000    
2023-06-16 04:13:40,302 - Epoch: [156][   30/   45]    Loss 0.926061    Top1 74.479167    
2023-06-16 04:13:44,502 - Epoch: [156][   40/   45]    Loss 0.903572    Top1 73.828125    
2023-06-16 04:13:45,848 - Epoch: [156][   45/   45]    Loss 0.928278    Top1 73.699015    
2023-06-16 04:13:46,484 - ==> Top1: 73.699    Loss: 0.928

2023-06-16 04:13:46,486 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:13:46,486 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:13:46,507 - 

2023-06-16 04:13:46,507 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:13:55,676 - Epoch: [157][   10/  142]    Overall Loss 0.459696    Objective Loss 0.459696                                        LR 0.000016    Time 0.916695    
2023-06-16 04:14:00,606 - Epoch: [157][   20/  142]    Overall Loss 0.464501    Objective Loss 0.464501                                        LR 0.000016    Time 0.704845    
2023-06-16 04:14:05,432 - Epoch: [157][   30/  142]    Overall Loss 0.471815    Objective Loss 0.471815                                        LR 0.000016    Time 0.630716    
2023-06-16 04:14:10,363 - Epoch: [157][   40/  142]    Overall Loss 0.476501    Objective Loss 0.476501                                        LR 0.000016    Time 0.596306    
2023-06-16 04:14:15,312 - Epoch: [157][   50/  142]    Overall Loss 0.479201    Objective Loss 0.479201                                        LR 0.000016    Time 0.576011    
2023-06-16 04:14:20,183 - Epoch: [157][   60/  142]    Overall Loss 0.475939    Objective Loss 0.475939                                        LR 0.000016    Time 0.561183    
2023-06-16 04:14:25,098 - Epoch: [157][   70/  142]    Overall Loss 0.473026    Objective Loss 0.473026                                        LR 0.000016    Time 0.551218    
2023-06-16 04:14:30,007 - Epoch: [157][   80/  142]    Overall Loss 0.467832    Objective Loss 0.467832                                        LR 0.000016    Time 0.543681    
2023-06-16 04:14:34,974 - Epoch: [157][   90/  142]    Overall Loss 0.464833    Objective Loss 0.464833                                        LR 0.000016    Time 0.538450    
2023-06-16 04:14:39,954 - Epoch: [157][  100/  142]    Overall Loss 0.466439    Objective Loss 0.466439                                        LR 0.000016    Time 0.534402    
2023-06-16 04:14:44,785 - Epoch: [157][  110/  142]    Overall Loss 0.469574    Objective Loss 0.469574                                        LR 0.000016    Time 0.529734    
2023-06-16 04:14:49,713 - Epoch: [157][  120/  142]    Overall Loss 0.469286    Objective Loss 0.469286                                        LR 0.000016    Time 0.526653    
2023-06-16 04:14:54,630 - Epoch: [157][  130/  142]    Overall Loss 0.475419    Objective Loss 0.475419                                        LR 0.000016    Time 0.523957    
2023-06-16 04:14:59,202 - Epoch: [157][  140/  142]    Overall Loss 0.471611    Objective Loss 0.471611                                        LR 0.000016    Time 0.519185    
2023-06-16 04:15:00,042 - Epoch: [157][  142/  142]    Overall Loss 0.471182    Objective Loss 0.471182    Top1 82.812500    LR 0.000016    Time 0.517789    
2023-06-16 04:15:00,681 - --- validate (epoch=157)-----------
2023-06-16 04:15:00,682 - 1422 samples (32 per mini-batch)
2023-06-16 04:15:08,777 - Epoch: [157][   10/   45]    Loss 0.802776    Top1 78.125000    
2023-06-16 04:15:12,825 - Epoch: [157][   20/   45]    Loss 0.861360    Top1 77.187500    
2023-06-16 04:15:17,474 - Epoch: [157][   30/   45]    Loss 0.923760    Top1 75.208333    
2023-06-16 04:15:21,601 - Epoch: [157][   40/   45]    Loss 0.896888    Top1 75.078125    
2023-06-16 04:15:23,251 - Epoch: [157][   45/   45]    Loss 0.886340    Top1 74.824191    
2023-06-16 04:15:23,901 - ==> Top1: 74.824    Loss: 0.886

2023-06-16 04:15:23,903 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:15:23,904 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:15:23,925 - 

2023-06-16 04:15:23,925 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:15:33,426 - Epoch: [158][   10/  142]    Overall Loss 0.425509    Objective Loss 0.425509                                        LR 0.000016    Time 0.949960    
2023-06-16 04:15:38,497 - Epoch: [158][   20/  142]    Overall Loss 0.429253    Objective Loss 0.429253                                        LR 0.000016    Time 0.728518    
2023-06-16 04:15:43,579 - Epoch: [158][   30/  142]    Overall Loss 0.444553    Objective Loss 0.444553                                        LR 0.000016    Time 0.655044    
2023-06-16 04:15:48,632 - Epoch: [158][   40/  142]    Overall Loss 0.432742    Objective Loss 0.432742                                        LR 0.000016    Time 0.617588    
2023-06-16 04:15:53,655 - Epoch: [158][   50/  142]    Overall Loss 0.452766    Objective Loss 0.452766                                        LR 0.000016    Time 0.594524    
2023-06-16 04:15:58,682 - Epoch: [158][   60/  142]    Overall Loss 0.461580    Objective Loss 0.461580                                        LR 0.000016    Time 0.579219    
2023-06-16 04:16:03,842 - Epoch: [158][   70/  142]    Overall Loss 0.449395    Objective Loss 0.449395                                        LR 0.000016    Time 0.570174    
2023-06-16 04:16:08,838 - Epoch: [158][   80/  142]    Overall Loss 0.456801    Objective Loss 0.456801                                        LR 0.000016    Time 0.561346    
2023-06-16 04:16:13,902 - Epoch: [158][   90/  142]    Overall Loss 0.452776    Objective Loss 0.452776                                        LR 0.000016    Time 0.555230    
2023-06-16 04:16:18,976 - Epoch: [158][  100/  142]    Overall Loss 0.449942    Objective Loss 0.449942                                        LR 0.000016    Time 0.550447    
2023-06-16 04:16:24,012 - Epoch: [158][  110/  142]    Overall Loss 0.449099    Objective Loss 0.449099                                        LR 0.000016    Time 0.546180    
2023-06-16 04:16:29,081 - Epoch: [158][  120/  142]    Overall Loss 0.445982    Objective Loss 0.445982                                        LR 0.000016    Time 0.542907    
2023-06-16 04:16:34,171 - Epoch: [158][  130/  142]    Overall Loss 0.444645    Objective Loss 0.444645                                        LR 0.000016    Time 0.540290    
2023-06-16 04:16:38,762 - Epoch: [158][  140/  142]    Overall Loss 0.439647    Objective Loss 0.439647                                        LR 0.000016    Time 0.534490    
2023-06-16 04:16:39,618 - Epoch: [158][  142/  142]    Overall Loss 0.439253    Objective Loss 0.439253    Top1 85.937500    LR 0.000016    Time 0.532988    
2023-06-16 04:16:40,248 - --- validate (epoch=158)-----------
2023-06-16 04:16:40,248 - 1422 samples (32 per mini-batch)
2023-06-16 04:16:48,396 - Epoch: [158][   10/   45]    Loss 0.932852    Top1 73.750000    
2023-06-16 04:16:52,613 - Epoch: [158][   20/   45]    Loss 0.893488    Top1 75.468750    
2023-06-16 04:16:57,114 - Epoch: [158][   30/   45]    Loss 0.904624    Top1 75.520833    
2023-06-16 04:17:01,494 - Epoch: [158][   40/   45]    Loss 0.868598    Top1 75.312500    
2023-06-16 04:17:02,900 - Epoch: [158][   45/   45]    Loss 0.882439    Top1 74.542897    
2023-06-16 04:17:03,546 - ==> Top1: 74.543    Loss: 0.882

2023-06-16 04:17:03,548 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:17:03,549 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:17:03,570 - 

2023-06-16 04:17:03,570 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:17:12,964 - Epoch: [159][   10/  142]    Overall Loss 0.426663    Objective Loss 0.426663                                        LR 0.000016    Time 0.939339    
2023-06-16 04:17:17,824 - Epoch: [159][   20/  142]    Overall Loss 0.440738    Objective Loss 0.440738                                        LR 0.000016    Time 0.712644    
2023-06-16 04:17:22,771 - Epoch: [159][   30/  142]    Overall Loss 0.434934    Objective Loss 0.434934                                        LR 0.000016    Time 0.639959    
2023-06-16 04:17:27,667 - Epoch: [159][   40/  142]    Overall Loss 0.435422    Objective Loss 0.435422                                        LR 0.000016    Time 0.602344    
2023-06-16 04:17:32,625 - Epoch: [159][   50/  142]    Overall Loss 0.415465    Objective Loss 0.415465                                        LR 0.000016    Time 0.581033    
2023-06-16 04:17:37,577 - Epoch: [159][   60/  142]    Overall Loss 0.438410    Objective Loss 0.438410                                        LR 0.000016    Time 0.566717    
2023-06-16 04:17:42,472 - Epoch: [159][   70/  142]    Overall Loss 0.446998    Objective Loss 0.446998                                        LR 0.000016    Time 0.555677    
2023-06-16 04:17:47,361 - Epoch: [159][   80/  142]    Overall Loss 0.440794    Objective Loss 0.440794                                        LR 0.000016    Time 0.547325    
2023-06-16 04:17:52,322 - Epoch: [159][   90/  142]    Overall Loss 0.438309    Objective Loss 0.438309                                        LR 0.000016    Time 0.541629    
2023-06-16 04:17:57,286 - Epoch: [159][  100/  142]    Overall Loss 0.433207    Objective Loss 0.433207                                        LR 0.000016    Time 0.537093    
2023-06-16 04:18:02,220 - Epoch: [159][  110/  142]    Overall Loss 0.441009    Objective Loss 0.441009                                        LR 0.000016    Time 0.533125    
2023-06-16 04:18:07,032 - Epoch: [159][  120/  142]    Overall Loss 0.437196    Objective Loss 0.437196                                        LR 0.000016    Time 0.528791    
2023-06-16 04:18:11,937 - Epoch: [159][  130/  142]    Overall Loss 0.434858    Objective Loss 0.434858                                        LR 0.000016    Time 0.525845    
2023-06-16 04:18:16,584 - Epoch: [159][  140/  142]    Overall Loss 0.439119    Objective Loss 0.439119                                        LR 0.000016    Time 0.521470    
2023-06-16 04:18:17,440 - Epoch: [159][  142/  142]    Overall Loss 0.440354    Objective Loss 0.440354    Top1 81.250000    LR 0.000016    Time 0.520153    
2023-06-16 04:18:18,071 - --- validate (epoch=159)-----------
2023-06-16 04:18:18,071 - 1422 samples (32 per mini-batch)
2023-06-16 04:18:25,929 - Epoch: [159][   10/   45]    Loss 0.942421    Top1 73.437500    
2023-06-16 04:18:30,942 - Epoch: [159][   20/   45]    Loss 0.896105    Top1 74.531250    
2023-06-16 04:18:35,403 - Epoch: [159][   30/   45]    Loss 0.914365    Top1 74.791667    
2023-06-16 04:18:39,507 - Epoch: [159][   40/   45]    Loss 0.895693    Top1 75.000000    
2023-06-16 04:18:40,904 - Epoch: [159][   45/   45]    Loss 0.903636    Top1 74.542897    
2023-06-16 04:18:41,502 - ==> Top1: 74.543    Loss: 0.904

2023-06-16 04:18:41,504 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:18:41,504 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:18:41,525 - 

2023-06-16 04:18:41,525 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:18:50,997 - Epoch: [160][   10/  142]    Overall Loss 0.429603    Objective Loss 0.429603                                        LR 0.000008    Time 0.947049    
2023-06-16 04:18:56,005 - Epoch: [160][   20/  142]    Overall Loss 0.447803    Objective Loss 0.447803                                        LR 0.000008    Time 0.723925    
2023-06-16 04:19:00,947 - Epoch: [160][   30/  142]    Overall Loss 0.464563    Objective Loss 0.464563                                        LR 0.000008    Time 0.647310    
2023-06-16 04:19:06,111 - Epoch: [160][   40/  142]    Overall Loss 0.473850    Objective Loss 0.473850                                        LR 0.000008    Time 0.614562    
2023-06-16 04:19:11,159 - Epoch: [160][   50/  142]    Overall Loss 0.471054    Objective Loss 0.471054                                        LR 0.000008    Time 0.592611    
2023-06-16 04:19:16,199 - Epoch: [160][   60/  142]    Overall Loss 0.465731    Objective Loss 0.465731                                        LR 0.000008    Time 0.577828    
2023-06-16 04:19:21,378 - Epoch: [160][   70/  142]    Overall Loss 0.467825    Objective Loss 0.467825                                        LR 0.000008    Time 0.569264    
2023-06-16 04:19:26,390 - Epoch: [160][   80/  142]    Overall Loss 0.460366    Objective Loss 0.460366                                        LR 0.000008    Time 0.560745    
2023-06-16 04:19:31,432 - Epoch: [160][   90/  142]    Overall Loss 0.458463    Objective Loss 0.458463                                        LR 0.000008    Time 0.554453    
2023-06-16 04:19:36,502 - Epoch: [160][  100/  142]    Overall Loss 0.457128    Objective Loss 0.457128                                        LR 0.000008    Time 0.549707    
2023-06-16 04:19:41,568 - Epoch: [160][  110/  142]    Overall Loss 0.457020    Objective Loss 0.457020                                        LR 0.000008    Time 0.545781    
2023-06-16 04:19:46,692 - Epoch: [160][  120/  142]    Overall Loss 0.458748    Objective Loss 0.458748                                        LR 0.000008    Time 0.542993    
2023-06-16 04:19:51,824 - Epoch: [160][  130/  142]    Overall Loss 0.453524    Objective Loss 0.453524                                        LR 0.000008    Time 0.540699    
2023-06-16 04:19:56,490 - Epoch: [160][  140/  142]    Overall Loss 0.454154    Objective Loss 0.454154                                        LR 0.000008    Time 0.535404    
2023-06-16 04:19:57,347 - Epoch: [160][  142/  142]    Overall Loss 0.454306    Objective Loss 0.454306    Top1 84.375000    LR 0.000008    Time 0.533899    
2023-06-16 04:19:57,939 - --- validate (epoch=160)-----------
2023-06-16 04:19:57,940 - 1422 samples (32 per mini-batch)
2023-06-16 04:20:05,914 - Epoch: [160][   10/   45]    Loss 0.864511    Top1 76.562500    
2023-06-16 04:20:10,332 - Epoch: [160][   20/   45]    Loss 0.821372    Top1 76.718750    
2023-06-16 04:20:14,899 - Epoch: [160][   30/   45]    Loss 0.880373    Top1 75.729167    
2023-06-16 04:20:19,488 - Epoch: [160][   40/   45]    Loss 0.898757    Top1 74.921875    
2023-06-16 04:20:20,888 - Epoch: [160][   45/   45]    Loss 0.895547    Top1 74.824191    
2023-06-16 04:20:21,483 - ==> Top1: 74.824    Loss: 0.896

2023-06-16 04:20:21,485 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:20:21,485 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:20:21,506 - 

2023-06-16 04:20:21,506 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:20:30,837 - Epoch: [161][   10/  142]    Overall Loss 0.360131    Objective Loss 0.360131                                        LR 0.000008    Time 0.932931    
2023-06-16 04:20:35,727 - Epoch: [161][   20/  142]    Overall Loss 0.366365    Objective Loss 0.366365                                        LR 0.000008    Time 0.710929    
2023-06-16 04:20:40,555 - Epoch: [161][   30/  142]    Overall Loss 0.414534    Objective Loss 0.414534                                        LR 0.000008    Time 0.634875    
2023-06-16 04:20:45,528 - Epoch: [161][   40/  142]    Overall Loss 0.422991    Objective Loss 0.422991                                        LR 0.000008    Time 0.600464    
2023-06-16 04:20:50,465 - Epoch: [161][   50/  142]    Overall Loss 0.428188    Objective Loss 0.428188                                        LR 0.000008    Time 0.579105    
2023-06-16 04:20:55,395 - Epoch: [161][   60/  142]    Overall Loss 0.445053    Objective Loss 0.445053                                        LR 0.000008    Time 0.564742    
2023-06-16 04:21:00,433 - Epoch: [161][   70/  142]    Overall Loss 0.438357    Objective Loss 0.438357                                        LR 0.000008    Time 0.556019    
2023-06-16 04:21:05,446 - Epoch: [161][   80/  142]    Overall Loss 0.434582    Objective Loss 0.434582                                        LR 0.000008    Time 0.548529    
2023-06-16 04:21:10,362 - Epoch: [161][   90/  142]    Overall Loss 0.430446    Objective Loss 0.430446                                        LR 0.000008    Time 0.542206    
2023-06-16 04:21:15,248 - Epoch: [161][  100/  142]    Overall Loss 0.428601    Objective Loss 0.428601                                        LR 0.000008    Time 0.536841    
2023-06-16 04:21:20,266 - Epoch: [161][  110/  142]    Overall Loss 0.432175    Objective Loss 0.432175                                        LR 0.000008    Time 0.533643    
2023-06-16 04:21:25,271 - Epoch: [161][  120/  142]    Overall Loss 0.433000    Objective Loss 0.433000                                        LR 0.000008    Time 0.530879    
2023-06-16 04:21:30,285 - Epoch: [161][  130/  142]    Overall Loss 0.435278    Objective Loss 0.435278                                        LR 0.000008    Time 0.528606    
2023-06-16 04:21:34,858 - Epoch: [161][  140/  142]    Overall Loss 0.430746    Objective Loss 0.430746                                        LR 0.000008    Time 0.523514    
2023-06-16 04:21:35,715 - Epoch: [161][  142/  142]    Overall Loss 0.430298    Objective Loss 0.430298    Top1 89.062500    LR 0.000008    Time 0.522171    
2023-06-16 04:21:36,342 - --- validate (epoch=161)-----------
2023-06-16 04:21:36,342 - 1422 samples (32 per mini-batch)
2023-06-16 04:21:44,647 - Epoch: [161][   10/   45]    Loss 0.847833    Top1 75.000000    
2023-06-16 04:21:48,663 - Epoch: [161][   20/   45]    Loss 0.897884    Top1 75.312500    
2023-06-16 04:21:53,165 - Epoch: [161][   30/   45]    Loss 0.922745    Top1 74.583333    
2023-06-16 04:21:57,728 - Epoch: [161][   40/   45]    Loss 0.956518    Top1 74.609375    
2023-06-16 04:21:59,122 - Epoch: [161][   45/   45]    Loss 0.909711    Top1 75.105485    
2023-06-16 04:21:59,780 - ==> Top1: 75.105    Loss: 0.910

2023-06-16 04:21:59,782 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:21:59,782 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:21:59,804 - 

2023-06-16 04:21:59,804 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:22:09,027 - Epoch: [162][   10/  142]    Overall Loss 0.352815    Objective Loss 0.352815                                        LR 0.000008    Time 0.922148    
2023-06-16 04:22:14,125 - Epoch: [162][   20/  142]    Overall Loss 0.371397    Objective Loss 0.371397                                        LR 0.000008    Time 0.715923    
2023-06-16 04:22:18,989 - Epoch: [162][   30/  142]    Overall Loss 0.401406    Objective Loss 0.401406                                        LR 0.000008    Time 0.639394    
2023-06-16 04:22:23,887 - Epoch: [162][   40/  142]    Overall Loss 0.419737    Objective Loss 0.419737                                        LR 0.000008    Time 0.601983    
2023-06-16 04:22:28,780 - Epoch: [162][   50/  142]    Overall Loss 0.434547    Objective Loss 0.434547                                        LR 0.000008    Time 0.579445    
2023-06-16 04:22:33,614 - Epoch: [162][   60/  142]    Overall Loss 0.434671    Objective Loss 0.434671                                        LR 0.000008    Time 0.563434    
2023-06-16 04:22:38,535 - Epoch: [162][   70/  142]    Overall Loss 0.431035    Objective Loss 0.431035                                        LR 0.000008    Time 0.553234    
2023-06-16 04:22:43,472 - Epoch: [162][   80/  142]    Overall Loss 0.429711    Objective Loss 0.429711                                        LR 0.000008    Time 0.545785    
2023-06-16 04:22:48,463 - Epoch: [162][   90/  142]    Overall Loss 0.429104    Objective Loss 0.429104                                        LR 0.000008    Time 0.540588    
2023-06-16 04:22:53,360 - Epoch: [162][  100/  142]    Overall Loss 0.434694    Objective Loss 0.434694                                        LR 0.000008    Time 0.535492    
2023-06-16 04:22:58,282 - Epoch: [162][  110/  142]    Overall Loss 0.437333    Objective Loss 0.437333                                        LR 0.000008    Time 0.531549    
2023-06-16 04:23:03,166 - Epoch: [162][  120/  142]    Overall Loss 0.442195    Objective Loss 0.442195                                        LR 0.000008    Time 0.527952    
2023-06-16 04:23:08,151 - Epoch: [162][  130/  142]    Overall Loss 0.442793    Objective Loss 0.442793                                        LR 0.000008    Time 0.525682    
2023-06-16 04:23:12,817 - Epoch: [162][  140/  142]    Overall Loss 0.444058    Objective Loss 0.444058                                        LR 0.000008    Time 0.521461    
2023-06-16 04:23:13,660 - Epoch: [162][  142/  142]    Overall Loss 0.444732    Objective Loss 0.444732    Top1 84.375000    LR 0.000008    Time 0.520049    
2023-06-16 04:23:14,307 - --- validate (epoch=162)-----------
2023-06-16 04:23:14,307 - 1422 samples (32 per mini-batch)
2023-06-16 04:23:22,444 - Epoch: [162][   10/   45]    Loss 0.848436    Top1 74.062500    
2023-06-16 04:23:26,460 - Epoch: [162][   20/   45]    Loss 0.925997    Top1 73.125000    
2023-06-16 04:23:30,823 - Epoch: [162][   30/   45]    Loss 0.936217    Top1 73.125000    
2023-06-16 04:23:35,333 - Epoch: [162][   40/   45]    Loss 0.914772    Top1 74.453125    
2023-06-16 04:23:36,654 - Epoch: [162][   45/   45]    Loss 0.911947    Top1 74.402250    
2023-06-16 04:23:37,306 - ==> Top1: 74.402    Loss: 0.912

2023-06-16 04:23:37,309 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:23:37,309 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:23:37,330 - 

2023-06-16 04:23:37,330 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:23:46,664 - Epoch: [163][   10/  142]    Overall Loss 0.503761    Objective Loss 0.503761                                        LR 0.000008    Time 0.933350    
2023-06-16 04:23:51,661 - Epoch: [163][   20/  142]    Overall Loss 0.432700    Objective Loss 0.432700                                        LR 0.000008    Time 0.716489    
2023-06-16 04:23:56,668 - Epoch: [163][   30/  142]    Overall Loss 0.449546    Objective Loss 0.449546                                        LR 0.000008    Time 0.644519    
2023-06-16 04:24:01,550 - Epoch: [163][   40/  142]    Overall Loss 0.445930    Objective Loss 0.445930                                        LR 0.000008    Time 0.605427    
2023-06-16 04:24:06,505 - Epoch: [163][   50/  142]    Overall Loss 0.447912    Objective Loss 0.447912                                        LR 0.000008    Time 0.583421    
2023-06-16 04:24:11,454 - Epoch: [163][   60/  142]    Overall Loss 0.465015    Objective Loss 0.465015                                        LR 0.000008    Time 0.568660    
2023-06-16 04:24:16,526 - Epoch: [163][   70/  142]    Overall Loss 0.454594    Objective Loss 0.454594                                        LR 0.000008    Time 0.559877    
2023-06-16 04:24:21,498 - Epoch: [163][   80/  142]    Overall Loss 0.447675    Objective Loss 0.447675                                        LR 0.000008    Time 0.552038    
2023-06-16 04:24:26,328 - Epoch: [163][   90/  142]    Overall Loss 0.437456    Objective Loss 0.437456                                        LR 0.000008    Time 0.544355    
2023-06-16 04:24:31,283 - Epoch: [163][  100/  142]    Overall Loss 0.441689    Objective Loss 0.441689                                        LR 0.000008    Time 0.539466    
2023-06-16 04:24:36,278 - Epoch: [163][  110/  142]    Overall Loss 0.438279    Objective Loss 0.438279                                        LR 0.000008    Time 0.535829    
2023-06-16 04:24:41,186 - Epoch: [163][  120/  142]    Overall Loss 0.439166    Objective Loss 0.439166                                        LR 0.000008    Time 0.532076    
2023-06-16 04:24:46,101 - Epoch: [163][  130/  142]    Overall Loss 0.444552    Objective Loss 0.444552                                        LR 0.000008    Time 0.528946    
2023-06-16 04:24:50,770 - Epoch: [163][  140/  142]    Overall Loss 0.439746    Objective Loss 0.439746                                        LR 0.000008    Time 0.524509    
2023-06-16 04:24:51,623 - Epoch: [163][  142/  142]    Overall Loss 0.438808    Objective Loss 0.438808    Top1 90.625000    LR 0.000008    Time 0.523131    
2023-06-16 04:24:52,272 - --- validate (epoch=163)-----------
2023-06-16 04:24:52,273 - 1422 samples (32 per mini-batch)
2023-06-16 04:25:00,178 - Epoch: [163][   10/   45]    Loss 0.869603    Top1 75.000000    
2023-06-16 04:25:04,533 - Epoch: [163][   20/   45]    Loss 0.938854    Top1 75.312500    
2023-06-16 04:25:08,596 - Epoch: [163][   30/   45]    Loss 0.914312    Top1 74.687500    
2023-06-16 04:25:12,828 - Epoch: [163][   40/   45]    Loss 0.907374    Top1 75.390625    
2023-06-16 04:25:14,353 - Epoch: [163][   45/   45]    Loss 0.901402    Top1 74.683544    
2023-06-16 04:25:15,008 - ==> Top1: 74.684    Loss: 0.901

2023-06-16 04:25:15,011 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:25:15,011 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:25:15,032 - 

2023-06-16 04:25:15,032 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:25:24,445 - Epoch: [164][   10/  142]    Overall Loss 0.474618    Objective Loss 0.474618                                        LR 0.000008    Time 0.941165    
2023-06-16 04:25:29,331 - Epoch: [164][   20/  142]    Overall Loss 0.464313    Objective Loss 0.464313                                        LR 0.000008    Time 0.714854    
2023-06-16 04:25:34,329 - Epoch: [164][   30/  142]    Overall Loss 0.455618    Objective Loss 0.455618                                        LR 0.000008    Time 0.643129    
2023-06-16 04:25:39,335 - Epoch: [164][   40/  142]    Overall Loss 0.467211    Objective Loss 0.467211                                        LR 0.000008    Time 0.607484    
2023-06-16 04:25:44,222 - Epoch: [164][   50/  142]    Overall Loss 0.446703    Objective Loss 0.446703                                        LR 0.000008    Time 0.583722    
2023-06-16 04:25:49,221 - Epoch: [164][   60/  142]    Overall Loss 0.439782    Objective Loss 0.439782                                        LR 0.000008    Time 0.569754    
2023-06-16 04:25:54,074 - Epoch: [164][   70/  142]    Overall Loss 0.446298    Objective Loss 0.446298                                        LR 0.000008    Time 0.557680    
2023-06-16 04:25:59,026 - Epoch: [164][   80/  142]    Overall Loss 0.436568    Objective Loss 0.436568                                        LR 0.000008    Time 0.549857    
2023-06-16 04:26:03,937 - Epoch: [164][   90/  142]    Overall Loss 0.434747    Objective Loss 0.434747                                        LR 0.000008    Time 0.543326    
2023-06-16 04:26:08,922 - Epoch: [164][  100/  142]    Overall Loss 0.430829    Objective Loss 0.430829                                        LR 0.000008    Time 0.538840    
2023-06-16 04:26:13,884 - Epoch: [164][  110/  142]    Overall Loss 0.436525    Objective Loss 0.436525                                        LR 0.000008    Time 0.534959    
2023-06-16 04:26:18,734 - Epoch: [164][  120/  142]    Overall Loss 0.439459    Objective Loss 0.439459                                        LR 0.000008    Time 0.530787    
2023-06-16 04:26:23,815 - Epoch: [164][  130/  142]    Overall Loss 0.443510    Objective Loss 0.443510                                        LR 0.000008    Time 0.529035    
2023-06-16 04:26:28,352 - Epoch: [164][  140/  142]    Overall Loss 0.443044    Objective Loss 0.443044                                        LR 0.000008    Time 0.523651    
2023-06-16 04:26:29,206 - Epoch: [164][  142/  142]    Overall Loss 0.441138    Objective Loss 0.441138    Top1 90.625000    LR 0.000008    Time 0.522294    
2023-06-16 04:26:29,851 - --- validate (epoch=164)-----------
2023-06-16 04:26:29,851 - 1422 samples (32 per mini-batch)
2023-06-16 04:26:37,830 - Epoch: [164][   10/   45]    Loss 0.898067    Top1 78.750000    
2023-06-16 04:26:41,964 - Epoch: [164][   20/   45]    Loss 0.882403    Top1 76.562500    
2023-06-16 04:26:46,717 - Epoch: [164][   30/   45]    Loss 0.870389    Top1 76.458333    
2023-06-16 04:26:51,035 - Epoch: [164][   40/   45]    Loss 0.904839    Top1 74.609375    
2023-06-16 04:26:52,403 - Epoch: [164][   45/   45]    Loss 0.904756    Top1 74.824191    
2023-06-16 04:26:53,019 - ==> Top1: 74.824    Loss: 0.905

2023-06-16 04:26:53,022 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:26:53,022 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:26:53,043 - 

2023-06-16 04:26:53,043 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:27:02,558 - Epoch: [165][   10/  142]    Overall Loss 0.483498    Objective Loss 0.483498                                        LR 0.000008    Time 0.951402    
2023-06-16 04:27:07,517 - Epoch: [165][   20/  142]    Overall Loss 0.448567    Objective Loss 0.448567                                        LR 0.000008    Time 0.723605    
2023-06-16 04:27:12,394 - Epoch: [165][   30/  142]    Overall Loss 0.450122    Objective Loss 0.450122                                        LR 0.000008    Time 0.644952    
2023-06-16 04:27:17,402 - Epoch: [165][   40/  142]    Overall Loss 0.451022    Objective Loss 0.451022                                        LR 0.000008    Time 0.608892    
2023-06-16 04:27:22,377 - Epoch: [165][   50/  142]    Overall Loss 0.449297    Objective Loss 0.449297                                        LR 0.000008    Time 0.586599    
2023-06-16 04:27:27,280 - Epoch: [165][   60/  142]    Overall Loss 0.449707    Objective Loss 0.449707                                        LR 0.000008    Time 0.570537    
2023-06-16 04:27:32,185 - Epoch: [165][   70/  142]    Overall Loss 0.456637    Objective Loss 0.456637                                        LR 0.000008    Time 0.559094    
2023-06-16 04:27:37,120 - Epoch: [165][   80/  142]    Overall Loss 0.446569    Objective Loss 0.446569                                        LR 0.000008    Time 0.550885    
2023-06-16 04:27:42,022 - Epoch: [165][   90/  142]    Overall Loss 0.439245    Objective Loss 0.439245                                        LR 0.000008    Time 0.544136    
2023-06-16 04:27:47,061 - Epoch: [165][  100/  142]    Overall Loss 0.433585    Objective Loss 0.433585                                        LR 0.000008    Time 0.540106    
2023-06-16 04:27:51,891 - Epoch: [165][  110/  142]    Overall Loss 0.430531    Objective Loss 0.430531                                        LR 0.000008    Time 0.534910    
2023-06-16 04:27:56,919 - Epoch: [165][  120/  142]    Overall Loss 0.428789    Objective Loss 0.428789                                        LR 0.000008    Time 0.532234    
2023-06-16 04:28:01,840 - Epoch: [165][  130/  142]    Overall Loss 0.428894    Objective Loss 0.428894                                        LR 0.000008    Time 0.529139    
2023-06-16 04:28:06,448 - Epoch: [165][  140/  142]    Overall Loss 0.434673    Objective Loss 0.434673                                        LR 0.000008    Time 0.523856    
2023-06-16 04:28:07,294 - Epoch: [165][  142/  142]    Overall Loss 0.435484    Objective Loss 0.435484    Top1 85.937500    LR 0.000008    Time 0.522430    
2023-06-16 04:28:07,953 - --- validate (epoch=165)-----------
2023-06-16 04:28:07,954 - 1422 samples (32 per mini-batch)
2023-06-16 04:28:15,828 - Epoch: [165][   10/   45]    Loss 0.892071    Top1 76.250000    
2023-06-16 04:28:19,888 - Epoch: [165][   20/   45]    Loss 0.876959    Top1 75.468750    
2023-06-16 04:28:24,533 - Epoch: [165][   30/   45]    Loss 0.910279    Top1 75.208333    
2023-06-16 04:28:28,835 - Epoch: [165][   40/   45]    Loss 0.899258    Top1 75.000000    
2023-06-16 04:28:30,197 - Epoch: [165][   45/   45]    Loss 0.895133    Top1 75.035162    
2023-06-16 04:28:30,854 - ==> Top1: 75.035    Loss: 0.895

2023-06-16 04:28:30,856 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:28:30,856 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:28:30,877 - 

2023-06-16 04:28:30,877 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:28:40,076 - Epoch: [166][   10/  142]    Overall Loss 0.382825    Objective Loss 0.382825                                        LR 0.000008    Time 0.919779    
2023-06-16 04:28:44,990 - Epoch: [166][   20/  142]    Overall Loss 0.399014    Objective Loss 0.399014                                        LR 0.000008    Time 0.705521    
2023-06-16 04:28:49,806 - Epoch: [166][   30/  142]    Overall Loss 0.427651    Objective Loss 0.427651                                        LR 0.000008    Time 0.630853    
2023-06-16 04:28:54,627 - Epoch: [166][   40/  142]    Overall Loss 0.424948    Objective Loss 0.424948                                        LR 0.000008    Time 0.593651    
2023-06-16 04:28:59,590 - Epoch: [166][   50/  142]    Overall Loss 0.437216    Objective Loss 0.437216                                        LR 0.000008    Time 0.574166    
2023-06-16 04:29:04,406 - Epoch: [166][   60/  142]    Overall Loss 0.433152    Objective Loss 0.433152                                        LR 0.000008    Time 0.558746    
2023-06-16 04:29:09,425 - Epoch: [166][   70/  142]    Overall Loss 0.425281    Objective Loss 0.425281                                        LR 0.000008    Time 0.550614    
2023-06-16 04:29:14,247 - Epoch: [166][   80/  142]    Overall Loss 0.425880    Objective Loss 0.425880                                        LR 0.000008    Time 0.542059    
2023-06-16 04:29:19,336 - Epoch: [166][   90/  142]    Overall Loss 0.423983    Objective Loss 0.423983                                        LR 0.000008    Time 0.538367    
2023-06-16 04:29:24,199 - Epoch: [166][  100/  142]    Overall Loss 0.430562    Objective Loss 0.430562                                        LR 0.000008    Time 0.533148    
2023-06-16 04:29:28,994 - Epoch: [166][  110/  142]    Overall Loss 0.430807    Objective Loss 0.430807                                        LR 0.000008    Time 0.528266    
2023-06-16 04:29:33,953 - Epoch: [166][  120/  142]    Overall Loss 0.431574    Objective Loss 0.431574                                        LR 0.000008    Time 0.525565    
2023-06-16 04:29:38,781 - Epoch: [166][  130/  142]    Overall Loss 0.428774    Objective Loss 0.428774                                        LR 0.000008    Time 0.522273    
2023-06-16 04:29:43,356 - Epoch: [166][  140/  142]    Overall Loss 0.431508    Objective Loss 0.431508                                        LR 0.000008    Time 0.517643    
2023-06-16 04:29:44,196 - Epoch: [166][  142/  142]    Overall Loss 0.430320    Objective Loss 0.430320    Top1 84.375000    LR 0.000008    Time 0.516267    
2023-06-16 04:29:44,830 - --- validate (epoch=166)-----------
2023-06-16 04:29:44,831 - 1422 samples (32 per mini-batch)
2023-06-16 04:29:52,989 - Epoch: [166][   10/   45]    Loss 0.969577    Top1 73.437500    
2023-06-16 04:29:56,947 - Epoch: [166][   20/   45]    Loss 0.903924    Top1 76.093750    
2023-06-16 04:30:01,251 - Epoch: [166][   30/   45]    Loss 0.899388    Top1 74.895833    
2023-06-16 04:30:05,924 - Epoch: [166][   40/   45]    Loss 0.895551    Top1 75.000000    
2023-06-16 04:30:07,271 - Epoch: [166][   45/   45]    Loss 0.903909    Top1 75.105485    
2023-06-16 04:30:07,854 - ==> Top1: 75.105    Loss: 0.904

2023-06-16 04:30:07,856 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:30:07,856 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:30:07,877 - 

2023-06-16 04:30:07,877 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:30:17,097 - Epoch: [167][   10/  142]    Overall Loss 0.427146    Objective Loss 0.427146                                        LR 0.000008    Time 0.921831    
2023-06-16 04:30:22,039 - Epoch: [167][   20/  142]    Overall Loss 0.456806    Objective Loss 0.456806                                        LR 0.000008    Time 0.707973    
2023-06-16 04:30:26,941 - Epoch: [167][   30/  142]    Overall Loss 0.453186    Objective Loss 0.453186                                        LR 0.000008    Time 0.635358    
2023-06-16 04:30:31,928 - Epoch: [167][   40/  142]    Overall Loss 0.458035    Objective Loss 0.458035                                        LR 0.000008    Time 0.601191    
2023-06-16 04:30:36,967 - Epoch: [167][   50/  142]    Overall Loss 0.443989    Objective Loss 0.443989                                        LR 0.000008    Time 0.581716    
2023-06-16 04:30:41,966 - Epoch: [167][   60/  142]    Overall Loss 0.447100    Objective Loss 0.447100                                        LR 0.000008    Time 0.568067    
2023-06-16 04:30:46,896 - Epoch: [167][   70/  142]    Overall Loss 0.453681    Objective Loss 0.453681                                        LR 0.000008    Time 0.557338    
2023-06-16 04:30:51,849 - Epoch: [167][   80/  142]    Overall Loss 0.438314    Objective Loss 0.438314                                        LR 0.000008    Time 0.549579    
2023-06-16 04:30:56,797 - Epoch: [167][   90/  142]    Overall Loss 0.441702    Objective Loss 0.441702                                        LR 0.000008    Time 0.543482    
2023-06-16 04:31:01,905 - Epoch: [167][  100/  142]    Overall Loss 0.438871    Objective Loss 0.438871                                        LR 0.000008    Time 0.540208    
2023-06-16 04:31:06,788 - Epoch: [167][  110/  142]    Overall Loss 0.440963    Objective Loss 0.440963                                        LR 0.000008    Time 0.535490    
2023-06-16 04:31:11,743 - Epoch: [167][  120/  142]    Overall Loss 0.434855    Objective Loss 0.434855                                        LR 0.000008    Time 0.532147    
2023-06-16 04:31:16,743 - Epoch: [167][  130/  142]    Overall Loss 0.437287    Objective Loss 0.437287                                        LR 0.000008    Time 0.529670    
2023-06-16 04:31:21,512 - Epoch: [167][  140/  142]    Overall Loss 0.438446    Objective Loss 0.438446                                        LR 0.000008    Time 0.525896    
2023-06-16 04:31:22,353 - Epoch: [167][  142/  142]    Overall Loss 0.439077    Objective Loss 0.439077    Top1 82.812500    LR 0.000008    Time 0.524411    
2023-06-16 04:31:22,986 - --- validate (epoch=167)-----------
2023-06-16 04:31:22,987 - 1422 samples (32 per mini-batch)
2023-06-16 04:31:31,147 - Epoch: [167][   10/   45]    Loss 0.880402    Top1 78.750000    
2023-06-16 04:31:35,105 - Epoch: [167][   20/   45]    Loss 0.995379    Top1 75.156250    
2023-06-16 04:31:39,488 - Epoch: [167][   30/   45]    Loss 0.944288    Top1 75.312500    
2023-06-16 04:31:43,371 - Epoch: [167][   40/   45]    Loss 0.914207    Top1 75.546875    
2023-06-16 04:31:45,283 - Epoch: [167][   45/   45]    Loss 0.908830    Top1 75.457103    
2023-06-16 04:31:45,944 - ==> Top1: 75.457    Loss: 0.909

2023-06-16 04:31:45,946 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:31:45,946 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:31:45,967 - 

2023-06-16 04:31:45,968 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:31:55,321 - Epoch: [168][   10/  142]    Overall Loss 0.479829    Objective Loss 0.479829                                        LR 0.000008    Time 0.935235    
2023-06-16 04:32:00,299 - Epoch: [168][   20/  142]    Overall Loss 0.419869    Objective Loss 0.419869                                        LR 0.000008    Time 0.716511    
2023-06-16 04:32:05,175 - Epoch: [168][   30/  142]    Overall Loss 0.449440    Objective Loss 0.449440                                        LR 0.000008    Time 0.640175    
2023-06-16 04:32:10,179 - Epoch: [168][   40/  142]    Overall Loss 0.449670    Objective Loss 0.449670                                        LR 0.000008    Time 0.605221    
2023-06-16 04:32:15,074 - Epoch: [168][   50/  142]    Overall Loss 0.438617    Objective Loss 0.438617                                        LR 0.000008    Time 0.582063    
2023-06-16 04:32:20,075 - Epoch: [168][   60/  142]    Overall Loss 0.436432    Objective Loss 0.436432                                        LR 0.000008    Time 0.568382    
2023-06-16 04:32:25,010 - Epoch: [168][   70/  142]    Overall Loss 0.434260    Objective Loss 0.434260                                        LR 0.000008    Time 0.557681    
2023-06-16 04:32:29,956 - Epoch: [168][   80/  142]    Overall Loss 0.427246    Objective Loss 0.427246                                        LR 0.000008    Time 0.549787    
2023-06-16 04:32:34,807 - Epoch: [168][   90/  142]    Overall Loss 0.425079    Objective Loss 0.425079                                        LR 0.000008    Time 0.542591    
2023-06-16 04:32:39,742 - Epoch: [168][  100/  142]    Overall Loss 0.424421    Objective Loss 0.424421                                        LR 0.000008    Time 0.537677    
2023-06-16 04:32:44,715 - Epoch: [168][  110/  142]    Overall Loss 0.421513    Objective Loss 0.421513                                        LR 0.000008    Time 0.534001    
2023-06-16 04:32:49,646 - Epoch: [168][  120/  142]    Overall Loss 0.418033    Objective Loss 0.418033                                        LR 0.000008    Time 0.530585    
2023-06-16 04:32:54,640 - Epoch: [168][  130/  142]    Overall Loss 0.416570    Objective Loss 0.416570                                        LR 0.000008    Time 0.528184    
2023-06-16 04:32:59,282 - Epoch: [168][  140/  142]    Overall Loss 0.417662    Objective Loss 0.417662                                        LR 0.000008    Time 0.523608    
2023-06-16 04:33:00,123 - Epoch: [168][  142/  142]    Overall Loss 0.421713    Objective Loss 0.421713    Top1 82.812500    LR 0.000008    Time 0.522155    
2023-06-16 04:33:00,774 - --- validate (epoch=168)-----------
2023-06-16 04:33:00,775 - 1422 samples (32 per mini-batch)
2023-06-16 04:33:08,403 - Epoch: [168][   10/   45]    Loss 0.912123    Top1 73.750000    
2023-06-16 04:33:13,208 - Epoch: [168][   20/   45]    Loss 0.872705    Top1 75.312500    
2023-06-16 04:33:17,734 - Epoch: [168][   30/   45]    Loss 0.892731    Top1 75.937500    
2023-06-16 04:33:21,988 - Epoch: [168][   40/   45]    Loss 0.895018    Top1 75.078125    
2023-06-16 04:33:23,348 - Epoch: [168][   45/   45]    Loss 0.905274    Top1 75.105485    
2023-06-16 04:33:23,996 - ==> Top1: 75.105    Loss: 0.905

2023-06-16 04:33:23,998 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:33:23,998 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:33:24,014 - 

2023-06-16 04:33:24,014 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:33:33,368 - Epoch: [169][   10/  142]    Overall Loss 0.479443    Objective Loss 0.479443                                        LR 0.000008    Time 0.935351    
2023-06-16 04:33:38,231 - Epoch: [169][   20/  142]    Overall Loss 0.434694    Objective Loss 0.434694                                        LR 0.000008    Time 0.710747    
2023-06-16 04:33:43,142 - Epoch: [169][   30/  142]    Overall Loss 0.417252    Objective Loss 0.417252                                        LR 0.000008    Time 0.637537    
2023-06-16 04:33:48,098 - Epoch: [169][   40/  142]    Overall Loss 0.415329    Objective Loss 0.415329                                        LR 0.000008    Time 0.602035    
2023-06-16 04:33:53,006 - Epoch: [169][   50/  142]    Overall Loss 0.420420    Objective Loss 0.420420                                        LR 0.000008    Time 0.579784    
2023-06-16 04:33:57,916 - Epoch: [169][   60/  142]    Overall Loss 0.418432    Objective Loss 0.418432                                        LR 0.000008    Time 0.564978    
2023-06-16 04:34:02,828 - Epoch: [169][   70/  142]    Overall Loss 0.422558    Objective Loss 0.422558                                        LR 0.000008    Time 0.554432    
2023-06-16 04:34:07,745 - Epoch: [169][   80/  142]    Overall Loss 0.422915    Objective Loss 0.422915                                        LR 0.000008    Time 0.546584    
2023-06-16 04:34:12,638 - Epoch: [169][   90/  142]    Overall Loss 0.424699    Objective Loss 0.424699                                        LR 0.000008    Time 0.540208    
2023-06-16 04:34:17,571 - Epoch: [169][  100/  142]    Overall Loss 0.424389    Objective Loss 0.424389                                        LR 0.000008    Time 0.535521    
2023-06-16 04:34:22,478 - Epoch: [169][  110/  142]    Overall Loss 0.425470    Objective Loss 0.425470                                        LR 0.000008    Time 0.531436    
2023-06-16 04:34:27,402 - Epoch: [169][  120/  142]    Overall Loss 0.423286    Objective Loss 0.423286                                        LR 0.000008    Time 0.528183    
2023-06-16 04:34:32,322 - Epoch: [169][  130/  142]    Overall Loss 0.424393    Objective Loss 0.424393                                        LR 0.000008    Time 0.525392    
2023-06-16 04:34:36,949 - Epoch: [169][  140/  142]    Overall Loss 0.426421    Objective Loss 0.426421                                        LR 0.000008    Time 0.520915    
2023-06-16 04:34:37,804 - Epoch: [169][  142/  142]    Overall Loss 0.427910    Objective Loss 0.427910    Top1 85.937500    LR 0.000008    Time 0.519600    
2023-06-16 04:34:38,477 - --- validate (epoch=169)-----------
2023-06-16 04:34:38,478 - 1422 samples (32 per mini-batch)
2023-06-16 04:34:46,398 - Epoch: [169][   10/   45]    Loss 0.861496    Top1 76.562500    
2023-06-16 04:34:50,574 - Epoch: [169][   20/   45]    Loss 0.818674    Top1 76.875000    
2023-06-16 04:34:55,022 - Epoch: [169][   30/   45]    Loss 0.827629    Top1 76.770833    
2023-06-16 04:34:59,456 - Epoch: [169][   40/   45]    Loss 0.874650    Top1 75.312500    
2023-06-16 04:35:00,837 - Epoch: [169][   45/   45]    Loss 0.872720    Top1 74.964838    
2023-06-16 04:35:01,496 - ==> Top1: 74.965    Loss: 0.873

2023-06-16 04:35:01,499 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:35:01,499 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:35:01,520 - 

2023-06-16 04:35:01,520 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:35:10,607 - Epoch: [170][   10/  142]    Overall Loss 0.465267    Objective Loss 0.465267                                        LR 0.000008    Time 0.908575    
2023-06-16 04:35:15,562 - Epoch: [170][   20/  142]    Overall Loss 0.448452    Objective Loss 0.448452                                        LR 0.000008    Time 0.701998    
2023-06-16 04:35:20,329 - Epoch: [170][   30/  142]    Overall Loss 0.430509    Objective Loss 0.430509                                        LR 0.000008    Time 0.626881    
2023-06-16 04:35:25,270 - Epoch: [170][   40/  142]    Overall Loss 0.433503    Objective Loss 0.433503                                        LR 0.000008    Time 0.593667    
2023-06-16 04:35:30,251 - Epoch: [170][   50/  142]    Overall Loss 0.441036    Objective Loss 0.441036                                        LR 0.000008    Time 0.574548    
2023-06-16 04:35:35,180 - Epoch: [170][   60/  142]    Overall Loss 0.428889    Objective Loss 0.428889                                        LR 0.000008    Time 0.560921    
2023-06-16 04:35:40,007 - Epoch: [170][   70/  142]    Overall Loss 0.425168    Objective Loss 0.425168                                        LR 0.000008    Time 0.549751    
2023-06-16 04:35:44,876 - Epoch: [170][   80/  142]    Overall Loss 0.420290    Objective Loss 0.420290                                        LR 0.000008    Time 0.541888    
2023-06-16 04:35:49,896 - Epoch: [170][   90/  142]    Overall Loss 0.420100    Objective Loss 0.420100                                        LR 0.000008    Time 0.537439    
2023-06-16 04:35:54,684 - Epoch: [170][  100/  142]    Overall Loss 0.413236    Objective Loss 0.413236                                        LR 0.000008    Time 0.531571    
2023-06-16 04:35:59,557 - Epoch: [170][  110/  142]    Overall Loss 0.420272    Objective Loss 0.420272                                        LR 0.000008    Time 0.527543    
2023-06-16 04:36:04,547 - Epoch: [170][  120/  142]    Overall Loss 0.429531    Objective Loss 0.429531                                        LR 0.000008    Time 0.525162    
2023-06-16 04:36:09,474 - Epoch: [170][  130/  142]    Overall Loss 0.430782    Objective Loss 0.430782                                        LR 0.000008    Time 0.522659    
2023-06-16 04:36:14,031 - Epoch: [170][  140/  142]    Overall Loss 0.437637    Objective Loss 0.437637                                        LR 0.000008    Time 0.517870    
2023-06-16 04:36:14,885 - Epoch: [170][  142/  142]    Overall Loss 0.439014    Objective Loss 0.439014    Top1 79.687500    LR 0.000008    Time 0.516588    
2023-06-16 04:36:15,538 - --- validate (epoch=170)-----------
2023-06-16 04:36:15,538 - 1422 samples (32 per mini-batch)
2023-06-16 04:36:23,482 - Epoch: [170][   10/   45]    Loss 0.847945    Top1 75.000000    
2023-06-16 04:36:27,591 - Epoch: [170][   20/   45]    Loss 0.872085    Top1 73.125000    
2023-06-16 04:36:32,018 - Epoch: [170][   30/   45]    Loss 0.843493    Top1 74.687500    
2023-06-16 04:36:36,479 - Epoch: [170][   40/   45]    Loss 0.862366    Top1 74.765625    
2023-06-16 04:36:37,802 - Epoch: [170][   45/   45]    Loss 0.867174    Top1 74.472574    
2023-06-16 04:36:38,454 - ==> Top1: 74.473    Loss: 0.867

2023-06-16 04:36:38,456 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:36:38,456 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:36:38,477 - 

2023-06-16 04:36:38,477 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:36:47,783 - Epoch: [171][   10/  142]    Overall Loss 0.439643    Objective Loss 0.439643                                        LR 0.000008    Time 0.930446    
2023-06-16 04:36:52,607 - Epoch: [171][   20/  142]    Overall Loss 0.428976    Objective Loss 0.428976                                        LR 0.000008    Time 0.706400    
2023-06-16 04:36:57,498 - Epoch: [171][   30/  142]    Overall Loss 0.450726    Objective Loss 0.450726                                        LR 0.000008    Time 0.633965    
2023-06-16 04:37:02,441 - Epoch: [171][   40/  142]    Overall Loss 0.453950    Objective Loss 0.453950                                        LR 0.000008    Time 0.599029    
2023-06-16 04:37:07,576 - Epoch: [171][   50/  142]    Overall Loss 0.431082    Objective Loss 0.431082                                        LR 0.000008    Time 0.581903    
2023-06-16 04:37:12,395 - Epoch: [171][   60/  142]    Overall Loss 0.434873    Objective Loss 0.434873                                        LR 0.000008    Time 0.565232    
2023-06-16 04:37:17,406 - Epoch: [171][   70/  142]    Overall Loss 0.434166    Objective Loss 0.434166                                        LR 0.000008    Time 0.556059    
2023-06-16 04:37:22,390 - Epoch: [171][   80/  142]    Overall Loss 0.433319    Objective Loss 0.433319                                        LR 0.000008    Time 0.548848    
2023-06-16 04:37:27,281 - Epoch: [171][   90/  142]    Overall Loss 0.433843    Objective Loss 0.433843                                        LR 0.000008    Time 0.542201    
2023-06-16 04:37:32,246 - Epoch: [171][  100/  142]    Overall Loss 0.431539    Objective Loss 0.431539                                        LR 0.000008    Time 0.537627    
2023-06-16 04:37:37,132 - Epoch: [171][  110/  142]    Overall Loss 0.430244    Objective Loss 0.430244                                        LR 0.000008    Time 0.533167    
2023-06-16 04:37:42,134 - Epoch: [171][  120/  142]    Overall Loss 0.435856    Objective Loss 0.435856                                        LR 0.000008    Time 0.530410    
2023-06-16 04:37:46,997 - Epoch: [171][  130/  142]    Overall Loss 0.434347    Objective Loss 0.434347                                        LR 0.000008    Time 0.527012    
2023-06-16 04:37:51,680 - Epoch: [171][  140/  142]    Overall Loss 0.438241    Objective Loss 0.438241                                        LR 0.000008    Time 0.522820    
2023-06-16 04:37:52,536 - Epoch: [171][  142/  142]    Overall Loss 0.439244    Objective Loss 0.439244    Top1 84.375000    LR 0.000008    Time 0.521479    
2023-06-16 04:37:53,196 - --- validate (epoch=171)-----------
2023-06-16 04:37:53,197 - 1422 samples (32 per mini-batch)
2023-06-16 04:38:01,408 - Epoch: [171][   10/   45]    Loss 1.109369    Top1 72.812500    
2023-06-16 04:38:05,575 - Epoch: [171][   20/   45]    Loss 0.948843    Top1 75.312500    
2023-06-16 04:38:10,297 - Epoch: [171][   30/   45]    Loss 0.927017    Top1 74.687500    
2023-06-16 04:38:14,448 - Epoch: [171][   40/   45]    Loss 0.924152    Top1 75.078125    
2023-06-16 04:38:15,869 - Epoch: [171][   45/   45]    Loss 0.921335    Top1 74.964838    
2023-06-16 04:38:16,489 - ==> Top1: 74.965    Loss: 0.921

2023-06-16 04:38:16,491 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:38:16,491 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:38:16,512 - 

2023-06-16 04:38:16,513 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:38:26,004 - Epoch: [172][   10/  142]    Overall Loss 0.421505    Objective Loss 0.421505                                        LR 0.000008    Time 0.949060    
2023-06-16 04:38:30,898 - Epoch: [172][   20/  142]    Overall Loss 0.404054    Objective Loss 0.404054                                        LR 0.000008    Time 0.719194    
2023-06-16 04:38:35,804 - Epoch: [172][   30/  142]    Overall Loss 0.428112    Objective Loss 0.428112                                        LR 0.000008    Time 0.642978    
2023-06-16 04:38:40,744 - Epoch: [172][   40/  142]    Overall Loss 0.419732    Objective Loss 0.419732                                        LR 0.000008    Time 0.605708    
2023-06-16 04:38:45,688 - Epoch: [172][   50/  142]    Overall Loss 0.424444    Objective Loss 0.424444                                        LR 0.000008    Time 0.583434    
2023-06-16 04:38:50,677 - Epoch: [172][   60/  142]    Overall Loss 0.423882    Objective Loss 0.423882                                        LR 0.000008    Time 0.569343    
2023-06-16 04:38:55,675 - Epoch: [172][   70/  142]    Overall Loss 0.417427    Objective Loss 0.417427                                        LR 0.000008    Time 0.558654    
2023-06-16 04:39:00,628 - Epoch: [172][   80/  142]    Overall Loss 0.415316    Objective Loss 0.415316                                        LR 0.000008    Time 0.550723    
2023-06-16 04:39:05,531 - Epoch: [172][   90/  142]    Overall Loss 0.427166    Objective Loss 0.427166                                        LR 0.000008    Time 0.544008    
2023-06-16 04:39:10,625 - Epoch: [172][  100/  142]    Overall Loss 0.427341    Objective Loss 0.427341                                        LR 0.000008    Time 0.540546    
2023-06-16 04:39:15,452 - Epoch: [172][  110/  142]    Overall Loss 0.437178    Objective Loss 0.437178                                        LR 0.000008    Time 0.535283    
2023-06-16 04:39:20,408 - Epoch: [172][  120/  142]    Overall Loss 0.439430    Objective Loss 0.439430                                        LR 0.000008    Time 0.531965    
2023-06-16 04:39:25,355 - Epoch: [172][  130/  142]    Overall Loss 0.437718    Objective Loss 0.437718                                        LR 0.000008    Time 0.529098    
2023-06-16 04:39:29,996 - Epoch: [172][  140/  142]    Overall Loss 0.443757    Objective Loss 0.443757                                        LR 0.000008    Time 0.524450    
2023-06-16 04:39:30,842 - Epoch: [172][  142/  142]    Overall Loss 0.445990    Objective Loss 0.445990    Top1 78.125000    LR 0.000008    Time 0.523024    
2023-06-16 04:39:31,460 - --- validate (epoch=172)-----------
2023-06-16 04:39:31,461 - 1422 samples (32 per mini-batch)
2023-06-16 04:39:39,536 - Epoch: [172][   10/   45]    Loss 0.784610    Top1 76.562500    
2023-06-16 04:39:43,885 - Epoch: [172][   20/   45]    Loss 0.932554    Top1 73.281250    
2023-06-16 04:39:49,323 - Epoch: [172][   30/   45]    Loss 0.873922    Top1 74.583333    
2023-06-16 04:39:53,604 - Epoch: [172][   40/   45]    Loss 0.884258    Top1 75.078125    
2023-06-16 04:39:54,925 - Epoch: [172][   45/   45]    Loss 0.892859    Top1 74.753868    
2023-06-16 04:39:55,563 - ==> Top1: 74.754    Loss: 0.893

2023-06-16 04:39:55,565 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:39:55,565 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:39:55,586 - 

2023-06-16 04:39:55,586 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:40:04,756 - Epoch: [173][   10/  142]    Overall Loss 0.389460    Objective Loss 0.389460                                        LR 0.000008    Time 0.916816    
2023-06-16 04:40:09,817 - Epoch: [173][   20/  142]    Overall Loss 0.396483    Objective Loss 0.396483                                        LR 0.000008    Time 0.711443    
2023-06-16 04:40:14,620 - Epoch: [173][   30/  142]    Overall Loss 0.424546    Objective Loss 0.424546                                        LR 0.000008    Time 0.634365    
2023-06-16 04:40:19,786 - Epoch: [173][   40/  142]    Overall Loss 0.433433    Objective Loss 0.433433                                        LR 0.000008    Time 0.604913    
2023-06-16 04:40:24,748 - Epoch: [173][   50/  142]    Overall Loss 0.424397    Objective Loss 0.424397                                        LR 0.000008    Time 0.583174    
2023-06-16 04:40:29,646 - Epoch: [173][   60/  142]    Overall Loss 0.439124    Objective Loss 0.439124                                        LR 0.000008    Time 0.567602    
2023-06-16 04:40:34,693 - Epoch: [173][   70/  142]    Overall Loss 0.441610    Objective Loss 0.441610                                        LR 0.000008    Time 0.558606    
2023-06-16 04:40:39,626 - Epoch: [173][   80/  142]    Overall Loss 0.438186    Objective Loss 0.438186                                        LR 0.000008    Time 0.550440    
2023-06-16 04:40:44,582 - Epoch: [173][   90/  142]    Overall Loss 0.444657    Objective Loss 0.444657                                        LR 0.000008    Time 0.544339    
2023-06-16 04:40:49,536 - Epoch: [173][  100/  142]    Overall Loss 0.443409    Objective Loss 0.443409                                        LR 0.000008    Time 0.539442    
2023-06-16 04:40:54,552 - Epoch: [173][  110/  142]    Overall Loss 0.444411    Objective Loss 0.444411                                        LR 0.000008    Time 0.535990    
2023-06-16 04:40:59,505 - Epoch: [173][  120/  142]    Overall Loss 0.440532    Objective Loss 0.440532                                        LR 0.000008    Time 0.532599    
2023-06-16 04:41:04,458 - Epoch: [173][  130/  142]    Overall Loss 0.438418    Objective Loss 0.438418                                        LR 0.000008    Time 0.529721    
2023-06-16 04:41:09,034 - Epoch: [173][  140/  142]    Overall Loss 0.434463    Objective Loss 0.434463                                        LR 0.000008    Time 0.524566    
2023-06-16 04:41:09,882 - Epoch: [173][  142/  142]    Overall Loss 0.436461    Objective Loss 0.436461    Top1 81.250000    LR 0.000008    Time 0.523150    
2023-06-16 04:41:10,533 - --- validate (epoch=173)-----------
2023-06-16 04:41:10,533 - 1422 samples (32 per mini-batch)
2023-06-16 04:41:18,737 - Epoch: [173][   10/   45]    Loss 0.957301    Top1 74.687500    
2023-06-16 04:41:23,633 - Epoch: [173][   20/   45]    Loss 0.972743    Top1 74.062500    
2023-06-16 04:41:27,675 - Epoch: [173][   30/   45]    Loss 0.954838    Top1 74.583333    
2023-06-16 04:41:31,844 - Epoch: [173][   40/   45]    Loss 0.948017    Top1 74.531250    
2023-06-16 04:41:33,277 - Epoch: [173][   45/   45]    Loss 0.905840    Top1 75.105485    
2023-06-16 04:41:33,943 - ==> Top1: 75.105    Loss: 0.906

2023-06-16 04:41:33,945 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:41:33,945 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:41:33,966 - 

2023-06-16 04:41:33,967 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:41:43,343 - Epoch: [174][   10/  142]    Overall Loss 0.431849    Objective Loss 0.431849                                        LR 0.000008    Time 0.937537    
2023-06-16 04:41:48,466 - Epoch: [174][   20/  142]    Overall Loss 0.422729    Objective Loss 0.422729                                        LR 0.000008    Time 0.724845    
2023-06-16 04:41:53,489 - Epoch: [174][   30/  142]    Overall Loss 0.428654    Objective Loss 0.428654                                        LR 0.000008    Time 0.650656    
2023-06-16 04:41:58,568 - Epoch: [174][   40/  142]    Overall Loss 0.447968    Objective Loss 0.447968                                        LR 0.000008    Time 0.614955    
2023-06-16 04:42:03,585 - Epoch: [174][   50/  142]    Overall Loss 0.440849    Objective Loss 0.440849                                        LR 0.000008    Time 0.592286    
2023-06-16 04:42:08,525 - Epoch: [174][   60/  142]    Overall Loss 0.450085    Objective Loss 0.450085                                        LR 0.000008    Time 0.575901    
2023-06-16 04:42:13,509 - Epoch: [174][   70/  142]    Overall Loss 0.445770    Objective Loss 0.445770                                        LR 0.000008    Time 0.564823    
2023-06-16 04:42:18,552 - Epoch: [174][   80/  142]    Overall Loss 0.442350    Objective Loss 0.442350                                        LR 0.000008    Time 0.557243    
2023-06-16 04:42:23,604 - Epoch: [174][   90/  142]    Overall Loss 0.435807    Objective Loss 0.435807                                        LR 0.000008    Time 0.551455    
2023-06-16 04:42:28,728 - Epoch: [174][  100/  142]    Overall Loss 0.426730    Objective Loss 0.426730                                        LR 0.000008    Time 0.547546    
2023-06-16 04:42:33,638 - Epoch: [174][  110/  142]    Overall Loss 0.424337    Objective Loss 0.424337                                        LR 0.000008    Time 0.542399    
2023-06-16 04:42:38,762 - Epoch: [174][  120/  142]    Overall Loss 0.426405    Objective Loss 0.426405                                        LR 0.000008    Time 0.539896    
2023-06-16 04:42:43,725 - Epoch: [174][  130/  142]    Overall Loss 0.428386    Objective Loss 0.428386                                        LR 0.000008    Time 0.536540    
2023-06-16 04:42:48,312 - Epoch: [174][  140/  142]    Overall Loss 0.431045    Objective Loss 0.431045                                        LR 0.000008    Time 0.530978    
2023-06-16 04:42:49,155 - Epoch: [174][  142/  142]    Overall Loss 0.429397    Objective Loss 0.429397    Top1 89.062500    LR 0.000008    Time 0.529434    
2023-06-16 04:42:49,816 - --- validate (epoch=174)-----------
2023-06-16 04:42:49,817 - 1422 samples (32 per mini-batch)
2023-06-16 04:42:57,777 - Epoch: [174][   10/   45]    Loss 1.117772    Top1 70.937500    
2023-06-16 04:43:01,798 - Epoch: [174][   20/   45]    Loss 0.950227    Top1 73.437500    
2023-06-16 04:43:06,456 - Epoch: [174][   30/   45]    Loss 0.968750    Top1 73.437500    
2023-06-16 04:43:10,926 - Epoch: [174][   40/   45]    Loss 0.944327    Top1 74.375000    
2023-06-16 04:43:12,480 - Epoch: [174][   45/   45]    Loss 0.950472    Top1 74.191280    
2023-06-16 04:43:13,130 - ==> Top1: 74.191    Loss: 0.950

2023-06-16 04:43:13,133 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:43:13,133 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:43:13,154 - 

2023-06-16 04:43:13,154 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:43:22,613 - Epoch: [175][   10/  142]    Overall Loss 0.485213    Objective Loss 0.485213                                        LR 0.000008    Time 0.945768    
2023-06-16 04:43:27,652 - Epoch: [175][   20/  142]    Overall Loss 0.482002    Objective Loss 0.482002                                        LR 0.000008    Time 0.724798    
2023-06-16 04:43:32,620 - Epoch: [175][   30/  142]    Overall Loss 0.455574    Objective Loss 0.455574                                        LR 0.000008    Time 0.648774    
2023-06-16 04:43:37,676 - Epoch: [175][   40/  142]    Overall Loss 0.465479    Objective Loss 0.465479                                        LR 0.000008    Time 0.612972    
2023-06-16 04:43:42,718 - Epoch: [175][   50/  142]    Overall Loss 0.459964    Objective Loss 0.459964                                        LR 0.000008    Time 0.591212    
2023-06-16 04:43:47,788 - Epoch: [175][   60/  142]    Overall Loss 0.453047    Objective Loss 0.453047                                        LR 0.000008    Time 0.577156    
2023-06-16 04:43:52,709 - Epoch: [175][   70/  142]    Overall Loss 0.449642    Objective Loss 0.449642                                        LR 0.000008    Time 0.565003    
2023-06-16 04:43:57,647 - Epoch: [175][   80/  142]    Overall Loss 0.444872    Objective Loss 0.444872                                        LR 0.000008    Time 0.556099    
2023-06-16 04:44:02,741 - Epoch: [175][   90/  142]    Overall Loss 0.444090    Objective Loss 0.444090                                        LR 0.000008    Time 0.550899    
2023-06-16 04:44:07,767 - Epoch: [175][  100/  142]    Overall Loss 0.442163    Objective Loss 0.442163                                        LR 0.000008    Time 0.546064    
2023-06-16 04:44:12,784 - Epoch: [175][  110/  142]    Overall Loss 0.452836    Objective Loss 0.452836                                        LR 0.000008    Time 0.542029    
2023-06-16 04:44:17,761 - Epoch: [175][  120/  142]    Overall Loss 0.446395    Objective Loss 0.446395                                        LR 0.000008    Time 0.538332    
2023-06-16 04:44:22,699 - Epoch: [175][  130/  142]    Overall Loss 0.447608    Objective Loss 0.447608                                        LR 0.000008    Time 0.534894    
2023-06-16 04:44:27,340 - Epoch: [175][  140/  142]    Overall Loss 0.443970    Objective Loss 0.443970                                        LR 0.000008    Time 0.529840    
2023-06-16 04:44:28,182 - Epoch: [175][  142/  142]    Overall Loss 0.445825    Objective Loss 0.445825    Top1 82.812500    LR 0.000008    Time 0.528303    
2023-06-16 04:44:28,825 - --- validate (epoch=175)-----------
2023-06-16 04:44:28,826 - 1422 samples (32 per mini-batch)
2023-06-16 04:44:36,963 - Epoch: [175][   10/   45]    Loss 0.814252    Top1 78.437500    
2023-06-16 04:44:41,412 - Epoch: [175][   20/   45]    Loss 0.894009    Top1 76.406250    
2023-06-16 04:44:46,234 - Epoch: [175][   30/   45]    Loss 0.910716    Top1 74.583333    
2023-06-16 04:44:50,565 - Epoch: [175][   40/   45]    Loss 0.894103    Top1 75.234375    
2023-06-16 04:44:51,918 - Epoch: [175][   45/   45]    Loss 0.895586    Top1 74.894515    
2023-06-16 04:44:52,564 - ==> Top1: 74.895    Loss: 0.896

2023-06-16 04:44:52,566 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:44:52,566 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:44:52,587 - 

2023-06-16 04:44:52,587 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:45:01,921 - Epoch: [176][   10/  142]    Overall Loss 0.403743    Objective Loss 0.403743                                        LR 0.000008    Time 0.933273    
2023-06-16 04:45:06,853 - Epoch: [176][   20/  142]    Overall Loss 0.396700    Objective Loss 0.396700                                        LR 0.000008    Time 0.713166    
2023-06-16 04:45:11,846 - Epoch: [176][   30/  142]    Overall Loss 0.428220    Objective Loss 0.428220                                        LR 0.000008    Time 0.641882    
2023-06-16 04:45:16,859 - Epoch: [176][   40/  142]    Overall Loss 0.435461    Objective Loss 0.435461                                        LR 0.000008    Time 0.606708    
2023-06-16 04:45:21,814 - Epoch: [176][   50/  142]    Overall Loss 0.435574    Objective Loss 0.435574                                        LR 0.000008    Time 0.584469    
2023-06-16 04:45:26,654 - Epoch: [176][   60/  142]    Overall Loss 0.431328    Objective Loss 0.431328                                        LR 0.000008    Time 0.567716    
2023-06-16 04:45:31,693 - Epoch: [176][   70/  142]    Overall Loss 0.435766    Objective Loss 0.435766                                        LR 0.000008    Time 0.558584    
2023-06-16 04:45:36,552 - Epoch: [176][   80/  142]    Overall Loss 0.437272    Objective Loss 0.437272                                        LR 0.000008    Time 0.549491    
2023-06-16 04:45:41,514 - Epoch: [176][   90/  142]    Overall Loss 0.441947    Objective Loss 0.441947                                        LR 0.000008    Time 0.543571    
2023-06-16 04:45:46,485 - Epoch: [176][  100/  142]    Overall Loss 0.437339    Objective Loss 0.437339                                        LR 0.000008    Time 0.538915    
2023-06-16 04:45:51,468 - Epoch: [176][  110/  142]    Overall Loss 0.439092    Objective Loss 0.439092                                        LR 0.000008    Time 0.535218    
2023-06-16 04:45:56,430 - Epoch: [176][  120/  142]    Overall Loss 0.438655    Objective Loss 0.438655                                        LR 0.000008    Time 0.531963    
2023-06-16 04:46:01,414 - Epoch: [176][  130/  142]    Overall Loss 0.437506    Objective Loss 0.437506                                        LR 0.000008    Time 0.529374    
2023-06-16 04:46:06,060 - Epoch: [176][  140/  142]    Overall Loss 0.433296    Objective Loss 0.433296                                        LR 0.000008    Time 0.524745    
2023-06-16 04:46:06,901 - Epoch: [176][  142/  142]    Overall Loss 0.433704    Objective Loss 0.433704    Top1 84.375000    LR 0.000008    Time 0.523274    
2023-06-16 04:46:07,537 - --- validate (epoch=176)-----------
2023-06-16 04:46:07,538 - 1422 samples (32 per mini-batch)
2023-06-16 04:46:15,797 - Epoch: [176][   10/   45]    Loss 0.896844    Top1 75.937500    
2023-06-16 04:46:19,871 - Epoch: [176][   20/   45]    Loss 0.941314    Top1 74.531250    
2023-06-16 04:46:23,914 - Epoch: [176][   30/   45]    Loss 0.990037    Top1 74.270833    
2023-06-16 04:46:28,610 - Epoch: [176][   40/   45]    Loss 0.968615    Top1 74.843750    
2023-06-16 04:46:29,913 - Epoch: [176][   45/   45]    Loss 0.976078    Top1 74.753868    
2023-06-16 04:46:30,534 - ==> Top1: 74.754    Loss: 0.976

2023-06-16 04:46:30,537 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:46:30,537 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:46:30,558 - 

2023-06-16 04:46:30,558 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:46:39,876 - Epoch: [177][   10/  142]    Overall Loss 0.374690    Objective Loss 0.374690                                        LR 0.000008    Time 0.931649    
2023-06-16 04:46:44,921 - Epoch: [177][   20/  142]    Overall Loss 0.411716    Objective Loss 0.411716                                        LR 0.000008    Time 0.718075    
2023-06-16 04:46:49,915 - Epoch: [177][   30/  142]    Overall Loss 0.438560    Objective Loss 0.438560                                        LR 0.000008    Time 0.645158    
2023-06-16 04:46:54,822 - Epoch: [177][   40/  142]    Overall Loss 0.427919    Objective Loss 0.427919                                        LR 0.000008    Time 0.606517    
2023-06-16 04:46:59,798 - Epoch: [177][   50/  142]    Overall Loss 0.435250    Objective Loss 0.435250                                        LR 0.000008    Time 0.584735    
2023-06-16 04:47:04,782 - Epoch: [177][   60/  142]    Overall Loss 0.433767    Objective Loss 0.433767                                        LR 0.000008    Time 0.570323    
2023-06-16 04:47:09,729 - Epoch: [177][   70/  142]    Overall Loss 0.426608    Objective Loss 0.426608                                        LR 0.000008    Time 0.559522    
2023-06-16 04:47:14,736 - Epoch: [177][   80/  142]    Overall Loss 0.430630    Objective Loss 0.430630                                        LR 0.000008    Time 0.552153    
2023-06-16 04:47:19,651 - Epoch: [177][   90/  142]    Overall Loss 0.434562    Objective Loss 0.434562                                        LR 0.000008    Time 0.545406    
2023-06-16 04:47:24,593 - Epoch: [177][  100/  142]    Overall Loss 0.430195    Objective Loss 0.430195                                        LR 0.000008    Time 0.540283    
2023-06-16 04:47:29,565 - Epoch: [177][  110/  142]    Overall Loss 0.435170    Objective Loss 0.435170                                        LR 0.000008    Time 0.536366    
2023-06-16 04:47:34,348 - Epoch: [177][  120/  142]    Overall Loss 0.432962    Objective Loss 0.432962                                        LR 0.000008    Time 0.531522    
2023-06-16 04:47:39,389 - Epoch: [177][  130/  142]    Overall Loss 0.436458    Objective Loss 0.436458                                        LR 0.000008    Time 0.529409    
2023-06-16 04:47:44,007 - Epoch: [177][  140/  142]    Overall Loss 0.437495    Objective Loss 0.437495                                        LR 0.000008    Time 0.524572    
2023-06-16 04:47:44,859 - Epoch: [177][  142/  142]    Overall Loss 0.436901    Objective Loss 0.436901    Top1 87.500000    LR 0.000008    Time 0.523186    
2023-06-16 04:47:45,491 - --- validate (epoch=177)-----------
2023-06-16 04:47:45,491 - 1422 samples (32 per mini-batch)
2023-06-16 04:47:53,328 - Epoch: [177][   10/   45]    Loss 0.901870    Top1 74.687500    
2023-06-16 04:47:57,858 - Epoch: [177][   20/   45]    Loss 0.899636    Top1 74.531250    
2023-06-16 04:48:02,116 - Epoch: [177][   30/   45]    Loss 0.923750    Top1 74.791667    
2023-06-16 04:48:06,516 - Epoch: [177][   40/   45]    Loss 0.934000    Top1 74.609375    
2023-06-16 04:48:07,893 - Epoch: [177][   45/   45]    Loss 0.942740    Top1 74.402250    
2023-06-16 04:48:08,520 - ==> Top1: 74.402    Loss: 0.943

2023-06-16 04:48:08,522 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:48:08,522 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:48:08,544 - 

2023-06-16 04:48:08,544 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:48:17,849 - Epoch: [178][   10/  142]    Overall Loss 0.424286    Objective Loss 0.424286                                        LR 0.000008    Time 0.930426    
2023-06-16 04:48:22,876 - Epoch: [178][   20/  142]    Overall Loss 0.423499    Objective Loss 0.423499                                        LR 0.000008    Time 0.716509    
2023-06-16 04:48:27,805 - Epoch: [178][   30/  142]    Overall Loss 0.461541    Objective Loss 0.461541                                        LR 0.000008    Time 0.641949    
2023-06-16 04:48:32,744 - Epoch: [178][   40/  142]    Overall Loss 0.461917    Objective Loss 0.461917                                        LR 0.000008    Time 0.604926    
2023-06-16 04:48:37,693 - Epoch: [178][   50/  142]    Overall Loss 0.455641    Objective Loss 0.455641                                        LR 0.000008    Time 0.582917    
2023-06-16 04:48:42,652 - Epoch: [178][   60/  142]    Overall Loss 0.448232    Objective Loss 0.448232                                        LR 0.000008    Time 0.568406    
2023-06-16 04:48:47,710 - Epoch: [178][   70/  142]    Overall Loss 0.444969    Objective Loss 0.444969                                        LR 0.000008    Time 0.559442    
2023-06-16 04:48:52,569 - Epoch: [178][   80/  142]    Overall Loss 0.448324    Objective Loss 0.448324                                        LR 0.000008    Time 0.550244    
2023-06-16 04:48:57,555 - Epoch: [178][   90/  142]    Overall Loss 0.440104    Objective Loss 0.440104                                        LR 0.000008    Time 0.544497    
2023-06-16 04:49:02,491 - Epoch: [178][  100/  142]    Overall Loss 0.441704    Objective Loss 0.441704                                        LR 0.000008    Time 0.539409    
2023-06-16 04:49:07,494 - Epoch: [178][  110/  142]    Overall Loss 0.437244    Objective Loss 0.437244                                        LR 0.000008    Time 0.535845    
2023-06-16 04:49:12,406 - Epoch: [178][  120/  142]    Overall Loss 0.435262    Objective Loss 0.435262                                        LR 0.000008    Time 0.532122    
2023-06-16 04:49:17,336 - Epoch: [178][  130/  142]    Overall Loss 0.428623    Objective Loss 0.428623                                        LR 0.000008    Time 0.529103    
2023-06-16 04:49:22,055 - Epoch: [178][  140/  142]    Overall Loss 0.430249    Objective Loss 0.430249                                        LR 0.000008    Time 0.525013    
2023-06-16 04:49:22,913 - Epoch: [178][  142/  142]    Overall Loss 0.429406    Objective Loss 0.429406    Top1 85.937500    LR 0.000008    Time 0.523662    
2023-06-16 04:49:23,573 - --- validate (epoch=178)-----------
2023-06-16 04:49:23,574 - 1422 samples (32 per mini-batch)
2023-06-16 04:49:31,793 - Epoch: [178][   10/   45]    Loss 0.868603    Top1 75.937500    
2023-06-16 04:49:35,957 - Epoch: [178][   20/   45]    Loss 0.854634    Top1 76.250000    
2023-06-16 04:49:40,179 - Epoch: [178][   30/   45]    Loss 0.904901    Top1 75.312500    
2023-06-16 04:49:44,714 - Epoch: [178][   40/   45]    Loss 0.885848    Top1 75.078125    
2023-06-16 04:49:46,174 - Epoch: [178][   45/   45]    Loss 0.901824    Top1 74.894515    
2023-06-16 04:49:46,835 - ==> Top1: 74.895    Loss: 0.902

2023-06-16 04:49:46,837 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:49:46,837 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:49:46,858 - 

2023-06-16 04:49:46,858 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:49:56,216 - Epoch: [179][   10/  142]    Overall Loss 0.394805    Objective Loss 0.394805                                        LR 0.000008    Time 0.935622    
2023-06-16 04:50:01,311 - Epoch: [179][   20/  142]    Overall Loss 0.427840    Objective Loss 0.427840                                        LR 0.000008    Time 0.722527    
2023-06-16 04:50:06,299 - Epoch: [179][   30/  142]    Overall Loss 0.425143    Objective Loss 0.425143                                        LR 0.000008    Time 0.647927    
2023-06-16 04:50:11,290 - Epoch: [179][   40/  142]    Overall Loss 0.412529    Objective Loss 0.412529                                        LR 0.000008    Time 0.610706    
2023-06-16 04:50:16,269 - Epoch: [179][   50/  142]    Overall Loss 0.410477    Objective Loss 0.410477                                        LR 0.000008    Time 0.588147    
2023-06-16 04:50:21,294 - Epoch: [179][   60/  142]    Overall Loss 0.418597    Objective Loss 0.418597                                        LR 0.000008    Time 0.573859    
2023-06-16 04:50:26,305 - Epoch: [179][   70/  142]    Overall Loss 0.406498    Objective Loss 0.406498                                        LR 0.000008    Time 0.563453    
2023-06-16 04:50:31,293 - Epoch: [179][   80/  142]    Overall Loss 0.411918    Objective Loss 0.411918                                        LR 0.000008    Time 0.555370    
2023-06-16 04:50:36,300 - Epoch: [179][   90/  142]    Overall Loss 0.412809    Objective Loss 0.412809                                        LR 0.000008    Time 0.549290    
2023-06-16 04:50:41,281 - Epoch: [179][  100/  142]    Overall Loss 0.409874    Objective Loss 0.409874                                        LR 0.000008    Time 0.544166    
2023-06-16 04:50:46,385 - Epoch: [179][  110/  142]    Overall Loss 0.410262    Objective Loss 0.410262                                        LR 0.000008    Time 0.541083    
2023-06-16 04:50:51,360 - Epoch: [179][  120/  142]    Overall Loss 0.416449    Objective Loss 0.416449                                        LR 0.000008    Time 0.537452    
2023-06-16 04:50:56,436 - Epoch: [179][  130/  142]    Overall Loss 0.421673    Objective Loss 0.421673                                        LR 0.000008    Time 0.535153    
2023-06-16 04:51:01,088 - Epoch: [179][  140/  142]    Overall Loss 0.420315    Objective Loss 0.420315                                        LR 0.000008    Time 0.530154    
2023-06-16 04:51:01,941 - Epoch: [179][  142/  142]    Overall Loss 0.420607    Objective Loss 0.420607    Top1 84.375000    LR 0.000008    Time 0.528691    
2023-06-16 04:51:02,611 - --- validate (epoch=179)-----------
2023-06-16 04:51:02,611 - 1422 samples (32 per mini-batch)
2023-06-16 04:51:11,013 - Epoch: [179][   10/   45]    Loss 0.912468    Top1 77.500000    
2023-06-16 04:51:15,085 - Epoch: [179][   20/   45]    Loss 0.973945    Top1 73.281250    
2023-06-16 04:51:19,145 - Epoch: [179][   30/   45]    Loss 0.930376    Top1 74.479167    
2023-06-16 04:51:23,308 - Epoch: [179][   40/   45]    Loss 0.931736    Top1 74.453125    
2023-06-16 04:51:24,744 - Epoch: [179][   45/   45]    Loss 0.918257    Top1 75.105485    
2023-06-16 04:51:25,390 - ==> Top1: 75.105    Loss: 0.918

2023-06-16 04:51:25,392 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:51:25,392 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:51:25,414 - 

2023-06-16 04:51:25,414 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:51:34,774 - Epoch: [180][   10/  142]    Overall Loss 0.437247    Objective Loss 0.437247                                        LR 0.000008    Time 0.935925    
2023-06-16 04:51:39,584 - Epoch: [180][   20/  142]    Overall Loss 0.431412    Objective Loss 0.431412                                        LR 0.000008    Time 0.708398    
2023-06-16 04:51:44,604 - Epoch: [180][   30/  142]    Overall Loss 0.403650    Objective Loss 0.403650                                        LR 0.000008    Time 0.639601    
2023-06-16 04:51:49,488 - Epoch: [180][   40/  142]    Overall Loss 0.424466    Objective Loss 0.424466                                        LR 0.000008    Time 0.601782    
2023-06-16 04:51:54,459 - Epoch: [180][   50/  142]    Overall Loss 0.418889    Objective Loss 0.418889                                        LR 0.000008    Time 0.580834    
2023-06-16 04:51:59,438 - Epoch: [180][   60/  142]    Overall Loss 0.439562    Objective Loss 0.439562                                        LR 0.000008    Time 0.566991    
2023-06-16 04:52:04,361 - Epoch: [180][   70/  142]    Overall Loss 0.440510    Objective Loss 0.440510                                        LR 0.000008    Time 0.556322    
2023-06-16 04:52:09,309 - Epoch: [180][   80/  142]    Overall Loss 0.445880    Objective Loss 0.445880                                        LR 0.000008    Time 0.548625    
2023-06-16 04:52:14,265 - Epoch: [180][   90/  142]    Overall Loss 0.442841    Objective Loss 0.442841                                        LR 0.000008    Time 0.542723    
2023-06-16 04:52:19,150 - Epoch: [180][  100/  142]    Overall Loss 0.441284    Objective Loss 0.441284                                        LR 0.000008    Time 0.537293    
2023-06-16 04:52:24,219 - Epoch: [180][  110/  142]    Overall Loss 0.440863    Objective Loss 0.440863                                        LR 0.000008    Time 0.534518    
2023-06-16 04:52:29,010 - Epoch: [180][  120/  142]    Overall Loss 0.435192    Objective Loss 0.435192                                        LR 0.000008    Time 0.529894    
2023-06-16 04:52:33,970 - Epoch: [180][  130/  142]    Overall Loss 0.433571    Objective Loss 0.433571                                        LR 0.000008    Time 0.527288    
2023-06-16 04:52:38,649 - Epoch: [180][  140/  142]    Overall Loss 0.438399    Objective Loss 0.438399                                        LR 0.000008    Time 0.523039    
2023-06-16 04:52:39,506 - Epoch: [180][  142/  142]    Overall Loss 0.437384    Objective Loss 0.437384    Top1 92.187500    LR 0.000008    Time 0.521705    
2023-06-16 04:52:40,133 - --- validate (epoch=180)-----------
2023-06-16 04:52:40,134 - 1422 samples (32 per mini-batch)
2023-06-16 04:52:48,228 - Epoch: [180][   10/   45]    Loss 0.726078    Top1 78.437500    
2023-06-16 04:52:52,277 - Epoch: [180][   20/   45]    Loss 0.825608    Top1 75.625000    
2023-06-16 04:52:56,493 - Epoch: [180][   30/   45]    Loss 0.862875    Top1 75.520833    
2023-06-16 04:53:00,936 - Epoch: [180][   40/   45]    Loss 0.843710    Top1 75.468750    
2023-06-16 04:53:02,340 - Epoch: [180][   45/   45]    Loss 0.891255    Top1 74.542897    
2023-06-16 04:53:02,985 - ==> Top1: 74.543    Loss: 0.891

2023-06-16 04:53:02,987 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:53:02,987 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:53:03,008 - 

2023-06-16 04:53:03,008 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:53:12,253 - Epoch: [181][   10/  142]    Overall Loss 0.421248    Objective Loss 0.421248                                        LR 0.000008    Time 0.924382    
2023-06-16 04:53:17,214 - Epoch: [181][   20/  142]    Overall Loss 0.390777    Objective Loss 0.390777                                        LR 0.000008    Time 0.710193    
2023-06-16 04:53:22,180 - Epoch: [181][   30/  142]    Overall Loss 0.411573    Objective Loss 0.411573                                        LR 0.000008    Time 0.638973    
2023-06-16 04:53:27,126 - Epoch: [181][   40/  142]    Overall Loss 0.424040    Objective Loss 0.424040                                        LR 0.000008    Time 0.602868    
2023-06-16 04:53:31,942 - Epoch: [181][   50/  142]    Overall Loss 0.439614    Objective Loss 0.439614                                        LR 0.000008    Time 0.578619    
2023-06-16 04:53:36,873 - Epoch: [181][   60/  142]    Overall Loss 0.444835    Objective Loss 0.444835                                        LR 0.000008    Time 0.564338    
2023-06-16 04:53:41,914 - Epoch: [181][   70/  142]    Overall Loss 0.438864    Objective Loss 0.438864                                        LR 0.000008    Time 0.555737    
2023-06-16 04:53:46,860 - Epoch: [181][   80/  142]    Overall Loss 0.429147    Objective Loss 0.429147                                        LR 0.000008    Time 0.548087    
2023-06-16 04:53:51,849 - Epoch: [181][   90/  142]    Overall Loss 0.431180    Objective Loss 0.431180                                        LR 0.000008    Time 0.542614    
2023-06-16 04:53:56,786 - Epoch: [181][  100/  142]    Overall Loss 0.433243    Objective Loss 0.433243                                        LR 0.000008    Time 0.537716    
2023-06-16 04:54:01,801 - Epoch: [181][  110/  142]    Overall Loss 0.438546    Objective Loss 0.438546                                        LR 0.000008    Time 0.534413    
2023-06-16 04:54:06,658 - Epoch: [181][  120/  142]    Overall Loss 0.436259    Objective Loss 0.436259                                        LR 0.000008    Time 0.530351    
2023-06-16 04:54:11,544 - Epoch: [181][  130/  142]    Overall Loss 0.439404    Objective Loss 0.439404                                        LR 0.000008    Time 0.527132    
2023-06-16 04:54:16,238 - Epoch: [181][  140/  142]    Overall Loss 0.439486    Objective Loss 0.439486                                        LR 0.000008    Time 0.523008    
2023-06-16 04:54:17,094 - Epoch: [181][  142/  142]    Overall Loss 0.439532    Objective Loss 0.439532    Top1 84.375000    LR 0.000008    Time 0.521663    
2023-06-16 04:54:17,745 - --- validate (epoch=181)-----------
2023-06-16 04:54:17,746 - 1422 samples (32 per mini-batch)
2023-06-16 04:54:25,854 - Epoch: [181][   10/   45]    Loss 0.869715    Top1 76.562500    
2023-06-16 04:54:29,917 - Epoch: [181][   20/   45]    Loss 0.883217    Top1 76.406250    
2023-06-16 04:54:34,460 - Epoch: [181][   30/   45]    Loss 0.887802    Top1 75.000000    
2023-06-16 04:54:38,702 - Epoch: [181][   40/   45]    Loss 0.900574    Top1 74.843750    
2023-06-16 04:54:40,089 - Epoch: [181][   45/   45]    Loss 0.889492    Top1 74.964838    
2023-06-16 04:54:40,737 - ==> Top1: 74.965    Loss: 0.889

2023-06-16 04:54:40,740 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:54:40,740 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:54:40,754 - 

2023-06-16 04:54:40,754 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:54:50,218 - Epoch: [182][   10/  142]    Overall Loss 0.388272    Objective Loss 0.388272                                        LR 0.000008    Time 0.946287    
2023-06-16 04:54:55,147 - Epoch: [182][   20/  142]    Overall Loss 0.409095    Objective Loss 0.409095                                        LR 0.000008    Time 0.719553    
2023-06-16 04:55:00,133 - Epoch: [182][   30/  142]    Overall Loss 0.409301    Objective Loss 0.409301                                        LR 0.000008    Time 0.645887    
2023-06-16 04:55:05,151 - Epoch: [182][   40/  142]    Overall Loss 0.425117    Objective Loss 0.425117                                        LR 0.000008    Time 0.609862    
2023-06-16 04:55:10,118 - Epoch: [182][   50/  142]    Overall Loss 0.415762    Objective Loss 0.415762                                        LR 0.000008    Time 0.587204    
2023-06-16 04:55:15,012 - Epoch: [182][   60/  142]    Overall Loss 0.426001    Objective Loss 0.426001                                        LR 0.000008    Time 0.570888    
2023-06-16 04:55:19,958 - Epoch: [182][   70/  142]    Overall Loss 0.429794    Objective Loss 0.429794                                        LR 0.000008    Time 0.559986    
2023-06-16 04:55:24,918 - Epoch: [182][   80/  142]    Overall Loss 0.429545    Objective Loss 0.429545                                        LR 0.000008    Time 0.551987    
2023-06-16 04:55:29,923 - Epoch: [182][   90/  142]    Overall Loss 0.426952    Objective Loss 0.426952                                        LR 0.000008    Time 0.546258    
2023-06-16 04:55:34,915 - Epoch: [182][  100/  142]    Overall Loss 0.424310    Objective Loss 0.424310                                        LR 0.000008    Time 0.541545    
2023-06-16 04:55:39,955 - Epoch: [182][  110/  142]    Overall Loss 0.425208    Objective Loss 0.425208                                        LR 0.000008    Time 0.538126    
2023-06-16 04:55:44,957 - Epoch: [182][  120/  142]    Overall Loss 0.427076    Objective Loss 0.427076                                        LR 0.000008    Time 0.534963    
2023-06-16 04:55:49,860 - Epoch: [182][  130/  142]    Overall Loss 0.424487    Objective Loss 0.424487                                        LR 0.000008    Time 0.531520    
2023-06-16 04:55:54,490 - Epoch: [182][  140/  142]    Overall Loss 0.424962    Objective Loss 0.424962                                        LR 0.000008    Time 0.526624    
2023-06-16 04:55:55,337 - Epoch: [182][  142/  142]    Overall Loss 0.424987    Objective Loss 0.424987    Top1 92.187500    LR 0.000008    Time 0.525170    
2023-06-16 04:55:55,961 - --- validate (epoch=182)-----------
2023-06-16 04:55:55,961 - 1422 samples (32 per mini-batch)
2023-06-16 04:56:04,063 - Epoch: [182][   10/   45]    Loss 0.942789    Top1 74.062500    
2023-06-16 04:56:08,505 - Epoch: [182][   20/   45]    Loss 0.906072    Top1 75.312500    
2023-06-16 04:56:13,163 - Epoch: [182][   30/   45]    Loss 0.906626    Top1 75.208333    
2023-06-16 04:56:17,632 - Epoch: [182][   40/   45]    Loss 0.893836    Top1 76.015625    
2023-06-16 04:56:18,925 - Epoch: [182][   45/   45]    Loss 0.917544    Top1 75.316456    
2023-06-16 04:56:19,585 - ==> Top1: 75.316    Loss: 0.918

2023-06-16 04:56:19,587 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:56:19,587 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:56:19,608 - 

2023-06-16 04:56:19,608 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:56:28,776 - Epoch: [183][   10/  142]    Overall Loss 0.412866    Objective Loss 0.412866                                        LR 0.000008    Time 0.916638    
2023-06-16 04:56:33,699 - Epoch: [183][   20/  142]    Overall Loss 0.400181    Objective Loss 0.400181                                        LR 0.000008    Time 0.704482    
2023-06-16 04:56:38,653 - Epoch: [183][   30/  142]    Overall Loss 0.436973    Objective Loss 0.436973                                        LR 0.000008    Time 0.634770    
2023-06-16 04:56:43,546 - Epoch: [183][   40/  142]    Overall Loss 0.419938    Objective Loss 0.419938                                        LR 0.000008    Time 0.598378    
2023-06-16 04:56:48,498 - Epoch: [183][   50/  142]    Overall Loss 0.412954    Objective Loss 0.412954                                        LR 0.000008    Time 0.577732    
2023-06-16 04:56:53,534 - Epoch: [183][   60/  142]    Overall Loss 0.407548    Objective Loss 0.407548                                        LR 0.000008    Time 0.565372    
2023-06-16 04:56:58,390 - Epoch: [183][   70/  142]    Overall Loss 0.417030    Objective Loss 0.417030                                        LR 0.000008    Time 0.553957    
2023-06-16 04:57:03,475 - Epoch: [183][   80/  142]    Overall Loss 0.418940    Objective Loss 0.418940                                        LR 0.000008    Time 0.548269    
2023-06-16 04:57:08,311 - Epoch: [183][   90/  142]    Overall Loss 0.422261    Objective Loss 0.422261                                        LR 0.000008    Time 0.541081    
2023-06-16 04:57:13,264 - Epoch: [183][  100/  142]    Overall Loss 0.418757    Objective Loss 0.418757                                        LR 0.000008    Time 0.536501    
2023-06-16 04:57:18,178 - Epoch: [183][  110/  142]    Overall Loss 0.425218    Objective Loss 0.425218                                        LR 0.000008    Time 0.532400    
2023-06-16 04:57:23,127 - Epoch: [183][  120/  142]    Overall Loss 0.422744    Objective Loss 0.422744                                        LR 0.000008    Time 0.529264    
2023-06-16 04:57:28,039 - Epoch: [183][  130/  142]    Overall Loss 0.424389    Objective Loss 0.424389                                        LR 0.000008    Time 0.526336    
2023-06-16 04:57:32,701 - Epoch: [183][  140/  142]    Overall Loss 0.423580    Objective Loss 0.423580                                        LR 0.000008    Time 0.522037    
2023-06-16 04:57:33,543 - Epoch: [183][  142/  142]    Overall Loss 0.425497    Objective Loss 0.425497    Top1 79.687500    LR 0.000008    Time 0.520614    
2023-06-16 04:57:34,185 - --- validate (epoch=183)-----------
2023-06-16 04:57:34,186 - 1422 samples (32 per mini-batch)
2023-06-16 04:57:42,462 - Epoch: [183][   10/   45]    Loss 0.934516    Top1 74.062500    
2023-06-16 04:57:46,410 - Epoch: [183][   20/   45]    Loss 0.920834    Top1 74.531250    
2023-06-16 04:57:50,779 - Epoch: [183][   30/   45]    Loss 0.951439    Top1 73.541667    
2023-06-16 04:57:55,141 - Epoch: [183][   40/   45]    Loss 0.914267    Top1 75.234375    
2023-06-16 04:57:56,523 - Epoch: [183][   45/   45]    Loss 0.943062    Top1 75.105485    
2023-06-16 04:57:57,182 - ==> Top1: 75.105    Loss: 0.943

2023-06-16 04:57:57,184 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:57:57,184 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:57:57,205 - 

2023-06-16 04:57:57,206 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:58:06,459 - Epoch: [184][   10/  142]    Overall Loss 0.425324    Objective Loss 0.425324                                        LR 0.000008    Time 0.925223    
2023-06-16 04:58:11,315 - Epoch: [184][   20/  142]    Overall Loss 0.432873    Objective Loss 0.432873                                        LR 0.000008    Time 0.705384    
2023-06-16 04:58:16,406 - Epoch: [184][   30/  142]    Overall Loss 0.431255    Objective Loss 0.431255                                        LR 0.000008    Time 0.639965    
2023-06-16 04:58:21,203 - Epoch: [184][   40/  142]    Overall Loss 0.437028    Objective Loss 0.437028                                        LR 0.000008    Time 0.599882    
2023-06-16 04:58:26,180 - Epoch: [184][   50/  142]    Overall Loss 0.432824    Objective Loss 0.432824                                        LR 0.000008    Time 0.579428    
2023-06-16 04:58:31,084 - Epoch: [184][   60/  142]    Overall Loss 0.428773    Objective Loss 0.428773                                        LR 0.000008    Time 0.564576    
2023-06-16 04:58:36,036 - Epoch: [184][   70/  142]    Overall Loss 0.429559    Objective Loss 0.429559                                        LR 0.000008    Time 0.554662    
2023-06-16 04:58:40,951 - Epoch: [184][   80/  142]    Overall Loss 0.434636    Objective Loss 0.434636                                        LR 0.000008    Time 0.546757    
2023-06-16 04:58:45,908 - Epoch: [184][   90/  142]    Overall Loss 0.429713    Objective Loss 0.429713                                        LR 0.000008    Time 0.541080    
2023-06-16 04:58:50,895 - Epoch: [184][  100/  142]    Overall Loss 0.424670    Objective Loss 0.424670                                        LR 0.000008    Time 0.536836    
2023-06-16 04:58:55,786 - Epoch: [184][  110/  142]    Overall Loss 0.426836    Objective Loss 0.426836                                        LR 0.000008    Time 0.532493    
2023-06-16 04:59:00,740 - Epoch: [184][  120/  142]    Overall Loss 0.426448    Objective Loss 0.426448                                        LR 0.000008    Time 0.529395    
2023-06-16 04:59:05,587 - Epoch: [184][  130/  142]    Overall Loss 0.430215    Objective Loss 0.430215                                        LR 0.000008    Time 0.525949    
2023-06-16 04:59:10,306 - Epoch: [184][  140/  142]    Overall Loss 0.430625    Objective Loss 0.430625                                        LR 0.000008    Time 0.522088    
2023-06-16 04:59:11,150 - Epoch: [184][  142/  142]    Overall Loss 0.430144    Objective Loss 0.430144    Top1 89.062500    LR 0.000008    Time 0.520674    
2023-06-16 04:59:11,790 - --- validate (epoch=184)-----------
2023-06-16 04:59:11,790 - 1422 samples (32 per mini-batch)
2023-06-16 04:59:20,020 - Epoch: [184][   10/   45]    Loss 0.719883    Top1 79.375000    
2023-06-16 04:59:24,374 - Epoch: [184][   20/   45]    Loss 0.880222    Top1 75.468750    
2023-06-16 04:59:29,144 - Epoch: [184][   30/   45]    Loss 0.911780    Top1 75.416667    
2023-06-16 04:59:33,829 - Epoch: [184][   40/   45]    Loss 0.925387    Top1 74.687500    
2023-06-16 04:59:35,175 - Epoch: [184][   45/   45]    Loss 0.901041    Top1 74.964838    
2023-06-16 04:59:35,828 - ==> Top1: 74.965    Loss: 0.901

2023-06-16 04:59:35,830 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 04:59:35,830 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 04:59:35,851 - 

2023-06-16 04:59:35,851 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 04:59:45,047 - Epoch: [185][   10/  142]    Overall Loss 0.414667    Objective Loss 0.414667                                        LR 0.000008    Time 0.919422    
2023-06-16 04:59:49,824 - Epoch: [185][   20/  142]    Overall Loss 0.395043    Objective Loss 0.395043                                        LR 0.000008    Time 0.698556    
2023-06-16 04:59:54,775 - Epoch: [185][   30/  142]    Overall Loss 0.406824    Objective Loss 0.406824                                        LR 0.000008    Time 0.630704    
2023-06-16 04:59:59,689 - Epoch: [185][   40/  142]    Overall Loss 0.418811    Objective Loss 0.418811                                        LR 0.000008    Time 0.595874    
2023-06-16 05:00:04,590 - Epoch: [185][   50/  142]    Overall Loss 0.416086    Objective Loss 0.416086                                        LR 0.000008    Time 0.574710    
2023-06-16 05:00:09,548 - Epoch: [185][   60/  142]    Overall Loss 0.415507    Objective Loss 0.415507                                        LR 0.000008    Time 0.561539    
2023-06-16 05:00:14,380 - Epoch: [185][   70/  142]    Overall Loss 0.418161    Objective Loss 0.418161                                        LR 0.000008    Time 0.550338    
2023-06-16 05:00:19,239 - Epoch: [185][   80/  142]    Overall Loss 0.407542    Objective Loss 0.407542                                        LR 0.000008    Time 0.542278    
2023-06-16 05:00:24,245 - Epoch: [185][   90/  142]    Overall Loss 0.413317    Objective Loss 0.413317                                        LR 0.000008    Time 0.537644    
2023-06-16 05:00:29,083 - Epoch: [185][  100/  142]    Overall Loss 0.409422    Objective Loss 0.409422                                        LR 0.000008    Time 0.532250    
2023-06-16 05:00:34,097 - Epoch: [185][  110/  142]    Overall Loss 0.415581    Objective Loss 0.415581                                        LR 0.000008    Time 0.529446    
2023-06-16 05:00:38,952 - Epoch: [185][  120/  142]    Overall Loss 0.412831    Objective Loss 0.412831                                        LR 0.000008    Time 0.525774    
2023-06-16 05:00:43,852 - Epoch: [185][  130/  142]    Overall Loss 0.417506    Objective Loss 0.417506                                        LR 0.000008    Time 0.523019    
2023-06-16 05:00:48,546 - Epoch: [185][  140/  142]    Overall Loss 0.419664    Objective Loss 0.419664                                        LR 0.000008    Time 0.519183    
2023-06-16 05:00:49,403 - Epoch: [185][  142/  142]    Overall Loss 0.420834    Objective Loss 0.420834    Top1 87.500000    LR 0.000008    Time 0.517903    
2023-06-16 05:00:50,014 - --- validate (epoch=185)-----------
2023-06-16 05:00:50,014 - 1422 samples (32 per mini-batch)
2023-06-16 05:00:57,958 - Epoch: [185][   10/   45]    Loss 0.931504    Top1 74.062500    
2023-06-16 05:01:02,004 - Epoch: [185][   20/   45]    Loss 0.920716    Top1 74.218750    
2023-06-16 05:01:06,699 - Epoch: [185][   30/   45]    Loss 0.837885    Top1 75.833333    
2023-06-16 05:01:11,371 - Epoch: [185][   40/   45]    Loss 0.880229    Top1 74.843750    
2023-06-16 05:01:12,716 - Epoch: [185][   45/   45]    Loss 0.888979    Top1 75.035162    
2023-06-16 05:01:13,368 - ==> Top1: 75.035    Loss: 0.889

2023-06-16 05:01:13,370 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 05:01:13,370 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 05:01:13,391 - 

2023-06-16 05:01:13,391 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 05:01:22,774 - Epoch: [186][   10/  142]    Overall Loss 0.428720    Objective Loss 0.428720                                        LR 0.000008    Time 0.938180    
2023-06-16 05:01:27,705 - Epoch: [186][   20/  142]    Overall Loss 0.379007    Objective Loss 0.379007                                        LR 0.000008    Time 0.715571    
2023-06-16 05:01:32,727 - Epoch: [186][   30/  142]    Overall Loss 0.426593    Objective Loss 0.426593                                        LR 0.000008    Time 0.644422    
2023-06-16 05:01:37,652 - Epoch: [186][   40/  142]    Overall Loss 0.430728    Objective Loss 0.430728                                        LR 0.000008    Time 0.606438    
2023-06-16 05:01:42,615 - Epoch: [186][   50/  142]    Overall Loss 0.446914    Objective Loss 0.446914                                        LR 0.000008    Time 0.584404    
2023-06-16 05:01:47,499 - Epoch: [186][   60/  142]    Overall Loss 0.434069    Objective Loss 0.434069                                        LR 0.000008    Time 0.568389    
2023-06-16 05:01:52,531 - Epoch: [186][   70/  142]    Overall Loss 0.430876    Objective Loss 0.430876                                        LR 0.000008    Time 0.559057    
2023-06-16 05:01:57,484 - Epoch: [186][   80/  142]    Overall Loss 0.426351    Objective Loss 0.426351                                        LR 0.000008    Time 0.551087    
2023-06-16 05:02:02,514 - Epoch: [186][   90/  142]    Overall Loss 0.422532    Objective Loss 0.422532                                        LR 0.000008    Time 0.545738    
2023-06-16 05:02:07,472 - Epoch: [186][  100/  142]    Overall Loss 0.420349    Objective Loss 0.420349                                        LR 0.000008    Time 0.540732    
2023-06-16 05:02:12,382 - Epoch: [186][  110/  142]    Overall Loss 0.419846    Objective Loss 0.419846                                        LR 0.000008    Time 0.536211    
2023-06-16 05:02:17,377 - Epoch: [186][  120/  142]    Overall Loss 0.423371    Objective Loss 0.423371                                        LR 0.000008    Time 0.533139    
2023-06-16 05:02:22,324 - Epoch: [186][  130/  142]    Overall Loss 0.422283    Objective Loss 0.422283                                        LR 0.000008    Time 0.530181    
2023-06-16 05:02:26,918 - Epoch: [186][  140/  142]    Overall Loss 0.429401    Objective Loss 0.429401                                        LR 0.000008    Time 0.525121    
2023-06-16 05:02:27,758 - Epoch: [186][  142/  142]    Overall Loss 0.427915    Objective Loss 0.427915    Top1 89.062500    LR 0.000008    Time 0.523641    
2023-06-16 05:02:28,395 - --- validate (epoch=186)-----------
2023-06-16 05:02:28,396 - 1422 samples (32 per mini-batch)
2023-06-16 05:02:36,264 - Epoch: [186][   10/   45]    Loss 0.854248    Top1 74.062500    
2023-06-16 05:02:40,730 - Epoch: [186][   20/   45]    Loss 0.875567    Top1 75.312500    
2023-06-16 05:02:45,128 - Epoch: [186][   30/   45]    Loss 0.894938    Top1 75.520833    
2023-06-16 05:02:49,905 - Epoch: [186][   40/   45]    Loss 0.904907    Top1 75.000000    
2023-06-16 05:02:51,262 - Epoch: [186][   45/   45]    Loss 0.907584    Top1 75.105485    
2023-06-16 05:02:51,889 - ==> Top1: 75.105    Loss: 0.908

2023-06-16 05:02:51,891 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 05:02:51,891 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 05:02:51,913 - 

2023-06-16 05:02:51,913 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 05:03:01,468 - Epoch: [187][   10/  142]    Overall Loss 0.405551    Objective Loss 0.405551                                        LR 0.000008    Time 0.955425    
2023-06-16 05:03:06,382 - Epoch: [187][   20/  142]    Overall Loss 0.420199    Objective Loss 0.420199                                        LR 0.000008    Time 0.723360    
2023-06-16 05:03:11,507 - Epoch: [187][   30/  142]    Overall Loss 0.407900    Objective Loss 0.407900                                        LR 0.000008    Time 0.653033    
2023-06-16 05:03:16,517 - Epoch: [187][   40/  142]    Overall Loss 0.413474    Objective Loss 0.413474                                        LR 0.000008    Time 0.615031    
2023-06-16 05:03:21,523 - Epoch: [187][   50/  142]    Overall Loss 0.414669    Objective Loss 0.414669                                        LR 0.000008    Time 0.592134    
2023-06-16 05:03:26,564 - Epoch: [187][   60/  142]    Overall Loss 0.410039    Objective Loss 0.410039                                        LR 0.000008    Time 0.577453    
2023-06-16 05:03:31,590 - Epoch: [187][   70/  142]    Overall Loss 0.409057    Objective Loss 0.409057                                        LR 0.000008    Time 0.566743    
2023-06-16 05:03:36,490 - Epoch: [187][   80/  142]    Overall Loss 0.418215    Objective Loss 0.418215                                        LR 0.000008    Time 0.557145    
2023-06-16 05:03:41,577 - Epoch: [187][   90/  142]    Overall Loss 0.422906    Objective Loss 0.422906                                        LR 0.000008    Time 0.551754    
2023-06-16 05:03:46,505 - Epoch: [187][  100/  142]    Overall Loss 0.420259    Objective Loss 0.420259                                        LR 0.000008    Time 0.545858    
2023-06-16 05:03:51,519 - Epoch: [187][  110/  142]    Overall Loss 0.421943    Objective Loss 0.421943                                        LR 0.000008    Time 0.541807    
2023-06-16 05:03:56,538 - Epoch: [187][  120/  142]    Overall Loss 0.422897    Objective Loss 0.422897                                        LR 0.000008    Time 0.538484    
2023-06-16 05:04:01,642 - Epoch: [187][  130/  142]    Overall Loss 0.426169    Objective Loss 0.426169                                        LR 0.000008    Time 0.536317    
2023-06-16 05:04:06,297 - Epoch: [187][  140/  142]    Overall Loss 0.423391    Objective Loss 0.423391                                        LR 0.000008    Time 0.531255    
2023-06-16 05:04:07,139 - Epoch: [187][  142/  142]    Overall Loss 0.427196    Objective Loss 0.427196    Top1 79.687500    LR 0.000008    Time 0.529701    
2023-06-16 05:04:07,782 - --- validate (epoch=187)-----------
2023-06-16 05:04:07,782 - 1422 samples (32 per mini-batch)
2023-06-16 05:04:15,479 - Epoch: [187][   10/   45]    Loss 0.930275    Top1 76.250000    
2023-06-16 05:04:19,997 - Epoch: [187][   20/   45]    Loss 0.956523    Top1 76.093750    
2023-06-16 05:04:24,265 - Epoch: [187][   30/   45]    Loss 0.946485    Top1 74.791667    
2023-06-16 05:04:29,051 - Epoch: [187][   40/   45]    Loss 0.942539    Top1 75.390625    
2023-06-16 05:04:30,399 - Epoch: [187][   45/   45]    Loss 0.937356    Top1 74.894515    
2023-06-16 05:04:31,055 - ==> Top1: 74.895    Loss: 0.937

2023-06-16 05:04:31,057 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 05:04:31,057 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 05:04:31,078 - 

2023-06-16 05:04:31,078 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 05:04:40,482 - Epoch: [188][   10/  142]    Overall Loss 0.406627    Objective Loss 0.406627                                        LR 0.000008    Time 0.940269    
2023-06-16 05:04:45,429 - Epoch: [188][   20/  142]    Overall Loss 0.410878    Objective Loss 0.410878                                        LR 0.000008    Time 0.717462    
2023-06-16 05:04:50,328 - Epoch: [188][   30/  142]    Overall Loss 0.438147    Objective Loss 0.438147                                        LR 0.000008    Time 0.641616    
2023-06-16 05:04:55,311 - Epoch: [188][   40/  142]    Overall Loss 0.421363    Objective Loss 0.421363                                        LR 0.000008    Time 0.605756    
2023-06-16 05:05:00,233 - Epoch: [188][   50/  142]    Overall Loss 0.435519    Objective Loss 0.435519                                        LR 0.000008    Time 0.583046    
2023-06-16 05:05:05,185 - Epoch: [188][   60/  142]    Overall Loss 0.435566    Objective Loss 0.435566                                        LR 0.000008    Time 0.568394    
2023-06-16 05:05:10,159 - Epoch: [188][   70/  142]    Overall Loss 0.434706    Objective Loss 0.434706                                        LR 0.000008    Time 0.558244    
2023-06-16 05:05:15,054 - Epoch: [188][   80/  142]    Overall Loss 0.435263    Objective Loss 0.435263                                        LR 0.000008    Time 0.549640    
2023-06-16 05:05:19,966 - Epoch: [188][   90/  142]    Overall Loss 0.440048    Objective Loss 0.440048                                        LR 0.000008    Time 0.543144    
2023-06-16 05:05:24,880 - Epoch: [188][  100/  142]    Overall Loss 0.432736    Objective Loss 0.432736                                        LR 0.000008    Time 0.537959    
2023-06-16 05:05:29,838 - Epoch: [188][  110/  142]    Overall Loss 0.426932    Objective Loss 0.426932                                        LR 0.000008    Time 0.534124    
2023-06-16 05:05:34,760 - Epoch: [188][  120/  142]    Overall Loss 0.429674    Objective Loss 0.429674                                        LR 0.000008    Time 0.530611    
2023-06-16 05:05:39,706 - Epoch: [188][  130/  142]    Overall Loss 0.431167    Objective Loss 0.431167                                        LR 0.000008    Time 0.527837    
2023-06-16 05:05:44,378 - Epoch: [188][  140/  142]    Overall Loss 0.429389    Objective Loss 0.429389                                        LR 0.000008    Time 0.523498    
2023-06-16 05:05:45,219 - Epoch: [188][  142/  142]    Overall Loss 0.427863    Objective Loss 0.427863    Top1 84.375000    LR 0.000008    Time 0.522051    
2023-06-16 05:05:45,881 - --- validate (epoch=188)-----------
2023-06-16 05:05:45,882 - 1422 samples (32 per mini-batch)
2023-06-16 05:05:54,026 - Epoch: [188][   10/   45]    Loss 0.952260    Top1 72.812500    
2023-06-16 05:05:58,060 - Epoch: [188][   20/   45]    Loss 0.930166    Top1 74.375000    
2023-06-16 05:06:02,873 - Epoch: [188][   30/   45]    Loss 0.871016    Top1 75.937500    
2023-06-16 05:06:07,003 - Epoch: [188][   40/   45]    Loss 0.931328    Top1 74.921875    
2023-06-16 05:06:08,610 - Epoch: [188][   45/   45]    Loss 0.910236    Top1 75.105485    
2023-06-16 05:06:09,257 - ==> Top1: 75.105    Loss: 0.910

2023-06-16 05:06:09,259 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 05:06:09,259 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 05:06:09,280 - 

2023-06-16 05:06:09,280 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 05:06:18,509 - Epoch: [189][   10/  142]    Overall Loss 0.424884    Objective Loss 0.424884                                        LR 0.000008    Time 0.922816    
2023-06-16 05:06:23,635 - Epoch: [189][   20/  142]    Overall Loss 0.411204    Objective Loss 0.411204                                        LR 0.000008    Time 0.717653    
2023-06-16 05:06:28,557 - Epoch: [189][   30/  142]    Overall Loss 0.400431    Objective Loss 0.400431                                        LR 0.000008    Time 0.642468    
2023-06-16 05:06:33,701 - Epoch: [189][   40/  142]    Overall Loss 0.417535    Objective Loss 0.417535                                        LR 0.000008    Time 0.610434    
2023-06-16 05:06:38,640 - Epoch: [189][   50/  142]    Overall Loss 0.429547    Objective Loss 0.429547                                        LR 0.000008    Time 0.587123    
2023-06-16 05:06:43,625 - Epoch: [189][   60/  142]    Overall Loss 0.442814    Objective Loss 0.442814                                        LR 0.000008    Time 0.572331    
2023-06-16 05:06:48,602 - Epoch: [189][   70/  142]    Overall Loss 0.433888    Objective Loss 0.433888                                        LR 0.000008    Time 0.561668    
2023-06-16 05:06:53,673 - Epoch: [189][   80/  142]    Overall Loss 0.424225    Objective Loss 0.424225                                        LR 0.000008    Time 0.554833    
2023-06-16 05:06:58,619 - Epoch: [189][   90/  142]    Overall Loss 0.421167    Objective Loss 0.421167                                        LR 0.000008    Time 0.548129    
2023-06-16 05:07:03,617 - Epoch: [189][  100/  142]    Overall Loss 0.420022    Objective Loss 0.420022                                        LR 0.000008    Time 0.543296    
2023-06-16 05:07:08,558 - Epoch: [189][  110/  142]    Overall Loss 0.425314    Objective Loss 0.425314                                        LR 0.000008    Time 0.538814    
2023-06-16 05:07:13,561 - Epoch: [189][  120/  142]    Overall Loss 0.421310    Objective Loss 0.421310                                        LR 0.000008    Time 0.535603    
2023-06-16 05:07:18,547 - Epoch: [189][  130/  142]    Overall Loss 0.421585    Objective Loss 0.421585                                        LR 0.000008    Time 0.532756    
2023-06-16 05:07:23,235 - Epoch: [189][  140/  142]    Overall Loss 0.423873    Objective Loss 0.423873                                        LR 0.000008    Time 0.528184    
2023-06-16 05:07:24,091 - Epoch: [189][  142/  142]    Overall Loss 0.423983    Objective Loss 0.423983    Top1 89.062500    LR 0.000008    Time 0.526771    
2023-06-16 05:07:24,737 - --- validate (epoch=189)-----------
2023-06-16 05:07:24,738 - 1422 samples (32 per mini-batch)
2023-06-16 05:07:32,924 - Epoch: [189][   10/   45]    Loss 0.927780    Top1 74.687500    
2023-06-16 05:07:37,246 - Epoch: [189][   20/   45]    Loss 0.887566    Top1 75.156250    
2023-06-16 05:07:41,973 - Epoch: [189][   30/   45]    Loss 0.899655    Top1 75.000000    
2023-06-16 05:07:46,443 - Epoch: [189][   40/   45]    Loss 0.906476    Top1 75.156250    
2023-06-16 05:07:47,762 - Epoch: [189][   45/   45]    Loss 0.894980    Top1 75.175809    
2023-06-16 05:07:48,403 - ==> Top1: 75.176    Loss: 0.895

2023-06-16 05:07:48,405 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 05:07:48,405 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 05:07:48,426 - 

2023-06-16 05:07:48,426 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 05:07:57,823 - Epoch: [190][   10/  142]    Overall Loss 0.413612    Objective Loss 0.413612                                        LR 0.000008    Time 0.939555    
2023-06-16 05:08:02,712 - Epoch: [190][   20/  142]    Overall Loss 0.397650    Objective Loss 0.397650                                        LR 0.000008    Time 0.714176    
2023-06-16 05:08:07,672 - Epoch: [190][   30/  142]    Overall Loss 0.415435    Objective Loss 0.415435                                        LR 0.000008    Time 0.641455    
2023-06-16 05:08:12,671 - Epoch: [190][   40/  142]    Overall Loss 0.405403    Objective Loss 0.405403                                        LR 0.000008    Time 0.606032    
2023-06-16 05:08:17,665 - Epoch: [190][   50/  142]    Overall Loss 0.412202    Objective Loss 0.412202                                        LR 0.000008    Time 0.584708    
2023-06-16 05:08:22,532 - Epoch: [190][   60/  142]    Overall Loss 0.405462    Objective Loss 0.405462                                        LR 0.000008    Time 0.568365    
2023-06-16 05:08:27,540 - Epoch: [190][   70/  142]    Overall Loss 0.416064    Objective Loss 0.416064                                        LR 0.000008    Time 0.558705    
2023-06-16 05:08:32,535 - Epoch: [190][   80/  142]    Overall Loss 0.408940    Objective Loss 0.408940                                        LR 0.000008    Time 0.551290    
2023-06-16 05:08:37,410 - Epoch: [190][   90/  142]    Overall Loss 0.414524    Objective Loss 0.414524                                        LR 0.000008    Time 0.544198    
2023-06-16 05:08:42,341 - Epoch: [190][  100/  142]    Overall Loss 0.417847    Objective Loss 0.417847                                        LR 0.000008    Time 0.539084    
2023-06-16 05:08:47,268 - Epoch: [190][  110/  142]    Overall Loss 0.411674    Objective Loss 0.411674                                        LR 0.000008    Time 0.534860    
2023-06-16 05:08:52,231 - Epoch: [190][  120/  142]    Overall Loss 0.415003    Objective Loss 0.415003                                        LR 0.000008    Time 0.531643    
2023-06-16 05:08:57,327 - Epoch: [190][  130/  142]    Overall Loss 0.415319    Objective Loss 0.415319                                        LR 0.000008    Time 0.529944    
2023-06-16 05:09:01,815 - Epoch: [190][  140/  142]    Overall Loss 0.417800    Objective Loss 0.417800                                        LR 0.000008    Time 0.524145    
2023-06-16 05:09:02,655 - Epoch: [190][  142/  142]    Overall Loss 0.417515    Objective Loss 0.417515    Top1 89.062500    LR 0.000008    Time 0.522677    
2023-06-16 05:09:03,309 - --- validate (epoch=190)-----------
2023-06-16 05:09:03,310 - 1422 samples (32 per mini-batch)
2023-06-16 05:09:11,404 - Epoch: [190][   10/   45]    Loss 0.863732    Top1 75.625000    
2023-06-16 05:09:15,830 - Epoch: [190][   20/   45]    Loss 0.846138    Top1 76.406250    
2023-06-16 05:09:20,448 - Epoch: [190][   30/   45]    Loss 0.890623    Top1 75.520833    
2023-06-16 05:09:25,007 - Epoch: [190][   40/   45]    Loss 0.896527    Top1 75.468750    
2023-06-16 05:09:26,350 - Epoch: [190][   45/   45]    Loss 0.899299    Top1 75.105485    
2023-06-16 05:09:27,001 - ==> Top1: 75.105    Loss: 0.899

2023-06-16 05:09:27,004 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 05:09:27,004 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 05:09:27,024 - 

2023-06-16 05:09:27,025 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 05:09:36,403 - Epoch: [191][   10/  142]    Overall Loss 0.448005    Objective Loss 0.448005                                        LR 0.000008    Time 0.937715    
2023-06-16 05:09:41,417 - Epoch: [191][   20/  142]    Overall Loss 0.397304    Objective Loss 0.397304                                        LR 0.000008    Time 0.719540    
2023-06-16 05:09:46,445 - Epoch: [191][   30/  142]    Overall Loss 0.410741    Objective Loss 0.410741                                        LR 0.000008    Time 0.647278    
2023-06-16 05:09:51,385 - Epoch: [191][   40/  142]    Overall Loss 0.426681    Objective Loss 0.426681                                        LR 0.000008    Time 0.608937    
2023-06-16 05:09:56,355 - Epoch: [191][   50/  142]    Overall Loss 0.436390    Objective Loss 0.436390                                        LR 0.000008    Time 0.586547    
2023-06-16 05:10:01,383 - Epoch: [191][   60/  142]    Overall Loss 0.435720    Objective Loss 0.435720                                        LR 0.000008    Time 0.572565    
2023-06-16 05:10:06,483 - Epoch: [191][   70/  142]    Overall Loss 0.429408    Objective Loss 0.429408                                        LR 0.000008    Time 0.563624    
2023-06-16 05:10:11,448 - Epoch: [191][   80/  142]    Overall Loss 0.428652    Objective Loss 0.428652                                        LR 0.000008    Time 0.555238    
2023-06-16 05:10:16,370 - Epoch: [191][   90/  142]    Overall Loss 0.425067    Objective Loss 0.425067                                        LR 0.000008    Time 0.548224    
2023-06-16 05:10:21,488 - Epoch: [191][  100/  142]    Overall Loss 0.419801    Objective Loss 0.419801                                        LR 0.000008    Time 0.544572    
2023-06-16 05:10:26,500 - Epoch: [191][  110/  142]    Overall Loss 0.424675    Objective Loss 0.424675                                        LR 0.000008    Time 0.540627    
2023-06-16 05:10:31,591 - Epoch: [191][  120/  142]    Overall Loss 0.428806    Objective Loss 0.428806                                        LR 0.000008    Time 0.537994    
2023-06-16 05:10:36,481 - Epoch: [191][  130/  142]    Overall Loss 0.433208    Objective Loss 0.433208                                        LR 0.000008    Time 0.534221    
2023-06-16 05:10:41,183 - Epoch: [191][  140/  142]    Overall Loss 0.433237    Objective Loss 0.433237                                        LR 0.000008    Time 0.529651    
2023-06-16 05:10:42,026 - Epoch: [191][  142/  142]    Overall Loss 0.431543    Objective Loss 0.431543    Top1 90.625000    LR 0.000008    Time 0.528125    
2023-06-16 05:10:42,684 - --- validate (epoch=191)-----------
2023-06-16 05:10:42,684 - 1422 samples (32 per mini-batch)
2023-06-16 05:10:50,652 - Epoch: [191][   10/   45]    Loss 1.059226    Top1 71.562500    
2023-06-16 05:10:55,280 - Epoch: [191][   20/   45]    Loss 0.940082    Top1 74.062500    
2023-06-16 05:10:59,567 - Epoch: [191][   30/   45]    Loss 0.955306    Top1 74.375000    
2023-06-16 05:11:03,861 - Epoch: [191][   40/   45]    Loss 0.930282    Top1 75.156250    
2023-06-16 05:11:05,181 - Epoch: [191][   45/   45]    Loss 0.922970    Top1 75.316456    
2023-06-16 05:11:05,820 - ==> Top1: 75.316    Loss: 0.923

2023-06-16 05:11:05,822 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 05:11:05,822 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 05:11:05,843 - 

2023-06-16 05:11:05,843 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 05:11:15,250 - Epoch: [192][   10/  142]    Overall Loss 0.444018    Objective Loss 0.444018                                        LR 0.000008    Time 0.940527    
2023-06-16 05:11:20,091 - Epoch: [192][   20/  142]    Overall Loss 0.452148    Objective Loss 0.452148                                        LR 0.000008    Time 0.712314    
2023-06-16 05:11:25,224 - Epoch: [192][   30/  142]    Overall Loss 0.421764    Objective Loss 0.421764                                        LR 0.000008    Time 0.645943    
2023-06-16 05:11:30,141 - Epoch: [192][   40/  142]    Overall Loss 0.409793    Objective Loss 0.409793                                        LR 0.000008    Time 0.607373    
2023-06-16 05:11:35,154 - Epoch: [192][   50/  142]    Overall Loss 0.411787    Objective Loss 0.411787                                        LR 0.000008    Time 0.586156    
2023-06-16 05:11:40,209 - Epoch: [192][   60/  142]    Overall Loss 0.416412    Objective Loss 0.416412                                        LR 0.000008    Time 0.572688    
2023-06-16 05:11:45,128 - Epoch: [192][   70/  142]    Overall Loss 0.421765    Objective Loss 0.421765                                        LR 0.000008    Time 0.561144    
2023-06-16 05:11:50,187 - Epoch: [192][   80/  142]    Overall Loss 0.418435    Objective Loss 0.418435                                        LR 0.000008    Time 0.554227    
2023-06-16 05:11:55,211 - Epoch: [192][   90/  142]    Overall Loss 0.411448    Objective Loss 0.411448                                        LR 0.000008    Time 0.548462    
2023-06-16 05:12:00,115 - Epoch: [192][  100/  142]    Overall Loss 0.416628    Objective Loss 0.416628                                        LR 0.000008    Time 0.542649    
2023-06-16 05:12:05,204 - Epoch: [192][  110/  142]    Overall Loss 0.424214    Objective Loss 0.424214                                        LR 0.000008    Time 0.539577    
2023-06-16 05:12:10,118 - Epoch: [192][  120/  142]    Overall Loss 0.420904    Objective Loss 0.420904                                        LR 0.000008    Time 0.535556    
2023-06-16 05:12:15,107 - Epoch: [192][  130/  142]    Overall Loss 0.418864    Objective Loss 0.418864                                        LR 0.000008    Time 0.532729    
2023-06-16 05:12:19,737 - Epoch: [192][  140/  142]    Overall Loss 0.422254    Objective Loss 0.422254                                        LR 0.000008    Time 0.527742    
2023-06-16 05:12:20,592 - Epoch: [192][  142/  142]    Overall Loss 0.422684    Objective Loss 0.422684    Top1 82.812500    LR 0.000008    Time 0.526330    
2023-06-16 05:12:21,244 - --- validate (epoch=192)-----------
2023-06-16 05:12:21,245 - 1422 samples (32 per mini-batch)
2023-06-16 05:12:29,455 - Epoch: [192][   10/   45]    Loss 0.891339    Top1 78.437500    
2023-06-16 05:12:33,879 - Epoch: [192][   20/   45]    Loss 0.916083    Top1 75.625000    
2023-06-16 05:12:38,732 - Epoch: [192][   30/   45]    Loss 0.890949    Top1 74.791667    
2023-06-16 05:12:43,304 - Epoch: [192][   40/   45]    Loss 0.897472    Top1 74.843750    
2023-06-16 05:12:44,606 - Epoch: [192][   45/   45]    Loss 0.948924    Top1 74.613221    
2023-06-16 05:12:45,259 - ==> Top1: 74.613    Loss: 0.949

2023-06-16 05:12:45,261 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 05:12:45,261 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 05:12:45,282 - 

2023-06-16 05:12:45,282 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 05:12:54,464 - Epoch: [193][   10/  142]    Overall Loss 0.510996    Objective Loss 0.510996                                        LR 0.000008    Time 0.918048    
2023-06-16 05:12:59,505 - Epoch: [193][   20/  142]    Overall Loss 0.453728    Objective Loss 0.453728                                        LR 0.000008    Time 0.711082    
2023-06-16 05:13:04,358 - Epoch: [193][   30/  142]    Overall Loss 0.458411    Objective Loss 0.458411                                        LR 0.000008    Time 0.635770    
2023-06-16 05:13:09,288 - Epoch: [193][   40/  142]    Overall Loss 0.448771    Objective Loss 0.448771                                        LR 0.000008    Time 0.600081    
2023-06-16 05:13:14,345 - Epoch: [193][   50/  142]    Overall Loss 0.448728    Objective Loss 0.448728                                        LR 0.000008    Time 0.581186    
2023-06-16 05:13:19,212 - Epoch: [193][   60/  142]    Overall Loss 0.445405    Objective Loss 0.445405                                        LR 0.000008    Time 0.565421    
2023-06-16 05:13:24,238 - Epoch: [193][   70/  142]    Overall Loss 0.438879    Objective Loss 0.438879                                        LR 0.000008    Time 0.556443    
2023-06-16 05:13:29,110 - Epoch: [193][   80/  142]    Overall Loss 0.426408    Objective Loss 0.426408                                        LR 0.000008    Time 0.547782    
2023-06-16 05:13:34,154 - Epoch: [193][   90/  142]    Overall Loss 0.414368    Objective Loss 0.414368                                        LR 0.000008    Time 0.542961    
2023-06-16 05:13:39,102 - Epoch: [193][  100/  142]    Overall Loss 0.412551    Objective Loss 0.412551                                        LR 0.000008    Time 0.538132    
2023-06-16 05:13:44,116 - Epoch: [193][  110/  142]    Overall Loss 0.411073    Objective Loss 0.411073                                        LR 0.000008    Time 0.534789    
2023-06-16 05:13:49,044 - Epoch: [193][  120/  142]    Overall Loss 0.410922    Objective Loss 0.410922                                        LR 0.000008    Time 0.531288    
2023-06-16 05:13:53,986 - Epoch: [193][  130/  142]    Overall Loss 0.414648    Objective Loss 0.414648                                        LR 0.000008    Time 0.528431    
2023-06-16 05:13:58,752 - Epoch: [193][  140/  142]    Overall Loss 0.419006    Objective Loss 0.419006                                        LR 0.000008    Time 0.524728    
2023-06-16 05:13:59,609 - Epoch: [193][  142/  142]    Overall Loss 0.420713    Objective Loss 0.420713    Top1 82.812500    LR 0.000008    Time 0.523369    
2023-06-16 05:14:00,266 - --- validate (epoch=193)-----------
2023-06-16 05:14:00,267 - 1422 samples (32 per mini-batch)
2023-06-16 05:14:08,378 - Epoch: [193][   10/   45]    Loss 0.997452    Top1 71.875000    
2023-06-16 05:14:12,436 - Epoch: [193][   20/   45]    Loss 0.959931    Top1 73.593750    
2023-06-16 05:14:17,250 - Epoch: [193][   30/   45]    Loss 0.925082    Top1 74.166667    
2023-06-16 05:14:21,488 - Epoch: [193][   40/   45]    Loss 0.874007    Top1 75.078125    
2023-06-16 05:14:22,847 - Epoch: [193][   45/   45]    Loss 0.880521    Top1 74.894515    
2023-06-16 05:14:23,484 - ==> Top1: 74.895    Loss: 0.881

2023-06-16 05:14:23,487 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 05:14:23,487 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 05:14:23,508 - 

2023-06-16 05:14:23,508 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 05:14:32,997 - Epoch: [194][   10/  142]    Overall Loss 0.433967    Objective Loss 0.433967                                        LR 0.000008    Time 0.948812    
2023-06-16 05:14:37,935 - Epoch: [194][   20/  142]    Overall Loss 0.442210    Objective Loss 0.442210                                        LR 0.000008    Time 0.721249    
2023-06-16 05:14:42,802 - Epoch: [194][   30/  142]    Overall Loss 0.432554    Objective Loss 0.432554                                        LR 0.000008    Time 0.643049    
2023-06-16 05:14:47,788 - Epoch: [194][   40/  142]    Overall Loss 0.446032    Objective Loss 0.446032                                        LR 0.000008    Time 0.606937    
2023-06-16 05:14:52,681 - Epoch: [194][   50/  142]    Overall Loss 0.436678    Objective Loss 0.436678                                        LR 0.000008    Time 0.583390    
2023-06-16 05:14:57,627 - Epoch: [194][   60/  142]    Overall Loss 0.437648    Objective Loss 0.437648                                        LR 0.000008    Time 0.568573    
2023-06-16 05:15:02,504 - Epoch: [194][   70/  142]    Overall Loss 0.430892    Objective Loss 0.430892                                        LR 0.000008    Time 0.557019    
2023-06-16 05:15:07,416 - Epoch: [194][   80/  142]    Overall Loss 0.435844    Objective Loss 0.435844                                        LR 0.000008    Time 0.548780    
2023-06-16 05:15:12,492 - Epoch: [194][   90/  142]    Overall Loss 0.432485    Objective Loss 0.432485                                        LR 0.000008    Time 0.544197    
2023-06-16 05:15:17,336 - Epoch: [194][  100/  142]    Overall Loss 0.427526    Objective Loss 0.427526                                        LR 0.000008    Time 0.538209    
2023-06-16 05:15:22,315 - Epoch: [194][  110/  142]    Overall Loss 0.424208    Objective Loss 0.424208                                        LR 0.000008    Time 0.534543    
2023-06-16 05:15:27,331 - Epoch: [194][  120/  142]    Overall Loss 0.424120    Objective Loss 0.424120                                        LR 0.000008    Time 0.531795    
2023-06-16 05:15:32,317 - Epoch: [194][  130/  142]    Overall Loss 0.424858    Objective Loss 0.424858                                        LR 0.000008    Time 0.529232    
2023-06-16 05:15:36,985 - Epoch: [194][  140/  142]    Overall Loss 0.420016    Objective Loss 0.420016                                        LR 0.000008    Time 0.524774    
2023-06-16 05:15:37,832 - Epoch: [194][  142/  142]    Overall Loss 0.420997    Objective Loss 0.420997    Top1 84.375000    LR 0.000008    Time 0.523345    
2023-06-16 05:15:38,478 - --- validate (epoch=194)-----------
2023-06-16 05:15:38,478 - 1422 samples (32 per mini-batch)
2023-06-16 05:15:46,532 - Epoch: [194][   10/   45]    Loss 0.905244    Top1 78.125000    
2023-06-16 05:15:50,572 - Epoch: [194][   20/   45]    Loss 0.946174    Top1 75.625000    
2023-06-16 05:15:55,051 - Epoch: [194][   30/   45]    Loss 0.991013    Top1 74.479167    
2023-06-16 05:15:59,405 - Epoch: [194][   40/   45]    Loss 0.964811    Top1 74.296875    
2023-06-16 05:16:00,776 - Epoch: [194][   45/   45]    Loss 0.954806    Top1 74.683544    
2023-06-16 05:16:01,408 - ==> Top1: 74.684    Loss: 0.955

2023-06-16 05:16:01,410 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 05:16:01,410 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 05:16:01,432 - 

2023-06-16 05:16:01,432 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 05:16:10,683 - Epoch: [195][   10/  142]    Overall Loss 0.443274    Objective Loss 0.443274                                        LR 0.000008    Time 0.924971    
2023-06-16 05:16:15,692 - Epoch: [195][   20/  142]    Overall Loss 0.390644    Objective Loss 0.390644                                        LR 0.000008    Time 0.712908    
2023-06-16 05:16:20,688 - Epoch: [195][   30/  142]    Overall Loss 0.420318    Objective Loss 0.420318                                        LR 0.000008    Time 0.641793    
2023-06-16 05:16:25,660 - Epoch: [195][   40/  142]    Overall Loss 0.411212    Objective Loss 0.411212                                        LR 0.000008    Time 0.605642    
2023-06-16 05:16:30,580 - Epoch: [195][   50/  142]    Overall Loss 0.420844    Objective Loss 0.420844                                        LR 0.000008    Time 0.582893    
2023-06-16 05:16:35,490 - Epoch: [195][   60/  142]    Overall Loss 0.424005    Objective Loss 0.424005                                        LR 0.000008    Time 0.567564    
2023-06-16 05:16:40,469 - Epoch: [195][   70/  142]    Overall Loss 0.429900    Objective Loss 0.429900                                        LR 0.000008    Time 0.557610    
2023-06-16 05:16:45,443 - Epoch: [195][   80/  142]    Overall Loss 0.434543    Objective Loss 0.434543                                        LR 0.000008    Time 0.550066    
2023-06-16 05:16:50,421 - Epoch: [195][   90/  142]    Overall Loss 0.440220    Objective Loss 0.440220                                        LR 0.000008    Time 0.544258    
2023-06-16 05:16:55,470 - Epoch: [195][  100/  142]    Overall Loss 0.436965    Objective Loss 0.436965                                        LR 0.000008    Time 0.540315    
2023-06-16 05:17:00,373 - Epoch: [195][  110/  142]    Overall Loss 0.443038    Objective Loss 0.443038                                        LR 0.000008    Time 0.535765    
2023-06-16 05:17:05,511 - Epoch: [195][  120/  142]    Overall Loss 0.438847    Objective Loss 0.438847                                        LR 0.000008    Time 0.533928    
2023-06-16 05:17:10,357 - Epoch: [195][  130/  142]    Overall Loss 0.438107    Objective Loss 0.438107                                        LR 0.000008    Time 0.530132    
2023-06-16 05:17:14,991 - Epoch: [195][  140/  142]    Overall Loss 0.433574    Objective Loss 0.433574                                        LR 0.000008    Time 0.525363    
2023-06-16 05:17:15,848 - Epoch: [195][  142/  142]    Overall Loss 0.432581    Objective Loss 0.432581    Top1 89.062500    LR 0.000008    Time 0.523993    
2023-06-16 05:17:16,478 - --- validate (epoch=195)-----------
2023-06-16 05:17:16,478 - 1422 samples (32 per mini-batch)
2023-06-16 05:17:24,477 - Epoch: [195][   10/   45]    Loss 0.921246    Top1 73.750000    
2023-06-16 05:17:28,661 - Epoch: [195][   20/   45]    Loss 0.923037    Top1 73.593750    
2023-06-16 05:17:32,992 - Epoch: [195][   30/   45]    Loss 0.950845    Top1 74.062500    
2023-06-16 05:17:37,386 - Epoch: [195][   40/   45]    Loss 0.972510    Top1 74.062500    
2023-06-16 05:17:38,819 - Epoch: [195][   45/   45]    Loss 0.940795    Top1 74.683544    
2023-06-16 05:17:39,427 - ==> Top1: 74.684    Loss: 0.941

2023-06-16 05:17:39,430 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 05:17:39,430 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 05:17:39,451 - 

2023-06-16 05:17:39,451 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 05:17:48,566 - Epoch: [196][   10/  142]    Overall Loss 0.385636    Objective Loss 0.385636                                        LR 0.000008    Time 0.911325    
2023-06-16 05:17:53,525 - Epoch: [196][   20/  142]    Overall Loss 0.397478    Objective Loss 0.397478                                        LR 0.000008    Time 0.703539    
2023-06-16 05:17:58,446 - Epoch: [196][   30/  142]    Overall Loss 0.406012    Objective Loss 0.406012                                        LR 0.000008    Time 0.633046    
2023-06-16 05:18:03,461 - Epoch: [196][   40/  142]    Overall Loss 0.418465    Objective Loss 0.418465                                        LR 0.000008    Time 0.600135    
2023-06-16 05:18:08,298 - Epoch: [196][   50/  142]    Overall Loss 0.415986    Objective Loss 0.415986                                        LR 0.000008    Time 0.576844    
2023-06-16 05:18:13,317 - Epoch: [196][   60/  142]    Overall Loss 0.412476    Objective Loss 0.412476                                        LR 0.000008    Time 0.564350    
2023-06-16 05:18:18,199 - Epoch: [196][   70/  142]    Overall Loss 0.416776    Objective Loss 0.416776                                        LR 0.000008    Time 0.553455    
2023-06-16 05:18:23,097 - Epoch: [196][   80/  142]    Overall Loss 0.423034    Objective Loss 0.423034                                        LR 0.000008    Time 0.545489    
2023-06-16 05:18:28,087 - Epoch: [196][   90/  142]    Overall Loss 0.424193    Objective Loss 0.424193                                        LR 0.000008    Time 0.540314    
2023-06-16 05:18:32,926 - Epoch: [196][  100/  142]    Overall Loss 0.420435    Objective Loss 0.420435                                        LR 0.000008    Time 0.534667    
2023-06-16 05:18:37,732 - Epoch: [196][  110/  142]    Overall Loss 0.422414    Objective Loss 0.422414                                        LR 0.000008    Time 0.529752    
2023-06-16 05:18:42,656 - Epoch: [196][  120/  142]    Overall Loss 0.422626    Objective Loss 0.422626                                        LR 0.000008    Time 0.526631    
2023-06-16 05:18:47,575 - Epoch: [196][  130/  142]    Overall Loss 0.426178    Objective Loss 0.426178                                        LR 0.000008    Time 0.523957    
2023-06-16 05:18:52,225 - Epoch: [196][  140/  142]    Overall Loss 0.430222    Objective Loss 0.430222                                        LR 0.000008    Time 0.519741    
2023-06-16 05:18:53,079 - Epoch: [196][  142/  142]    Overall Loss 0.429502    Objective Loss 0.429502    Top1 90.625000    LR 0.000008    Time 0.518436    
2023-06-16 05:18:53,741 - --- validate (epoch=196)-----------
2023-06-16 05:18:53,742 - 1422 samples (32 per mini-batch)
2023-06-16 05:19:02,034 - Epoch: [196][   10/   45]    Loss 0.723931    Top1 76.250000    
2023-06-16 05:19:06,118 - Epoch: [196][   20/   45]    Loss 0.894766    Top1 74.218750    
2023-06-16 05:19:10,467 - Epoch: [196][   30/   45]    Loss 0.922054    Top1 74.166667    
2023-06-16 05:19:14,871 - Epoch: [196][   40/   45]    Loss 0.904809    Top1 75.390625    
2023-06-16 05:19:16,223 - Epoch: [196][   45/   45]    Loss 0.916505    Top1 74.894515    
2023-06-16 05:19:16,884 - ==> Top1: 74.895    Loss: 0.917

2023-06-16 05:19:16,886 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 139]
2023-06-16 05:19:16,886 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 05:19:16,900 - 

2023-06-16 05:19:16,900 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 05:19:26,210 - Epoch: [197][   10/  142]    Overall Loss 0.434891    Objective Loss 0.434891                                        LR 0.000008    Time 0.930958    
2023-06-16 05:19:31,083 - Epoch: [197][   20/  142]    Overall Loss 0.437677    Objective Loss 0.437677                                        LR 0.000008    Time 0.709093    
2023-06-16 05:19:36,018 - Epoch: [197][   30/  142]    Overall Loss 0.425860    Objective Loss 0.425860                                        LR 0.000008    Time 0.637196    
2023-06-16 05:19:40,889 - Epoch: [197][   40/  142]    Overall Loss 0.425763    Objective Loss 0.425763                                        LR 0.000008    Time 0.599669    
2023-06-16 05:19:45,885 - Epoch: [197][   50/  142]    Overall Loss 0.432997    Objective Loss 0.432997                                        LR 0.000008    Time 0.579633    
2023-06-16 05:19:50,749 - Epoch: [197][   60/  142]    Overall Loss 0.436371    Objective Loss 0.436371                                        LR 0.000008    Time 0.564081    
2023-06-16 05:19:55,662 - Epoch: [197][   70/  142]    Overall Loss 0.436815    Objective Loss 0.436815                                        LR 0.000008    Time 0.553677    
2023-06-16 05:20:00,659 - Epoch: [197][   80/  142]    Overall Loss 0.425665    Objective Loss 0.425665                                        LR 0.000008    Time 0.546917    
2023-06-16 05:20:05,664 - Epoch: [197][   90/  142]    Overall Loss 0.420132    Objective Loss 0.420132                                        LR 0.000008    Time 0.541759    
2023-06-16 05:20:10,454 - Epoch: [197][  100/  142]    Overall Loss 0.424378    Objective Loss 0.424378                                        LR 0.000008    Time 0.535481    
2023-06-16 05:20:15,429 - Epoch: [197][  110/  142]    Overall Loss 0.429854    Objective Loss 0.429854                                        LR 0.000008    Time 0.532018    
2023-06-16 05:20:20,296 - Epoch: [197][  120/  142]    Overall Loss 0.423888    Objective Loss 0.423888                                        LR 0.000008    Time 0.528239    
2023-06-16 05:20:25,111 - Epoch: [197][  130/  142]    Overall Loss 0.421157    Objective Loss 0.421157                                        LR 0.000008    Time 0.524640    
2023-06-16 05:20:29,844 - Epoch: [197][  140/  142]    Overall Loss 0.421716    Objective Loss 0.421716                                        LR 0.000008    Time 0.520967    
2023-06-16 05:20:30,703 - Epoch: [197][  142/  142]    Overall Loss 0.420217    Objective Loss 0.420217    Top1 89.062500    LR 0.000008    Time 0.519681    
2023-06-16 05:20:31,350 - --- validate (epoch=197)-----------
2023-06-16 05:20:31,351 - 1422 samples (32 per mini-batch)
2023-06-16 05:20:39,617 - Epoch: [197][   10/   45]    Loss 0.909392    Top1 76.250000    
2023-06-16 05:20:43,758 - Epoch: [197][   20/   45]    Loss 0.881422    Top1 77.656250    
2023-06-16 05:20:48,303 - Epoch: [197][   30/   45]    Loss 0.902502    Top1 75.937500    
2023-06-16 05:20:53,580 - Epoch: [197][   40/   45]    Loss 0.918459    Top1 75.390625    
2023-06-16 05:20:54,878 - Epoch: [197][   45/   45]    Loss 0.908508    Top1 75.527426    
2023-06-16 05:20:55,506 - ==> Top1: 75.527    Loss: 0.909

2023-06-16 05:20:55,509 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 197]
2023-06-16 05:20:55,509 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 05:20:55,534 - 

2023-06-16 05:20:55,534 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 05:21:04,715 - Epoch: [198][   10/  142]    Overall Loss 0.369822    Objective Loss 0.369822                                        LR 0.000008    Time 0.918026    
2023-06-16 05:21:09,709 - Epoch: [198][   20/  142]    Overall Loss 0.395806    Objective Loss 0.395806                                        LR 0.000008    Time 0.708647    
2023-06-16 05:21:14,694 - Epoch: [198][   30/  142]    Overall Loss 0.395579    Objective Loss 0.395579                                        LR 0.000008    Time 0.638602    
2023-06-16 05:21:19,734 - Epoch: [198][   40/  142]    Overall Loss 0.412947    Objective Loss 0.412947                                        LR 0.000008    Time 0.604928    
2023-06-16 05:21:24,707 - Epoch: [198][   50/  142]    Overall Loss 0.422415    Objective Loss 0.422415                                        LR 0.000008    Time 0.583387    
2023-06-16 05:21:29,697 - Epoch: [198][   60/  142]    Overall Loss 0.409005    Objective Loss 0.409005                                        LR 0.000008    Time 0.569311    
2023-06-16 05:21:34,664 - Epoch: [198][   70/  142]    Overall Loss 0.398628    Objective Loss 0.398628                                        LR 0.000008    Time 0.558925    
2023-06-16 05:21:39,713 - Epoch: [198][   80/  142]    Overall Loss 0.406539    Objective Loss 0.406539                                        LR 0.000008    Time 0.552171    
2023-06-16 05:21:44,728 - Epoch: [198][   90/  142]    Overall Loss 0.417744    Objective Loss 0.417744                                        LR 0.000008    Time 0.546527    
2023-06-16 05:21:49,753 - Epoch: [198][  100/  142]    Overall Loss 0.422467    Objective Loss 0.422467                                        LR 0.000008    Time 0.542120    
2023-06-16 05:21:54,797 - Epoch: [198][  110/  142]    Overall Loss 0.427046    Objective Loss 0.427046                                        LR 0.000008    Time 0.538688    
2023-06-16 05:21:59,882 - Epoch: [198][  120/  142]    Overall Loss 0.428057    Objective Loss 0.428057                                        LR 0.000008    Time 0.536170    
2023-06-16 05:22:04,761 - Epoch: [198][  130/  142]    Overall Loss 0.425509    Objective Loss 0.425509                                        LR 0.000008    Time 0.532419    
2023-06-16 05:22:09,435 - Epoch: [198][  140/  142]    Overall Loss 0.429264    Objective Loss 0.429264                                        LR 0.000008    Time 0.527768    
2023-06-16 05:22:10,275 - Epoch: [198][  142/  142]    Overall Loss 0.427346    Objective Loss 0.427346    Top1 95.312500    LR 0.000008    Time 0.526251    
2023-06-16 05:22:10,917 - --- validate (epoch=198)-----------
2023-06-16 05:22:10,918 - 1422 samples (32 per mini-batch)
2023-06-16 05:22:18,911 - Epoch: [198][   10/   45]    Loss 0.859248    Top1 75.312500    
2023-06-16 05:22:23,092 - Epoch: [198][   20/   45]    Loss 0.882113    Top1 76.718750    
2023-06-16 05:22:28,321 - Epoch: [198][   30/   45]    Loss 0.875505    Top1 76.562500    
2023-06-16 05:22:32,366 - Epoch: [198][   40/   45]    Loss 0.905810    Top1 74.921875    
2023-06-16 05:22:33,682 - Epoch: [198][   45/   45]    Loss 0.910200    Top1 75.175809    
2023-06-16 05:22:34,295 - ==> Top1: 75.176    Loss: 0.910

2023-06-16 05:22:34,297 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375264 on epoch: 197]
2023-06-16 05:22:34,297 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 05:22:34,318 - 

2023-06-16 05:22:34,318 - Training epoch: 4544 samples (32 per mini-batch)
2023-06-16 05:22:43,646 - Epoch: [199][   10/  142]    Overall Loss 0.374517    Objective Loss 0.374517                                        LR 0.000008    Time 0.932684    
2023-06-16 05:22:48,697 - Epoch: [199][   20/  142]    Overall Loss 0.380480    Objective Loss 0.380480                                        LR 0.000008    Time 0.718819    
2023-06-16 05:22:53,632 - Epoch: [199][   30/  142]    Overall Loss 0.396953    Objective Loss 0.396953                                        LR 0.000008    Time 0.643724    
2023-06-16 05:22:58,527 - Epoch: [199][   40/  142]    Overall Loss 0.397733    Objective Loss 0.397733                                        LR 0.000008    Time 0.605145    
2023-06-16 05:23:03,474 - Epoch: [199][   50/  142]    Overall Loss 0.413837    Objective Loss 0.413837                                        LR 0.000008    Time 0.583048    
2023-06-16 05:23:08,438 - Epoch: [199][   60/  142]    Overall Loss 0.405038    Objective Loss 0.405038                                        LR 0.000008    Time 0.568589    
2023-06-16 05:23:13,432 - Epoch: [199][   70/  142]    Overall Loss 0.413933    Objective Loss 0.413933                                        LR 0.000008    Time 0.558697    
2023-06-16 05:23:18,467 - Epoch: [199][   80/  142]    Overall Loss 0.411516    Objective Loss 0.411516                                        LR 0.000008    Time 0.551785    
2023-06-16 05:23:23,409 - Epoch: [199][   90/  142]    Overall Loss 0.407147    Objective Loss 0.407147                                        LR 0.000008    Time 0.545388    
2023-06-16 05:23:28,298 - Epoch: [199][  100/  142]    Overall Loss 0.405241    Objective Loss 0.405241                                        LR 0.000008    Time 0.539728    
2023-06-16 05:23:33,274 - Epoch: [199][  110/  142]    Overall Loss 0.408075    Objective Loss 0.408075                                        LR 0.000008    Time 0.535894    
2023-06-16 05:23:38,290 - Epoch: [199][  120/  142]    Overall Loss 0.408827    Objective Loss 0.408827                                        LR 0.000008    Time 0.533032    
2023-06-16 05:23:43,194 - Epoch: [199][  130/  142]    Overall Loss 0.418061    Objective Loss 0.418061                                        LR 0.000008    Time 0.529751    
2023-06-16 05:23:47,845 - Epoch: [199][  140/  142]    Overall Loss 0.420875    Objective Loss 0.420875                                        LR 0.000008    Time 0.525127    
2023-06-16 05:23:48,704 - Epoch: [199][  142/  142]    Overall Loss 0.421117    Objective Loss 0.421117    Top1 89.062500    LR 0.000008    Time 0.523777    
2023-06-16 05:23:49,363 - --- validate (epoch=199)-----------
2023-06-16 05:23:49,364 - 1422 samples (32 per mini-batch)
2023-06-16 05:23:57,494 - Epoch: [199][   10/   45]    Loss 0.953888    Top1 74.062500    
2023-06-16 05:24:01,602 - Epoch: [199][   20/   45]    Loss 0.948662    Top1 73.593750    
2023-06-16 05:24:06,494 - Epoch: [199][   30/   45]    Loss 0.900745    Top1 75.000000    
2023-06-16 05:24:11,225 - Epoch: [199][   40/   45]    Loss 0.894013    Top1 75.781250    
2023-06-16 05:24:12,568 - Epoch: [199][   45/   45]    Loss 0.888193    Top1 75.527426    
2023-06-16 05:24:13,205 - ==> Top1: 75.527    Loss: 0.888

2023-06-16 05:24:13,207 - ==> Best [Top1: 75.527   Sparsity:0.00   Params: 375263 on epoch: 199]
2023-06-16 05:24:13,208 - Saving checkpoint to: logs/2023.06.15-235432/qat_checkpoint.pth.tar
2023-06-16 05:24:13,233 - --- test ---------------------
2023-06-16 05:24:13,233 - 1422 samples (32 per mini-batch)
2023-06-16 05:24:21,508 - Test: [   10/   45]    Loss 0.951561    Top1 72.500000    
2023-06-16 05:24:25,620 - Test: [   20/   45]    Loss 0.981761    Top1 73.125000    
2023-06-16 05:24:30,213 - Test: [   30/   45]    Loss 0.947001    Top1 73.645833    
2023-06-16 05:24:34,733 - Test: [   40/   45]    Loss 0.890337    Top1 75.234375    
2023-06-16 05:24:36,140 - Test: [   45/   45]    Loss 0.879661    Top1 75.527426    
2023-06-16 05:24:36,799 - ==> Top1: 75.527    Loss: 0.880

2023-06-16 05:24:37,026 - 
2023-06-16 05:24:37,026 - Log file for this run: /home/alicangok/Projects/AI8X/ai8x-training/logs/2023.06.15-235432/2023.06.15-235432.log
