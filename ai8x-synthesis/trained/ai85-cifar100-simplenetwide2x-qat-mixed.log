2022-01-10 16:12:14,100 - Log file for this run: /home/gorkemulkar/Workspace/Python/AI8X_GitHub/ai8x-training/logs/2022.01.10-161214/2022.01.10-161214.log
2022-01-10 16:12:14,100 - Number of CPUs: 24
2022-01-10 16:12:14,106 - Number of GPUs: 1
2022-01-10 16:12:14,106 - CUDA version: 11.1
2022-01-10 16:12:14,106 - CUDNN version: 8005
2022-01-10 16:12:14,106 - Kernel: 5.4.0-90-generic
2022-01-10 16:12:14,106 - Python: 3.8.11 (default, Aug  3 2021, 15:09:35) 
[GCC 7.5.0]
2022-01-10 16:12:14,106 - pip freeze: {'absl-py': '0.13.0', 'aiohttp': '3.7.4', 'appdirs': '1.4.4', 'argon2-cffi': '20.1.0', 'astroid': '2.5', 'async-generator': '1.10', 'async-timeout': '3.0.1', 'atomicwrites': '1.4.0', 'attrs': '21.2.0', 'audioread': '2.1.9', 'backcall': '0.2.0', 'bleach': '3.3.0', 'blinker': '1.4', 'bottleneck': '1.3.2', 'bqplot': '0.11.5', 'brotlipy': '0.7.0', 'cachetools': '4.2.2', 'certifi': '2021.5.30', 'cffi': '1.14.6', 'chardet': '4.0.0', 'click': '8.0.1', 'cloudpickle': '1.6.0', 'colorama': '0.4.4', 'coverage': '5.5', 'cryptography': '3.4.7', 'cycler': '0.10.0', 'cython': '0.29.24', 'cytoolz': '0.11.0', 'dask': '2021.7.2', 'decorator': '5.0.9', 'defusedxml': '0.7.1', 'deprecated': '1.2.12', 'distiller': '0.4.0rc0', 'entrypoints': '0.3', 'fsspec': '2021.7.0', 'gitdb': '4.0.7', 'gitpython': '3.1.0', 'google-auth': '1.33.0', 'google-auth-oauthlib': '0.4.4', 'graphviz': '0.10.1', 'grpcio': '1.36.1', 'gym': '0.12.5', 'idna': '2.10', 'imageio': '2.9.0', 'importlib-metadata': '3.10.0', 'ipykernel': '5.5.3', 'ipython': '7.22.0', 'ipython-genutils': '0.2.0', 'ipywidgets': '7.4.2', 'isort': '5.9.2', 'jedi': '0.17.0', 'jinja2': '2.11.3', 'joblib': '1.0.1', 'jsonpatch': '1.32', 'jsonpointer': '2.1', 'jsonschema': '3.2.0', 'jupyter': '1.0.0', 'jupyter-client': '6.1.12', 'jupyter-console': '6.4.0', 'jupyter-core': '4.7.1', 'jupyterlab-pygments': '0.1.2', 'kiwisolver': '1.3.1', 'lazy-object-proxy': '1.6.0', 'librosa': '0.8.1', 'llvmlite': '0.36.0', 'locket': '0.2.1', 'markdown': '3.3.4', 'markupsafe': '1.1.1', 'matplotlib': '3.3.4', 'mccabe': '0.6.1', 'mistune': '0.8.4', 'mkl-fft': '1.3.0', 'mkl-random': '1.1.1', 'mkl-service': '2.3.0', 'more-itertools': '8.7.0', 'multidict': '5.1.0', 'munch': '2.5.0', 'nbclient': '0.5.3', 'nbconvert': '6.0.7', 'nbformat': '5.1.3', 'nest-asyncio': '1.5.1', 'networkx': '2.6.2', 'notebook': '6.3.0', 'numba': '0.53.1', 'numexpr': '2.7.3', 'numpy': '1.20.2', 'oauthlib': '3.1.1', 'olefile': '0.46', 'onnx': '1.10.1', 'opencv-python': '4.5.2.54', 'packaging': '21.0', 'pafy': '0.5.5', 'pandas': '1.3.1', 'pandocfilters': '1.4.3', 'parso': '0.8.2', 'partd': '1.2.0', 'pathspec': '0.8.1', 'pexpect': '4.8.0', 'pickleshare': '0.7.5', 'pillow': '8.3.1', 'pip': '21.2.2', 'pluggy': '0.13.1', 'pooch': '1.4.0', 'pretrainedmodels': '0.7.4', 'prometheus-client': '0.10.0', 'prompt-toolkit': '3.0.17', 'protobuf': '3.16.0', 'ptyprocess': '0.7.0', 'py': '1.10.0', 'pyasn1': '0.4.8', 'pyasn1-modules': '0.2.8', 'pycparser': '2.20', 'pydot': '1.4.1', 'pygithub': '1.55', 'pyglet': '1.5.15', 'pygments': '2.9.0', 'pyjwt': '2.1.0', 'pylint': '2.6.0', 'pynacl': '1.4.0', 'pyopenssl': '20.0.1', 'pyparsing': '2.4.7', 'pyqt5': '5.12.3', 'pyqt5-sip': '4.19.18', 'pyqtchart': '5.12', 'pyqtwebengine': '5.12.1', 'pyrsistent': '0.17.3', 'pysocks': '1.7.1', 'pytest': '4.6.11', 'python-dateutil': '2.8.2', 'pytsmod': '0.3.3', 'pytz': '2021.1', 'pywavelets': '1.1.1', 'pyyaml': '5.4.1', 'pyzmq': '22.0.3', 'qgrid': '1.1.1', 'qtconsole': '5.0.3', 'qtpy': '1.9.0', 'requests': '2.25.1', 'requests-oauthlib': '1.3.0', 'resampy': '0.2.2', 'rsa': '4.7.2', 'scikit-image': '0.18.1', 'scikit-learn': '0.23.2', 'scipy': '1.6.2', 'send2trash': '1.5.0', 'setuptools': '52.0.0.post20210125', 'shap': '0.37.0', 'six': '1.16.0', 'slicer': '0.0.7', 'smmap': '4.0.0', 'soundfile': '0.10.3.post1', 'tabulate': '0.8.3', 'tensorboard': '2.4.0', 'tensorboard-plugin-wit': '1.6.0', 'terminado': '0.9.4', 'testpath': '0.4.4', 'threadpoolctl': '2.2.0', 'tifffile': '2020.10.1', 'toml': '0.10.2', 'toolz': '0.11.1', 'torch': '1.8.1+cu111', 'torchaudio': '0.8.0a0+e4e171a', 'torchfile': '0.1.0', 'torchnet': '0.0.4', 'torchvision': '0.9.1+cu111', 'tornado': '6.1', 'tqdm': '4.33.0', 'traitlets': '5.0.5', 'traittypes': '0.2.1', 'typing-extensions': '3.10.0.0', 'urllib3': '1.26.6', 'visdom': '0.1.8.9', 'wcwidth': '0.2.5', 'webencodings': '0.5.1', 'websocket-client': '0.58.0', 'werkzeug': '1.0.1', 'wheel': '0.36.2', 'widgetsnbextension': '3.4.2', 'wrapt': '1.12.1', 'xlsxwriter': '1.3.8', 'yamllint': '1.26.1', 'yarl': '1.6.3', 'youtube-dl': '2021.6.6', 'zipp': '3.5.0'}
2022-01-10 16:12:14,106 - Command line: train.py --epochs 300 --optimizer Adam --lr 0.001 --compress schedule-cifar100.yaml --model ai85simplenetwide2x --dataset CIFAR100 --device MAX78000 --batch-size 100 --print-freq 100 --validation-split 0 --qat-policy qat_policy_cifar100.yaml --use-bias
2022-01-10 16:12:14,106 - Distiller: 0.4.0rc0
2022-01-10 16:12:15,833 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2022-01-10 16:12:15,833 - Optimizer Args: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}
2022-01-10 16:12:16,933 - Dataset sizes:
	training=50000
	validation=10000
	test=10000
2022-01-10 16:12:16,933 - Reading compression schedule from: schedule-cifar100.yaml
2022-01-10 16:12:16,934 - Schedule contents:
{
  "lr_schedulers": {
    "training_lr": {
      "class": "MultiStepLR",
      "milestones": [
        100,
        150,
        200
      ],
      "gamma": 0.25
    }
  },
  "policies": [
    {
      "lr_scheduler": {
        "instance_name": "training_lr"
      },
      "starting_epoch": 0,
      "ending_epoch": 250,
      "frequency": 1
    }
  ]
}
2022-01-10 16:12:16,936 - 

2022-01-10 16:12:16,936 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:12:18,904 - Epoch: [0][  100/  500]    Overall Loss 4.400431    Objective Loss 4.400431                                        LR 0.001000    Time 0.019669    
2022-01-10 16:12:20,299 - Epoch: [0][  200/  500]    Overall Loss 4.246567    Objective Loss 4.246567                                        LR 0.001000    Time 0.016803    
2022-01-10 16:12:21,681 - Epoch: [0][  300/  500]    Overall Loss 4.119799    Objective Loss 4.119799                                        LR 0.001000    Time 0.015809    
2022-01-10 16:12:23,065 - Epoch: [0][  400/  500]    Overall Loss 4.024682    Objective Loss 4.024682                                        LR 0.001000    Time 0.015313    
2022-01-10 16:12:24,465 - Epoch: [0][  500/  500]    Overall Loss 3.943652    Objective Loss 3.943652    Top1 9.500000    Top5 35.500000    LR 0.001000    Time 0.015050    
2022-01-10 16:12:24,507 - --- validate (epoch=0)-----------
2022-01-10 16:12:24,507 - 10000 samples (100 per mini-batch)
2022-01-10 16:12:25,224 - Epoch: [0][  100/  100]    Loss 3.708732    Top1 12.010000    Top5 35.180000    
2022-01-10 16:12:25,267 - ==> Top1: 12.010    Top5: 35.180    Loss: 3.709

2022-01-10 16:12:25,269 - ==> Best [Top1: 12.010   Top5: 35.180   Sparsity:0.00   Params: 725128 on epoch: 0]
2022-01-10 16:12:25,269 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:12:25,290 - 

2022-01-10 16:12:25,290 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:12:26,877 - Epoch: [1][  100/  500]    Overall Loss 3.479926    Objective Loss 3.479926                                        LR 0.001000    Time 0.015861    
2022-01-10 16:12:28,306 - Epoch: [1][  200/  500]    Overall Loss 3.420507    Objective Loss 3.420507                                        LR 0.001000    Time 0.015072    
2022-01-10 16:12:29,778 - Epoch: [1][  300/  500]    Overall Loss 3.361261    Objective Loss 3.361261                                        LR 0.001000    Time 0.014952    
2022-01-10 16:12:31,196 - Epoch: [1][  400/  500]    Overall Loss 3.306794    Objective Loss 3.306794                                        LR 0.001000    Time 0.014756    
2022-01-10 16:12:32,624 - Epoch: [1][  500/  500]    Overall Loss 3.254648    Objective Loss 3.254648    Top1 25.500000    Top5 50.500000    LR 0.001000    Time 0.014658    
2022-01-10 16:12:32,670 - --- validate (epoch=1)-----------
2022-01-10 16:12:32,670 - 10000 samples (100 per mini-batch)
2022-01-10 16:12:33,398 - Epoch: [1][  100/  100]    Loss 3.110216    Top1 21.480000    Top5 51.870000    
2022-01-10 16:12:33,444 - ==> Top1: 21.480    Top5: 51.870    Loss: 3.110

2022-01-10 16:12:33,445 - ==> Best [Top1: 21.480   Top5: 51.870   Sparsity:0.00   Params: 725128 on epoch: 1]
2022-01-10 16:12:33,445 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:12:33,478 - 

2022-01-10 16:12:33,479 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:12:35,127 - Epoch: [2][  100/  500]    Overall Loss 2.927523    Objective Loss 2.927523                                        LR 0.001000    Time 0.016472    
2022-01-10 16:12:36,542 - Epoch: [2][  200/  500]    Overall Loss 2.886841    Objective Loss 2.886841                                        LR 0.001000    Time 0.015306    
2022-01-10 16:12:37,954 - Epoch: [2][  300/  500]    Overall Loss 2.859574    Objective Loss 2.859574                                        LR 0.001000    Time 0.014910    
2022-01-10 16:12:39,365 - Epoch: [2][  400/  500]    Overall Loss 2.828193    Objective Loss 2.828193                                        LR 0.001000    Time 0.014708    
2022-01-10 16:12:40,784 - Epoch: [2][  500/  500]    Overall Loss 2.801874    Objective Loss 2.801874    Top1 30.500000    Top5 60.500000    LR 0.001000    Time 0.014602    
2022-01-10 16:12:40,834 - --- validate (epoch=2)-----------
2022-01-10 16:12:40,835 - 10000 samples (100 per mini-batch)
2022-01-10 16:12:41,583 - Epoch: [2][  100/  100]    Loss 2.668086    Top1 29.880000    Top5 63.690000    
2022-01-10 16:12:41,626 - ==> Top1: 29.880    Top5: 63.690    Loss: 2.668

2022-01-10 16:12:41,628 - ==> Best [Top1: 29.880   Top5: 63.690   Sparsity:0.00   Params: 725128 on epoch: 2]
2022-01-10 16:12:41,628 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:12:41,655 - 

2022-01-10 16:12:41,655 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:12:43,280 - Epoch: [3][  100/  500]    Overall Loss 2.615018    Objective Loss 2.615018                                        LR 0.001000    Time 0.016230    
2022-01-10 16:12:44,721 - Epoch: [3][  200/  500]    Overall Loss 2.599827    Objective Loss 2.599827                                        LR 0.001000    Time 0.015317    
2022-01-10 16:12:46,153 - Epoch: [3][  300/  500]    Overall Loss 2.572650    Objective Loss 2.572650                                        LR 0.001000    Time 0.014983    
2022-01-10 16:12:47,592 - Epoch: [3][  400/  500]    Overall Loss 2.543370    Objective Loss 2.543370                                        LR 0.001000    Time 0.014832    
2022-01-10 16:12:49,028 - Epoch: [3][  500/  500]    Overall Loss 2.522191    Objective Loss 2.522191    Top1 38.500000    Top5 68.000000    LR 0.001000    Time 0.014736    
2022-01-10 16:12:49,078 - --- validate (epoch=3)-----------
2022-01-10 16:12:49,079 - 10000 samples (100 per mini-batch)
2022-01-10 16:12:49,914 - Epoch: [3][  100/  100]    Loss 2.473059    Top1 34.460000    Top5 67.990000    
2022-01-10 16:12:49,960 - ==> Top1: 34.460    Top5: 67.990    Loss: 2.473

2022-01-10 16:12:49,962 - ==> Best [Top1: 34.460   Top5: 67.990   Sparsity:0.00   Params: 725128 on epoch: 3]
2022-01-10 16:12:49,962 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:12:49,996 - 

2022-01-10 16:12:49,996 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:12:51,596 - Epoch: [4][  100/  500]    Overall Loss 2.357161    Objective Loss 2.357161                                        LR 0.001000    Time 0.015984    
2022-01-10 16:12:53,032 - Epoch: [4][  200/  500]    Overall Loss 2.347900    Objective Loss 2.347900                                        LR 0.001000    Time 0.015168    
2022-01-10 16:12:54,497 - Epoch: [4][  300/  500]    Overall Loss 2.329280    Objective Loss 2.329280                                        LR 0.001000    Time 0.014993    
2022-01-10 16:12:55,966 - Epoch: [4][  400/  500]    Overall Loss 2.325287    Objective Loss 2.325287                                        LR 0.001000    Time 0.014916    
2022-01-10 16:12:57,440 - Epoch: [4][  500/  500]    Overall Loss 2.315445    Objective Loss 2.315445    Top1 35.000000    Top5 68.500000    LR 0.001000    Time 0.014879    
2022-01-10 16:12:57,490 - --- validate (epoch=4)-----------
2022-01-10 16:12:57,490 - 10000 samples (100 per mini-batch)
2022-01-10 16:12:58,232 - Epoch: [4][  100/  100]    Loss 2.319386    Top1 37.060000    Top5 70.920000    
2022-01-10 16:12:58,287 - ==> Top1: 37.060    Top5: 70.920    Loss: 2.319

2022-01-10 16:12:58,289 - ==> Best [Top1: 37.060   Top5: 70.920   Sparsity:0.00   Params: 725128 on epoch: 4]
2022-01-10 16:12:58,289 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:12:58,316 - 

2022-01-10 16:12:58,316 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:12:59,974 - Epoch: [5][  100/  500]    Overall Loss 2.176687    Objective Loss 2.176687                                        LR 0.001000    Time 0.016559    
2022-01-10 16:13:01,389 - Epoch: [5][  200/  500]    Overall Loss 2.174199    Objective Loss 2.174199                                        LR 0.001000    Time 0.015350    
2022-01-10 16:13:02,804 - Epoch: [5][  300/  500]    Overall Loss 2.171436    Objective Loss 2.171436                                        LR 0.001000    Time 0.014949    
2022-01-10 16:13:04,220 - Epoch: [5][  400/  500]    Overall Loss 2.167156    Objective Loss 2.167156                                        LR 0.001000    Time 0.014750    
2022-01-10 16:13:05,642 - Epoch: [5][  500/  500]    Overall Loss 2.160101    Objective Loss 2.160101    Top1 39.000000    Top5 72.500000    LR 0.001000    Time 0.014642    
2022-01-10 16:13:05,697 - --- validate (epoch=5)-----------
2022-01-10 16:13:05,698 - 10000 samples (100 per mini-batch)
2022-01-10 16:13:06,455 - Epoch: [5][  100/  100]    Loss 2.171482    Top1 41.290000    Top5 74.270000    
2022-01-10 16:13:06,499 - ==> Top1: 41.290    Top5: 74.270    Loss: 2.171

2022-01-10 16:13:06,501 - ==> Best [Top1: 41.290   Top5: 74.270   Sparsity:0.00   Params: 725128 on epoch: 5]
2022-01-10 16:13:06,501 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:13:06,535 - 

2022-01-10 16:13:06,535 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:13:08,120 - Epoch: [6][  100/  500]    Overall Loss 2.054375    Objective Loss 2.054375                                        LR 0.001000    Time 0.015832    
2022-01-10 16:13:09,533 - Epoch: [6][  200/  500]    Overall Loss 2.040226    Objective Loss 2.040226                                        LR 0.001000    Time 0.014976    
2022-01-10 16:13:11,028 - Epoch: [6][  300/  500]    Overall Loss 2.035238    Objective Loss 2.035238                                        LR 0.001000    Time 0.014965    
2022-01-10 16:13:12,581 - Epoch: [6][  400/  500]    Overall Loss 2.030233    Objective Loss 2.030233                                        LR 0.001000    Time 0.015104    
2022-01-10 16:13:14,141 - Epoch: [6][  500/  500]    Overall Loss 2.026018    Objective Loss 2.026018    Top1 49.000000    Top5 78.000000    LR 0.001000    Time 0.015203    
2022-01-10 16:13:14,186 - --- validate (epoch=6)-----------
2022-01-10 16:13:14,186 - 10000 samples (100 per mini-batch)
2022-01-10 16:13:15,014 - Epoch: [6][  100/  100]    Loss 2.230130    Top1 40.300000    Top5 73.320000    
2022-01-10 16:13:15,065 - ==> Top1: 40.300    Top5: 73.320    Loss: 2.230

2022-01-10 16:13:15,066 - ==> Best [Top1: 41.290   Top5: 74.270   Sparsity:0.00   Params: 725128 on epoch: 5]
2022-01-10 16:13:15,067 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:13:15,094 - 

2022-01-10 16:13:15,094 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:13:16,724 - Epoch: [7][  100/  500]    Overall Loss 1.939956    Objective Loss 1.939956                                        LR 0.001000    Time 0.016289    
2022-01-10 16:13:18,186 - Epoch: [7][  200/  500]    Overall Loss 1.942958    Objective Loss 1.942958                                        LR 0.001000    Time 0.015448    
2022-01-10 16:13:19,636 - Epoch: [7][  300/  500]    Overall Loss 1.934366    Objective Loss 1.934366                                        LR 0.001000    Time 0.015129    
2022-01-10 16:13:21,070 - Epoch: [7][  400/  500]    Overall Loss 1.928066    Objective Loss 1.928066                                        LR 0.001000    Time 0.014931    
2022-01-10 16:13:22,514 - Epoch: [7][  500/  500]    Overall Loss 1.921655    Objective Loss 1.921655    Top1 45.000000    Top5 76.500000    LR 0.001000    Time 0.014831    
2022-01-10 16:13:22,568 - --- validate (epoch=7)-----------
2022-01-10 16:13:22,568 - 10000 samples (100 per mini-batch)
2022-01-10 16:13:23,336 - Epoch: [7][  100/  100]    Loss 1.997709    Top1 45.500000    Top5 77.430000    
2022-01-10 16:13:23,390 - ==> Top1: 45.500    Top5: 77.430    Loss: 1.998

2022-01-10 16:13:23,392 - ==> Best [Top1: 45.500   Top5: 77.430   Sparsity:0.00   Params: 725128 on epoch: 7]
2022-01-10 16:13:23,392 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:13:23,426 - 

2022-01-10 16:13:23,426 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:13:25,124 - Epoch: [8][  100/  500]    Overall Loss 1.838644    Objective Loss 1.838644                                        LR 0.001000    Time 0.016970    
2022-01-10 16:13:26,557 - Epoch: [8][  200/  500]    Overall Loss 1.844767    Objective Loss 1.844767                                        LR 0.001000    Time 0.015643    
2022-01-10 16:13:27,989 - Epoch: [8][  300/  500]    Overall Loss 1.836502    Objective Loss 1.836502                                        LR 0.001000    Time 0.015202    
2022-01-10 16:13:29,420 - Epoch: [8][  400/  500]    Overall Loss 1.832269    Objective Loss 1.832269                                        LR 0.001000    Time 0.014977    
2022-01-10 16:13:30,862 - Epoch: [8][  500/  500]    Overall Loss 1.831356    Objective Loss 1.831356    Top1 43.000000    Top5 81.500000    LR 0.001000    Time 0.014864    
2022-01-10 16:13:30,918 - --- validate (epoch=8)-----------
2022-01-10 16:13:30,918 - 10000 samples (100 per mini-batch)
2022-01-10 16:13:31,680 - Epoch: [8][  100/  100]    Loss 1.886073    Top1 47.970000    Top5 79.430000    
2022-01-10 16:13:31,720 - ==> Top1: 47.970    Top5: 79.430    Loss: 1.886

2022-01-10 16:13:31,722 - ==> Best [Top1: 47.970   Top5: 79.430   Sparsity:0.00   Params: 725128 on epoch: 8]
2022-01-10 16:13:31,722 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:13:31,756 - 

2022-01-10 16:13:31,756 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:13:33,353 - Epoch: [9][  100/  500]    Overall Loss 1.757880    Objective Loss 1.757880                                        LR 0.001000    Time 0.015958    
2022-01-10 16:13:34,775 - Epoch: [9][  200/  500]    Overall Loss 1.766192    Objective Loss 1.766192                                        LR 0.001000    Time 0.015083    
2022-01-10 16:13:36,194 - Epoch: [9][  300/  500]    Overall Loss 1.766298    Objective Loss 1.766298                                        LR 0.001000    Time 0.014785    
2022-01-10 16:13:37,616 - Epoch: [9][  400/  500]    Overall Loss 1.763045    Objective Loss 1.763045                                        LR 0.001000    Time 0.014640    
2022-01-10 16:13:39,043 - Epoch: [9][  500/  500]    Overall Loss 1.764969    Objective Loss 1.764969    Top1 49.500000    Top5 82.000000    LR 0.001000    Time 0.014566    
2022-01-10 16:13:39,081 - --- validate (epoch=9)-----------
2022-01-10 16:13:39,081 - 10000 samples (100 per mini-batch)
2022-01-10 16:13:39,918 - Epoch: [9][  100/  100]    Loss 1.875214    Top1 48.580000    Top5 79.840000    
2022-01-10 16:13:39,969 - ==> Top1: 48.580    Top5: 79.840    Loss: 1.875

2022-01-10 16:13:39,971 - ==> Best [Top1: 48.580   Top5: 79.840   Sparsity:0.00   Params: 725128 on epoch: 9]
2022-01-10 16:13:39,971 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:13:39,998 - 

2022-01-10 16:13:39,998 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:13:41,591 - Epoch: [10][  100/  500]    Overall Loss 1.671473    Objective Loss 1.671473                                        LR 0.001000    Time 0.015918    
2022-01-10 16:13:43,100 - Epoch: [10][  200/  500]    Overall Loss 1.679274    Objective Loss 1.679274                                        LR 0.001000    Time 0.015499    
2022-01-10 16:13:44,527 - Epoch: [10][  300/  500]    Overall Loss 1.686498    Objective Loss 1.686498                                        LR 0.001000    Time 0.015086    
2022-01-10 16:13:45,958 - Epoch: [10][  400/  500]    Overall Loss 1.689049    Objective Loss 1.689049                                        LR 0.001000    Time 0.014890    
2022-01-10 16:13:47,392 - Epoch: [10][  500/  500]    Overall Loss 1.694023    Objective Loss 1.694023    Top1 54.000000    Top5 85.000000    LR 0.001000    Time 0.014780    
2022-01-10 16:13:47,440 - --- validate (epoch=10)-----------
2022-01-10 16:13:47,440 - 10000 samples (100 per mini-batch)
2022-01-10 16:13:48,182 - Epoch: [10][  100/  100]    Loss 1.805385    Top1 50.140000    Top5 80.700000    
2022-01-10 16:13:48,221 - ==> Top1: 50.140    Top5: 80.700    Loss: 1.805

2022-01-10 16:13:48,222 - ==> Best [Top1: 50.140   Top5: 80.700   Sparsity:0.00   Params: 725128 on epoch: 10]
2022-01-10 16:13:48,222 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:13:48,257 - 

2022-01-10 16:13:48,257 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:13:49,929 - Epoch: [11][  100/  500]    Overall Loss 1.633818    Objective Loss 1.633818                                        LR 0.001000    Time 0.016706    
2022-01-10 16:13:51,367 - Epoch: [11][  200/  500]    Overall Loss 1.638370    Objective Loss 1.638370                                        LR 0.001000    Time 0.015538    
2022-01-10 16:13:52,791 - Epoch: [11][  300/  500]    Overall Loss 1.635801    Objective Loss 1.635801                                        LR 0.001000    Time 0.015104    
2022-01-10 16:13:54,218 - Epoch: [11][  400/  500]    Overall Loss 1.636693    Objective Loss 1.636693                                        LR 0.001000    Time 0.014892    
2022-01-10 16:13:55,660 - Epoch: [11][  500/  500]    Overall Loss 1.634028    Objective Loss 1.634028    Top1 56.500000    Top5 85.500000    LR 0.001000    Time 0.014797    
2022-01-10 16:13:55,699 - --- validate (epoch=11)-----------
2022-01-10 16:13:55,699 - 10000 samples (100 per mini-batch)
2022-01-10 16:13:56,451 - Epoch: [11][  100/  100]    Loss 1.784799    Top1 50.560000    Top5 81.100000    
2022-01-10 16:13:56,495 - ==> Top1: 50.560    Top5: 81.100    Loss: 1.785

2022-01-10 16:13:56,496 - ==> Best [Top1: 50.560   Top5: 81.100   Sparsity:0.00   Params: 725128 on epoch: 11]
2022-01-10 16:13:56,496 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:13:56,530 - 

2022-01-10 16:13:56,530 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:13:58,135 - Epoch: [12][  100/  500]    Overall Loss 1.564990    Objective Loss 1.564990                                        LR 0.001000    Time 0.016037    
2022-01-10 16:13:59,669 - Epoch: [12][  200/  500]    Overall Loss 1.566082    Objective Loss 1.566082                                        LR 0.001000    Time 0.015686    
2022-01-10 16:14:01,150 - Epoch: [12][  300/  500]    Overall Loss 1.573292    Objective Loss 1.573292                                        LR 0.001000    Time 0.015389    
2022-01-10 16:14:02,575 - Epoch: [12][  400/  500]    Overall Loss 1.573797    Objective Loss 1.573797                                        LR 0.001000    Time 0.015102    
2022-01-10 16:14:04,008 - Epoch: [12][  500/  500]    Overall Loss 1.581719    Objective Loss 1.581719    Top1 54.000000    Top5 86.500000    LR 0.001000    Time 0.014947    
2022-01-10 16:14:04,054 - --- validate (epoch=12)-----------
2022-01-10 16:14:04,054 - 10000 samples (100 per mini-batch)
2022-01-10 16:14:04,875 - Epoch: [12][  100/  100]    Loss 1.763157    Top1 50.840000    Top5 81.240000    
2022-01-10 16:14:04,922 - ==> Top1: 50.840    Top5: 81.240    Loss: 1.763

2022-01-10 16:14:04,923 - ==> Best [Top1: 50.840   Top5: 81.240   Sparsity:0.00   Params: 725128 on epoch: 12]
2022-01-10 16:14:04,924 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:14:04,957 - 

2022-01-10 16:14:04,957 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:14:06,537 - Epoch: [13][  100/  500]    Overall Loss 1.509309    Objective Loss 1.509309                                        LR 0.001000    Time 0.015784    
2022-01-10 16:14:07,961 - Epoch: [13][  200/  500]    Overall Loss 1.523745    Objective Loss 1.523745                                        LR 0.001000    Time 0.015008    
2022-01-10 16:14:09,402 - Epoch: [13][  300/  500]    Overall Loss 1.528815    Objective Loss 1.528815                                        LR 0.001000    Time 0.014807    
2022-01-10 16:14:10,848 - Epoch: [13][  400/  500]    Overall Loss 1.534416    Objective Loss 1.534416                                        LR 0.001000    Time 0.014719    
2022-01-10 16:14:12,290 - Epoch: [13][  500/  500]    Overall Loss 1.539818    Objective Loss 1.539818    Top1 56.500000    Top5 86.000000    LR 0.001000    Time 0.014656    
2022-01-10 16:14:12,330 - --- validate (epoch=13)-----------
2022-01-10 16:14:12,330 - 10000 samples (100 per mini-batch)
2022-01-10 16:14:13,066 - Epoch: [13][  100/  100]    Loss 1.854986    Top1 49.590000    Top5 79.960000    
2022-01-10 16:14:13,112 - ==> Top1: 49.590    Top5: 79.960    Loss: 1.855

2022-01-10 16:14:13,113 - ==> Best [Top1: 50.840   Top5: 81.240   Sparsity:0.00   Params: 725128 on epoch: 12]
2022-01-10 16:14:13,114 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:14:13,141 - 

2022-01-10 16:14:13,141 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:14:14,809 - Epoch: [14][  100/  500]    Overall Loss 1.470532    Objective Loss 1.470532                                        LR 0.001000    Time 0.016662    
2022-01-10 16:14:16,227 - Epoch: [14][  200/  500]    Overall Loss 1.479964    Objective Loss 1.479964                                        LR 0.001000    Time 0.015417    
2022-01-10 16:14:17,698 - Epoch: [14][  300/  500]    Overall Loss 1.490288    Objective Loss 1.490288                                        LR 0.001000    Time 0.015180    
2022-01-10 16:14:19,275 - Epoch: [14][  400/  500]    Overall Loss 1.489614    Objective Loss 1.489614                                        LR 0.001000    Time 0.015324    
2022-01-10 16:14:20,856 - Epoch: [14][  500/  500]    Overall Loss 1.489428    Objective Loss 1.489428    Top1 55.500000    Top5 83.500000    LR 0.001000    Time 0.015420    
2022-01-10 16:14:20,899 - --- validate (epoch=14)-----------
2022-01-10 16:14:20,899 - 10000 samples (100 per mini-batch)
2022-01-10 16:14:21,657 - Epoch: [14][  100/  100]    Loss 1.666532    Top1 53.940000    Top5 83.450000    
2022-01-10 16:14:21,696 - ==> Top1: 53.940    Top5: 83.450    Loss: 1.667

2022-01-10 16:14:21,698 - ==> Best [Top1: 53.940   Top5: 83.450   Sparsity:0.00   Params: 725128 on epoch: 14]
2022-01-10 16:14:21,698 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:14:21,727 - 

2022-01-10 16:14:21,728 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:14:23,306 - Epoch: [15][  100/  500]    Overall Loss 1.403650    Objective Loss 1.403650                                        LR 0.001000    Time 0.015768    
2022-01-10 16:14:24,747 - Epoch: [15][  200/  500]    Overall Loss 1.439351    Objective Loss 1.439351                                        LR 0.001000    Time 0.015086    
2022-01-10 16:14:26,182 - Epoch: [15][  300/  500]    Overall Loss 1.449568    Objective Loss 1.449568                                        LR 0.001000    Time 0.014839    
2022-01-10 16:14:27,592 - Epoch: [15][  400/  500]    Overall Loss 1.454820    Objective Loss 1.454820                                        LR 0.001000    Time 0.014652    
2022-01-10 16:14:29,024 - Epoch: [15][  500/  500]    Overall Loss 1.453337    Objective Loss 1.453337    Top1 56.000000    Top5 86.500000    LR 0.001000    Time 0.014584    
2022-01-10 16:14:29,074 - --- validate (epoch=15)-----------
2022-01-10 16:14:29,075 - 10000 samples (100 per mini-batch)
2022-01-10 16:14:29,866 - Epoch: [15][  100/  100]    Loss 1.706348    Top1 53.090000    Top5 82.720000    
2022-01-10 16:14:29,905 - ==> Top1: 53.090    Top5: 82.720    Loss: 1.706

2022-01-10 16:14:29,907 - ==> Best [Top1: 53.940   Top5: 83.450   Sparsity:0.00   Params: 725128 on epoch: 14]
2022-01-10 16:14:29,907 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:14:29,933 - 

2022-01-10 16:14:29,934 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:14:31,522 - Epoch: [16][  100/  500]    Overall Loss 1.391035    Objective Loss 1.391035                                        LR 0.001000    Time 0.015874    
2022-01-10 16:14:32,934 - Epoch: [16][  200/  500]    Overall Loss 1.394360    Objective Loss 1.394360                                        LR 0.001000    Time 0.014992    
2022-01-10 16:14:34,381 - Epoch: [16][  300/  500]    Overall Loss 1.405648    Objective Loss 1.405648                                        LR 0.001000    Time 0.014814    
2022-01-10 16:14:35,922 - Epoch: [16][  400/  500]    Overall Loss 1.404832    Objective Loss 1.404832                                        LR 0.001000    Time 0.014961    
2022-01-10 16:14:37,456 - Epoch: [16][  500/  500]    Overall Loss 1.414247    Objective Loss 1.414247    Top1 49.500000    Top5 85.500000    LR 0.001000    Time 0.015036    
2022-01-10 16:14:37,499 - --- validate (epoch=16)-----------
2022-01-10 16:14:37,499 - 10000 samples (100 per mini-batch)
2022-01-10 16:14:38,239 - Epoch: [16][  100/  100]    Loss 1.705380    Top1 52.750000    Top5 82.710000    
2022-01-10 16:14:38,277 - ==> Top1: 52.750    Top5: 82.710    Loss: 1.705

2022-01-10 16:14:38,279 - ==> Best [Top1: 53.940   Top5: 83.450   Sparsity:0.00   Params: 725128 on epoch: 14]
2022-01-10 16:14:38,279 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:14:38,306 - 

2022-01-10 16:14:38,306 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:14:39,981 - Epoch: [17][  100/  500]    Overall Loss 1.353323    Objective Loss 1.353323                                        LR 0.001000    Time 0.016736    
2022-01-10 16:14:41,397 - Epoch: [17][  200/  500]    Overall Loss 1.363685    Objective Loss 1.363685                                        LR 0.001000    Time 0.015444    
2022-01-10 16:14:42,805 - Epoch: [17][  300/  500]    Overall Loss 1.366525    Objective Loss 1.366525                                        LR 0.001000    Time 0.014989    
2022-01-10 16:14:44,214 - Epoch: [17][  400/  500]    Overall Loss 1.367699    Objective Loss 1.367699                                        LR 0.001000    Time 0.014760    
2022-01-10 16:14:45,638 - Epoch: [17][  500/  500]    Overall Loss 1.379948    Objective Loss 1.379948    Top1 58.000000    Top5 86.000000    LR 0.001000    Time 0.014655    
2022-01-10 16:14:45,687 - --- validate (epoch=17)-----------
2022-01-10 16:14:45,687 - 10000 samples (100 per mini-batch)
2022-01-10 16:14:46,442 - Epoch: [17][  100/  100]    Loss 1.643016    Top1 54.770000    Top5 83.910000    
2022-01-10 16:14:46,485 - ==> Top1: 54.770    Top5: 83.910    Loss: 1.643

2022-01-10 16:14:46,486 - ==> Best [Top1: 54.770   Top5: 83.910   Sparsity:0.00   Params: 725128 on epoch: 17]
2022-01-10 16:14:46,486 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:14:46,517 - 

2022-01-10 16:14:46,517 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:14:48,118 - Epoch: [18][  100/  500]    Overall Loss 1.327597    Objective Loss 1.327597                                        LR 0.001000    Time 0.015995    
2022-01-10 16:14:49,542 - Epoch: [18][  200/  500]    Overall Loss 1.336764    Objective Loss 1.336764                                        LR 0.001000    Time 0.015117    
2022-01-10 16:14:50,966 - Epoch: [18][  300/  500]    Overall Loss 1.346702    Objective Loss 1.346702                                        LR 0.001000    Time 0.014822    
2022-01-10 16:14:52,388 - Epoch: [18][  400/  500]    Overall Loss 1.350126    Objective Loss 1.350126                                        LR 0.001000    Time 0.014669    
2022-01-10 16:14:53,822 - Epoch: [18][  500/  500]    Overall Loss 1.352532    Objective Loss 1.352532    Top1 62.500000    Top5 92.000000    LR 0.001000    Time 0.014601    
2022-01-10 16:14:53,870 - --- validate (epoch=18)-----------
2022-01-10 16:14:53,870 - 10000 samples (100 per mini-batch)
2022-01-10 16:14:54,600 - Epoch: [18][  100/  100]    Loss 1.716722    Top1 52.760000    Top5 82.540000    
2022-01-10 16:14:54,639 - ==> Top1: 52.760    Top5: 82.540    Loss: 1.717

2022-01-10 16:14:54,640 - ==> Best [Top1: 54.770   Top5: 83.910   Sparsity:0.00   Params: 725128 on epoch: 17]
2022-01-10 16:14:54,640 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:14:54,727 - 

2022-01-10 16:14:54,727 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:14:56,421 - Epoch: [19][  100/  500]    Overall Loss 1.290024    Objective Loss 1.290024                                        LR 0.001000    Time 0.016928    
2022-01-10 16:14:57,839 - Epoch: [19][  200/  500]    Overall Loss 1.303816    Objective Loss 1.303816                                        LR 0.001000    Time 0.015550    
2022-01-10 16:14:59,257 - Epoch: [19][  300/  500]    Overall Loss 1.306118    Objective Loss 1.306118                                        LR 0.001000    Time 0.015088    
2022-01-10 16:15:00,683 - Epoch: [19][  400/  500]    Overall Loss 1.311327    Objective Loss 1.311327                                        LR 0.001000    Time 0.014881    
2022-01-10 16:15:02,127 - Epoch: [19][  500/  500]    Overall Loss 1.316861    Objective Loss 1.316861    Top1 62.000000    Top5 87.500000    LR 0.001000    Time 0.014791    
2022-01-10 16:15:02,178 - --- validate (epoch=19)-----------
2022-01-10 16:15:02,179 - 10000 samples (100 per mini-batch)
2022-01-10 16:15:02,907 - Epoch: [19][  100/  100]    Loss 1.620356    Top1 55.000000    Top5 84.050000    
2022-01-10 16:15:02,946 - ==> Top1: 55.000    Top5: 84.050    Loss: 1.620

2022-01-10 16:15:02,948 - ==> Best [Top1: 55.000   Top5: 84.050   Sparsity:0.00   Params: 725128 on epoch: 19]
2022-01-10 16:15:02,948 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:15:02,982 - 

2022-01-10 16:15:02,983 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:15:04,671 - Epoch: [20][  100/  500]    Overall Loss 1.247600    Objective Loss 1.247600                                        LR 0.001000    Time 0.016868    
2022-01-10 16:15:06,118 - Epoch: [20][  200/  500]    Overall Loss 1.265670    Objective Loss 1.265670                                        LR 0.001000    Time 0.015664    
2022-01-10 16:15:07,565 - Epoch: [20][  300/  500]    Overall Loss 1.275445    Objective Loss 1.275445                                        LR 0.001000    Time 0.015263    
2022-01-10 16:15:09,010 - Epoch: [20][  400/  500]    Overall Loss 1.276599    Objective Loss 1.276599                                        LR 0.001000    Time 0.015059    
2022-01-10 16:15:10,463 - Epoch: [20][  500/  500]    Overall Loss 1.282558    Objective Loss 1.282558    Top1 64.500000    Top5 86.000000    LR 0.001000    Time 0.014952    
2022-01-10 16:15:10,507 - --- validate (epoch=20)-----------
2022-01-10 16:15:10,507 - 10000 samples (100 per mini-batch)
2022-01-10 16:15:11,240 - Epoch: [20][  100/  100]    Loss 1.587963    Top1 55.760000    Top5 84.620000    
2022-01-10 16:15:11,284 - ==> Top1: 55.760    Top5: 84.620    Loss: 1.588

2022-01-10 16:15:11,286 - ==> Best [Top1: 55.760   Top5: 84.620   Sparsity:0.00   Params: 725128 on epoch: 20]
2022-01-10 16:15:11,286 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:15:11,320 - 

2022-01-10 16:15:11,321 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:15:12,941 - Epoch: [21][  100/  500]    Overall Loss 1.245467    Objective Loss 1.245467                                        LR 0.001000    Time 0.016191    
2022-01-10 16:15:14,387 - Epoch: [21][  200/  500]    Overall Loss 1.247526    Objective Loss 1.247526                                        LR 0.001000    Time 0.015319    
2022-01-10 16:15:15,831 - Epoch: [21][  300/  500]    Overall Loss 1.248613    Objective Loss 1.248613                                        LR 0.001000    Time 0.015023    
2022-01-10 16:15:17,277 - Epoch: [21][  400/  500]    Overall Loss 1.253465    Objective Loss 1.253465                                        LR 0.001000    Time 0.014881    
2022-01-10 16:15:18,730 - Epoch: [21][  500/  500]    Overall Loss 1.258671    Objective Loss 1.258671    Top1 61.500000    Top5 88.500000    LR 0.001000    Time 0.014809    
2022-01-10 16:15:18,780 - --- validate (epoch=21)-----------
2022-01-10 16:15:18,780 - 10000 samples (100 per mini-batch)
2022-01-10 16:15:19,520 - Epoch: [21][  100/  100]    Loss 1.586883    Top1 56.050000    Top5 84.440000    
2022-01-10 16:15:19,565 - ==> Top1: 56.050    Top5: 84.440    Loss: 1.587

2022-01-10 16:15:19,567 - ==> Best [Top1: 56.050   Top5: 84.440   Sparsity:0.00   Params: 725128 on epoch: 21]
2022-01-10 16:15:19,567 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:15:19,598 - 

2022-01-10 16:15:19,598 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:15:21,293 - Epoch: [22][  100/  500]    Overall Loss 1.191113    Objective Loss 1.191113                                        LR 0.001000    Time 0.016936    
2022-01-10 16:15:22,739 - Epoch: [22][  200/  500]    Overall Loss 1.206076    Objective Loss 1.206076                                        LR 0.001000    Time 0.015693    
2022-01-10 16:15:24,181 - Epoch: [22][  300/  500]    Overall Loss 1.212788    Objective Loss 1.212788                                        LR 0.001000    Time 0.015265    
2022-01-10 16:15:25,618 - Epoch: [22][  400/  500]    Overall Loss 1.223050    Objective Loss 1.223050                                        LR 0.001000    Time 0.015040    
2022-01-10 16:15:27,118 - Epoch: [22][  500/  500]    Overall Loss 1.235938    Objective Loss 1.235938    Top1 63.500000    Top5 89.500000    LR 0.001000    Time 0.015031    
2022-01-10 16:15:27,159 - --- validate (epoch=22)-----------
2022-01-10 16:15:27,159 - 10000 samples (100 per mini-batch)
2022-01-10 16:15:27,937 - Epoch: [22][  100/  100]    Loss 1.555971    Top1 57.010000    Top5 85.030000    
2022-01-10 16:15:27,978 - ==> Top1: 57.010    Top5: 85.030    Loss: 1.556

2022-01-10 16:15:27,980 - ==> Best [Top1: 57.010   Top5: 85.030   Sparsity:0.00   Params: 725128 on epoch: 22]
2022-01-10 16:15:27,980 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:15:28,014 - 

2022-01-10 16:15:28,014 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:15:29,691 - Epoch: [23][  100/  500]    Overall Loss 1.196352    Objective Loss 1.196352                                        LR 0.001000    Time 0.016752    
2022-01-10 16:15:31,110 - Epoch: [23][  200/  500]    Overall Loss 1.185902    Objective Loss 1.185902                                        LR 0.001000    Time 0.015468    
2022-01-10 16:15:32,529 - Epoch: [23][  300/  500]    Overall Loss 1.199956    Objective Loss 1.199956                                        LR 0.001000    Time 0.015041    
2022-01-10 16:15:33,950 - Epoch: [23][  400/  500]    Overall Loss 1.208498    Objective Loss 1.208498                                        LR 0.001000    Time 0.014831    
2022-01-10 16:15:35,379 - Epoch: [23][  500/  500]    Overall Loss 1.213300    Objective Loss 1.213300    Top1 65.000000    Top5 87.500000    LR 0.001000    Time 0.014720    
2022-01-10 16:15:35,416 - --- validate (epoch=23)-----------
2022-01-10 16:15:35,416 - 10000 samples (100 per mini-batch)
2022-01-10 16:15:36,146 - Epoch: [23][  100/  100]    Loss 1.578458    Top1 56.340000    Top5 84.810000    
2022-01-10 16:15:36,190 - ==> Top1: 56.340    Top5: 84.810    Loss: 1.578

2022-01-10 16:15:36,192 - ==> Best [Top1: 57.010   Top5: 85.030   Sparsity:0.00   Params: 725128 on epoch: 22]
2022-01-10 16:15:36,192 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:15:36,219 - 

2022-01-10 16:15:36,219 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:15:37,848 - Epoch: [24][  100/  500]    Overall Loss 1.147271    Objective Loss 1.147271                                        LR 0.001000    Time 0.016274    
2022-01-10 16:15:39,287 - Epoch: [24][  200/  500]    Overall Loss 1.152766    Objective Loss 1.152766                                        LR 0.001000    Time 0.015326    
2022-01-10 16:15:40,730 - Epoch: [24][  300/  500]    Overall Loss 1.158191    Objective Loss 1.158191                                        LR 0.001000    Time 0.015026    
2022-01-10 16:15:42,173 - Epoch: [24][  400/  500]    Overall Loss 1.167125    Objective Loss 1.167125                                        LR 0.001000    Time 0.014875    
2022-01-10 16:15:43,624 - Epoch: [24][  500/  500]    Overall Loss 1.185433    Objective Loss 1.185433    Top1 65.000000    Top5 93.500000    LR 0.001000    Time 0.014800    
2022-01-10 16:15:43,665 - --- validate (epoch=24)-----------
2022-01-10 16:15:43,665 - 10000 samples (100 per mini-batch)
2022-01-10 16:15:44,428 - Epoch: [24][  100/  100]    Loss 1.621339    Top1 55.260000    Top5 84.460000    
2022-01-10 16:15:44,473 - ==> Top1: 55.260    Top5: 84.460    Loss: 1.621

2022-01-10 16:15:44,475 - ==> Best [Top1: 57.010   Top5: 85.030   Sparsity:0.00   Params: 725128 on epoch: 22]
2022-01-10 16:15:44,475 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:15:44,502 - 

2022-01-10 16:15:44,502 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:15:46,159 - Epoch: [25][  100/  500]    Overall Loss 1.124591    Objective Loss 1.124591                                        LR 0.001000    Time 0.016552    
2022-01-10 16:15:47,600 - Epoch: [25][  200/  500]    Overall Loss 1.143720    Objective Loss 1.143720                                        LR 0.001000    Time 0.015478    
2022-01-10 16:15:49,021 - Epoch: [25][  300/  500]    Overall Loss 1.152284    Objective Loss 1.152284                                        LR 0.001000    Time 0.015053    
2022-01-10 16:15:50,444 - Epoch: [25][  400/  500]    Overall Loss 1.165949    Objective Loss 1.165949                                        LR 0.001000    Time 0.014845    
2022-01-10 16:15:51,874 - Epoch: [25][  500/  500]    Overall Loss 1.164852    Objective Loss 1.164852    Top1 64.500000    Top5 90.000000    LR 0.001000    Time 0.014734    
2022-01-10 16:15:51,922 - --- validate (epoch=25)-----------
2022-01-10 16:15:51,923 - 10000 samples (100 per mini-batch)
2022-01-10 16:15:52,651 - Epoch: [25][  100/  100]    Loss 1.535303    Top1 57.830000    Top5 85.740000    
2022-01-10 16:15:52,695 - ==> Top1: 57.830    Top5: 85.740    Loss: 1.535

2022-01-10 16:15:52,697 - ==> Best [Top1: 57.830   Top5: 85.740   Sparsity:0.00   Params: 725128 on epoch: 25]
2022-01-10 16:15:52,697 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:15:52,724 - 

2022-01-10 16:15:52,724 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:15:54,387 - Epoch: [26][  100/  500]    Overall Loss 1.118945    Objective Loss 1.118945                                        LR 0.001000    Time 0.016613    
2022-01-10 16:15:55,814 - Epoch: [26][  200/  500]    Overall Loss 1.119410    Objective Loss 1.119410                                        LR 0.001000    Time 0.015436    
2022-01-10 16:15:57,241 - Epoch: [26][  300/  500]    Overall Loss 1.126466    Objective Loss 1.126466                                        LR 0.001000    Time 0.015045    
2022-01-10 16:15:58,669 - Epoch: [26][  400/  500]    Overall Loss 1.135722    Objective Loss 1.135722                                        LR 0.001000    Time 0.014852    
2022-01-10 16:16:00,107 - Epoch: [26][  500/  500]    Overall Loss 1.139976    Objective Loss 1.139976    Top1 65.000000    Top5 88.000000    LR 0.001000    Time 0.014756    
2022-01-10 16:16:00,152 - --- validate (epoch=26)-----------
2022-01-10 16:16:00,153 - 10000 samples (100 per mini-batch)
2022-01-10 16:16:00,888 - Epoch: [26][  100/  100]    Loss 1.582811    Top1 56.690000    Top5 84.380000    
2022-01-10 16:16:00,938 - ==> Top1: 56.690    Top5: 84.380    Loss: 1.583

2022-01-10 16:16:00,940 - ==> Best [Top1: 57.830   Top5: 85.740   Sparsity:0.00   Params: 725128 on epoch: 25]
2022-01-10 16:16:00,940 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:16:00,967 - 

2022-01-10 16:16:00,967 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:16:02,575 - Epoch: [27][  100/  500]    Overall Loss 1.096031    Objective Loss 1.096031                                        LR 0.001000    Time 0.016061    
2022-01-10 16:16:03,996 - Epoch: [27][  200/  500]    Overall Loss 1.107375    Objective Loss 1.107375                                        LR 0.001000    Time 0.015131    
2022-01-10 16:16:05,408 - Epoch: [27][  300/  500]    Overall Loss 1.111537    Objective Loss 1.111537                                        LR 0.001000    Time 0.014792    
2022-01-10 16:16:06,824 - Epoch: [27][  400/  500]    Overall Loss 1.114580    Objective Loss 1.114580                                        LR 0.001000    Time 0.014632    
2022-01-10 16:16:08,247 - Epoch: [27][  500/  500]    Overall Loss 1.121481    Objective Loss 1.121481    Top1 62.500000    Top5 90.500000    LR 0.001000    Time 0.014550    
2022-01-10 16:16:08,298 - --- validate (epoch=27)-----------
2022-01-10 16:16:08,299 - 10000 samples (100 per mini-batch)
2022-01-10 16:16:09,051 - Epoch: [27][  100/  100]    Loss 1.541150    Top1 57.220000    Top5 85.150000    
2022-01-10 16:16:09,094 - ==> Top1: 57.220    Top5: 85.150    Loss: 1.541

2022-01-10 16:16:09,095 - ==> Best [Top1: 57.830   Top5: 85.740   Sparsity:0.00   Params: 725128 on epoch: 25]
2022-01-10 16:16:09,095 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:16:09,116 - 

2022-01-10 16:16:09,116 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:16:10,830 - Epoch: [28][  100/  500]    Overall Loss 1.085049    Objective Loss 1.085049                                        LR 0.001000    Time 0.017123    
2022-01-10 16:16:12,332 - Epoch: [28][  200/  500]    Overall Loss 1.093968    Objective Loss 1.093968                                        LR 0.001000    Time 0.016068    
2022-01-10 16:16:13,907 - Epoch: [28][  300/  500]    Overall Loss 1.103757    Objective Loss 1.103757                                        LR 0.001000    Time 0.015959    
2022-01-10 16:16:15,486 - Epoch: [28][  400/  500]    Overall Loss 1.105705    Objective Loss 1.105705                                        LR 0.001000    Time 0.015913    
2022-01-10 16:16:17,061 - Epoch: [28][  500/  500]    Overall Loss 1.110432    Objective Loss 1.110432    Top1 65.500000    Top5 91.000000    LR 0.001000    Time 0.015879    
2022-01-10 16:16:17,108 - --- validate (epoch=28)-----------
2022-01-10 16:16:17,109 - 10000 samples (100 per mini-batch)
2022-01-10 16:16:17,861 - Epoch: [28][  100/  100]    Loss 1.527993    Top1 57.620000    Top5 85.610000    
2022-01-10 16:16:17,906 - ==> Top1: 57.620    Top5: 85.610    Loss: 1.528

2022-01-10 16:16:17,908 - ==> Best [Top1: 57.830   Top5: 85.740   Sparsity:0.00   Params: 725128 on epoch: 25]
2022-01-10 16:16:17,908 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:16:17,936 - 

2022-01-10 16:16:17,936 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:16:19,648 - Epoch: [29][  100/  500]    Overall Loss 1.053844    Objective Loss 1.053844                                        LR 0.001000    Time 0.017107    
2022-01-10 16:16:21,156 - Epoch: [29][  200/  500]    Overall Loss 1.054241    Objective Loss 1.054241                                        LR 0.001000    Time 0.016089    
2022-01-10 16:16:22,755 - Epoch: [29][  300/  500]    Overall Loss 1.066584    Objective Loss 1.066584                                        LR 0.001000    Time 0.016052    
2022-01-10 16:16:24,347 - Epoch: [29][  400/  500]    Overall Loss 1.069452    Objective Loss 1.069452                                        LR 0.001000    Time 0.016017    
2022-01-10 16:16:25,932 - Epoch: [29][  500/  500]    Overall Loss 1.077361    Objective Loss 1.077361    Top1 67.000000    Top5 94.500000    LR 0.001000    Time 0.015982    
2022-01-10 16:16:25,976 - --- validate (epoch=29)-----------
2022-01-10 16:16:25,976 - 10000 samples (100 per mini-batch)
2022-01-10 16:16:26,719 - Epoch: [29][  100/  100]    Loss 1.522413    Top1 58.060000    Top5 86.060000    
2022-01-10 16:16:26,769 - ==> Top1: 58.060    Top5: 86.060    Loss: 1.522

2022-01-10 16:16:26,771 - ==> Best [Top1: 58.060   Top5: 86.060   Sparsity:0.00   Params: 725128 on epoch: 29]
2022-01-10 16:16:26,771 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:16:26,805 - 

2022-01-10 16:16:26,805 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:16:28,415 - Epoch: [30][  100/  500]    Overall Loss 1.011976    Objective Loss 1.011976                                        LR 0.001000    Time 0.016084    
2022-01-10 16:16:29,855 - Epoch: [30][  200/  500]    Overall Loss 1.034016    Objective Loss 1.034016                                        LR 0.001000    Time 0.015235    
2022-01-10 16:16:31,291 - Epoch: [30][  300/  500]    Overall Loss 1.042844    Objective Loss 1.042844                                        LR 0.001000    Time 0.014942    
2022-01-10 16:16:32,726 - Epoch: [30][  400/  500]    Overall Loss 1.054604    Objective Loss 1.054604                                        LR 0.001000    Time 0.014794    
2022-01-10 16:16:34,174 - Epoch: [30][  500/  500]    Overall Loss 1.060756    Objective Loss 1.060756    Top1 63.000000    Top5 90.000000    LR 0.001000    Time 0.014728    
2022-01-10 16:16:34,216 - --- validate (epoch=30)-----------
2022-01-10 16:16:34,216 - 10000 samples (100 per mini-batch)
2022-01-10 16:16:34,955 - Epoch: [30][  100/  100]    Loss 1.522747    Top1 58.330000    Top5 86.080000    
2022-01-10 16:16:34,995 - ==> Top1: 58.330    Top5: 86.080    Loss: 1.523

2022-01-10 16:16:34,997 - ==> Best [Top1: 58.330   Top5: 86.080   Sparsity:0.00   Params: 725128 on epoch: 30]
2022-01-10 16:16:34,997 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:16:35,031 - 

2022-01-10 16:16:35,031 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:16:36,728 - Epoch: [31][  100/  500]    Overall Loss 1.043025    Objective Loss 1.043025                                        LR 0.001000    Time 0.016949    
2022-01-10 16:16:38,276 - Epoch: [31][  200/  500]    Overall Loss 1.040384    Objective Loss 1.040384                                        LR 0.001000    Time 0.016213    
2022-01-10 16:16:39,748 - Epoch: [31][  300/  500]    Overall Loss 1.039378    Objective Loss 1.039378                                        LR 0.001000    Time 0.015711    
2022-01-10 16:16:41,163 - Epoch: [31][  400/  500]    Overall Loss 1.040567    Objective Loss 1.040567                                        LR 0.001000    Time 0.015319    
2022-01-10 16:16:42,586 - Epoch: [31][  500/  500]    Overall Loss 1.045203    Objective Loss 1.045203    Top1 61.500000    Top5 91.500000    LR 0.001000    Time 0.015099    
2022-01-10 16:16:42,635 - --- validate (epoch=31)-----------
2022-01-10 16:16:42,636 - 10000 samples (100 per mini-batch)
2022-01-10 16:16:43,371 - Epoch: [31][  100/  100]    Loss 1.538038    Top1 57.930000    Top5 85.590000    
2022-01-10 16:16:43,417 - ==> Top1: 57.930    Top5: 85.590    Loss: 1.538

2022-01-10 16:16:43,418 - ==> Best [Top1: 58.330   Top5: 86.080   Sparsity:0.00   Params: 725128 on epoch: 30]
2022-01-10 16:16:43,418 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:16:43,445 - 

2022-01-10 16:16:43,446 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:16:45,093 - Epoch: [32][  100/  500]    Overall Loss 0.999256    Objective Loss 0.999256                                        LR 0.001000    Time 0.016462    
2022-01-10 16:16:46,503 - Epoch: [32][  200/  500]    Overall Loss 1.007142    Objective Loss 1.007142                                        LR 0.001000    Time 0.015278    
2022-01-10 16:16:47,917 - Epoch: [32][  300/  500]    Overall Loss 1.014316    Objective Loss 1.014316                                        LR 0.001000    Time 0.014894    
2022-01-10 16:16:49,330 - Epoch: [32][  400/  500]    Overall Loss 1.025657    Objective Loss 1.025657                                        LR 0.001000    Time 0.014703    
2022-01-10 16:16:50,751 - Epoch: [32][  500/  500]    Overall Loss 1.031939    Objective Loss 1.031939    Top1 73.000000    Top5 95.000000    LR 0.001000    Time 0.014601    
2022-01-10 16:16:50,792 - --- validate (epoch=32)-----------
2022-01-10 16:16:50,792 - 10000 samples (100 per mini-batch)
2022-01-10 16:16:51,509 - Epoch: [32][  100/  100]    Loss 1.555546    Top1 57.680000    Top5 85.520000    
2022-01-10 16:16:51,552 - ==> Top1: 57.680    Top5: 85.520    Loss: 1.556

2022-01-10 16:16:51,554 - ==> Best [Top1: 58.330   Top5: 86.080   Sparsity:0.00   Params: 725128 on epoch: 30]
2022-01-10 16:16:51,554 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:16:51,581 - 

2022-01-10 16:16:51,581 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:16:53,166 - Epoch: [33][  100/  500]    Overall Loss 0.965028    Objective Loss 0.965028                                        LR 0.001000    Time 0.015832    
2022-01-10 16:16:54,584 - Epoch: [33][  200/  500]    Overall Loss 0.979147    Objective Loss 0.979147                                        LR 0.001000    Time 0.015002    
2022-01-10 16:16:56,014 - Epoch: [33][  300/  500]    Overall Loss 0.989718    Objective Loss 0.989718                                        LR 0.001000    Time 0.014766    
2022-01-10 16:16:57,425 - Epoch: [33][  400/  500]    Overall Loss 1.003690    Objective Loss 1.003690                                        LR 0.001000    Time 0.014600    
2022-01-10 16:16:58,870 - Epoch: [33][  500/  500]    Overall Loss 1.013019    Objective Loss 1.013019    Top1 70.500000    Top5 96.500000    LR 0.001000    Time 0.014570    
2022-01-10 16:16:58,909 - --- validate (epoch=33)-----------
2022-01-10 16:16:58,909 - 10000 samples (100 per mini-batch)
2022-01-10 16:16:59,666 - Epoch: [33][  100/  100]    Loss 1.515100    Top1 58.550000    Top5 86.010000    
2022-01-10 16:16:59,719 - ==> Top1: 58.550    Top5: 86.010    Loss: 1.515

2022-01-10 16:16:59,721 - ==> Best [Top1: 58.550   Top5: 86.010   Sparsity:0.00   Params: 725128 on epoch: 33]
2022-01-10 16:16:59,721 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:16:59,755 - 

2022-01-10 16:16:59,755 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:17:01,474 - Epoch: [34][  100/  500]    Overall Loss 0.959243    Objective Loss 0.959243                                        LR 0.001000    Time 0.017176    
2022-01-10 16:17:02,887 - Epoch: [34][  200/  500]    Overall Loss 0.976660    Objective Loss 0.976660                                        LR 0.001000    Time 0.015646    
2022-01-10 16:17:04,288 - Epoch: [34][  300/  500]    Overall Loss 0.987694    Objective Loss 0.987694                                        LR 0.001000    Time 0.015099    
2022-01-10 16:17:05,690 - Epoch: [34][  400/  500]    Overall Loss 0.997929    Objective Loss 0.997929                                        LR 0.001000    Time 0.014828    
2022-01-10 16:17:07,102 - Epoch: [34][  500/  500]    Overall Loss 0.999871    Objective Loss 0.999871    Top1 69.500000    Top5 97.500000    LR 0.001000    Time 0.014684    
2022-01-10 16:17:07,148 - --- validate (epoch=34)-----------
2022-01-10 16:17:07,148 - 10000 samples (100 per mini-batch)
2022-01-10 16:17:07,901 - Epoch: [34][  100/  100]    Loss 1.526971    Top1 58.740000    Top5 86.230000    
2022-01-10 16:17:07,950 - ==> Top1: 58.740    Top5: 86.230    Loss: 1.527

2022-01-10 16:17:07,952 - ==> Best [Top1: 58.740   Top5: 86.230   Sparsity:0.00   Params: 725128 on epoch: 34]
2022-01-10 16:17:07,952 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:17:07,985 - 

2022-01-10 16:17:07,986 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:17:09,646 - Epoch: [35][  100/  500]    Overall Loss 0.954341    Objective Loss 0.954341                                        LR 0.001000    Time 0.016587    
2022-01-10 16:17:11,057 - Epoch: [35][  200/  500]    Overall Loss 0.956176    Objective Loss 0.956176                                        LR 0.001000    Time 0.015346    
2022-01-10 16:17:12,466 - Epoch: [35][  300/  500]    Overall Loss 0.965259    Objective Loss 0.965259                                        LR 0.001000    Time 0.014925    
2022-01-10 16:17:13,892 - Epoch: [35][  400/  500]    Overall Loss 0.975620    Objective Loss 0.975620                                        LR 0.001000    Time 0.014757    
2022-01-10 16:17:15,331 - Epoch: [35][  500/  500]    Overall Loss 0.984250    Objective Loss 0.984250    Top1 72.500000    Top5 94.500000    LR 0.001000    Time 0.014682    
2022-01-10 16:17:15,384 - --- validate (epoch=35)-----------
2022-01-10 16:17:15,385 - 10000 samples (100 per mini-batch)
2022-01-10 16:17:16,164 - Epoch: [35][  100/  100]    Loss 1.533520    Top1 58.320000    Top5 86.200000    
2022-01-10 16:17:16,203 - ==> Top1: 58.320    Top5: 86.200    Loss: 1.534

2022-01-10 16:17:16,205 - ==> Best [Top1: 58.740   Top5: 86.230   Sparsity:0.00   Params: 725128 on epoch: 34]
2022-01-10 16:17:16,205 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:17:16,232 - 

2022-01-10 16:17:16,233 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:17:17,843 - Epoch: [36][  100/  500]    Overall Loss 0.945551    Objective Loss 0.945551                                        LR 0.001000    Time 0.016096    
2022-01-10 16:17:19,289 - Epoch: [36][  200/  500]    Overall Loss 0.946278    Objective Loss 0.946278                                        LR 0.001000    Time 0.015273    
2022-01-10 16:17:20,748 - Epoch: [36][  300/  500]    Overall Loss 0.957143    Objective Loss 0.957143                                        LR 0.001000    Time 0.015042    
2022-01-10 16:17:22,209 - Epoch: [36][  400/  500]    Overall Loss 0.954273    Objective Loss 0.954273                                        LR 0.001000    Time 0.014932    
2022-01-10 16:17:23,674 - Epoch: [36][  500/  500]    Overall Loss 0.960369    Objective Loss 0.960369    Top1 67.500000    Top5 92.500000    LR 0.001000    Time 0.014874    
2022-01-10 16:17:23,712 - --- validate (epoch=36)-----------
2022-01-10 16:17:23,712 - 10000 samples (100 per mini-batch)
2022-01-10 16:17:24,483 - Epoch: [36][  100/  100]    Loss 1.526024    Top1 58.590000    Top5 86.370000    
2022-01-10 16:17:24,531 - ==> Top1: 58.590    Top5: 86.370    Loss: 1.526

2022-01-10 16:17:24,533 - ==> Best [Top1: 58.740   Top5: 86.230   Sparsity:0.00   Params: 725128 on epoch: 34]
2022-01-10 16:17:24,533 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:17:24,554 - 

2022-01-10 16:17:24,554 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:17:26,221 - Epoch: [37][  100/  500]    Overall Loss 0.892310    Objective Loss 0.892310                                        LR 0.001000    Time 0.016652    
2022-01-10 16:17:27,652 - Epoch: [37][  200/  500]    Overall Loss 0.921250    Objective Loss 0.921250                                        LR 0.001000    Time 0.015479    
2022-01-10 16:17:29,079 - Epoch: [37][  300/  500]    Overall Loss 0.928517    Objective Loss 0.928517                                        LR 0.001000    Time 0.015073    
2022-01-10 16:17:30,509 - Epoch: [37][  400/  500]    Overall Loss 0.936137    Objective Loss 0.936137                                        LR 0.001000    Time 0.014876    
2022-01-10 16:17:31,946 - Epoch: [37][  500/  500]    Overall Loss 0.948928    Objective Loss 0.948928    Top1 65.000000    Top5 90.000000    LR 0.001000    Time 0.014773    
2022-01-10 16:17:31,987 - --- validate (epoch=37)-----------
2022-01-10 16:17:31,987 - 10000 samples (100 per mini-batch)
2022-01-10 16:17:32,729 - Epoch: [37][  100/  100]    Loss 1.507809    Top1 58.810000    Top5 86.520000    
2022-01-10 16:17:32,776 - ==> Top1: 58.810    Top5: 86.520    Loss: 1.508

2022-01-10 16:17:32,778 - ==> Best [Top1: 58.810   Top5: 86.520   Sparsity:0.00   Params: 725128 on epoch: 37]
2022-01-10 16:17:32,778 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:17:32,812 - 

2022-01-10 16:17:32,812 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:17:34,466 - Epoch: [38][  100/  500]    Overall Loss 0.897194    Objective Loss 0.897194                                        LR 0.001000    Time 0.016524    
2022-01-10 16:17:35,901 - Epoch: [38][  200/  500]    Overall Loss 0.907119    Objective Loss 0.907119                                        LR 0.001000    Time 0.015435    
2022-01-10 16:17:37,333 - Epoch: [38][  300/  500]    Overall Loss 0.921378    Objective Loss 0.921378                                        LR 0.001000    Time 0.015060    
2022-01-10 16:17:38,785 - Epoch: [38][  400/  500]    Overall Loss 0.929282    Objective Loss 0.929282                                        LR 0.001000    Time 0.014924    
2022-01-10 16:17:40,244 - Epoch: [38][  500/  500]    Overall Loss 0.929318    Objective Loss 0.929318    Top1 76.500000    Top5 96.000000    LR 0.001000    Time 0.014855    
2022-01-10 16:17:40,289 - --- validate (epoch=38)-----------
2022-01-10 16:17:40,289 - 10000 samples (100 per mini-batch)
2022-01-10 16:17:41,038 - Epoch: [38][  100/  100]    Loss 1.492272    Top1 59.070000    Top5 86.600000    
2022-01-10 16:17:41,081 - ==> Top1: 59.070    Top5: 86.600    Loss: 1.492

2022-01-10 16:17:41,083 - ==> Best [Top1: 59.070   Top5: 86.600   Sparsity:0.00   Params: 725128 on epoch: 38]
2022-01-10 16:17:41,083 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:17:41,107 - 

2022-01-10 16:17:41,107 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:17:42,690 - Epoch: [39][  100/  500]    Overall Loss 0.871318    Objective Loss 0.871318                                        LR 0.001000    Time 0.015824    
2022-01-10 16:17:44,108 - Epoch: [39][  200/  500]    Overall Loss 0.893601    Objective Loss 0.893601                                        LR 0.001000    Time 0.014997    
2022-01-10 16:17:45,553 - Epoch: [39][  300/  500]    Overall Loss 0.903668    Objective Loss 0.903668                                        LR 0.001000    Time 0.014811    
2022-01-10 16:17:47,008 - Epoch: [39][  400/  500]    Overall Loss 0.907676    Objective Loss 0.907676                                        LR 0.001000    Time 0.014743    
2022-01-10 16:17:48,474 - Epoch: [39][  500/  500]    Overall Loss 0.914447    Objective Loss 0.914447    Top1 75.000000    Top5 95.000000    LR 0.001000    Time 0.014724    
2022-01-10 16:17:48,522 - --- validate (epoch=39)-----------
2022-01-10 16:17:48,522 - 10000 samples (100 per mini-batch)
2022-01-10 16:17:49,274 - Epoch: [39][  100/  100]    Loss 1.521820    Top1 59.280000    Top5 85.950000    
2022-01-10 16:17:49,328 - ==> Top1: 59.280    Top5: 85.950    Loss: 1.522

2022-01-10 16:17:49,330 - ==> Best [Top1: 59.280   Top5: 85.950   Sparsity:0.00   Params: 725128 on epoch: 39]
2022-01-10 16:17:49,330 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:17:49,364 - 

2022-01-10 16:17:49,364 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:17:51,040 - Epoch: [40][  100/  500]    Overall Loss 0.852531    Objective Loss 0.852531                                        LR 0.001000    Time 0.016752    
2022-01-10 16:17:52,466 - Epoch: [40][  200/  500]    Overall Loss 0.873117    Objective Loss 0.873117                                        LR 0.001000    Time 0.015498    
2022-01-10 16:17:54,009 - Epoch: [40][  300/  500]    Overall Loss 0.874916    Objective Loss 0.874916                                        LR 0.001000    Time 0.015474    
2022-01-10 16:17:55,562 - Epoch: [40][  400/  500]    Overall Loss 0.888854    Objective Loss 0.888854                                        LR 0.001000    Time 0.015485    
2022-01-10 16:17:57,112 - Epoch: [40][  500/  500]    Overall Loss 0.901617    Objective Loss 0.901617    Top1 73.500000    Top5 95.500000    LR 0.001000    Time 0.015486    
2022-01-10 16:17:57,157 - --- validate (epoch=40)-----------
2022-01-10 16:17:57,158 - 10000 samples (100 per mini-batch)
2022-01-10 16:17:57,910 - Epoch: [40][  100/  100]    Loss 1.602820    Top1 57.490000    Top5 85.400000    
2022-01-10 16:17:57,954 - ==> Top1: 57.490    Top5: 85.400    Loss: 1.603

2022-01-10 16:17:57,956 - ==> Best [Top1: 59.280   Top5: 85.950   Sparsity:0.00   Params: 725128 on epoch: 39]
2022-01-10 16:17:57,956 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:17:57,978 - 

2022-01-10 16:17:57,978 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:17:59,574 - Epoch: [41][  100/  500]    Overall Loss 0.861585    Objective Loss 0.861585                                        LR 0.001000    Time 0.015943    
2022-01-10 16:18:01,004 - Epoch: [41][  200/  500]    Overall Loss 0.872421    Objective Loss 0.872421                                        LR 0.001000    Time 0.015119    
2022-01-10 16:18:02,435 - Epoch: [41][  300/  500]    Overall Loss 0.887977    Objective Loss 0.887977                                        LR 0.001000    Time 0.014848    
2022-01-10 16:18:03,867 - Epoch: [41][  400/  500]    Overall Loss 0.889043    Objective Loss 0.889043                                        LR 0.001000    Time 0.014714    
2022-01-10 16:18:05,306 - Epoch: [41][  500/  500]    Overall Loss 0.897190    Objective Loss 0.897190    Top1 73.000000    Top5 94.500000    LR 0.001000    Time 0.014647    
2022-01-10 16:18:05,360 - --- validate (epoch=41)-----------
2022-01-10 16:18:05,360 - 10000 samples (100 per mini-batch)
2022-01-10 16:18:06,178 - Epoch: [41][  100/  100]    Loss 1.504894    Top1 59.880000    Top5 86.510000    
2022-01-10 16:18:06,223 - ==> Top1: 59.880    Top5: 86.510    Loss: 1.505

2022-01-10 16:18:06,224 - ==> Best [Top1: 59.880   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 41]
2022-01-10 16:18:06,224 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:18:06,258 - 

2022-01-10 16:18:06,258 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:18:07,898 - Epoch: [42][  100/  500]    Overall Loss 0.861067    Objective Loss 0.861067                                        LR 0.001000    Time 0.016378    
2022-01-10 16:18:09,330 - Epoch: [42][  200/  500]    Overall Loss 0.859124    Objective Loss 0.859124                                        LR 0.001000    Time 0.015349    
2022-01-10 16:18:10,888 - Epoch: [42][  300/  500]    Overall Loss 0.869866    Objective Loss 0.869866                                        LR 0.001000    Time 0.015421    
2022-01-10 16:18:12,437 - Epoch: [42][  400/  500]    Overall Loss 0.870021    Objective Loss 0.870021                                        LR 0.001000    Time 0.015437    
2022-01-10 16:18:13,985 - Epoch: [42][  500/  500]    Overall Loss 0.880030    Objective Loss 0.880030    Top1 73.500000    Top5 97.000000    LR 0.001000    Time 0.015445    
2022-01-10 16:18:14,026 - --- validate (epoch=42)-----------
2022-01-10 16:18:14,027 - 10000 samples (100 per mini-batch)
2022-01-10 16:18:14,767 - Epoch: [42][  100/  100]    Loss 1.590381    Top1 58.420000    Top5 85.940000    
2022-01-10 16:18:14,821 - ==> Top1: 58.420    Top5: 85.940    Loss: 1.590

2022-01-10 16:18:14,822 - ==> Best [Top1: 59.880   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 41]
2022-01-10 16:18:14,823 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:18:14,845 - 

2022-01-10 16:18:14,846 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:18:16,521 - Epoch: [43][  100/  500]    Overall Loss 0.843535    Objective Loss 0.843535                                        LR 0.001000    Time 0.016745    
2022-01-10 16:18:17,954 - Epoch: [43][  200/  500]    Overall Loss 0.832172    Objective Loss 0.832172                                        LR 0.001000    Time 0.015530    
2022-01-10 16:18:19,375 - Epoch: [43][  300/  500]    Overall Loss 0.845544    Objective Loss 0.845544                                        LR 0.001000    Time 0.015088    
2022-01-10 16:18:20,798 - Epoch: [43][  400/  500]    Overall Loss 0.853909    Objective Loss 0.853909                                        LR 0.001000    Time 0.014871    
2022-01-10 16:18:22,228 - Epoch: [43][  500/  500]    Overall Loss 0.866045    Objective Loss 0.866045    Top1 71.000000    Top5 96.000000    LR 0.001000    Time 0.014755    
2022-01-10 16:18:22,279 - --- validate (epoch=43)-----------
2022-01-10 16:18:22,279 - 10000 samples (100 per mini-batch)
2022-01-10 16:18:23,024 - Epoch: [43][  100/  100]    Loss 1.494831    Top1 59.540000    Top5 86.920000    
2022-01-10 16:18:23,078 - ==> Top1: 59.540    Top5: 86.920    Loss: 1.495

2022-01-10 16:18:23,080 - ==> Best [Top1: 59.880   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 41]
2022-01-10 16:18:23,080 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:18:23,107 - 

2022-01-10 16:18:23,108 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:18:24,722 - Epoch: [44][  100/  500]    Overall Loss 0.816426    Objective Loss 0.816426                                        LR 0.001000    Time 0.016126    
2022-01-10 16:18:26,150 - Epoch: [44][  200/  500]    Overall Loss 0.823406    Objective Loss 0.823406                                        LR 0.001000    Time 0.015198    
2022-01-10 16:18:27,573 - Epoch: [44][  300/  500]    Overall Loss 0.839686    Objective Loss 0.839686                                        LR 0.001000    Time 0.014876    
2022-01-10 16:18:28,975 - Epoch: [44][  400/  500]    Overall Loss 0.849081    Objective Loss 0.849081                                        LR 0.001000    Time 0.014660    
2022-01-10 16:18:30,422 - Epoch: [44][  500/  500]    Overall Loss 0.856889    Objective Loss 0.856889    Top1 69.500000    Top5 95.500000    LR 0.001000    Time 0.014619    
2022-01-10 16:18:30,463 - --- validate (epoch=44)-----------
2022-01-10 16:18:30,463 - 10000 samples (100 per mini-batch)
2022-01-10 16:18:31,346 - Epoch: [44][  100/  100]    Loss 1.497598    Top1 59.640000    Top5 87.030000    
2022-01-10 16:18:31,394 - ==> Top1: 59.640    Top5: 87.030    Loss: 1.498

2022-01-10 16:18:31,395 - ==> Best [Top1: 59.880   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 41]
2022-01-10 16:18:31,395 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:18:31,417 - 

2022-01-10 16:18:31,417 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:18:33,035 - Epoch: [45][  100/  500]    Overall Loss 0.808411    Objective Loss 0.808411                                        LR 0.001000    Time 0.016166    
2022-01-10 16:18:34,486 - Epoch: [45][  200/  500]    Overall Loss 0.816194    Objective Loss 0.816194                                        LR 0.001000    Time 0.015335    
2022-01-10 16:18:36,057 - Epoch: [45][  300/  500]    Overall Loss 0.826165    Objective Loss 0.826165                                        LR 0.001000    Time 0.015456    
2022-01-10 16:18:37,630 - Epoch: [45][  400/  500]    Overall Loss 0.838222    Objective Loss 0.838222                                        LR 0.001000    Time 0.015524    
2022-01-10 16:18:39,202 - Epoch: [45][  500/  500]    Overall Loss 0.845420    Objective Loss 0.845420    Top1 71.500000    Top5 97.000000    LR 0.001000    Time 0.015560    
2022-01-10 16:18:39,243 - --- validate (epoch=45)-----------
2022-01-10 16:18:39,244 - 10000 samples (100 per mini-batch)
2022-01-10 16:18:39,989 - Epoch: [45][  100/  100]    Loss 1.535448    Top1 59.520000    Top5 86.560000    
2022-01-10 16:18:40,027 - ==> Top1: 59.520    Top5: 86.560    Loss: 1.535

2022-01-10 16:18:40,028 - ==> Best [Top1: 59.880   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 41]
2022-01-10 16:18:40,028 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:18:40,056 - 

2022-01-10 16:18:40,056 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:18:41,758 - Epoch: [46][  100/  500]    Overall Loss 0.796245    Objective Loss 0.796245                                        LR 0.001000    Time 0.017006    
2022-01-10 16:18:43,200 - Epoch: [46][  200/  500]    Overall Loss 0.803633    Objective Loss 0.803633                                        LR 0.001000    Time 0.015708    
2022-01-10 16:18:44,629 - Epoch: [46][  300/  500]    Overall Loss 0.810952    Objective Loss 0.810952                                        LR 0.001000    Time 0.015234    
2022-01-10 16:18:46,077 - Epoch: [46][  400/  500]    Overall Loss 0.821254    Objective Loss 0.821254                                        LR 0.001000    Time 0.015042    
2022-01-10 16:18:47,518 - Epoch: [46][  500/  500]    Overall Loss 0.825043    Objective Loss 0.825043    Top1 74.500000    Top5 94.500000    LR 0.001000    Time 0.014915    
2022-01-10 16:18:47,559 - --- validate (epoch=46)-----------
2022-01-10 16:18:47,559 - 10000 samples (100 per mini-batch)
2022-01-10 16:18:48,294 - Epoch: [46][  100/  100]    Loss 1.510284    Top1 59.800000    Top5 86.500000    
2022-01-10 16:18:48,341 - ==> Top1: 59.800    Top5: 86.500    Loss: 1.510

2022-01-10 16:18:48,343 - ==> Best [Top1: 59.880   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 41]
2022-01-10 16:18:48,343 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:18:48,371 - 

2022-01-10 16:18:48,371 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:18:49,972 - Epoch: [47][  100/  500]    Overall Loss 0.797735    Objective Loss 0.797735                                        LR 0.001000    Time 0.015994    
2022-01-10 16:18:51,406 - Epoch: [47][  200/  500]    Overall Loss 0.800514    Objective Loss 0.800514                                        LR 0.001000    Time 0.015165    
2022-01-10 16:18:52,838 - Epoch: [47][  300/  500]    Overall Loss 0.804946    Objective Loss 0.804946                                        LR 0.001000    Time 0.014880    
2022-01-10 16:18:54,271 - Epoch: [47][  400/  500]    Overall Loss 0.812981    Objective Loss 0.812981                                        LR 0.001000    Time 0.014740    
2022-01-10 16:18:55,712 - Epoch: [47][  500/  500]    Overall Loss 0.819721    Objective Loss 0.819721    Top1 73.000000    Top5 95.000000    LR 0.001000    Time 0.014673    
2022-01-10 16:18:55,767 - --- validate (epoch=47)-----------
2022-01-10 16:18:55,767 - 10000 samples (100 per mini-batch)
2022-01-10 16:18:56,551 - Epoch: [47][  100/  100]    Loss 1.522106    Top1 59.920000    Top5 86.450000    
2022-01-10 16:18:56,595 - ==> Top1: 59.920    Top5: 86.450    Loss: 1.522

2022-01-10 16:18:56,596 - ==> Best [Top1: 59.920   Top5: 86.450   Sparsity:0.00   Params: 725128 on epoch: 47]
2022-01-10 16:18:56,596 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:18:56,631 - 

2022-01-10 16:18:56,631 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:18:58,311 - Epoch: [48][  100/  500]    Overall Loss 0.761919    Objective Loss 0.761919                                        LR 0.001000    Time 0.016787    
2022-01-10 16:18:59,769 - Epoch: [48][  200/  500]    Overall Loss 0.784399    Objective Loss 0.784399                                        LR 0.001000    Time 0.015678    
2022-01-10 16:19:01,332 - Epoch: [48][  300/  500]    Overall Loss 0.789488    Objective Loss 0.789488                                        LR 0.001000    Time 0.015659    
2022-01-10 16:19:02,905 - Epoch: [48][  400/  500]    Overall Loss 0.804485    Objective Loss 0.804485                                        LR 0.001000    Time 0.015674    
2022-01-10 16:19:04,465 - Epoch: [48][  500/  500]    Overall Loss 0.806778    Objective Loss 0.806778    Top1 73.500000    Top5 98.000000    LR 0.001000    Time 0.015657    
2022-01-10 16:19:04,516 - --- validate (epoch=48)-----------
2022-01-10 16:19:04,517 - 10000 samples (100 per mini-batch)
2022-01-10 16:19:05,260 - Epoch: [48][  100/  100]    Loss 1.566469    Top1 58.760000    Top5 86.200000    
2022-01-10 16:19:05,304 - ==> Top1: 58.760    Top5: 86.200    Loss: 1.566

2022-01-10 16:19:05,306 - ==> Best [Top1: 59.920   Top5: 86.450   Sparsity:0.00   Params: 725128 on epoch: 47]
2022-01-10 16:19:05,306 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:19:05,333 - 

2022-01-10 16:19:05,334 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:19:07,010 - Epoch: [49][  100/  500]    Overall Loss 0.768935    Objective Loss 0.768935                                        LR 0.001000    Time 0.016749    
2022-01-10 16:19:08,451 - Epoch: [49][  200/  500]    Overall Loss 0.769436    Objective Loss 0.769436                                        LR 0.001000    Time 0.015575    
2022-01-10 16:19:09,902 - Epoch: [49][  300/  500]    Overall Loss 0.777283    Objective Loss 0.777283                                        LR 0.001000    Time 0.015219    
2022-01-10 16:19:11,351 - Epoch: [49][  400/  500]    Overall Loss 0.787579    Objective Loss 0.787579                                        LR 0.001000    Time 0.015034    
2022-01-10 16:19:12,809 - Epoch: [49][  500/  500]    Overall Loss 0.797182    Objective Loss 0.797182    Top1 74.500000    Top5 96.500000    LR 0.001000    Time 0.014940    
2022-01-10 16:19:12,853 - --- validate (epoch=49)-----------
2022-01-10 16:19:12,854 - 10000 samples (100 per mini-batch)
2022-01-10 16:19:13,590 - Epoch: [49][  100/  100]    Loss 1.543435    Top1 60.120000    Top5 86.530000    
2022-01-10 16:19:13,635 - ==> Top1: 60.120    Top5: 86.530    Loss: 1.543

2022-01-10 16:19:13,637 - ==> Best [Top1: 60.120   Top5: 86.530   Sparsity:0.00   Params: 725128 on epoch: 49]
2022-01-10 16:19:13,637 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:19:13,672 - 

2022-01-10 16:19:13,672 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:19:15,275 - Epoch: [50][  100/  500]    Overall Loss 0.750525    Objective Loss 0.750525                                        LR 0.001000    Time 0.016017    
2022-01-10 16:19:16,713 - Epoch: [50][  200/  500]    Overall Loss 0.758038    Objective Loss 0.758038                                        LR 0.001000    Time 0.015192    
2022-01-10 16:19:18,147 - Epoch: [50][  300/  500]    Overall Loss 0.767625    Objective Loss 0.767625                                        LR 0.001000    Time 0.014907    
2022-01-10 16:19:19,583 - Epoch: [50][  400/  500]    Overall Loss 0.777079    Objective Loss 0.777079                                        LR 0.001000    Time 0.014769    
2022-01-10 16:19:21,026 - Epoch: [50][  500/  500]    Overall Loss 0.782601    Objective Loss 0.782601    Top1 78.000000    Top5 96.000000    LR 0.001000    Time 0.014698    
2022-01-10 16:19:21,065 - --- validate (epoch=50)-----------
2022-01-10 16:19:21,065 - 10000 samples (100 per mini-batch)
2022-01-10 16:19:21,796 - Epoch: [50][  100/  100]    Loss 1.562906    Top1 58.890000    Top5 86.000000    
2022-01-10 16:19:21,853 - ==> Top1: 58.890    Top5: 86.000    Loss: 1.563

2022-01-10 16:19:21,855 - ==> Best [Top1: 60.120   Top5: 86.530   Sparsity:0.00   Params: 725128 on epoch: 49]
2022-01-10 16:19:21,855 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:19:21,883 - 

2022-01-10 16:19:21,883 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:19:23,574 - Epoch: [51][  100/  500]    Overall Loss 0.757297    Objective Loss 0.757297                                        LR 0.001000    Time 0.016899    
2022-01-10 16:19:25,030 - Epoch: [51][  200/  500]    Overall Loss 0.756075    Objective Loss 0.756075                                        LR 0.001000    Time 0.015726    
2022-01-10 16:19:26,487 - Epoch: [51][  300/  500]    Overall Loss 0.757156    Objective Loss 0.757156                                        LR 0.001000    Time 0.015338    
2022-01-10 16:19:27,922 - Epoch: [51][  400/  500]    Overall Loss 0.766160    Objective Loss 0.766160                                        LR 0.001000    Time 0.015087    
2022-01-10 16:19:29,361 - Epoch: [51][  500/  500]    Overall Loss 0.769938    Objective Loss 0.769938    Top1 78.000000    Top5 94.000000    LR 0.001000    Time 0.014947    
2022-01-10 16:19:29,413 - --- validate (epoch=51)-----------
2022-01-10 16:19:29,413 - 10000 samples (100 per mini-batch)
2022-01-10 16:19:30,159 - Epoch: [51][  100/  100]    Loss 1.577465    Top1 59.340000    Top5 86.100000    
2022-01-10 16:19:30,203 - ==> Top1: 59.340    Top5: 86.100    Loss: 1.577

2022-01-10 16:19:30,205 - ==> Best [Top1: 60.120   Top5: 86.530   Sparsity:0.00   Params: 725128 on epoch: 49]
2022-01-10 16:19:30,205 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:19:30,232 - 

2022-01-10 16:19:30,233 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:19:31,921 - Epoch: [52][  100/  500]    Overall Loss 0.735810    Objective Loss 0.735810                                        LR 0.001000    Time 0.016867    
2022-01-10 16:19:33,356 - Epoch: [52][  200/  500]    Overall Loss 0.743453    Objective Loss 0.743453                                        LR 0.001000    Time 0.015604    
2022-01-10 16:19:34,791 - Epoch: [52][  300/  500]    Overall Loss 0.743411    Objective Loss 0.743411                                        LR 0.001000    Time 0.015186    
2022-01-10 16:19:36,230 - Epoch: [52][  400/  500]    Overall Loss 0.760343    Objective Loss 0.760343                                        LR 0.001000    Time 0.014973    
2022-01-10 16:19:37,672 - Epoch: [52][  500/  500]    Overall Loss 0.765986    Objective Loss 0.765986    Top1 74.500000    Top5 96.500000    LR 0.001000    Time 0.014861    
2022-01-10 16:19:37,710 - --- validate (epoch=52)-----------
2022-01-10 16:19:37,711 - 10000 samples (100 per mini-batch)
2022-01-10 16:19:38,465 - Epoch: [52][  100/  100]    Loss 1.625887    Top1 58.430000    Top5 85.290000    
2022-01-10 16:19:38,513 - ==> Top1: 58.430    Top5: 85.290    Loss: 1.626

2022-01-10 16:19:38,515 - ==> Best [Top1: 60.120   Top5: 86.530   Sparsity:0.00   Params: 725128 on epoch: 49]
2022-01-10 16:19:38,515 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:19:38,542 - 

2022-01-10 16:19:38,543 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:19:40,152 - Epoch: [53][  100/  500]    Overall Loss 0.738579    Objective Loss 0.738579                                        LR 0.001000    Time 0.016077    
2022-01-10 16:19:41,591 - Epoch: [53][  200/  500]    Overall Loss 0.738052    Objective Loss 0.738052                                        LR 0.001000    Time 0.015231    
2022-01-10 16:19:43,032 - Epoch: [53][  300/  500]    Overall Loss 0.743284    Objective Loss 0.743284                                        LR 0.001000    Time 0.014954    
2022-01-10 16:19:44,471 - Epoch: [53][  400/  500]    Overall Loss 0.746677    Objective Loss 0.746677                                        LR 0.001000    Time 0.014812    
2022-01-10 16:19:45,922 - Epoch: [53][  500/  500]    Overall Loss 0.753856    Objective Loss 0.753856    Top1 77.500000    Top5 97.000000    LR 0.001000    Time 0.014749    
2022-01-10 16:19:45,960 - --- validate (epoch=53)-----------
2022-01-10 16:19:45,960 - 10000 samples (100 per mini-batch)
2022-01-10 16:19:46,705 - Epoch: [53][  100/  100]    Loss 1.540168    Top1 60.170000    Top5 86.690000    
2022-01-10 16:19:46,754 - ==> Top1: 60.170    Top5: 86.690    Loss: 1.540

2022-01-10 16:19:46,756 - ==> Best [Top1: 60.170   Top5: 86.690   Sparsity:0.00   Params: 725128 on epoch: 53]
2022-01-10 16:19:46,756 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:19:46,786 - 

2022-01-10 16:19:46,786 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:19:48,466 - Epoch: [54][  100/  500]    Overall Loss 0.700415    Objective Loss 0.700415                                        LR 0.001000    Time 0.016781    
2022-01-10 16:19:49,906 - Epoch: [54][  200/  500]    Overall Loss 0.718266    Objective Loss 0.718266                                        LR 0.001000    Time 0.015589    
2022-01-10 16:19:51,349 - Epoch: [54][  300/  500]    Overall Loss 0.726785    Objective Loss 0.726785                                        LR 0.001000    Time 0.015200    
2022-01-10 16:19:52,794 - Epoch: [54][  400/  500]    Overall Loss 0.737008    Objective Loss 0.737008                                        LR 0.001000    Time 0.015009    
2022-01-10 16:19:54,244 - Epoch: [54][  500/  500]    Overall Loss 0.745281    Objective Loss 0.745281    Top1 71.000000    Top5 94.500000    LR 0.001000    Time 0.014906    
2022-01-10 16:19:54,286 - --- validate (epoch=54)-----------
2022-01-10 16:19:54,286 - 10000 samples (100 per mini-batch)
2022-01-10 16:19:55,033 - Epoch: [54][  100/  100]    Loss 1.537404    Top1 60.450000    Top5 86.790000    
2022-01-10 16:19:55,073 - ==> Top1: 60.450    Top5: 86.790    Loss: 1.537

2022-01-10 16:19:55,075 - ==> Best [Top1: 60.450   Top5: 86.790   Sparsity:0.00   Params: 725128 on epoch: 54]
2022-01-10 16:19:55,075 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:19:55,109 - 

2022-01-10 16:19:55,110 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:19:56,861 - Epoch: [55][  100/  500]    Overall Loss 0.705716    Objective Loss 0.705716                                        LR 0.001000    Time 0.017495    
2022-01-10 16:19:58,424 - Epoch: [55][  200/  500]    Overall Loss 0.706847    Objective Loss 0.706847                                        LR 0.001000    Time 0.016562    
2022-01-10 16:19:59,997 - Epoch: [55][  300/  500]    Overall Loss 0.711706    Objective Loss 0.711706                                        LR 0.001000    Time 0.016281    
2022-01-10 16:20:01,565 - Epoch: [55][  400/  500]    Overall Loss 0.727676    Objective Loss 0.727676                                        LR 0.001000    Time 0.016129    
2022-01-10 16:20:03,134 - Epoch: [55][  500/  500]    Overall Loss 0.731920    Objective Loss 0.731920    Top1 74.500000    Top5 93.500000    LR 0.001000    Time 0.016039    
2022-01-10 16:20:03,190 - --- validate (epoch=55)-----------
2022-01-10 16:20:03,190 - 10000 samples (100 per mini-batch)
2022-01-10 16:20:04,019 - Epoch: [55][  100/  100]    Loss 1.534849    Top1 60.200000    Top5 86.950000    
2022-01-10 16:20:04,063 - ==> Top1: 60.200    Top5: 86.950    Loss: 1.535

2022-01-10 16:20:04,064 - ==> Best [Top1: 60.450   Top5: 86.790   Sparsity:0.00   Params: 725128 on epoch: 54]
2022-01-10 16:20:04,065 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:20:04,092 - 

2022-01-10 16:20:04,092 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:20:05,702 - Epoch: [56][  100/  500]    Overall Loss 0.664067    Objective Loss 0.664067                                        LR 0.001000    Time 0.016082    
2022-01-10 16:20:07,157 - Epoch: [56][  200/  500]    Overall Loss 0.693885    Objective Loss 0.693885                                        LR 0.001000    Time 0.015315    
2022-01-10 16:20:08,583 - Epoch: [56][  300/  500]    Overall Loss 0.707853    Objective Loss 0.707853                                        LR 0.001000    Time 0.014958    
2022-01-10 16:20:10,009 - Epoch: [56][  400/  500]    Overall Loss 0.717354    Objective Loss 0.717354                                        LR 0.001000    Time 0.014783    
2022-01-10 16:20:11,442 - Epoch: [56][  500/  500]    Overall Loss 0.727041    Objective Loss 0.727041    Top1 75.000000    Top5 96.000000    LR 0.001000    Time 0.014690    
2022-01-10 16:20:11,482 - --- validate (epoch=56)-----------
2022-01-10 16:20:11,483 - 10000 samples (100 per mini-batch)
2022-01-10 16:20:12,225 - Epoch: [56][  100/  100]    Loss 1.556653    Top1 60.190000    Top5 86.580000    
2022-01-10 16:20:12,269 - ==> Top1: 60.190    Top5: 86.580    Loss: 1.557

2022-01-10 16:20:12,271 - ==> Best [Top1: 60.450   Top5: 86.790   Sparsity:0.00   Params: 725128 on epoch: 54]
2022-01-10 16:20:12,271 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:20:12,297 - 

2022-01-10 16:20:12,298 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:20:13,952 - Epoch: [57][  100/  500]    Overall Loss 0.667161    Objective Loss 0.667161                                        LR 0.001000    Time 0.016534    
2022-01-10 16:20:15,384 - Epoch: [57][  200/  500]    Overall Loss 0.687671    Objective Loss 0.687671                                        LR 0.001000    Time 0.015420    
2022-01-10 16:20:16,810 - Epoch: [57][  300/  500]    Overall Loss 0.702926    Objective Loss 0.702926                                        LR 0.001000    Time 0.015032    
2022-01-10 16:20:18,236 - Epoch: [57][  400/  500]    Overall Loss 0.711241    Objective Loss 0.711241                                        LR 0.001000    Time 0.014836    
2022-01-10 16:20:19,671 - Epoch: [57][  500/  500]    Overall Loss 0.718689    Objective Loss 0.718689    Top1 77.500000    Top5 98.000000    LR 0.001000    Time 0.014737    
2022-01-10 16:20:19,724 - --- validate (epoch=57)-----------
2022-01-10 16:20:19,725 - 10000 samples (100 per mini-batch)
2022-01-10 16:20:20,472 - Epoch: [57][  100/  100]    Loss 1.553608    Top1 60.150000    Top5 86.600000    
2022-01-10 16:20:20,518 - ==> Top1: 60.150    Top5: 86.600    Loss: 1.554

2022-01-10 16:20:20,520 - ==> Best [Top1: 60.450   Top5: 86.790   Sparsity:0.00   Params: 725128 on epoch: 54]
2022-01-10 16:20:20,520 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:20:20,547 - 

2022-01-10 16:20:20,547 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:20:22,219 - Epoch: [58][  100/  500]    Overall Loss 0.680386    Objective Loss 0.680386                                        LR 0.001000    Time 0.016705    
2022-01-10 16:20:23,655 - Epoch: [58][  200/  500]    Overall Loss 0.687862    Objective Loss 0.687862                                        LR 0.001000    Time 0.015530    
2022-01-10 16:20:25,082 - Epoch: [58][  300/  500]    Overall Loss 0.694009    Objective Loss 0.694009                                        LR 0.001000    Time 0.015107    
2022-01-10 16:20:26,510 - Epoch: [58][  400/  500]    Overall Loss 0.695820    Objective Loss 0.695820                                        LR 0.001000    Time 0.014898    
2022-01-10 16:20:27,945 - Epoch: [58][  500/  500]    Overall Loss 0.701908    Objective Loss 0.701908    Top1 74.500000    Top5 95.500000    LR 0.001000    Time 0.014788    
2022-01-10 16:20:27,988 - --- validate (epoch=58)-----------
2022-01-10 16:20:27,989 - 10000 samples (100 per mini-batch)
2022-01-10 16:20:28,719 - Epoch: [58][  100/  100]    Loss 1.606411    Top1 59.210000    Top5 86.460000    
2022-01-10 16:20:28,769 - ==> Top1: 59.210    Top5: 86.460    Loss: 1.606

2022-01-10 16:20:28,770 - ==> Best [Top1: 60.450   Top5: 86.790   Sparsity:0.00   Params: 725128 on epoch: 54]
2022-01-10 16:20:28,770 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:20:28,797 - 

2022-01-10 16:20:28,798 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:20:30,402 - Epoch: [59][  100/  500]    Overall Loss 0.669016    Objective Loss 0.669016                                        LR 0.001000    Time 0.016030    
2022-01-10 16:20:31,859 - Epoch: [59][  200/  500]    Overall Loss 0.675547    Objective Loss 0.675547                                        LR 0.001000    Time 0.015298    
2022-01-10 16:20:33,301 - Epoch: [59][  300/  500]    Overall Loss 0.685607    Objective Loss 0.685607                                        LR 0.001000    Time 0.015003    
2022-01-10 16:20:34,744 - Epoch: [59][  400/  500]    Overall Loss 0.691881    Objective Loss 0.691881                                        LR 0.001000    Time 0.014857    
2022-01-10 16:20:36,194 - Epoch: [59][  500/  500]    Overall Loss 0.698009    Objective Loss 0.698009    Top1 78.500000    Top5 96.500000    LR 0.001000    Time 0.014784    
2022-01-10 16:20:36,241 - --- validate (epoch=59)-----------
2022-01-10 16:20:36,241 - 10000 samples (100 per mini-batch)
2022-01-10 16:20:37,006 - Epoch: [59][  100/  100]    Loss 1.536327    Top1 61.020000    Top5 86.900000    
2022-01-10 16:20:37,051 - ==> Top1: 61.020    Top5: 86.900    Loss: 1.536

2022-01-10 16:20:37,053 - ==> Best [Top1: 61.020   Top5: 86.900   Sparsity:0.00   Params: 725128 on epoch: 59]
2022-01-10 16:20:37,053 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:20:37,083 - 

2022-01-10 16:20:37,083 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:20:38,753 - Epoch: [60][  100/  500]    Overall Loss 0.655269    Objective Loss 0.655269                                        LR 0.001000    Time 0.016684    
2022-01-10 16:20:40,181 - Epoch: [60][  200/  500]    Overall Loss 0.663623    Objective Loss 0.663623                                        LR 0.001000    Time 0.015479    
2022-01-10 16:20:41,620 - Epoch: [60][  300/  500]    Overall Loss 0.665462    Objective Loss 0.665462                                        LR 0.001000    Time 0.015114    
2022-01-10 16:20:43,060 - Epoch: [60][  400/  500]    Overall Loss 0.677674    Objective Loss 0.677674                                        LR 0.001000    Time 0.014934    
2022-01-10 16:20:44,508 - Epoch: [60][  500/  500]    Overall Loss 0.687788    Objective Loss 0.687788    Top1 75.000000    Top5 96.500000    LR 0.001000    Time 0.014840    
2022-01-10 16:20:44,547 - --- validate (epoch=60)-----------
2022-01-10 16:20:44,548 - 10000 samples (100 per mini-batch)
2022-01-10 16:20:45,292 - Epoch: [60][  100/  100]    Loss 1.534239    Top1 60.460000    Top5 87.310000    
2022-01-10 16:20:45,341 - ==> Top1: 60.460    Top5: 87.310    Loss: 1.534

2022-01-10 16:20:45,342 - ==> Best [Top1: 61.020   Top5: 86.900   Sparsity:0.00   Params: 725128 on epoch: 59]
2022-01-10 16:20:45,342 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:20:45,370 - 

2022-01-10 16:20:45,370 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:20:47,055 - Epoch: [61][  100/  500]    Overall Loss 0.669228    Objective Loss 0.669228                                        LR 0.001000    Time 0.016835    
2022-01-10 16:20:48,493 - Epoch: [61][  200/  500]    Overall Loss 0.660605    Objective Loss 0.660605                                        LR 0.001000    Time 0.015600    
2022-01-10 16:20:49,935 - Epoch: [61][  300/  500]    Overall Loss 0.669021    Objective Loss 0.669021                                        LR 0.001000    Time 0.015206    
2022-01-10 16:20:51,380 - Epoch: [61][  400/  500]    Overall Loss 0.674690    Objective Loss 0.674690                                        LR 0.001000    Time 0.015014    
2022-01-10 16:20:52,831 - Epoch: [61][  500/  500]    Overall Loss 0.682442    Objective Loss 0.682442    Top1 77.000000    Top5 97.500000    LR 0.001000    Time 0.014913    
2022-01-10 16:20:52,875 - --- validate (epoch=61)-----------
2022-01-10 16:20:52,875 - 10000 samples (100 per mini-batch)
2022-01-10 16:20:53,597 - Epoch: [61][  100/  100]    Loss 1.582074    Top1 60.240000    Top5 86.630000    
2022-01-10 16:20:53,651 - ==> Top1: 60.240    Top5: 86.630    Loss: 1.582

2022-01-10 16:20:53,653 - ==> Best [Top1: 61.020   Top5: 86.900   Sparsity:0.00   Params: 725128 on epoch: 59]
2022-01-10 16:20:53,653 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:20:53,674 - 

2022-01-10 16:20:53,674 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:20:55,289 - Epoch: [62][  100/  500]    Overall Loss 0.635564    Objective Loss 0.635564                                        LR 0.001000    Time 0.016136    
2022-01-10 16:20:56,800 - Epoch: [62][  200/  500]    Overall Loss 0.662933    Objective Loss 0.662933                                        LR 0.001000    Time 0.015618    
2022-01-10 16:20:58,309 - Epoch: [62][  300/  500]    Overall Loss 0.669406    Objective Loss 0.669406                                        LR 0.001000    Time 0.015439    
2022-01-10 16:20:59,746 - Epoch: [62][  400/  500]    Overall Loss 0.675160    Objective Loss 0.675160                                        LR 0.001000    Time 0.015170    
2022-01-10 16:21:01,191 - Epoch: [62][  500/  500]    Overall Loss 0.678944    Objective Loss 0.678944    Top1 70.500000    Top5 95.500000    LR 0.001000    Time 0.015025    
2022-01-10 16:21:01,233 - --- validate (epoch=62)-----------
2022-01-10 16:21:01,233 - 10000 samples (100 per mini-batch)
2022-01-10 16:21:01,970 - Epoch: [62][  100/  100]    Loss 1.559173    Top1 60.700000    Top5 87.320000    
2022-01-10 16:21:02,020 - ==> Top1: 60.700    Top5: 87.320    Loss: 1.559

2022-01-10 16:21:02,022 - ==> Best [Top1: 61.020   Top5: 86.900   Sparsity:0.00   Params: 725128 on epoch: 59]
2022-01-10 16:21:02,022 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:21:02,049 - 

2022-01-10 16:21:02,050 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:21:03,725 - Epoch: [63][  100/  500]    Overall Loss 0.626967    Objective Loss 0.626967                                        LR 0.001000    Time 0.016739    
2022-01-10 16:21:05,162 - Epoch: [63][  200/  500]    Overall Loss 0.631271    Objective Loss 0.631271                                        LR 0.001000    Time 0.015552    
2022-01-10 16:21:06,606 - Epoch: [63][  300/  500]    Overall Loss 0.651193    Objective Loss 0.651193                                        LR 0.001000    Time 0.015179    
2022-01-10 16:21:08,072 - Epoch: [63][  400/  500]    Overall Loss 0.657095    Objective Loss 0.657095                                        LR 0.001000    Time 0.015048    
2022-01-10 16:21:09,533 - Epoch: [63][  500/  500]    Overall Loss 0.664746    Objective Loss 0.664746    Top1 76.000000    Top5 94.000000    LR 0.001000    Time 0.014957    
2022-01-10 16:21:09,587 - --- validate (epoch=63)-----------
2022-01-10 16:21:09,587 - 10000 samples (100 per mini-batch)
2022-01-10 16:21:10,313 - Epoch: [63][  100/  100]    Loss 1.564904    Top1 60.370000    Top5 86.870000    
2022-01-10 16:21:10,360 - ==> Top1: 60.370    Top5: 86.870    Loss: 1.565

2022-01-10 16:21:10,362 - ==> Best [Top1: 61.020   Top5: 86.900   Sparsity:0.00   Params: 725128 on epoch: 59]
2022-01-10 16:21:10,362 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:21:10,384 - 

2022-01-10 16:21:10,384 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:21:12,079 - Epoch: [64][  100/  500]    Overall Loss 0.622265    Objective Loss 0.622265                                        LR 0.001000    Time 0.016934    
2022-01-10 16:21:13,510 - Epoch: [64][  200/  500]    Overall Loss 0.636468    Objective Loss 0.636468                                        LR 0.001000    Time 0.015619    
2022-01-10 16:21:14,947 - Epoch: [64][  300/  500]    Overall Loss 0.651556    Objective Loss 0.651556                                        LR 0.001000    Time 0.015202    
2022-01-10 16:21:16,379 - Epoch: [64][  400/  500]    Overall Loss 0.658309    Objective Loss 0.658309                                        LR 0.001000    Time 0.014978    
2022-01-10 16:21:17,815 - Epoch: [64][  500/  500]    Overall Loss 0.659694    Objective Loss 0.659694    Top1 80.000000    Top5 96.500000    LR 0.001000    Time 0.014853    
2022-01-10 16:21:17,863 - --- validate (epoch=64)-----------
2022-01-10 16:21:17,863 - 10000 samples (100 per mini-batch)
2022-01-10 16:21:18,594 - Epoch: [64][  100/  100]    Loss 1.599929    Top1 59.690000    Top5 86.250000    
2022-01-10 16:21:18,638 - ==> Top1: 59.690    Top5: 86.250    Loss: 1.600

2022-01-10 16:21:18,639 - ==> Best [Top1: 61.020   Top5: 86.900   Sparsity:0.00   Params: 725128 on epoch: 59]
2022-01-10 16:21:18,639 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:21:18,667 - 

2022-01-10 16:21:18,667 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:21:20,316 - Epoch: [65][  100/  500]    Overall Loss 0.602653    Objective Loss 0.602653                                        LR 0.001000    Time 0.016470    
2022-01-10 16:21:21,895 - Epoch: [65][  200/  500]    Overall Loss 0.618573    Objective Loss 0.618573                                        LR 0.001000    Time 0.016125    
2022-01-10 16:21:23,432 - Epoch: [65][  300/  500]    Overall Loss 0.630924    Objective Loss 0.630924                                        LR 0.001000    Time 0.015872    
2022-01-10 16:21:24,888 - Epoch: [65][  400/  500]    Overall Loss 0.643226    Objective Loss 0.643226                                        LR 0.001000    Time 0.015541    
2022-01-10 16:21:26,351 - Epoch: [65][  500/  500]    Overall Loss 0.648325    Objective Loss 0.648325    Top1 79.500000    Top5 98.500000    LR 0.001000    Time 0.015358    
2022-01-10 16:21:26,393 - --- validate (epoch=65)-----------
2022-01-10 16:21:26,394 - 10000 samples (100 per mini-batch)
2022-01-10 16:21:27,141 - Epoch: [65][  100/  100]    Loss 1.580964    Top1 60.540000    Top5 86.520000    
2022-01-10 16:21:27,186 - ==> Top1: 60.540    Top5: 86.520    Loss: 1.581

2022-01-10 16:21:27,188 - ==> Best [Top1: 61.020   Top5: 86.900   Sparsity:0.00   Params: 725128 on epoch: 59]
2022-01-10 16:21:27,188 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:21:27,212 - 

2022-01-10 16:21:27,212 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:21:28,897 - Epoch: [66][  100/  500]    Overall Loss 0.623470    Objective Loss 0.623470                                        LR 0.001000    Time 0.016837    
2022-01-10 16:21:30,331 - Epoch: [66][  200/  500]    Overall Loss 0.626811    Objective Loss 0.626811                                        LR 0.001000    Time 0.015582    
2022-01-10 16:21:31,758 - Epoch: [66][  300/  500]    Overall Loss 0.629697    Objective Loss 0.629697                                        LR 0.001000    Time 0.015143    
2022-01-10 16:21:33,186 - Epoch: [66][  400/  500]    Overall Loss 0.640185    Objective Loss 0.640185                                        LR 0.001000    Time 0.014927    
2022-01-10 16:21:34,616 - Epoch: [66][  500/  500]    Overall Loss 0.643451    Objective Loss 0.643451    Top1 78.000000    Top5 98.500000    LR 0.001000    Time 0.014799    
2022-01-10 16:21:34,661 - --- validate (epoch=66)-----------
2022-01-10 16:21:34,661 - 10000 samples (100 per mini-batch)
2022-01-10 16:21:35,396 - Epoch: [66][  100/  100]    Loss 1.598898    Top1 60.850000    Top5 86.940000    
2022-01-10 16:21:35,440 - ==> Top1: 60.850    Top5: 86.940    Loss: 1.599

2022-01-10 16:21:35,442 - ==> Best [Top1: 61.020   Top5: 86.900   Sparsity:0.00   Params: 725128 on epoch: 59]
2022-01-10 16:21:35,442 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:21:35,470 - 

2022-01-10 16:21:35,470 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:21:37,183 - Epoch: [67][  100/  500]    Overall Loss 0.584897    Objective Loss 0.584897                                        LR 0.001000    Time 0.017121    
2022-01-10 16:21:38,641 - Epoch: [67][  200/  500]    Overall Loss 0.597413    Objective Loss 0.597413                                        LR 0.001000    Time 0.015843    
2022-01-10 16:21:40,087 - Epoch: [67][  300/  500]    Overall Loss 0.606592    Objective Loss 0.606592                                        LR 0.001000    Time 0.015383    
2022-01-10 16:21:41,540 - Epoch: [67][  400/  500]    Overall Loss 0.614570    Objective Loss 0.614570                                        LR 0.001000    Time 0.015166    
2022-01-10 16:21:43,000 - Epoch: [67][  500/  500]    Overall Loss 0.623852    Objective Loss 0.623852    Top1 85.500000    Top5 98.500000    LR 0.001000    Time 0.015051    
2022-01-10 16:21:43,041 - --- validate (epoch=67)-----------
2022-01-10 16:21:43,041 - 10000 samples (100 per mini-batch)
2022-01-10 16:21:43,798 - Epoch: [67][  100/  100]    Loss 1.612173    Top1 59.790000    Top5 86.640000    
2022-01-10 16:21:43,841 - ==> Top1: 59.790    Top5: 86.640    Loss: 1.612

2022-01-10 16:21:43,843 - ==> Best [Top1: 61.020   Top5: 86.900   Sparsity:0.00   Params: 725128 on epoch: 59]
2022-01-10 16:21:43,843 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:21:43,866 - 

2022-01-10 16:21:43,866 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:21:45,472 - Epoch: [68][  100/  500]    Overall Loss 0.616847    Objective Loss 0.616847                                        LR 0.001000    Time 0.016040    
2022-01-10 16:21:46,906 - Epoch: [68][  200/  500]    Overall Loss 0.605640    Objective Loss 0.605640                                        LR 0.001000    Time 0.015185    
2022-01-10 16:21:48,338 - Epoch: [68][  300/  500]    Overall Loss 0.615027    Objective Loss 0.615027                                        LR 0.001000    Time 0.014894    
2022-01-10 16:21:49,772 - Epoch: [68][  400/  500]    Overall Loss 0.619814    Objective Loss 0.619814                                        LR 0.001000    Time 0.014755    
2022-01-10 16:21:51,212 - Epoch: [68][  500/  500]    Overall Loss 0.622224    Objective Loss 0.622224    Top1 79.500000    Top5 97.500000    LR 0.001000    Time 0.014683    
2022-01-10 16:21:51,257 - --- validate (epoch=68)-----------
2022-01-10 16:21:51,257 - 10000 samples (100 per mini-batch)
2022-01-10 16:21:52,006 - Epoch: [68][  100/  100]    Loss 1.583201    Top1 60.520000    Top5 87.240000    
2022-01-10 16:21:52,060 - ==> Top1: 60.520    Top5: 87.240    Loss: 1.583

2022-01-10 16:21:52,061 - ==> Best [Top1: 61.020   Top5: 86.900   Sparsity:0.00   Params: 725128 on epoch: 59]
2022-01-10 16:21:52,062 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:21:52,089 - 

2022-01-10 16:21:52,089 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:21:53,854 - Epoch: [69][  100/  500]    Overall Loss 0.566303    Objective Loss 0.566303                                        LR 0.001000    Time 0.017635    
2022-01-10 16:21:55,451 - Epoch: [69][  200/  500]    Overall Loss 0.577137    Objective Loss 0.577137                                        LR 0.001000    Time 0.016796    
2022-01-10 16:21:56,903 - Epoch: [69][  300/  500]    Overall Loss 0.597510    Objective Loss 0.597510                                        LR 0.001000    Time 0.016036    
2022-01-10 16:21:58,325 - Epoch: [69][  400/  500]    Overall Loss 0.610746    Objective Loss 0.610746                                        LR 0.001000    Time 0.015579    
2022-01-10 16:21:59,755 - Epoch: [69][  500/  500]    Overall Loss 0.620817    Objective Loss 0.620817    Top1 77.500000    Top5 96.000000    LR 0.001000    Time 0.015323    
2022-01-10 16:21:59,798 - --- validate (epoch=69)-----------
2022-01-10 16:21:59,799 - 10000 samples (100 per mini-batch)
2022-01-10 16:22:00,581 - Epoch: [69][  100/  100]    Loss 1.584654    Top1 60.180000    Top5 87.220000    
2022-01-10 16:22:00,628 - ==> Top1: 60.180    Top5: 87.220    Loss: 1.585

2022-01-10 16:22:00,630 - ==> Best [Top1: 61.020   Top5: 86.900   Sparsity:0.00   Params: 725128 on epoch: 59]
2022-01-10 16:22:00,630 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:22:00,651 - 

2022-01-10 16:22:00,651 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:22:02,248 - Epoch: [70][  100/  500]    Overall Loss 0.589015    Objective Loss 0.589015                                        LR 0.001000    Time 0.015958    
2022-01-10 16:22:03,676 - Epoch: [70][  200/  500]    Overall Loss 0.589796    Objective Loss 0.589796                                        LR 0.001000    Time 0.015112    
2022-01-10 16:22:05,121 - Epoch: [70][  300/  500]    Overall Loss 0.603275    Objective Loss 0.603275                                        LR 0.001000    Time 0.014888    
2022-01-10 16:22:06,571 - Epoch: [70][  400/  500]    Overall Loss 0.604819    Objective Loss 0.604819                                        LR 0.001000    Time 0.014789    
2022-01-10 16:22:08,023 - Epoch: [70][  500/  500]    Overall Loss 0.609216    Objective Loss 0.609216    Top1 75.500000    Top5 95.000000    LR 0.001000    Time 0.014733    
2022-01-10 16:22:08,069 - --- validate (epoch=70)-----------
2022-01-10 16:22:08,069 - 10000 samples (100 per mini-batch)
2022-01-10 16:22:08,878 - Epoch: [70][  100/  100]    Loss 1.629907    Top1 59.350000    Top5 86.360000    
2022-01-10 16:22:08,923 - ==> Top1: 59.350    Top5: 86.360    Loss: 1.630

2022-01-10 16:22:08,924 - ==> Best [Top1: 61.020   Top5: 86.900   Sparsity:0.00   Params: 725128 on epoch: 59]
2022-01-10 16:22:08,925 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:22:08,951 - 

2022-01-10 16:22:08,952 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:22:10,615 - Epoch: [71][  100/  500]    Overall Loss 0.584612    Objective Loss 0.584612                                        LR 0.001000    Time 0.016618    
2022-01-10 16:22:12,155 - Epoch: [71][  200/  500]    Overall Loss 0.587657    Objective Loss 0.587657                                        LR 0.001000    Time 0.016005    
2022-01-10 16:22:13,593 - Epoch: [71][  300/  500]    Overall Loss 0.590049    Objective Loss 0.590049                                        LR 0.001000    Time 0.015460    
2022-01-10 16:22:15,013 - Epoch: [71][  400/  500]    Overall Loss 0.599006    Objective Loss 0.599006                                        LR 0.001000    Time 0.015143    
2022-01-10 16:22:16,442 - Epoch: [71][  500/  500]    Overall Loss 0.606528    Objective Loss 0.606528    Top1 77.000000    Top5 96.000000    LR 0.001000    Time 0.014972    
2022-01-10 16:22:16,483 - --- validate (epoch=71)-----------
2022-01-10 16:22:16,483 - 10000 samples (100 per mini-batch)
2022-01-10 16:22:17,250 - Epoch: [71][  100/  100]    Loss 1.592876    Top1 60.110000    Top5 86.770000    
2022-01-10 16:22:17,291 - ==> Top1: 60.110    Top5: 86.770    Loss: 1.593

2022-01-10 16:22:17,293 - ==> Best [Top1: 61.020   Top5: 86.900   Sparsity:0.00   Params: 725128 on epoch: 59]
2022-01-10 16:22:17,293 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:22:17,312 - 

2022-01-10 16:22:17,312 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:22:19,052 - Epoch: [72][  100/  500]    Overall Loss 0.573033    Objective Loss 0.573033                                        LR 0.001000    Time 0.017380    
2022-01-10 16:22:20,627 - Epoch: [72][  200/  500]    Overall Loss 0.582909    Objective Loss 0.582909                                        LR 0.001000    Time 0.016563    
2022-01-10 16:22:22,202 - Epoch: [72][  300/  500]    Overall Loss 0.583844    Objective Loss 0.583844                                        LR 0.001000    Time 0.016289    
2022-01-10 16:22:23,780 - Epoch: [72][  400/  500]    Overall Loss 0.593233    Objective Loss 0.593233                                        LR 0.001000    Time 0.016159    
2022-01-10 16:22:25,343 - Epoch: [72][  500/  500]    Overall Loss 0.594359    Objective Loss 0.594359    Top1 79.500000    Top5 99.000000    LR 0.001000    Time 0.016051    
2022-01-10 16:22:25,388 - --- validate (epoch=72)-----------
2022-01-10 16:22:25,388 - 10000 samples (100 per mini-batch)
2022-01-10 16:22:26,130 - Epoch: [72][  100/  100]    Loss 1.635247    Top1 60.230000    Top5 86.390000    
2022-01-10 16:22:26,183 - ==> Top1: 60.230    Top5: 86.390    Loss: 1.635

2022-01-10 16:22:26,184 - ==> Best [Top1: 61.020   Top5: 86.900   Sparsity:0.00   Params: 725128 on epoch: 59]
2022-01-10 16:22:26,184 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:22:26,212 - 

2022-01-10 16:22:26,212 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:22:27,848 - Epoch: [73][  100/  500]    Overall Loss 0.546962    Objective Loss 0.546962                                        LR 0.001000    Time 0.016342    
2022-01-10 16:22:29,295 - Epoch: [73][  200/  500]    Overall Loss 0.571912    Objective Loss 0.571912                                        LR 0.001000    Time 0.015402    
2022-01-10 16:22:30,742 - Epoch: [73][  300/  500]    Overall Loss 0.573694    Objective Loss 0.573694                                        LR 0.001000    Time 0.015087    
2022-01-10 16:22:32,189 - Epoch: [73][  400/  500]    Overall Loss 0.580006    Objective Loss 0.580006                                        LR 0.001000    Time 0.014932    
2022-01-10 16:22:33,645 - Epoch: [73][  500/  500]    Overall Loss 0.588042    Objective Loss 0.588042    Top1 82.000000    Top5 98.500000    LR 0.001000    Time 0.014855    
2022-01-10 16:22:33,682 - --- validate (epoch=73)-----------
2022-01-10 16:22:33,682 - 10000 samples (100 per mini-batch)
2022-01-10 16:22:34,486 - Epoch: [73][  100/  100]    Loss 1.595772    Top1 60.700000    Top5 87.020000    
2022-01-10 16:22:34,531 - ==> Top1: 60.700    Top5: 87.020    Loss: 1.596

2022-01-10 16:22:34,533 - ==> Best [Top1: 61.020   Top5: 86.900   Sparsity:0.00   Params: 725128 on epoch: 59]
2022-01-10 16:22:34,533 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:22:34,561 - 

2022-01-10 16:22:34,561 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:22:36,168 - Epoch: [74][  100/  500]    Overall Loss 0.547947    Objective Loss 0.547947                                        LR 0.001000    Time 0.016056    
2022-01-10 16:22:37,684 - Epoch: [74][  200/  500]    Overall Loss 0.561293    Objective Loss 0.561293                                        LR 0.001000    Time 0.015602    
2022-01-10 16:22:39,253 - Epoch: [74][  300/  500]    Overall Loss 0.571632    Objective Loss 0.571632                                        LR 0.001000    Time 0.015629    
2022-01-10 16:22:40,821 - Epoch: [74][  400/  500]    Overall Loss 0.576384    Objective Loss 0.576384                                        LR 0.001000    Time 0.015639    
2022-01-10 16:22:42,280 - Epoch: [74][  500/  500]    Overall Loss 0.581955    Objective Loss 0.581955    Top1 78.500000    Top5 97.500000    LR 0.001000    Time 0.015429    
2022-01-10 16:22:42,335 - --- validate (epoch=74)-----------
2022-01-10 16:22:42,335 - 10000 samples (100 per mini-batch)
2022-01-10 16:22:43,137 - Epoch: [74][  100/  100]    Loss 1.683983    Top1 59.370000    Top5 85.760000    
2022-01-10 16:22:43,190 - ==> Top1: 59.370    Top5: 85.760    Loss: 1.684

2022-01-10 16:22:43,192 - ==> Best [Top1: 61.020   Top5: 86.900   Sparsity:0.00   Params: 725128 on epoch: 59]
2022-01-10 16:22:43,192 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:22:43,219 - 

2022-01-10 16:22:43,220 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:22:44,878 - Epoch: [75][  100/  500]    Overall Loss 0.528342    Objective Loss 0.528342                                        LR 0.001000    Time 0.016566    
2022-01-10 16:22:46,304 - Epoch: [75][  200/  500]    Overall Loss 0.537161    Objective Loss 0.537161                                        LR 0.001000    Time 0.015411    
2022-01-10 16:22:47,735 - Epoch: [75][  300/  500]    Overall Loss 0.556822    Objective Loss 0.556822                                        LR 0.001000    Time 0.015042    
2022-01-10 16:22:49,185 - Epoch: [75][  400/  500]    Overall Loss 0.559872    Objective Loss 0.559872                                        LR 0.001000    Time 0.014903    
2022-01-10 16:22:50,657 - Epoch: [75][  500/  500]    Overall Loss 0.572197    Objective Loss 0.572197    Top1 79.000000    Top5 99.000000    LR 0.001000    Time 0.014865    
2022-01-10 16:22:50,711 - --- validate (epoch=75)-----------
2022-01-10 16:22:50,711 - 10000 samples (100 per mini-batch)
2022-01-10 16:22:51,484 - Epoch: [75][  100/  100]    Loss 1.632723    Top1 60.500000    Top5 86.770000    
2022-01-10 16:22:51,527 - ==> Top1: 60.500    Top5: 86.770    Loss: 1.633

2022-01-10 16:22:51,529 - ==> Best [Top1: 61.020   Top5: 86.900   Sparsity:0.00   Params: 725128 on epoch: 59]
2022-01-10 16:22:51,529 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:22:51,557 - 

2022-01-10 16:22:51,557 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:22:53,155 - Epoch: [76][  100/  500]    Overall Loss 0.548899    Objective Loss 0.548899                                        LR 0.001000    Time 0.015965    
2022-01-10 16:22:54,597 - Epoch: [76][  200/  500]    Overall Loss 0.549749    Objective Loss 0.549749                                        LR 0.001000    Time 0.015190    
2022-01-10 16:22:56,048 - Epoch: [76][  300/  500]    Overall Loss 0.555678    Objective Loss 0.555678                                        LR 0.001000    Time 0.014960    
2022-01-10 16:22:57,504 - Epoch: [76][  400/  500]    Overall Loss 0.571643    Objective Loss 0.571643                                        LR 0.001000    Time 0.014857    
2022-01-10 16:22:58,968 - Epoch: [76][  500/  500]    Overall Loss 0.575197    Objective Loss 0.575197    Top1 85.000000    Top5 99.500000    LR 0.001000    Time 0.014813    
2022-01-10 16:22:59,018 - --- validate (epoch=76)-----------
2022-01-10 16:22:59,018 - 10000 samples (100 per mini-batch)
2022-01-10 16:22:59,842 - Epoch: [76][  100/  100]    Loss 1.627726    Top1 60.170000    Top5 87.200000    
2022-01-10 16:22:59,892 - ==> Top1: 60.170    Top5: 87.200    Loss: 1.628

2022-01-10 16:22:59,894 - ==> Best [Top1: 61.020   Top5: 86.900   Sparsity:0.00   Params: 725128 on epoch: 59]
2022-01-10 16:22:59,894 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:22:59,922 - 

2022-01-10 16:22:59,922 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:23:01,542 - Epoch: [77][  100/  500]    Overall Loss 0.539671    Objective Loss 0.539671                                        LR 0.001000    Time 0.016184    
2022-01-10 16:23:02,973 - Epoch: [77][  200/  500]    Overall Loss 0.555005    Objective Loss 0.555005                                        LR 0.001000    Time 0.015243    
2022-01-10 16:23:04,398 - Epoch: [77][  300/  500]    Overall Loss 0.556037    Objective Loss 0.556037                                        LR 0.001000    Time 0.014911    
2022-01-10 16:23:05,823 - Epoch: [77][  400/  500]    Overall Loss 0.560544    Objective Loss 0.560544                                        LR 0.001000    Time 0.014743    
2022-01-10 16:23:07,254 - Epoch: [77][  500/  500]    Overall Loss 0.568255    Objective Loss 0.568255    Top1 80.500000    Top5 96.500000    LR 0.001000    Time 0.014656    
2022-01-10 16:23:07,305 - --- validate (epoch=77)-----------
2022-01-10 16:23:07,305 - 10000 samples (100 per mini-batch)
2022-01-10 16:23:08,042 - Epoch: [77][  100/  100]    Loss 1.609061    Top1 61.070000    Top5 87.050000    
2022-01-10 16:23:08,086 - ==> Top1: 61.070    Top5: 87.050    Loss: 1.609

2022-01-10 16:23:08,087 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:23:08,088 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:23:08,121 - 

2022-01-10 16:23:08,121 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:23:09,775 - Epoch: [78][  100/  500]    Overall Loss 0.532243    Objective Loss 0.532243                                        LR 0.001000    Time 0.016525    
2022-01-10 16:23:11,193 - Epoch: [78][  200/  500]    Overall Loss 0.548353    Objective Loss 0.548353                                        LR 0.001000    Time 0.015348    
2022-01-10 16:23:12,634 - Epoch: [78][  300/  500]    Overall Loss 0.550382    Objective Loss 0.550382                                        LR 0.001000    Time 0.015032    
2022-01-10 16:23:14,068 - Epoch: [78][  400/  500]    Overall Loss 0.553874    Objective Loss 0.553874                                        LR 0.001000    Time 0.014858    
2022-01-10 16:23:15,511 - Epoch: [78][  500/  500]    Overall Loss 0.564354    Objective Loss 0.564354    Top1 81.000000    Top5 97.500000    LR 0.001000    Time 0.014771    
2022-01-10 16:23:15,559 - --- validate (epoch=78)-----------
2022-01-10 16:23:15,559 - 10000 samples (100 per mini-batch)
2022-01-10 16:23:16,302 - Epoch: [78][  100/  100]    Loss 1.619496    Top1 60.360000    Top5 86.920000    
2022-01-10 16:23:16,347 - ==> Top1: 60.360    Top5: 86.920    Loss: 1.619

2022-01-10 16:23:16,348 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:23:16,349 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:23:16,376 - 

2022-01-10 16:23:16,376 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:23:17,994 - Epoch: [79][  100/  500]    Overall Loss 0.526416    Objective Loss 0.526416                                        LR 0.001000    Time 0.016164    
2022-01-10 16:23:19,433 - Epoch: [79][  200/  500]    Overall Loss 0.534034    Objective Loss 0.534034                                        LR 0.001000    Time 0.015274    
2022-01-10 16:23:21,007 - Epoch: [79][  300/  500]    Overall Loss 0.543979    Objective Loss 0.543979                                        LR 0.001000    Time 0.015426    
2022-01-10 16:23:22,573 - Epoch: [79][  400/  500]    Overall Loss 0.548685    Objective Loss 0.548685                                        LR 0.001000    Time 0.015481    
2022-01-10 16:23:24,133 - Epoch: [79][  500/  500]    Overall Loss 0.552413    Objective Loss 0.552413    Top1 78.500000    Top5 99.000000    LR 0.001000    Time 0.015503    
2022-01-10 16:23:24,180 - --- validate (epoch=79)-----------
2022-01-10 16:23:24,180 - 10000 samples (100 per mini-batch)
2022-01-10 16:23:24,976 - Epoch: [79][  100/  100]    Loss 1.596282    Top1 60.830000    Top5 87.380000    
2022-01-10 16:23:25,021 - ==> Top1: 60.830    Top5: 87.380    Loss: 1.596

2022-01-10 16:23:25,023 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:23:25,023 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:23:25,051 - 

2022-01-10 16:23:25,051 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:23:26,657 - Epoch: [80][  100/  500]    Overall Loss 0.504726    Objective Loss 0.504726                                        LR 0.001000    Time 0.016044    
2022-01-10 16:23:28,113 - Epoch: [80][  200/  500]    Overall Loss 0.515530    Objective Loss 0.515530                                        LR 0.001000    Time 0.015298    
2022-01-10 16:23:29,569 - Epoch: [80][  300/  500]    Overall Loss 0.529556    Objective Loss 0.529556                                        LR 0.001000    Time 0.015051    
2022-01-10 16:23:31,027 - Epoch: [80][  400/  500]    Overall Loss 0.535255    Objective Loss 0.535255                                        LR 0.001000    Time 0.014932    
2022-01-10 16:23:32,490 - Epoch: [80][  500/  500]    Overall Loss 0.546100    Objective Loss 0.546100    Top1 82.500000    Top5 97.500000    LR 0.001000    Time 0.014870    
2022-01-10 16:23:32,535 - --- validate (epoch=80)-----------
2022-01-10 16:23:32,535 - 10000 samples (100 per mini-batch)
2022-01-10 16:23:33,310 - Epoch: [80][  100/  100]    Loss 1.653988    Top1 60.540000    Top5 86.740000    
2022-01-10 16:23:33,352 - ==> Top1: 60.540    Top5: 86.740    Loss: 1.654

2022-01-10 16:23:33,354 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:23:33,354 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:23:33,382 - 

2022-01-10 16:23:33,382 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:23:35,032 - Epoch: [81][  100/  500]    Overall Loss 0.508079    Objective Loss 0.508079                                        LR 0.001000    Time 0.016489    
2022-01-10 16:23:36,448 - Epoch: [81][  200/  500]    Overall Loss 0.517161    Objective Loss 0.517161                                        LR 0.001000    Time 0.015319    
2022-01-10 16:23:37,904 - Epoch: [81][  300/  500]    Overall Loss 0.525557    Objective Loss 0.525557                                        LR 0.001000    Time 0.015063    
2022-01-10 16:23:39,453 - Epoch: [81][  400/  500]    Overall Loss 0.533003    Objective Loss 0.533003                                        LR 0.001000    Time 0.015167    
2022-01-10 16:23:41,007 - Epoch: [81][  500/  500]    Overall Loss 0.540734    Objective Loss 0.540734    Top1 79.000000    Top5 99.000000    LR 0.001000    Time 0.015241    
2022-01-10 16:23:41,046 - --- validate (epoch=81)-----------
2022-01-10 16:23:41,046 - 10000 samples (100 per mini-batch)
2022-01-10 16:23:41,815 - Epoch: [81][  100/  100]    Loss 1.658491    Top1 60.070000    Top5 86.570000    
2022-01-10 16:23:41,859 - ==> Top1: 60.070    Top5: 86.570    Loss: 1.658

2022-01-10 16:23:41,861 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:23:41,861 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:23:41,889 - 

2022-01-10 16:23:41,889 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:23:43,506 - Epoch: [82][  100/  500]    Overall Loss 0.486032    Objective Loss 0.486032                                        LR 0.001000    Time 0.016154    
2022-01-10 16:23:44,960 - Epoch: [82][  200/  500]    Overall Loss 0.502206    Objective Loss 0.502206                                        LR 0.001000    Time 0.015341    
2022-01-10 16:23:46,412 - Epoch: [82][  300/  500]    Overall Loss 0.513431    Objective Loss 0.513431                                        LR 0.001000    Time 0.015064    
2022-01-10 16:23:47,863 - Epoch: [82][  400/  500]    Overall Loss 0.523980    Objective Loss 0.523980                                        LR 0.001000    Time 0.014924    
2022-01-10 16:23:49,316 - Epoch: [82][  500/  500]    Overall Loss 0.530424    Objective Loss 0.530424    Top1 86.500000    Top5 98.000000    LR 0.001000    Time 0.014844    
2022-01-10 16:23:49,363 - --- validate (epoch=82)-----------
2022-01-10 16:23:49,363 - 10000 samples (100 per mini-batch)
2022-01-10 16:23:50,170 - Epoch: [82][  100/  100]    Loss 1.674794    Top1 60.150000    Top5 86.540000    
2022-01-10 16:23:50,223 - ==> Top1: 60.150    Top5: 86.540    Loss: 1.675

2022-01-10 16:23:50,225 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:23:50,225 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:23:50,253 - 

2022-01-10 16:23:50,253 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:23:51,868 - Epoch: [83][  100/  500]    Overall Loss 0.513893    Objective Loss 0.513893                                        LR 0.001000    Time 0.016135    
2022-01-10 16:23:53,390 - Epoch: [83][  200/  500]    Overall Loss 0.513388    Objective Loss 0.513388                                        LR 0.001000    Time 0.015673    
2022-01-10 16:23:54,851 - Epoch: [83][  300/  500]    Overall Loss 0.517021    Objective Loss 0.517021                                        LR 0.001000    Time 0.015318    
2022-01-10 16:23:56,267 - Epoch: [83][  400/  500]    Overall Loss 0.522823    Objective Loss 0.522823                                        LR 0.001000    Time 0.015026    
2022-01-10 16:23:57,699 - Epoch: [83][  500/  500]    Overall Loss 0.527814    Objective Loss 0.527814    Top1 80.000000    Top5 98.000000    LR 0.001000    Time 0.014882    
2022-01-10 16:23:57,743 - --- validate (epoch=83)-----------
2022-01-10 16:23:57,743 - 10000 samples (100 per mini-batch)
2022-01-10 16:23:58,530 - Epoch: [83][  100/  100]    Loss 1.678608    Top1 60.590000    Top5 86.660000    
2022-01-10 16:23:58,569 - ==> Top1: 60.590    Top5: 86.660    Loss: 1.679

2022-01-10 16:23:58,571 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:23:58,571 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:23:58,598 - 

2022-01-10 16:23:58,598 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:24:00,254 - Epoch: [84][  100/  500]    Overall Loss 0.484729    Objective Loss 0.484729                                        LR 0.001000    Time 0.016540    
2022-01-10 16:24:01,667 - Epoch: [84][  200/  500]    Overall Loss 0.489279    Objective Loss 0.489279                                        LR 0.001000    Time 0.015334    
2022-01-10 16:24:03,109 - Epoch: [84][  300/  500]    Overall Loss 0.500811    Objective Loss 0.500811                                        LR 0.001000    Time 0.015026    
2022-01-10 16:24:04,595 - Epoch: [84][  400/  500]    Overall Loss 0.510901    Objective Loss 0.510901                                        LR 0.001000    Time 0.014982    
2022-01-10 16:24:06,022 - Epoch: [84][  500/  500]    Overall Loss 0.516290    Objective Loss 0.516290    Top1 81.500000    Top5 97.000000    LR 0.001000    Time 0.014838    
2022-01-10 16:24:06,075 - --- validate (epoch=84)-----------
2022-01-10 16:24:06,075 - 10000 samples (100 per mini-batch)
2022-01-10 16:24:06,843 - Epoch: [84][  100/  100]    Loss 1.661596    Top1 60.520000    Top5 87.010000    
2022-01-10 16:24:06,878 - ==> Top1: 60.520    Top5: 87.010    Loss: 1.662

2022-01-10 16:24:06,880 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:24:06,880 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:24:06,907 - 

2022-01-10 16:24:06,907 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:24:08,525 - Epoch: [85][  100/  500]    Overall Loss 0.494981    Objective Loss 0.494981                                        LR 0.001000    Time 0.016163    
2022-01-10 16:24:09,981 - Epoch: [85][  200/  500]    Overall Loss 0.501632    Objective Loss 0.501632                                        LR 0.001000    Time 0.015355    
2022-01-10 16:24:11,507 - Epoch: [85][  300/  500]    Overall Loss 0.508028    Objective Loss 0.508028                                        LR 0.001000    Time 0.015323    
2022-01-10 16:24:13,078 - Epoch: [85][  400/  500]    Overall Loss 0.513823    Objective Loss 0.513823                                        LR 0.001000    Time 0.015417    
2022-01-10 16:24:14,650 - Epoch: [85][  500/  500]    Overall Loss 0.521456    Objective Loss 0.521456    Top1 84.500000    Top5 98.500000    LR 0.001000    Time 0.015475    
2022-01-10 16:24:14,700 - --- validate (epoch=85)-----------
2022-01-10 16:24:14,701 - 10000 samples (100 per mini-batch)
2022-01-10 16:24:15,498 - Epoch: [85][  100/  100]    Loss 1.679308    Top1 59.860000    Top5 86.780000    
2022-01-10 16:24:15,542 - ==> Top1: 59.860    Top5: 86.780    Loss: 1.679

2022-01-10 16:24:15,543 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:24:15,543 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:24:15,571 - 

2022-01-10 16:24:15,571 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:24:17,167 - Epoch: [86][  100/  500]    Overall Loss 0.481339    Objective Loss 0.481339                                        LR 0.001000    Time 0.015941    
2022-01-10 16:24:18,601 - Epoch: [86][  200/  500]    Overall Loss 0.497383    Objective Loss 0.497383                                        LR 0.001000    Time 0.015136    
2022-01-10 16:24:20,050 - Epoch: [86][  300/  500]    Overall Loss 0.501892    Objective Loss 0.501892                                        LR 0.001000    Time 0.014918    
2022-01-10 16:24:21,493 - Epoch: [86][  400/  500]    Overall Loss 0.509432    Objective Loss 0.509432                                        LR 0.001000    Time 0.014794    
2022-01-10 16:24:22,960 - Epoch: [86][  500/  500]    Overall Loss 0.518004    Objective Loss 0.518004    Top1 79.500000    Top5 99.500000    LR 0.001000    Time 0.014767    
2022-01-10 16:24:23,013 - --- validate (epoch=86)-----------
2022-01-10 16:24:23,013 - 10000 samples (100 per mini-batch)
2022-01-10 16:24:23,778 - Epoch: [86][  100/  100]    Loss 1.710044    Top1 60.460000    Top5 86.970000    
2022-01-10 16:24:23,820 - ==> Top1: 60.460    Top5: 86.970    Loss: 1.710

2022-01-10 16:24:23,822 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:24:23,822 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:24:23,850 - 

2022-01-10 16:24:23,850 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:24:25,514 - Epoch: [87][  100/  500]    Overall Loss 0.477085    Objective Loss 0.477085                                        LR 0.001000    Time 0.016632    
2022-01-10 16:24:27,056 - Epoch: [87][  200/  500]    Overall Loss 0.493127    Objective Loss 0.493127                                        LR 0.001000    Time 0.016020    
2022-01-10 16:24:28,597 - Epoch: [87][  300/  500]    Overall Loss 0.504237    Objective Loss 0.504237                                        LR 0.001000    Time 0.015812    
2022-01-10 16:24:30,171 - Epoch: [87][  400/  500]    Overall Loss 0.505913    Objective Loss 0.505913                                        LR 0.001000    Time 0.015793    
2022-01-10 16:24:31,747 - Epoch: [87][  500/  500]    Overall Loss 0.509258    Objective Loss 0.509258    Top1 84.000000    Top5 98.500000    LR 0.001000    Time 0.015784    
2022-01-10 16:24:31,787 - --- validate (epoch=87)-----------
2022-01-10 16:24:31,788 - 10000 samples (100 per mini-batch)
2022-01-10 16:24:32,523 - Epoch: [87][  100/  100]    Loss 1.677726    Top1 60.010000    Top5 86.770000    
2022-01-10 16:24:32,569 - ==> Top1: 60.010    Top5: 86.770    Loss: 1.678

2022-01-10 16:24:32,571 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:24:32,571 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:24:32,598 - 

2022-01-10 16:24:32,598 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:24:34,251 - Epoch: [88][  100/  500]    Overall Loss 0.472710    Objective Loss 0.472710                                        LR 0.001000    Time 0.016510    
2022-01-10 16:24:35,799 - Epoch: [88][  200/  500]    Overall Loss 0.477816    Objective Loss 0.477816                                        LR 0.001000    Time 0.015994    
2022-01-10 16:24:37,377 - Epoch: [88][  300/  500]    Overall Loss 0.489824    Objective Loss 0.489824                                        LR 0.001000    Time 0.015917    
2022-01-10 16:24:38,956 - Epoch: [88][  400/  500]    Overall Loss 0.493902    Objective Loss 0.493902                                        LR 0.001000    Time 0.015884    
2022-01-10 16:24:40,527 - Epoch: [88][  500/  500]    Overall Loss 0.497717    Objective Loss 0.497717    Top1 82.500000    Top5 98.000000    LR 0.001000    Time 0.015846    
2022-01-10 16:24:40,577 - --- validate (epoch=88)-----------
2022-01-10 16:24:40,577 - 10000 samples (100 per mini-batch)
2022-01-10 16:24:41,353 - Epoch: [88][  100/  100]    Loss 1.652660    Top1 60.630000    Top5 87.110000    
2022-01-10 16:24:41,402 - ==> Top1: 60.630    Top5: 87.110    Loss: 1.653

2022-01-10 16:24:41,404 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:24:41,404 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:24:41,432 - 

2022-01-10 16:24:41,432 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:24:43,121 - Epoch: [89][  100/  500]    Overall Loss 0.482119    Objective Loss 0.482119                                        LR 0.001000    Time 0.016878    
2022-01-10 16:24:44,548 - Epoch: [89][  200/  500]    Overall Loss 0.490567    Objective Loss 0.490567                                        LR 0.001000    Time 0.015570    
2022-01-10 16:24:45,973 - Epoch: [89][  300/  500]    Overall Loss 0.488693    Objective Loss 0.488693                                        LR 0.001000    Time 0.015128    
2022-01-10 16:24:47,399 - Epoch: [89][  400/  500]    Overall Loss 0.493874    Objective Loss 0.493874                                        LR 0.001000    Time 0.014908    
2022-01-10 16:24:48,831 - Epoch: [89][  500/  500]    Overall Loss 0.499069    Objective Loss 0.499069    Top1 82.000000    Top5 99.000000    LR 0.001000    Time 0.014790    
2022-01-10 16:24:48,871 - --- validate (epoch=89)-----------
2022-01-10 16:24:48,871 - 10000 samples (100 per mini-batch)
2022-01-10 16:24:49,608 - Epoch: [89][  100/  100]    Loss 1.709557    Top1 60.510000    Top5 86.290000    
2022-01-10 16:24:49,648 - ==> Top1: 60.510    Top5: 86.290    Loss: 1.710

2022-01-10 16:24:49,650 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:24:49,650 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:24:49,678 - 

2022-01-10 16:24:49,678 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:24:51,372 - Epoch: [90][  100/  500]    Overall Loss 0.454282    Objective Loss 0.454282                                        LR 0.001000    Time 0.016927    
2022-01-10 16:24:52,795 - Epoch: [90][  200/  500]    Overall Loss 0.459324    Objective Loss 0.459324                                        LR 0.001000    Time 0.015576    
2022-01-10 16:24:54,340 - Epoch: [90][  300/  500]    Overall Loss 0.467632    Objective Loss 0.467632                                        LR 0.001000    Time 0.015529    
2022-01-10 16:24:55,909 - Epoch: [90][  400/  500]    Overall Loss 0.478804    Objective Loss 0.478804                                        LR 0.001000    Time 0.015567    
2022-01-10 16:24:57,484 - Epoch: [90][  500/  500]    Overall Loss 0.486277    Objective Loss 0.486277    Top1 86.000000    Top5 97.000000    LR 0.001000    Time 0.015602    
2022-01-10 16:24:57,531 - --- validate (epoch=90)-----------
2022-01-10 16:24:57,531 - 10000 samples (100 per mini-batch)
2022-01-10 16:24:58,277 - Epoch: [90][  100/  100]    Loss 1.725490    Top1 59.950000    Top5 86.190000    
2022-01-10 16:24:58,326 - ==> Top1: 59.950    Top5: 86.190    Loss: 1.725

2022-01-10 16:24:58,328 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:24:58,328 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:24:58,350 - 

2022-01-10 16:24:58,350 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:25:00,046 - Epoch: [91][  100/  500]    Overall Loss 0.463251    Objective Loss 0.463251                                        LR 0.001000    Time 0.016945    
2022-01-10 16:25:01,621 - Epoch: [91][  200/  500]    Overall Loss 0.477699    Objective Loss 0.477699                                        LR 0.001000    Time 0.016341    
2022-01-10 16:25:03,192 - Epoch: [91][  300/  500]    Overall Loss 0.482721    Objective Loss 0.482721                                        LR 0.001000    Time 0.016128    
2022-01-10 16:25:04,766 - Epoch: [91][  400/  500]    Overall Loss 0.488847    Objective Loss 0.488847                                        LR 0.001000    Time 0.016030    
2022-01-10 16:25:06,343 - Epoch: [91][  500/  500]    Overall Loss 0.492599    Objective Loss 0.492599    Top1 85.000000    Top5 99.500000    LR 0.001000    Time 0.015976    
2022-01-10 16:25:06,397 - --- validate (epoch=91)-----------
2022-01-10 16:25:06,397 - 10000 samples (100 per mini-batch)
2022-01-10 16:25:07,153 - Epoch: [91][  100/  100]    Loss 1.680627    Top1 60.670000    Top5 86.690000    
2022-01-10 16:25:07,190 - ==> Top1: 60.670    Top5: 86.690    Loss: 1.681

2022-01-10 16:25:07,192 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:25:07,192 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:25:07,220 - 

2022-01-10 16:25:07,220 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:25:08,898 - Epoch: [92][  100/  500]    Overall Loss 0.445616    Objective Loss 0.445616                                        LR 0.001000    Time 0.016762    
2022-01-10 16:25:10,323 - Epoch: [92][  200/  500]    Overall Loss 0.451760    Objective Loss 0.451760                                        LR 0.001000    Time 0.015504    
2022-01-10 16:25:11,748 - Epoch: [92][  300/  500]    Overall Loss 0.459781    Objective Loss 0.459781                                        LR 0.001000    Time 0.015085    
2022-01-10 16:25:13,177 - Epoch: [92][  400/  500]    Overall Loss 0.467027    Objective Loss 0.467027                                        LR 0.001000    Time 0.014882    
2022-01-10 16:25:14,621 - Epoch: [92][  500/  500]    Overall Loss 0.477258    Objective Loss 0.477258    Top1 88.000000    Top5 99.000000    LR 0.001000    Time 0.014794    
2022-01-10 16:25:14,666 - --- validate (epoch=92)-----------
2022-01-10 16:25:14,667 - 10000 samples (100 per mini-batch)
2022-01-10 16:25:15,393 - Epoch: [92][  100/  100]    Loss 1.739469    Top1 59.620000    Top5 86.300000    
2022-01-10 16:25:15,433 - ==> Top1: 59.620    Top5: 86.300    Loss: 1.739

2022-01-10 16:25:15,435 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:25:15,435 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:25:15,462 - 

2022-01-10 16:25:15,462 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:25:17,153 - Epoch: [93][  100/  500]    Overall Loss 0.447622    Objective Loss 0.447622                                        LR 0.001000    Time 0.016890    
2022-01-10 16:25:18,596 - Epoch: [93][  200/  500]    Overall Loss 0.453211    Objective Loss 0.453211                                        LR 0.001000    Time 0.015658    
2022-01-10 16:25:20,034 - Epoch: [93][  300/  500]    Overall Loss 0.457024    Objective Loss 0.457024                                        LR 0.001000    Time 0.015228    
2022-01-10 16:25:21,472 - Epoch: [93][  400/  500]    Overall Loss 0.469078    Objective Loss 0.469078                                        LR 0.001000    Time 0.015014    
2022-01-10 16:25:22,917 - Epoch: [93][  500/  500]    Overall Loss 0.476723    Objective Loss 0.476723    Top1 83.500000    Top5 98.500000    LR 0.001000    Time 0.014900    
2022-01-10 16:25:22,959 - --- validate (epoch=93)-----------
2022-01-10 16:25:22,959 - 10000 samples (100 per mini-batch)
2022-01-10 16:25:23,728 - Epoch: [93][  100/  100]    Loss 1.719262    Top1 60.600000    Top5 86.720000    
2022-01-10 16:25:23,771 - ==> Top1: 60.600    Top5: 86.720    Loss: 1.719

2022-01-10 16:25:23,773 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:25:23,773 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:25:23,800 - 

2022-01-10 16:25:23,800 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:25:25,394 - Epoch: [94][  100/  500]    Overall Loss 0.435745    Objective Loss 0.435745                                        LR 0.001000    Time 0.015920    
2022-01-10 16:25:26,825 - Epoch: [94][  200/  500]    Overall Loss 0.444513    Objective Loss 0.444513                                        LR 0.001000    Time 0.015112    
2022-01-10 16:25:28,259 - Epoch: [94][  300/  500]    Overall Loss 0.453170    Objective Loss 0.453170                                        LR 0.001000    Time 0.014851    
2022-01-10 16:25:29,693 - Epoch: [94][  400/  500]    Overall Loss 0.463847    Objective Loss 0.463847                                        LR 0.001000    Time 0.014723    
2022-01-10 16:25:31,138 - Epoch: [94][  500/  500]    Overall Loss 0.473250    Objective Loss 0.473250    Top1 86.000000    Top5 98.500000    LR 0.001000    Time 0.014666    
2022-01-10 16:25:31,191 - --- validate (epoch=94)-----------
2022-01-10 16:25:31,191 - 10000 samples (100 per mini-batch)
2022-01-10 16:25:31,913 - Epoch: [94][  100/  100]    Loss 1.744908    Top1 59.720000    Top5 85.990000    
2022-01-10 16:25:31,953 - ==> Top1: 59.720    Top5: 85.990    Loss: 1.745

2022-01-10 16:25:31,954 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:25:31,954 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:25:31,976 - 

2022-01-10 16:25:31,976 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:25:33,671 - Epoch: [95][  100/  500]    Overall Loss 0.438702    Objective Loss 0.438702                                        LR 0.001000    Time 0.016938    
2022-01-10 16:25:35,152 - Epoch: [95][  200/  500]    Overall Loss 0.446278    Objective Loss 0.446278                                        LR 0.001000    Time 0.015871    
2022-01-10 16:25:36,694 - Epoch: [95][  300/  500]    Overall Loss 0.451523    Objective Loss 0.451523                                        LR 0.001000    Time 0.015715    
2022-01-10 16:25:38,190 - Epoch: [95][  400/  500]    Overall Loss 0.457814    Objective Loss 0.457814                                        LR 0.001000    Time 0.015525    
2022-01-10 16:25:39,630 - Epoch: [95][  500/  500]    Overall Loss 0.467219    Objective Loss 0.467219    Top1 85.000000    Top5 99.000000    LR 0.001000    Time 0.015298    
2022-01-10 16:25:39,674 - --- validate (epoch=95)-----------
2022-01-10 16:25:39,674 - 10000 samples (100 per mini-batch)
2022-01-10 16:25:40,422 - Epoch: [95][  100/  100]    Loss 1.768615    Top1 59.910000    Top5 86.100000    
2022-01-10 16:25:40,467 - ==> Top1: 59.910    Top5: 86.100    Loss: 1.769

2022-01-10 16:25:40,469 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:25:40,469 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:25:40,496 - 

2022-01-10 16:25:40,497 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:25:42,196 - Epoch: [96][  100/  500]    Overall Loss 0.444208    Objective Loss 0.444208                                        LR 0.001000    Time 0.016980    
2022-01-10 16:25:43,692 - Epoch: [96][  200/  500]    Overall Loss 0.443623    Objective Loss 0.443623                                        LR 0.001000    Time 0.015967    
2022-01-10 16:25:45,170 - Epoch: [96][  300/  500]    Overall Loss 0.444131    Objective Loss 0.444131                                        LR 0.001000    Time 0.015568    
2022-01-10 16:25:46,613 - Epoch: [96][  400/  500]    Overall Loss 0.453256    Objective Loss 0.453256                                        LR 0.001000    Time 0.015281    
2022-01-10 16:25:48,059 - Epoch: [96][  500/  500]    Overall Loss 0.461444    Objective Loss 0.461444    Top1 84.500000    Top5 98.000000    LR 0.001000    Time 0.015116    
2022-01-10 16:25:48,104 - --- validate (epoch=96)-----------
2022-01-10 16:25:48,104 - 10000 samples (100 per mini-batch)
2022-01-10 16:25:48,854 - Epoch: [96][  100/  100]    Loss 1.747733    Top1 60.030000    Top5 86.070000    
2022-01-10 16:25:48,908 - ==> Top1: 60.030    Top5: 86.070    Loss: 1.748

2022-01-10 16:25:48,910 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:25:48,910 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:25:48,937 - 

2022-01-10 16:25:48,937 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:25:50,521 - Epoch: [97][  100/  500]    Overall Loss 0.437905    Objective Loss 0.437905                                        LR 0.001000    Time 0.015829    
2022-01-10 16:25:51,983 - Epoch: [97][  200/  500]    Overall Loss 0.439990    Objective Loss 0.439990                                        LR 0.001000    Time 0.015221    
2022-01-10 16:25:53,426 - Epoch: [97][  300/  500]    Overall Loss 0.453358    Objective Loss 0.453358                                        LR 0.001000    Time 0.014954    
2022-01-10 16:25:54,871 - Epoch: [97][  400/  500]    Overall Loss 0.462333    Objective Loss 0.462333                                        LR 0.001000    Time 0.014826    
2022-01-10 16:25:56,320 - Epoch: [97][  500/  500]    Overall Loss 0.470274    Objective Loss 0.470274    Top1 80.000000    Top5 98.500000    LR 0.001000    Time 0.014758    
2022-01-10 16:25:56,366 - --- validate (epoch=97)-----------
2022-01-10 16:25:56,366 - 10000 samples (100 per mini-batch)
2022-01-10 16:25:57,116 - Epoch: [97][  100/  100]    Loss 1.732213    Top1 60.740000    Top5 86.190000    
2022-01-10 16:25:57,162 - ==> Top1: 60.740    Top5: 86.190    Loss: 1.732

2022-01-10 16:25:57,164 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:25:57,164 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:25:57,191 - 

2022-01-10 16:25:57,191 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:25:58,861 - Epoch: [98][  100/  500]    Overall Loss 0.432738    Objective Loss 0.432738                                        LR 0.001000    Time 0.016691    
2022-01-10 16:26:00,296 - Epoch: [98][  200/  500]    Overall Loss 0.448009    Objective Loss 0.448009                                        LR 0.001000    Time 0.015514    
2022-01-10 16:26:01,752 - Epoch: [98][  300/  500]    Overall Loss 0.454520    Objective Loss 0.454520                                        LR 0.001000    Time 0.015196    
2022-01-10 16:26:03,213 - Epoch: [98][  400/  500]    Overall Loss 0.460473    Objective Loss 0.460473                                        LR 0.001000    Time 0.015046    
2022-01-10 16:26:04,678 - Epoch: [98][  500/  500]    Overall Loss 0.463355    Objective Loss 0.463355    Top1 83.000000    Top5 98.500000    LR 0.001000    Time 0.014966    
2022-01-10 16:26:04,731 - --- validate (epoch=98)-----------
2022-01-10 16:26:04,731 - 10000 samples (100 per mini-batch)
2022-01-10 16:26:05,465 - Epoch: [98][  100/  100]    Loss 1.746216    Top1 60.350000    Top5 86.120000    
2022-01-10 16:26:05,511 - ==> Top1: 60.350    Top5: 86.120    Loss: 1.746

2022-01-10 16:26:05,513 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:26:05,513 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:26:05,540 - 

2022-01-10 16:26:05,540 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:26:07,229 - Epoch: [99][  100/  500]    Overall Loss 0.419789    Objective Loss 0.419789                                        LR 0.001000    Time 0.016871    
2022-01-10 16:26:08,674 - Epoch: [99][  200/  500]    Overall Loss 0.425815    Objective Loss 0.425815                                        LR 0.001000    Time 0.015656    
2022-01-10 16:26:10,122 - Epoch: [99][  300/  500]    Overall Loss 0.434850    Objective Loss 0.434850                                        LR 0.001000    Time 0.015261    
2022-01-10 16:26:11,571 - Epoch: [99][  400/  500]    Overall Loss 0.441706    Objective Loss 0.441706                                        LR 0.001000    Time 0.015067    
2022-01-10 16:26:13,029 - Epoch: [99][  500/  500]    Overall Loss 0.452241    Objective Loss 0.452241    Top1 86.000000    Top5 98.500000    LR 0.001000    Time 0.014968    
2022-01-10 16:26:13,078 - --- validate (epoch=99)-----------
2022-01-10 16:26:13,078 - 10000 samples (100 per mini-batch)
2022-01-10 16:26:13,836 - Epoch: [99][  100/  100]    Loss 1.727266    Top1 60.060000    Top5 86.960000    
2022-01-10 16:26:13,881 - ==> Top1: 60.060    Top5: 86.960    Loss: 1.727

2022-01-10 16:26:13,883 - ==> Best [Top1: 61.070   Top5: 87.050   Sparsity:0.00   Params: 725128 on epoch: 77]
2022-01-10 16:26:13,883 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:26:13,911 - 

2022-01-10 16:26:13,911 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:26:15,543 - Epoch: [100][  100/  500]    Overall Loss 0.394032    Objective Loss 0.394032                                        LR 0.000250    Time 0.016303    
2022-01-10 16:26:16,999 - Epoch: [100][  200/  500]    Overall Loss 0.373934    Objective Loss 0.373934                                        LR 0.000250    Time 0.015431    
2022-01-10 16:26:18,435 - Epoch: [100][  300/  500]    Overall Loss 0.366113    Objective Loss 0.366113                                        LR 0.000250    Time 0.015071    
2022-01-10 16:26:19,874 - Epoch: [100][  400/  500]    Overall Loss 0.359194    Objective Loss 0.359194                                        LR 0.000250    Time 0.014898    
2022-01-10 16:26:21,318 - Epoch: [100][  500/  500]    Overall Loss 0.354587    Objective Loss 0.354587    Top1 90.000000    Top5 99.500000    LR 0.000250    Time 0.014806    
2022-01-10 16:26:21,363 - --- validate (epoch=100)-----------
2022-01-10 16:26:21,364 - 10000 samples (100 per mini-batch)
2022-01-10 16:26:22,151 - Epoch: [100][  100/  100]    Loss 1.606942    Top1 62.840000    Top5 87.960000    
2022-01-10 16:26:22,196 - ==> Top1: 62.840    Top5: 87.960    Loss: 1.607

2022-01-10 16:26:22,198 - ==> Best [Top1: 62.840   Top5: 87.960   Sparsity:0.00   Params: 725128 on epoch: 100]
2022-01-10 16:26:22,198 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:26:22,232 - 

2022-01-10 16:26:22,232 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:26:23,902 - Epoch: [101][  100/  500]    Overall Loss 0.312046    Objective Loss 0.312046                                        LR 0.000250    Time 0.016681    
2022-01-10 16:26:25,338 - Epoch: [101][  200/  500]    Overall Loss 0.313572    Objective Loss 0.313572                                        LR 0.000250    Time 0.015515    
2022-01-10 16:26:26,770 - Epoch: [101][  300/  500]    Overall Loss 0.313691    Objective Loss 0.313691                                        LR 0.000250    Time 0.015115    
2022-01-10 16:26:28,203 - Epoch: [101][  400/  500]    Overall Loss 0.314489    Objective Loss 0.314489                                        LR 0.000250    Time 0.014917    
2022-01-10 16:26:29,645 - Epoch: [101][  500/  500]    Overall Loss 0.315250    Objective Loss 0.315250    Top1 93.500000    Top5 100.000000    LR 0.000250    Time 0.014816    
2022-01-10 16:26:29,687 - --- validate (epoch=101)-----------
2022-01-10 16:26:29,687 - 10000 samples (100 per mini-batch)
2022-01-10 16:26:30,446 - Epoch: [101][  100/  100]    Loss 1.624712    Top1 62.630000    Top5 87.910000    
2022-01-10 16:26:30,493 - ==> Top1: 62.630    Top5: 87.910    Loss: 1.625

2022-01-10 16:26:30,494 - ==> Best [Top1: 62.840   Top5: 87.960   Sparsity:0.00   Params: 725128 on epoch: 100]
2022-01-10 16:26:30,495 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:26:30,522 - 

2022-01-10 16:26:30,522 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:26:32,223 - Epoch: [102][  100/  500]    Overall Loss 0.298667    Objective Loss 0.298667                                        LR 0.000250    Time 0.016994    
2022-01-10 16:26:33,681 - Epoch: [102][  200/  500]    Overall Loss 0.305021    Objective Loss 0.305021                                        LR 0.000250    Time 0.015787    
2022-01-10 16:26:35,139 - Epoch: [102][  300/  500]    Overall Loss 0.308942    Objective Loss 0.308942                                        LR 0.000250    Time 0.015382    
2022-01-10 16:26:36,585 - Epoch: [102][  400/  500]    Overall Loss 0.312125    Objective Loss 0.312125                                        LR 0.000250    Time 0.015148    
2022-01-10 16:26:38,018 - Epoch: [102][  500/  500]    Overall Loss 0.312455    Objective Loss 0.312455    Top1 87.500000    Top5 99.500000    LR 0.000250    Time 0.014983    
2022-01-10 16:26:38,066 - --- validate (epoch=102)-----------
2022-01-10 16:26:38,066 - 10000 samples (100 per mini-batch)
2022-01-10 16:26:38,819 - Epoch: [102][  100/  100]    Loss 1.613492    Top1 63.160000    Top5 87.910000    
2022-01-10 16:26:38,867 - ==> Top1: 63.160    Top5: 87.910    Loss: 1.613

2022-01-10 16:26:38,869 - ==> Best [Top1: 63.160   Top5: 87.910   Sparsity:0.00   Params: 725128 on epoch: 102]
2022-01-10 16:26:38,869 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:26:38,903 - 

2022-01-10 16:26:38,904 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:26:40,495 - Epoch: [103][  100/  500]    Overall Loss 0.299740    Objective Loss 0.299740                                        LR 0.000250    Time 0.015905    
2022-01-10 16:26:41,931 - Epoch: [103][  200/  500]    Overall Loss 0.298266    Objective Loss 0.298266                                        LR 0.000250    Time 0.015128    
2022-01-10 16:26:43,376 - Epoch: [103][  300/  500]    Overall Loss 0.298347    Objective Loss 0.298347                                        LR 0.000250    Time 0.014896    
2022-01-10 16:26:44,818 - Epoch: [103][  400/  500]    Overall Loss 0.300477    Objective Loss 0.300477                                        LR 0.000250    Time 0.014777    
2022-01-10 16:26:46,259 - Epoch: [103][  500/  500]    Overall Loss 0.298756    Objective Loss 0.298756    Top1 91.000000    Top5 100.000000    LR 0.000250    Time 0.014701    
2022-01-10 16:26:46,305 - --- validate (epoch=103)-----------
2022-01-10 16:26:46,305 - 10000 samples (100 per mini-batch)
2022-01-10 16:26:47,084 - Epoch: [103][  100/  100]    Loss 1.613080    Top1 62.980000    Top5 87.930000    
2022-01-10 16:26:47,136 - ==> Top1: 62.980    Top5: 87.930    Loss: 1.613

2022-01-10 16:26:47,138 - ==> Best [Top1: 63.160   Top5: 87.910   Sparsity:0.00   Params: 725128 on epoch: 102]
2022-01-10 16:26:47,138 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:26:47,159 - 

2022-01-10 16:26:47,159 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:26:48,853 - Epoch: [104][  100/  500]    Overall Loss 0.286800    Objective Loss 0.286800                                        LR 0.000250    Time 0.016920    
2022-01-10 16:26:50,309 - Epoch: [104][  200/  500]    Overall Loss 0.284142    Objective Loss 0.284142                                        LR 0.000250    Time 0.015736    
2022-01-10 16:26:51,745 - Epoch: [104][  300/  500]    Overall Loss 0.285920    Objective Loss 0.285920                                        LR 0.000250    Time 0.015275    
2022-01-10 16:26:53,182 - Epoch: [104][  400/  500]    Overall Loss 0.286520    Objective Loss 0.286520                                        LR 0.000250    Time 0.015047    
2022-01-10 16:26:54,641 - Epoch: [104][  500/  500]    Overall Loss 0.289971    Objective Loss 0.289971    Top1 92.500000    Top5 100.000000    LR 0.000250    Time 0.014954    
2022-01-10 16:26:54,695 - --- validate (epoch=104)-----------
2022-01-10 16:26:54,695 - 10000 samples (100 per mini-batch)
2022-01-10 16:26:55,440 - Epoch: [104][  100/  100]    Loss 1.627917    Top1 62.580000    Top5 87.850000    
2022-01-10 16:26:55,487 - ==> Top1: 62.580    Top5: 87.850    Loss: 1.628

2022-01-10 16:26:55,489 - ==> Best [Top1: 63.160   Top5: 87.910   Sparsity:0.00   Params: 725128 on epoch: 102]
2022-01-10 16:26:55,489 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:26:55,516 - 

2022-01-10 16:26:55,517 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:26:57,255 - Epoch: [105][  100/  500]    Overall Loss 0.286370    Objective Loss 0.286370                                        LR 0.000250    Time 0.017367    
2022-01-10 16:26:58,714 - Epoch: [105][  200/  500]    Overall Loss 0.286469    Objective Loss 0.286469                                        LR 0.000250    Time 0.015978    
2022-01-10 16:27:00,185 - Epoch: [105][  300/  500]    Overall Loss 0.286487    Objective Loss 0.286487                                        LR 0.000250    Time 0.015551    
2022-01-10 16:27:01,626 - Epoch: [105][  400/  500]    Overall Loss 0.288377    Objective Loss 0.288377                                        LR 0.000250    Time 0.015264    
2022-01-10 16:27:03,078 - Epoch: [105][  500/  500]    Overall Loss 0.287473    Objective Loss 0.287473    Top1 88.500000    Top5 99.500000    LR 0.000250    Time 0.015113    
2022-01-10 16:27:03,121 - --- validate (epoch=105)-----------
2022-01-10 16:27:03,121 - 10000 samples (100 per mini-batch)
2022-01-10 16:27:03,856 - Epoch: [105][  100/  100]    Loss 1.620792    Top1 63.030000    Top5 88.140000    
2022-01-10 16:27:03,901 - ==> Top1: 63.030    Top5: 88.140    Loss: 1.621

2022-01-10 16:27:03,903 - ==> Best [Top1: 63.160   Top5: 87.910   Sparsity:0.00   Params: 725128 on epoch: 102]
2022-01-10 16:27:03,903 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:27:03,927 - 

2022-01-10 16:27:03,927 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:27:05,542 - Epoch: [106][  100/  500]    Overall Loss 0.281111    Objective Loss 0.281111                                        LR 0.000250    Time 0.016136    
2022-01-10 16:27:06,973 - Epoch: [106][  200/  500]    Overall Loss 0.279799    Objective Loss 0.279799                                        LR 0.000250    Time 0.015220    
2022-01-10 16:27:08,402 - Epoch: [106][  300/  500]    Overall Loss 0.277732    Objective Loss 0.277732                                        LR 0.000250    Time 0.014906    
2022-01-10 16:27:09,829 - Epoch: [106][  400/  500]    Overall Loss 0.279356    Objective Loss 0.279356                                        LR 0.000250    Time 0.014746    
2022-01-10 16:27:11,263 - Epoch: [106][  500/  500]    Overall Loss 0.283187    Objective Loss 0.283187    Top1 88.500000    Top5 100.000000    LR 0.000250    Time 0.014663    
2022-01-10 16:27:11,311 - --- validate (epoch=106)-----------
2022-01-10 16:27:11,312 - 10000 samples (100 per mini-batch)
2022-01-10 16:27:12,052 - Epoch: [106][  100/  100]    Loss 1.625231    Top1 63.200000    Top5 88.210000    
2022-01-10 16:27:12,093 - ==> Top1: 63.200    Top5: 88.210    Loss: 1.625

2022-01-10 16:27:12,095 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:27:12,096 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:27:12,130 - 

2022-01-10 16:27:12,130 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:27:13,961 - Epoch: [107][  100/  500]    Overall Loss 0.260868    Objective Loss 0.260868                                        LR 0.000250    Time 0.018294    
2022-01-10 16:27:15,479 - Epoch: [107][  200/  500]    Overall Loss 0.268267    Objective Loss 0.268267                                        LR 0.000250    Time 0.016735    
2022-01-10 16:27:16,912 - Epoch: [107][  300/  500]    Overall Loss 0.271392    Objective Loss 0.271392                                        LR 0.000250    Time 0.015930    
2022-01-10 16:27:18,345 - Epoch: [107][  400/  500]    Overall Loss 0.270181    Objective Loss 0.270181                                        LR 0.000250    Time 0.015527    
2022-01-10 16:27:19,784 - Epoch: [107][  500/  500]    Overall Loss 0.272853    Objective Loss 0.272853    Top1 88.000000    Top5 100.000000    LR 0.000250    Time 0.015299    
2022-01-10 16:27:19,831 - --- validate (epoch=107)-----------
2022-01-10 16:27:19,832 - 10000 samples (100 per mini-batch)
2022-01-10 16:27:20,566 - Epoch: [107][  100/  100]    Loss 1.641833    Top1 62.850000    Top5 87.880000    
2022-01-10 16:27:20,614 - ==> Top1: 62.850    Top5: 87.880    Loss: 1.642

2022-01-10 16:27:20,616 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:27:20,616 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:27:20,644 - 

2022-01-10 16:27:20,644 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:27:22,329 - Epoch: [108][  100/  500]    Overall Loss 0.261223    Objective Loss 0.261223                                        LR 0.000250    Time 0.016842    
2022-01-10 16:27:23,768 - Epoch: [108][  200/  500]    Overall Loss 0.264759    Objective Loss 0.264759                                        LR 0.000250    Time 0.015608    
2022-01-10 16:27:25,226 - Epoch: [108][  300/  500]    Overall Loss 0.266953    Objective Loss 0.266953                                        LR 0.000250    Time 0.015263    
2022-01-10 16:27:26,681 - Epoch: [108][  400/  500]    Overall Loss 0.269908    Objective Loss 0.269908                                        LR 0.000250    Time 0.015084    
2022-01-10 16:27:28,143 - Epoch: [108][  500/  500]    Overall Loss 0.273809    Objective Loss 0.273809    Top1 88.500000    Top5 99.500000    LR 0.000250    Time 0.014988    
2022-01-10 16:27:28,195 - --- validate (epoch=108)-----------
2022-01-10 16:27:28,196 - 10000 samples (100 per mini-batch)
2022-01-10 16:27:28,952 - Epoch: [108][  100/  100]    Loss 1.637592    Top1 62.850000    Top5 88.200000    
2022-01-10 16:27:29,005 - ==> Top1: 62.850    Top5: 88.200    Loss: 1.638

2022-01-10 16:27:29,007 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:27:29,007 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:27:29,035 - 

2022-01-10 16:27:29,035 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:27:30,634 - Epoch: [109][  100/  500]    Overall Loss 0.258608    Objective Loss 0.258608                                        LR 0.000250    Time 0.015975    
2022-01-10 16:27:32,115 - Epoch: [109][  200/  500]    Overall Loss 0.264416    Objective Loss 0.264416                                        LR 0.000250    Time 0.015387    
2022-01-10 16:27:33,577 - Epoch: [109][  300/  500]    Overall Loss 0.265225    Objective Loss 0.265225                                        LR 0.000250    Time 0.015131    
2022-01-10 16:27:34,994 - Epoch: [109][  400/  500]    Overall Loss 0.267207    Objective Loss 0.267207                                        LR 0.000250    Time 0.014888    
2022-01-10 16:27:36,412 - Epoch: [109][  500/  500]    Overall Loss 0.268638    Objective Loss 0.268638    Top1 92.000000    Top5 99.500000    LR 0.000250    Time 0.014745    
2022-01-10 16:27:36,467 - --- validate (epoch=109)-----------
2022-01-10 16:27:36,467 - 10000 samples (100 per mini-batch)
2022-01-10 16:27:37,196 - Epoch: [109][  100/  100]    Loss 1.644818    Top1 62.980000    Top5 88.010000    
2022-01-10 16:27:37,238 - ==> Top1: 62.980    Top5: 88.010    Loss: 1.645

2022-01-10 16:27:37,240 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:27:37,240 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:27:37,267 - 

2022-01-10 16:27:37,267 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:27:38,920 - Epoch: [110][  100/  500]    Overall Loss 0.253151    Objective Loss 0.253151                                        LR 0.000250    Time 0.016518    
2022-01-10 16:27:40,335 - Epoch: [110][  200/  500]    Overall Loss 0.259033    Objective Loss 0.259033                                        LR 0.000250    Time 0.015328    
2022-01-10 16:27:41,797 - Epoch: [110][  300/  500]    Overall Loss 0.261275    Objective Loss 0.261275                                        LR 0.000250    Time 0.015092    
2022-01-10 16:27:43,343 - Epoch: [110][  400/  500]    Overall Loss 0.261775    Objective Loss 0.261775                                        LR 0.000250    Time 0.015182    
2022-01-10 16:27:44,801 - Epoch: [110][  500/  500]    Overall Loss 0.263621    Objective Loss 0.263621    Top1 90.000000    Top5 100.000000    LR 0.000250    Time 0.015059    
2022-01-10 16:27:44,842 - --- validate (epoch=110)-----------
2022-01-10 16:27:44,842 - 10000 samples (100 per mini-batch)
2022-01-10 16:27:45,570 - Epoch: [110][  100/  100]    Loss 1.660877    Top1 62.520000    Top5 87.700000    
2022-01-10 16:27:45,616 - ==> Top1: 62.520    Top5: 87.700    Loss: 1.661

2022-01-10 16:27:45,618 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:27:45,618 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:27:45,645 - 

2022-01-10 16:27:45,646 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:27:47,237 - Epoch: [111][  100/  500]    Overall Loss 0.261712    Objective Loss 0.261712                                        LR 0.000250    Time 0.015896    
2022-01-10 16:27:48,677 - Epoch: [111][  200/  500]    Overall Loss 0.265026    Objective Loss 0.265026                                        LR 0.000250    Time 0.015144    
2022-01-10 16:27:50,115 - Epoch: [111][  300/  500]    Overall Loss 0.260785    Objective Loss 0.260785                                        LR 0.000250    Time 0.014887    
2022-01-10 16:27:51,553 - Epoch: [111][  400/  500]    Overall Loss 0.261208    Objective Loss 0.261208                                        LR 0.000250    Time 0.014760    
2022-01-10 16:27:53,001 - Epoch: [111][  500/  500]    Overall Loss 0.262631    Objective Loss 0.262631    Top1 90.500000    Top5 100.000000    LR 0.000250    Time 0.014701    
2022-01-10 16:27:53,041 - --- validate (epoch=111)-----------
2022-01-10 16:27:53,042 - 10000 samples (100 per mini-batch)
2022-01-10 16:27:53,867 - Epoch: [111][  100/  100]    Loss 1.671365    Top1 62.730000    Top5 87.920000    
2022-01-10 16:27:53,904 - ==> Top1: 62.730    Top5: 87.920    Loss: 1.671

2022-01-10 16:27:53,906 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:27:53,906 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:27:53,930 - 

2022-01-10 16:27:53,930 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:27:55,552 - Epoch: [112][  100/  500]    Overall Loss 0.255554    Objective Loss 0.255554                                        LR 0.000250    Time 0.016203    
2022-01-10 16:27:57,001 - Epoch: [112][  200/  500]    Overall Loss 0.252646    Objective Loss 0.252646                                        LR 0.000250    Time 0.015342    
2022-01-10 16:27:58,444 - Epoch: [112][  300/  500]    Overall Loss 0.253019    Objective Loss 0.253019                                        LR 0.000250    Time 0.015037    
2022-01-10 16:27:59,967 - Epoch: [112][  400/  500]    Overall Loss 0.255112    Objective Loss 0.255112                                        LR 0.000250    Time 0.015083    
2022-01-10 16:28:01,443 - Epoch: [112][  500/  500]    Overall Loss 0.255381    Objective Loss 0.255381    Top1 93.500000    Top5 99.500000    LR 0.000250    Time 0.015016    
2022-01-10 16:28:01,485 - --- validate (epoch=112)-----------
2022-01-10 16:28:01,485 - 10000 samples (100 per mini-batch)
2022-01-10 16:28:02,237 - Epoch: [112][  100/  100]    Loss 1.670935    Top1 62.670000    Top5 87.940000    
2022-01-10 16:28:02,276 - ==> Top1: 62.670    Top5: 87.940    Loss: 1.671

2022-01-10 16:28:02,278 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:28:02,278 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:28:02,305 - 

2022-01-10 16:28:02,306 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:28:03,998 - Epoch: [113][  100/  500]    Overall Loss 0.256126    Objective Loss 0.256126                                        LR 0.000250    Time 0.016915    
2022-01-10 16:28:05,431 - Epoch: [113][  200/  500]    Overall Loss 0.257749    Objective Loss 0.257749                                        LR 0.000250    Time 0.015614    
2022-01-10 16:28:06,860 - Epoch: [113][  300/  500]    Overall Loss 0.258733    Objective Loss 0.258733                                        LR 0.000250    Time 0.015170    
2022-01-10 16:28:08,289 - Epoch: [113][  400/  500]    Overall Loss 0.257543    Objective Loss 0.257543                                        LR 0.000250    Time 0.014949    
2022-01-10 16:28:09,725 - Epoch: [113][  500/  500]    Overall Loss 0.257324    Objective Loss 0.257324    Top1 91.000000    Top5 99.000000    LR 0.000250    Time 0.014830    
2022-01-10 16:28:09,772 - --- validate (epoch=113)-----------
2022-01-10 16:28:09,772 - 10000 samples (100 per mini-batch)
2022-01-10 16:28:10,538 - Epoch: [113][  100/  100]    Loss 1.681494    Top1 62.550000    Top5 87.930000    
2022-01-10 16:28:10,584 - ==> Top1: 62.550    Top5: 87.930    Loss: 1.681

2022-01-10 16:28:10,586 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:28:10,586 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:28:10,605 - 

2022-01-10 16:28:10,605 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:28:12,202 - Epoch: [114][  100/  500]    Overall Loss 0.241377    Objective Loss 0.241377                                        LR 0.000250    Time 0.015954    
2022-01-10 16:28:13,634 - Epoch: [114][  200/  500]    Overall Loss 0.250040    Objective Loss 0.250040                                        LR 0.000250    Time 0.015132    
2022-01-10 16:28:15,101 - Epoch: [114][  300/  500]    Overall Loss 0.249294    Objective Loss 0.249294                                        LR 0.000250    Time 0.014977    
2022-01-10 16:28:16,636 - Epoch: [114][  400/  500]    Overall Loss 0.251267    Objective Loss 0.251267                                        LR 0.000250    Time 0.015066    
2022-01-10 16:28:18,203 - Epoch: [114][  500/  500]    Overall Loss 0.254480    Objective Loss 0.254480    Top1 91.000000    Top5 99.500000    LR 0.000250    Time 0.015185    
2022-01-10 16:28:18,250 - --- validate (epoch=114)-----------
2022-01-10 16:28:18,251 - 10000 samples (100 per mini-batch)
2022-01-10 16:28:19,083 - Epoch: [114][  100/  100]    Loss 1.680255    Top1 62.670000    Top5 87.880000    
2022-01-10 16:28:19,126 - ==> Top1: 62.670    Top5: 87.880    Loss: 1.680

2022-01-10 16:28:19,128 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:28:19,128 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:28:19,156 - 

2022-01-10 16:28:19,156 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:28:20,775 - Epoch: [115][  100/  500]    Overall Loss 0.245564    Objective Loss 0.245564                                        LR 0.000250    Time 0.016175    
2022-01-10 16:28:22,217 - Epoch: [115][  200/  500]    Overall Loss 0.245219    Objective Loss 0.245219                                        LR 0.000250    Time 0.015293    
2022-01-10 16:28:23,658 - Epoch: [115][  300/  500]    Overall Loss 0.248335    Objective Loss 0.248335                                        LR 0.000250    Time 0.014999    
2022-01-10 16:28:25,099 - Epoch: [115][  400/  500]    Overall Loss 0.249611    Objective Loss 0.249611                                        LR 0.000250    Time 0.014849    
2022-01-10 16:28:26,547 - Epoch: [115][  500/  500]    Overall Loss 0.250026    Objective Loss 0.250026    Top1 93.500000    Top5 99.000000    LR 0.000250    Time 0.014773    
2022-01-10 16:28:26,596 - --- validate (epoch=115)-----------
2022-01-10 16:28:26,596 - 10000 samples (100 per mini-batch)
2022-01-10 16:28:27,421 - Epoch: [115][  100/  100]    Loss 1.682565    Top1 62.210000    Top5 87.610000    
2022-01-10 16:28:27,475 - ==> Top1: 62.210    Top5: 87.610    Loss: 1.683

2022-01-10 16:28:27,477 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:28:27,477 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:28:27,498 - 

2022-01-10 16:28:27,498 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:28:29,185 - Epoch: [116][  100/  500]    Overall Loss 0.253548    Objective Loss 0.253548                                        LR 0.000250    Time 0.016854    
2022-01-10 16:28:30,620 - Epoch: [116][  200/  500]    Overall Loss 0.253195    Objective Loss 0.253195                                        LR 0.000250    Time 0.015600    
2022-01-10 16:28:32,056 - Epoch: [116][  300/  500]    Overall Loss 0.249964    Objective Loss 0.249964                                        LR 0.000250    Time 0.015184    
2022-01-10 16:28:33,494 - Epoch: [116][  400/  500]    Overall Loss 0.250061    Objective Loss 0.250061                                        LR 0.000250    Time 0.014981    
2022-01-10 16:28:34,940 - Epoch: [116][  500/  500]    Overall Loss 0.251480    Objective Loss 0.251480    Top1 93.000000    Top5 100.000000    LR 0.000250    Time 0.014875    
2022-01-10 16:28:34,983 - --- validate (epoch=116)-----------
2022-01-10 16:28:34,983 - 10000 samples (100 per mini-batch)
2022-01-10 16:28:35,735 - Epoch: [116][  100/  100]    Loss 1.690219    Top1 62.440000    Top5 87.620000    
2022-01-10 16:28:35,781 - ==> Top1: 62.440    Top5: 87.620    Loss: 1.690

2022-01-10 16:28:35,783 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:28:35,783 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:28:35,811 - 

2022-01-10 16:28:35,811 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:28:37,440 - Epoch: [117][  100/  500]    Overall Loss 0.252963    Objective Loss 0.252963                                        LR 0.000250    Time 0.016278    
2022-01-10 16:28:38,903 - Epoch: [117][  200/  500]    Overall Loss 0.253290    Objective Loss 0.253290                                        LR 0.000250    Time 0.015450    
2022-01-10 16:28:40,344 - Epoch: [117][  300/  500]    Overall Loss 0.250292    Objective Loss 0.250292                                        LR 0.000250    Time 0.015101    
2022-01-10 16:28:41,782 - Epoch: [117][  400/  500]    Overall Loss 0.251205    Objective Loss 0.251205                                        LR 0.000250    Time 0.014918    
2022-01-10 16:28:43,228 - Epoch: [117][  500/  500]    Overall Loss 0.251368    Objective Loss 0.251368    Top1 93.500000    Top5 100.000000    LR 0.000250    Time 0.014825    
2022-01-10 16:28:43,285 - --- validate (epoch=117)-----------
2022-01-10 16:28:43,285 - 10000 samples (100 per mini-batch)
2022-01-10 16:28:44,089 - Epoch: [117][  100/  100]    Loss 1.683986    Top1 62.740000    Top5 87.780000    
2022-01-10 16:28:44,138 - ==> Top1: 62.740    Top5: 87.780    Loss: 1.684

2022-01-10 16:28:44,140 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:28:44,140 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:28:44,161 - 

2022-01-10 16:28:44,161 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:28:45,771 - Epoch: [118][  100/  500]    Overall Loss 0.232573    Objective Loss 0.232573                                        LR 0.000250    Time 0.016082    
2022-01-10 16:28:47,291 - Epoch: [118][  200/  500]    Overall Loss 0.232520    Objective Loss 0.232520                                        LR 0.000250    Time 0.015636    
2022-01-10 16:28:48,860 - Epoch: [118][  300/  500]    Overall Loss 0.236378    Objective Loss 0.236378                                        LR 0.000250    Time 0.015652    
2022-01-10 16:28:50,430 - Epoch: [118][  400/  500]    Overall Loss 0.238856    Objective Loss 0.238856                                        LR 0.000250    Time 0.015661    
2022-01-10 16:28:51,997 - Epoch: [118][  500/  500]    Overall Loss 0.240712    Objective Loss 0.240712    Top1 90.500000    Top5 99.000000    LR 0.000250    Time 0.015662    
2022-01-10 16:28:52,049 - --- validate (epoch=118)-----------
2022-01-10 16:28:52,050 - 10000 samples (100 per mini-batch)
2022-01-10 16:28:52,774 - Epoch: [118][  100/  100]    Loss 1.695768    Top1 62.910000    Top5 87.770000    
2022-01-10 16:28:52,819 - ==> Top1: 62.910    Top5: 87.770    Loss: 1.696

2022-01-10 16:28:52,821 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:28:52,821 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:28:52,848 - 

2022-01-10 16:28:52,849 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:28:54,547 - Epoch: [119][  100/  500]    Overall Loss 0.237517    Objective Loss 0.237517                                        LR 0.000250    Time 0.016966    
2022-01-10 16:28:55,978 - Epoch: [119][  200/  500]    Overall Loss 0.243179    Objective Loss 0.243179                                        LR 0.000250    Time 0.015637    
2022-01-10 16:28:57,397 - Epoch: [119][  300/  500]    Overall Loss 0.241418    Objective Loss 0.241418                                        LR 0.000250    Time 0.015150    
2022-01-10 16:28:58,812 - Epoch: [119][  400/  500]    Overall Loss 0.241455    Objective Loss 0.241455                                        LR 0.000250    Time 0.014898    
2022-01-10 16:29:00,234 - Epoch: [119][  500/  500]    Overall Loss 0.242915    Objective Loss 0.242915    Top1 90.500000    Top5 100.000000    LR 0.000250    Time 0.014761    
2022-01-10 16:29:00,287 - --- validate (epoch=119)-----------
2022-01-10 16:29:00,287 - 10000 samples (100 per mini-batch)
2022-01-10 16:29:01,039 - Epoch: [119][  100/  100]    Loss 1.692963    Top1 63.040000    Top5 87.980000    
2022-01-10 16:29:01,090 - ==> Top1: 63.040    Top5: 87.980    Loss: 1.693

2022-01-10 16:29:01,092 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:29:01,092 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:29:01,120 - 

2022-01-10 16:29:01,120 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:29:02,727 - Epoch: [120][  100/  500]    Overall Loss 0.231840    Objective Loss 0.231840                                        LR 0.000250    Time 0.016057    
2022-01-10 16:29:04,152 - Epoch: [120][  200/  500]    Overall Loss 0.238138    Objective Loss 0.238138                                        LR 0.000250    Time 0.015147    
2022-01-10 16:29:05,582 - Epoch: [120][  300/  500]    Overall Loss 0.241438    Objective Loss 0.241438                                        LR 0.000250    Time 0.014862    
2022-01-10 16:29:07,013 - Epoch: [120][  400/  500]    Overall Loss 0.240181    Objective Loss 0.240181                                        LR 0.000250    Time 0.014723    
2022-01-10 16:29:08,452 - Epoch: [120][  500/  500]    Overall Loss 0.243421    Objective Loss 0.243421    Top1 90.500000    Top5 100.000000    LR 0.000250    Time 0.014655    
2022-01-10 16:29:08,502 - --- validate (epoch=120)-----------
2022-01-10 16:29:08,502 - 10000 samples (100 per mini-batch)
2022-01-10 16:29:09,319 - Epoch: [120][  100/  100]    Loss 1.706008    Top1 62.930000    Top5 87.500000    
2022-01-10 16:29:09,368 - ==> Top1: 62.930    Top5: 87.500    Loss: 1.706

2022-01-10 16:29:09,370 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:29:09,370 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:29:09,397 - 

2022-01-10 16:29:09,398 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:29:11,002 - Epoch: [121][  100/  500]    Overall Loss 0.238541    Objective Loss 0.238541                                        LR 0.000250    Time 0.016033    
2022-01-10 16:29:12,442 - Epoch: [121][  200/  500]    Overall Loss 0.238018    Objective Loss 0.238018                                        LR 0.000250    Time 0.015213    
2022-01-10 16:29:13,906 - Epoch: [121][  300/  500]    Overall Loss 0.237451    Objective Loss 0.237451                                        LR 0.000250    Time 0.015018    
2022-01-10 16:29:15,371 - Epoch: [121][  400/  500]    Overall Loss 0.238490    Objective Loss 0.238490                                        LR 0.000250    Time 0.014925    
2022-01-10 16:29:16,840 - Epoch: [121][  500/  500]    Overall Loss 0.239956    Objective Loss 0.239956    Top1 92.500000    Top5 99.500000    LR 0.000250    Time 0.014875    
2022-01-10 16:29:16,878 - --- validate (epoch=121)-----------
2022-01-10 16:29:16,878 - 10000 samples (100 per mini-batch)
2022-01-10 16:29:17,634 - Epoch: [121][  100/  100]    Loss 1.708819    Top1 62.810000    Top5 87.700000    
2022-01-10 16:29:17,685 - ==> Top1: 62.810    Top5: 87.700    Loss: 1.709

2022-01-10 16:29:17,687 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:29:17,687 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:29:17,708 - 

2022-01-10 16:29:17,708 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:29:19,382 - Epoch: [122][  100/  500]    Overall Loss 0.228782    Objective Loss 0.228782                                        LR 0.000250    Time 0.016725    
2022-01-10 16:29:20,805 - Epoch: [122][  200/  500]    Overall Loss 0.234379    Objective Loss 0.234379                                        LR 0.000250    Time 0.015474    
2022-01-10 16:29:22,235 - Epoch: [122][  300/  500]    Overall Loss 0.233417    Objective Loss 0.233417                                        LR 0.000250    Time 0.015081    
2022-01-10 16:29:23,664 - Epoch: [122][  400/  500]    Overall Loss 0.232997    Objective Loss 0.232997                                        LR 0.000250    Time 0.014882    
2022-01-10 16:29:25,099 - Epoch: [122][  500/  500]    Overall Loss 0.233528    Objective Loss 0.233528    Top1 95.000000    Top5 99.000000    LR 0.000250    Time 0.014773    
2022-01-10 16:29:25,137 - --- validate (epoch=122)-----------
2022-01-10 16:29:25,138 - 10000 samples (100 per mini-batch)
2022-01-10 16:29:25,869 - Epoch: [122][  100/  100]    Loss 1.715524    Top1 62.350000    Top5 87.850000    
2022-01-10 16:29:25,918 - ==> Top1: 62.350    Top5: 87.850    Loss: 1.716

2022-01-10 16:29:25,920 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:29:25,920 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:29:25,947 - 

2022-01-10 16:29:25,947 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:29:27,577 - Epoch: [123][  100/  500]    Overall Loss 0.230425    Objective Loss 0.230425                                        LR 0.000250    Time 0.016283    
2022-01-10 16:29:29,030 - Epoch: [123][  200/  500]    Overall Loss 0.234330    Objective Loss 0.234330                                        LR 0.000250    Time 0.015402    
2022-01-10 16:29:30,463 - Epoch: [123][  300/  500]    Overall Loss 0.235593    Objective Loss 0.235593                                        LR 0.000250    Time 0.015043    
2022-01-10 16:29:31,897 - Epoch: [123][  400/  500]    Overall Loss 0.240467    Objective Loss 0.240467                                        LR 0.000250    Time 0.014865    
2022-01-10 16:29:33,336 - Epoch: [123][  500/  500]    Overall Loss 0.239110    Objective Loss 0.239110    Top1 88.500000    Top5 100.000000    LR 0.000250    Time 0.014768    
2022-01-10 16:29:33,380 - --- validate (epoch=123)-----------
2022-01-10 16:29:33,380 - 10000 samples (100 per mini-batch)
2022-01-10 16:29:34,189 - Epoch: [123][  100/  100]    Loss 1.718422    Top1 62.920000    Top5 87.870000    
2022-01-10 16:29:34,234 - ==> Top1: 62.920    Top5: 87.870    Loss: 1.718

2022-01-10 16:29:34,235 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:29:34,235 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:29:34,256 - 

2022-01-10 16:29:34,256 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:29:35,892 - Epoch: [124][  100/  500]    Overall Loss 0.229414    Objective Loss 0.229414                                        LR 0.000250    Time 0.016342    
2022-01-10 16:29:37,351 - Epoch: [124][  200/  500]    Overall Loss 0.228463    Objective Loss 0.228463                                        LR 0.000250    Time 0.015463    
2022-01-10 16:29:38,787 - Epoch: [124][  300/  500]    Overall Loss 0.229962    Objective Loss 0.229962                                        LR 0.000250    Time 0.015092    
2022-01-10 16:29:40,221 - Epoch: [124][  400/  500]    Overall Loss 0.233298    Objective Loss 0.233298                                        LR 0.000250    Time 0.014903    
2022-01-10 16:29:41,665 - Epoch: [124][  500/  500]    Overall Loss 0.235412    Objective Loss 0.235412    Top1 95.000000    Top5 100.000000    LR 0.000250    Time 0.014807    
2022-01-10 16:29:41,718 - --- validate (epoch=124)-----------
2022-01-10 16:29:41,718 - 10000 samples (100 per mini-batch)
2022-01-10 16:29:42,460 - Epoch: [124][  100/  100]    Loss 1.729103    Top1 62.420000    Top5 87.950000    
2022-01-10 16:29:42,513 - ==> Top1: 62.420    Top5: 87.950    Loss: 1.729

2022-01-10 16:29:42,515 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:29:42,515 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:29:42,542 - 

2022-01-10 16:29:42,543 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:29:44,220 - Epoch: [125][  100/  500]    Overall Loss 0.223755    Objective Loss 0.223755                                        LR 0.000250    Time 0.016757    
2022-01-10 16:29:45,646 - Epoch: [125][  200/  500]    Overall Loss 0.227740    Objective Loss 0.227740                                        LR 0.000250    Time 0.015505    
2022-01-10 16:29:47,135 - Epoch: [125][  300/  500]    Overall Loss 0.230431    Objective Loss 0.230431                                        LR 0.000250    Time 0.015299    
2022-01-10 16:29:48,557 - Epoch: [125][  400/  500]    Overall Loss 0.230224    Objective Loss 0.230224                                        LR 0.000250    Time 0.015026    
2022-01-10 16:29:49,985 - Epoch: [125][  500/  500]    Overall Loss 0.230327    Objective Loss 0.230327    Top1 93.000000    Top5 100.000000    LR 0.000250    Time 0.014875    
2022-01-10 16:29:50,023 - --- validate (epoch=125)-----------
2022-01-10 16:29:50,023 - 10000 samples (100 per mini-batch)
2022-01-10 16:29:50,767 - Epoch: [125][  100/  100]    Loss 1.729875    Top1 62.480000    Top5 87.910000    
2022-01-10 16:29:50,823 - ==> Top1: 62.480    Top5: 87.910    Loss: 1.730

2022-01-10 16:29:50,825 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:29:50,825 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:29:50,846 - 

2022-01-10 16:29:50,846 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:29:52,443 - Epoch: [126][  100/  500]    Overall Loss 0.216758    Objective Loss 0.216758                                        LR 0.000250    Time 0.015954    
2022-01-10 16:29:53,871 - Epoch: [126][  200/  500]    Overall Loss 0.223836    Objective Loss 0.223836                                        LR 0.000250    Time 0.015118    
2022-01-10 16:29:55,300 - Epoch: [126][  300/  500]    Overall Loss 0.229918    Objective Loss 0.229918                                        LR 0.000250    Time 0.014837    
2022-01-10 16:29:56,727 - Epoch: [126][  400/  500]    Overall Loss 0.232015    Objective Loss 0.232015                                        LR 0.000250    Time 0.014693    
2022-01-10 16:29:58,160 - Epoch: [126][  500/  500]    Overall Loss 0.232617    Objective Loss 0.232617    Top1 90.000000    Top5 98.500000    LR 0.000250    Time 0.014620    
2022-01-10 16:29:58,202 - --- validate (epoch=126)-----------
2022-01-10 16:29:58,202 - 10000 samples (100 per mini-batch)
2022-01-10 16:29:59,015 - Epoch: [126][  100/  100]    Loss 1.728712    Top1 62.850000    Top5 87.780000    
2022-01-10 16:29:59,067 - ==> Top1: 62.850    Top5: 87.780    Loss: 1.729

2022-01-10 16:29:59,069 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:29:59,069 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:29:59,097 - 

2022-01-10 16:29:59,097 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:30:00,786 - Epoch: [127][  100/  500]    Overall Loss 0.224191    Objective Loss 0.224191                                        LR 0.000250    Time 0.016879    
2022-01-10 16:30:02,355 - Epoch: [127][  200/  500]    Overall Loss 0.225000    Objective Loss 0.225000                                        LR 0.000250    Time 0.016277    
2022-01-10 16:30:03,917 - Epoch: [127][  300/  500]    Overall Loss 0.228520    Objective Loss 0.228520                                        LR 0.000250    Time 0.016055    
2022-01-10 16:30:05,479 - Epoch: [127][  400/  500]    Overall Loss 0.230788    Objective Loss 0.230788                                        LR 0.000250    Time 0.015944    
2022-01-10 16:30:07,015 - Epoch: [127][  500/  500]    Overall Loss 0.230519    Objective Loss 0.230519    Top1 94.500000    Top5 100.000000    LR 0.000250    Time 0.015826    
2022-01-10 16:30:07,052 - --- validate (epoch=127)-----------
2022-01-10 16:30:07,052 - 10000 samples (100 per mini-batch)
2022-01-10 16:30:07,785 - Epoch: [127][  100/  100]    Loss 1.734962    Top1 62.460000    Top5 87.850000    
2022-01-10 16:30:07,842 - ==> Top1: 62.460    Top5: 87.850    Loss: 1.735

2022-01-10 16:30:07,844 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:30:07,844 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:30:07,871 - 

2022-01-10 16:30:07,872 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:30:09,550 - Epoch: [128][  100/  500]    Overall Loss 0.214669    Objective Loss 0.214669                                        LR 0.000250    Time 0.016772    
2022-01-10 16:30:10,992 - Epoch: [128][  200/  500]    Overall Loss 0.217260    Objective Loss 0.217260                                        LR 0.000250    Time 0.015591    
2022-01-10 16:30:12,471 - Epoch: [128][  300/  500]    Overall Loss 0.220311    Objective Loss 0.220311                                        LR 0.000250    Time 0.015322    
2022-01-10 16:30:13,940 - Epoch: [128][  400/  500]    Overall Loss 0.222680    Objective Loss 0.222680                                        LR 0.000250    Time 0.015162    
2022-01-10 16:30:15,411 - Epoch: [128][  500/  500]    Overall Loss 0.223972    Objective Loss 0.223972    Top1 92.500000    Top5 99.500000    LR 0.000250    Time 0.015070    
2022-01-10 16:30:15,456 - --- validate (epoch=128)-----------
2022-01-10 16:30:15,457 - 10000 samples (100 per mini-batch)
2022-01-10 16:30:16,227 - Epoch: [128][  100/  100]    Loss 1.745708    Top1 62.800000    Top5 87.680000    
2022-01-10 16:30:16,270 - ==> Top1: 62.800    Top5: 87.680    Loss: 1.746

2022-01-10 16:30:16,272 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:30:16,272 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:30:16,300 - 

2022-01-10 16:30:16,300 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:30:17,903 - Epoch: [129][  100/  500]    Overall Loss 0.225154    Objective Loss 0.225154                                        LR 0.000250    Time 0.016017    
2022-01-10 16:30:19,346 - Epoch: [129][  200/  500]    Overall Loss 0.224560    Objective Loss 0.224560                                        LR 0.000250    Time 0.015219    
2022-01-10 16:30:20,801 - Epoch: [129][  300/  500]    Overall Loss 0.225975    Objective Loss 0.225975                                        LR 0.000250    Time 0.014992    
2022-01-10 16:30:22,250 - Epoch: [129][  400/  500]    Overall Loss 0.226160    Objective Loss 0.226160                                        LR 0.000250    Time 0.014866    
2022-01-10 16:30:23,703 - Epoch: [129][  500/  500]    Overall Loss 0.226775    Objective Loss 0.226775    Top1 95.000000    Top5 99.500000    LR 0.000250    Time 0.014797    
2022-01-10 16:30:23,749 - --- validate (epoch=129)-----------
2022-01-10 16:30:23,750 - 10000 samples (100 per mini-batch)
2022-01-10 16:30:24,511 - Epoch: [129][  100/  100]    Loss 1.753911    Top1 62.330000    Top5 87.370000    
2022-01-10 16:30:24,554 - ==> Top1: 62.330    Top5: 87.370    Loss: 1.754

2022-01-10 16:30:24,556 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:30:24,556 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:30:24,582 - 

2022-01-10 16:30:24,582 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:30:26,265 - Epoch: [130][  100/  500]    Overall Loss 0.220924    Objective Loss 0.220924                                        LR 0.000250    Time 0.016814    
2022-01-10 16:30:27,711 - Epoch: [130][  200/  500]    Overall Loss 0.219753    Objective Loss 0.219753                                        LR 0.000250    Time 0.015634    
2022-01-10 16:30:29,124 - Epoch: [130][  300/  500]    Overall Loss 0.222400    Objective Loss 0.222400                                        LR 0.000250    Time 0.015128    
2022-01-10 16:30:30,535 - Epoch: [130][  400/  500]    Overall Loss 0.221289    Objective Loss 0.221289                                        LR 0.000250    Time 0.014874    
2022-01-10 16:30:31,953 - Epoch: [130][  500/  500]    Overall Loss 0.221815    Objective Loss 0.221815    Top1 91.000000    Top5 99.000000    LR 0.000250    Time 0.014732    
2022-01-10 16:30:31,993 - --- validate (epoch=130)-----------
2022-01-10 16:30:31,993 - 10000 samples (100 per mini-batch)
2022-01-10 16:30:32,730 - Epoch: [130][  100/  100]    Loss 1.752262    Top1 62.590000    Top5 87.520000    
2022-01-10 16:30:32,777 - ==> Top1: 62.590    Top5: 87.520    Loss: 1.752

2022-01-10 16:30:32,779 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:30:32,779 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:30:32,807 - 

2022-01-10 16:30:32,807 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:30:34,483 - Epoch: [131][  100/  500]    Overall Loss 0.220847    Objective Loss 0.220847                                        LR 0.000250    Time 0.016740    
2022-01-10 16:30:35,925 - Epoch: [131][  200/  500]    Overall Loss 0.223866    Objective Loss 0.223866                                        LR 0.000250    Time 0.015578    
2022-01-10 16:30:37,368 - Epoch: [131][  300/  500]    Overall Loss 0.222729    Objective Loss 0.222729                                        LR 0.000250    Time 0.015194    
2022-01-10 16:30:38,805 - Epoch: [131][  400/  500]    Overall Loss 0.221154    Objective Loss 0.221154                                        LR 0.000250    Time 0.014984    
2022-01-10 16:30:40,253 - Epoch: [131][  500/  500]    Overall Loss 0.221589    Objective Loss 0.221589    Top1 92.500000    Top5 99.500000    LR 0.000250    Time 0.014883    
2022-01-10 16:30:40,301 - --- validate (epoch=131)-----------
2022-01-10 16:30:40,301 - 10000 samples (100 per mini-batch)
2022-01-10 16:30:41,051 - Epoch: [131][  100/  100]    Loss 1.763654    Top1 62.160000    Top5 87.920000    
2022-01-10 16:30:41,095 - ==> Top1: 62.160    Top5: 87.920    Loss: 1.764

2022-01-10 16:30:41,097 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:30:41,097 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:30:41,125 - 

2022-01-10 16:30:41,125 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:30:42,788 - Epoch: [132][  100/  500]    Overall Loss 0.218901    Objective Loss 0.218901                                        LR 0.000250    Time 0.016616    
2022-01-10 16:30:44,234 - Epoch: [132][  200/  500]    Overall Loss 0.216103    Objective Loss 0.216103                                        LR 0.000250    Time 0.015535    
2022-01-10 16:30:45,675 - Epoch: [132][  300/  500]    Overall Loss 0.218932    Objective Loss 0.218932                                        LR 0.000250    Time 0.015157    
2022-01-10 16:30:47,117 - Epoch: [132][  400/  500]    Overall Loss 0.217967    Objective Loss 0.217967                                        LR 0.000250    Time 0.014970    
2022-01-10 16:30:48,564 - Epoch: [132][  500/  500]    Overall Loss 0.220740    Objective Loss 0.220740    Top1 91.500000    Top5 100.000000    LR 0.000250    Time 0.014869    
2022-01-10 16:30:48,603 - --- validate (epoch=132)-----------
2022-01-10 16:30:48,603 - 10000 samples (100 per mini-batch)
2022-01-10 16:30:49,333 - Epoch: [132][  100/  100]    Loss 1.757897    Top1 62.560000    Top5 87.560000    
2022-01-10 16:30:49,377 - ==> Top1: 62.560    Top5: 87.560    Loss: 1.758

2022-01-10 16:30:49,379 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:30:49,379 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:30:49,406 - 

2022-01-10 16:30:49,406 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:30:51,082 - Epoch: [133][  100/  500]    Overall Loss 0.223815    Objective Loss 0.223815                                        LR 0.000250    Time 0.016750    
2022-01-10 16:30:52,490 - Epoch: [133][  200/  500]    Overall Loss 0.219860    Objective Loss 0.219860                                        LR 0.000250    Time 0.015409    
2022-01-10 16:30:53,914 - Epoch: [133][  300/  500]    Overall Loss 0.222440    Objective Loss 0.222440                                        LR 0.000250    Time 0.015018    
2022-01-10 16:30:55,358 - Epoch: [133][  400/  500]    Overall Loss 0.225565    Objective Loss 0.225565                                        LR 0.000250    Time 0.014872    
2022-01-10 16:30:56,802 - Epoch: [133][  500/  500]    Overall Loss 0.226100    Objective Loss 0.226100    Top1 93.500000    Top5 100.000000    LR 0.000250    Time 0.014783    
2022-01-10 16:30:56,846 - --- validate (epoch=133)-----------
2022-01-10 16:30:56,846 - 10000 samples (100 per mini-batch)
2022-01-10 16:30:57,603 - Epoch: [133][  100/  100]    Loss 1.759130    Top1 62.250000    Top5 87.790000    
2022-01-10 16:30:57,650 - ==> Top1: 62.250    Top5: 87.790    Loss: 1.759

2022-01-10 16:30:57,652 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:30:57,652 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:30:57,673 - 

2022-01-10 16:30:57,674 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:30:59,351 - Epoch: [134][  100/  500]    Overall Loss 0.210805    Objective Loss 0.210805                                        LR 0.000250    Time 0.016758    
2022-01-10 16:31:00,810 - Epoch: [134][  200/  500]    Overall Loss 0.215785    Objective Loss 0.215785                                        LR 0.000250    Time 0.015669    
2022-01-10 16:31:02,257 - Epoch: [134][  300/  500]    Overall Loss 0.219382    Objective Loss 0.219382                                        LR 0.000250    Time 0.015269    
2022-01-10 16:31:03,700 - Epoch: [134][  400/  500]    Overall Loss 0.222192    Objective Loss 0.222192                                        LR 0.000250    Time 0.015057    
2022-01-10 16:31:05,150 - Epoch: [134][  500/  500]    Overall Loss 0.221028    Objective Loss 0.221028    Top1 94.500000    Top5 100.000000    LR 0.000250    Time 0.014944    
2022-01-10 16:31:05,187 - --- validate (epoch=134)-----------
2022-01-10 16:31:05,187 - 10000 samples (100 per mini-batch)
2022-01-10 16:31:05,942 - Epoch: [134][  100/  100]    Loss 1.762312    Top1 62.000000    Top5 87.460000    
2022-01-10 16:31:05,984 - ==> Top1: 62.000    Top5: 87.460    Loss: 1.762

2022-01-10 16:31:05,986 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:31:05,986 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:31:06,013 - 

2022-01-10 16:31:06,013 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:31:07,622 - Epoch: [135][  100/  500]    Overall Loss 0.216071    Objective Loss 0.216071                                        LR 0.000250    Time 0.016075    
2022-01-10 16:31:09,152 - Epoch: [135][  200/  500]    Overall Loss 0.216064    Objective Loss 0.216064                                        LR 0.000250    Time 0.015685    
2022-01-10 16:31:10,586 - Epoch: [135][  300/  500]    Overall Loss 0.217829    Objective Loss 0.217829                                        LR 0.000250    Time 0.015234    
2022-01-10 16:31:11,988 - Epoch: [135][  400/  500]    Overall Loss 0.217894    Objective Loss 0.217894                                        LR 0.000250    Time 0.014928    
2022-01-10 16:31:13,396 - Epoch: [135][  500/  500]    Overall Loss 0.218890    Objective Loss 0.218890    Top1 93.500000    Top5 99.500000    LR 0.000250    Time 0.014757    
2022-01-10 16:31:13,444 - --- validate (epoch=135)-----------
2022-01-10 16:31:13,444 - 10000 samples (100 per mini-batch)
2022-01-10 16:31:14,175 - Epoch: [135][  100/  100]    Loss 1.766589    Top1 62.460000    Top5 87.360000    
2022-01-10 16:31:14,219 - ==> Top1: 62.460    Top5: 87.360    Loss: 1.767

2022-01-10 16:31:14,221 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:31:14,221 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:31:14,242 - 

2022-01-10 16:31:14,242 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:31:15,946 - Epoch: [136][  100/  500]    Overall Loss 0.205268    Objective Loss 0.205268                                        LR 0.000250    Time 0.017029    
2022-01-10 16:31:17,447 - Epoch: [136][  200/  500]    Overall Loss 0.213283    Objective Loss 0.213283                                        LR 0.000250    Time 0.016012    
2022-01-10 16:31:18,874 - Epoch: [136][  300/  500]    Overall Loss 0.210933    Objective Loss 0.210933                                        LR 0.000250    Time 0.015428    
2022-01-10 16:31:20,295 - Epoch: [136][  400/  500]    Overall Loss 0.214774    Objective Loss 0.214774                                        LR 0.000250    Time 0.015123    
2022-01-10 16:31:21,723 - Epoch: [136][  500/  500]    Overall Loss 0.216306    Objective Loss 0.216306    Top1 93.000000    Top5 99.000000    LR 0.000250    Time 0.014953    
2022-01-10 16:31:21,770 - --- validate (epoch=136)-----------
2022-01-10 16:31:21,770 - 10000 samples (100 per mini-batch)
2022-01-10 16:31:22,585 - Epoch: [136][  100/  100]    Loss 1.773432    Top1 62.330000    Top5 87.460000    
2022-01-10 16:31:22,630 - ==> Top1: 62.330    Top5: 87.460    Loss: 1.773

2022-01-10 16:31:22,632 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:31:22,632 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:31:22,659 - 

2022-01-10 16:31:22,659 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:31:24,343 - Epoch: [137][  100/  500]    Overall Loss 0.205182    Objective Loss 0.205182                                        LR 0.000250    Time 0.016826    
2022-01-10 16:31:25,792 - Epoch: [137][  200/  500]    Overall Loss 0.206089    Objective Loss 0.206089                                        LR 0.000250    Time 0.015657    
2022-01-10 16:31:27,227 - Epoch: [137][  300/  500]    Overall Loss 0.211756    Objective Loss 0.211756                                        LR 0.000250    Time 0.015219    
2022-01-10 16:31:28,654 - Epoch: [137][  400/  500]    Overall Loss 0.212667    Objective Loss 0.212667                                        LR 0.000250    Time 0.014977    
2022-01-10 16:31:30,086 - Epoch: [137][  500/  500]    Overall Loss 0.214158    Objective Loss 0.214158    Top1 93.000000    Top5 100.000000    LR 0.000250    Time 0.014845    
2022-01-10 16:31:30,135 - --- validate (epoch=137)-----------
2022-01-10 16:31:30,135 - 10000 samples (100 per mini-batch)
2022-01-10 16:31:30,890 - Epoch: [137][  100/  100]    Loss 1.760113    Top1 62.510000    Top5 87.760000    
2022-01-10 16:31:30,937 - ==> Top1: 62.510    Top5: 87.760    Loss: 1.760

2022-01-10 16:31:30,939 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:31:30,939 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:31:30,961 - 

2022-01-10 16:31:30,961 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:31:32,602 - Epoch: [138][  100/  500]    Overall Loss 0.216579    Objective Loss 0.216579                                        LR 0.000250    Time 0.016396    
2022-01-10 16:31:34,178 - Epoch: [138][  200/  500]    Overall Loss 0.214326    Objective Loss 0.214326                                        LR 0.000250    Time 0.016074    
2022-01-10 16:31:35,756 - Epoch: [138][  300/  500]    Overall Loss 0.213326    Objective Loss 0.213326                                        LR 0.000250    Time 0.015971    
2022-01-10 16:31:37,333 - Epoch: [138][  400/  500]    Overall Loss 0.213290    Objective Loss 0.213290                                        LR 0.000250    Time 0.015919    
2022-01-10 16:31:38,904 - Epoch: [138][  500/  500]    Overall Loss 0.216225    Objective Loss 0.216225    Top1 92.000000    Top5 100.000000    LR 0.000250    Time 0.015877    
2022-01-10 16:31:38,946 - --- validate (epoch=138)-----------
2022-01-10 16:31:38,946 - 10000 samples (100 per mini-batch)
2022-01-10 16:31:39,670 - Epoch: [138][  100/  100]    Loss 1.769401    Top1 62.700000    Top5 87.680000    
2022-01-10 16:31:39,715 - ==> Top1: 62.700    Top5: 87.680    Loss: 1.769

2022-01-10 16:31:39,716 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:31:39,717 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:31:39,744 - 

2022-01-10 16:31:39,744 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:31:41,409 - Epoch: [139][  100/  500]    Overall Loss 0.209271    Objective Loss 0.209271                                        LR 0.000250    Time 0.016632    
2022-01-10 16:31:42,822 - Epoch: [139][  200/  500]    Overall Loss 0.206949    Objective Loss 0.206949                                        LR 0.000250    Time 0.015380    
2022-01-10 16:31:44,237 - Epoch: [139][  300/  500]    Overall Loss 0.212097    Objective Loss 0.212097                                        LR 0.000250    Time 0.014966    
2022-01-10 16:31:45,675 - Epoch: [139][  400/  500]    Overall Loss 0.213433    Objective Loss 0.213433                                        LR 0.000250    Time 0.014817    
2022-01-10 16:31:47,119 - Epoch: [139][  500/  500]    Overall Loss 0.214453    Objective Loss 0.214453    Top1 94.500000    Top5 100.000000    LR 0.000250    Time 0.014740    
2022-01-10 16:31:47,160 - --- validate (epoch=139)-----------
2022-01-10 16:31:47,160 - 10000 samples (100 per mini-batch)
2022-01-10 16:31:47,908 - Epoch: [139][  100/  100]    Loss 1.778068    Top1 62.470000    Top5 87.650000    
2022-01-10 16:31:47,964 - ==> Top1: 62.470    Top5: 87.650    Loss: 1.778

2022-01-10 16:31:47,966 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:31:47,966 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:31:47,987 - 

2022-01-10 16:31:47,987 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:31:49,651 - Epoch: [140][  100/  500]    Overall Loss 0.204384    Objective Loss 0.204384                                        LR 0.000250    Time 0.016623    
2022-01-10 16:31:51,099 - Epoch: [140][  200/  500]    Overall Loss 0.207247    Objective Loss 0.207247                                        LR 0.000250    Time 0.015550    
2022-01-10 16:31:52,643 - Epoch: [140][  300/  500]    Overall Loss 0.210484    Objective Loss 0.210484                                        LR 0.000250    Time 0.015508    
2022-01-10 16:31:54,185 - Epoch: [140][  400/  500]    Overall Loss 0.211053    Objective Loss 0.211053                                        LR 0.000250    Time 0.015486    
2022-01-10 16:31:55,632 - Epoch: [140][  500/  500]    Overall Loss 0.213195    Objective Loss 0.213195    Top1 94.000000    Top5 100.000000    LR 0.000250    Time 0.015280    
2022-01-10 16:31:55,679 - --- validate (epoch=140)-----------
2022-01-10 16:31:55,680 - 10000 samples (100 per mini-batch)
2022-01-10 16:31:56,442 - Epoch: [140][  100/  100]    Loss 1.783924    Top1 62.150000    Top5 87.740000    
2022-01-10 16:31:56,490 - ==> Top1: 62.150    Top5: 87.740    Loss: 1.784

2022-01-10 16:31:56,492 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:31:56,492 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:31:56,519 - 

2022-01-10 16:31:56,519 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:31:58,136 - Epoch: [141][  100/  500]    Overall Loss 0.207614    Objective Loss 0.207614                                        LR 0.000250    Time 0.016154    
2022-01-10 16:31:59,566 - Epoch: [141][  200/  500]    Overall Loss 0.208038    Objective Loss 0.208038                                        LR 0.000250    Time 0.015222    
2022-01-10 16:32:00,991 - Epoch: [141][  300/  500]    Overall Loss 0.209120    Objective Loss 0.209120                                        LR 0.000250    Time 0.014897    
2022-01-10 16:32:02,417 - Epoch: [141][  400/  500]    Overall Loss 0.208248    Objective Loss 0.208248                                        LR 0.000250    Time 0.014736    
2022-01-10 16:32:03,852 - Epoch: [141][  500/  500]    Overall Loss 0.209775    Objective Loss 0.209775    Top1 96.000000    Top5 100.000000    LR 0.000250    Time 0.014657    
2022-01-10 16:32:03,894 - --- validate (epoch=141)-----------
2022-01-10 16:32:03,894 - 10000 samples (100 per mini-batch)
2022-01-10 16:32:04,673 - Epoch: [141][  100/  100]    Loss 1.790052    Top1 62.330000    Top5 87.440000    
2022-01-10 16:32:04,718 - ==> Top1: 62.330    Top5: 87.440    Loss: 1.790

2022-01-10 16:32:04,720 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:32:04,720 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:32:04,740 - 

2022-01-10 16:32:04,741 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:32:06,437 - Epoch: [142][  100/  500]    Overall Loss 0.195968    Objective Loss 0.195968                                        LR 0.000250    Time 0.016953    
2022-01-10 16:32:07,878 - Epoch: [142][  200/  500]    Overall Loss 0.202764    Objective Loss 0.202764                                        LR 0.000250    Time 0.015675    
2022-01-10 16:32:09,316 - Epoch: [142][  300/  500]    Overall Loss 0.203177    Objective Loss 0.203177                                        LR 0.000250    Time 0.015243    
2022-01-10 16:32:10,757 - Epoch: [142][  400/  500]    Overall Loss 0.205203    Objective Loss 0.205203                                        LR 0.000250    Time 0.015031    
2022-01-10 16:32:12,205 - Epoch: [142][  500/  500]    Overall Loss 0.205575    Objective Loss 0.205575    Top1 93.500000    Top5 100.000000    LR 0.000250    Time 0.014920    
2022-01-10 16:32:12,250 - --- validate (epoch=142)-----------
2022-01-10 16:32:12,251 - 10000 samples (100 per mini-batch)
2022-01-10 16:32:13,007 - Epoch: [142][  100/  100]    Loss 1.790902    Top1 62.360000    Top5 87.620000    
2022-01-10 16:32:13,050 - ==> Top1: 62.360    Top5: 87.620    Loss: 1.791

2022-01-10 16:32:13,052 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:32:13,052 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:32:13,079 - 

2022-01-10 16:32:13,079 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:32:14,754 - Epoch: [143][  100/  500]    Overall Loss 0.193681    Objective Loss 0.193681                                        LR 0.000250    Time 0.016734    
2022-01-10 16:32:16,190 - Epoch: [143][  200/  500]    Overall Loss 0.201202    Objective Loss 0.201202                                        LR 0.000250    Time 0.015545    
2022-01-10 16:32:17,623 - Epoch: [143][  300/  500]    Overall Loss 0.205489    Objective Loss 0.205489                                        LR 0.000250    Time 0.015138    
2022-01-10 16:32:19,051 - Epoch: [143][  400/  500]    Overall Loss 0.206243    Objective Loss 0.206243                                        LR 0.000250    Time 0.014921    
2022-01-10 16:32:20,466 - Epoch: [143][  500/  500]    Overall Loss 0.209264    Objective Loss 0.209264    Top1 89.500000    Top5 100.000000    LR 0.000250    Time 0.014765    
2022-01-10 16:32:20,506 - --- validate (epoch=143)-----------
2022-01-10 16:32:20,507 - 10000 samples (100 per mini-batch)
2022-01-10 16:32:21,238 - Epoch: [143][  100/  100]    Loss 1.795317    Top1 62.400000    Top5 87.800000    
2022-01-10 16:32:21,281 - ==> Top1: 62.400    Top5: 87.800    Loss: 1.795

2022-01-10 16:32:21,283 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:32:21,284 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:32:21,305 - 

2022-01-10 16:32:21,305 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:32:22,901 - Epoch: [144][  100/  500]    Overall Loss 0.190123    Objective Loss 0.190123                                        LR 0.000250    Time 0.015944    
2022-01-10 16:32:24,323 - Epoch: [144][  200/  500]    Overall Loss 0.203761    Objective Loss 0.203761                                        LR 0.000250    Time 0.015082    
2022-01-10 16:32:25,747 - Epoch: [144][  300/  500]    Overall Loss 0.206914    Objective Loss 0.206914                                        LR 0.000250    Time 0.014799    
2022-01-10 16:32:27,174 - Epoch: [144][  400/  500]    Overall Loss 0.207823    Objective Loss 0.207823                                        LR 0.000250    Time 0.014663    
2022-01-10 16:32:28,607 - Epoch: [144][  500/  500]    Overall Loss 0.209033    Objective Loss 0.209033    Top1 91.500000    Top5 100.000000    LR 0.000250    Time 0.014596    
2022-01-10 16:32:28,643 - --- validate (epoch=144)-----------
2022-01-10 16:32:28,643 - 10000 samples (100 per mini-batch)
2022-01-10 16:32:29,397 - Epoch: [144][  100/  100]    Loss 1.807580    Top1 62.520000    Top5 87.530000    
2022-01-10 16:32:29,441 - ==> Top1: 62.520    Top5: 87.530    Loss: 1.808

2022-01-10 16:32:29,443 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:32:29,443 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:32:29,470 - 

2022-01-10 16:32:29,470 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:32:31,165 - Epoch: [145][  100/  500]    Overall Loss 0.194808    Objective Loss 0.194808                                        LR 0.000250    Time 0.016937    
2022-01-10 16:32:32,598 - Epoch: [145][  200/  500]    Overall Loss 0.198143    Objective Loss 0.198143                                        LR 0.000250    Time 0.015629    
2022-01-10 16:32:34,030 - Epoch: [145][  300/  500]    Overall Loss 0.202229    Objective Loss 0.202229                                        LR 0.000250    Time 0.015190    
2022-01-10 16:32:35,462 - Epoch: [145][  400/  500]    Overall Loss 0.204554    Objective Loss 0.204554                                        LR 0.000250    Time 0.014969    
2022-01-10 16:32:36,902 - Epoch: [145][  500/  500]    Overall Loss 0.205196    Objective Loss 0.205196    Top1 93.500000    Top5 100.000000    LR 0.000250    Time 0.014854    
2022-01-10 16:32:36,940 - --- validate (epoch=145)-----------
2022-01-10 16:32:36,941 - 10000 samples (100 per mini-batch)
2022-01-10 16:32:37,710 - Epoch: [145][  100/  100]    Loss 1.800594    Top1 62.470000    Top5 87.530000    
2022-01-10 16:32:37,754 - ==> Top1: 62.470    Top5: 87.530    Loss: 1.801

2022-01-10 16:32:37,756 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:32:37,756 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:32:37,777 - 

2022-01-10 16:32:37,777 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:32:39,479 - Epoch: [146][  100/  500]    Overall Loss 0.204499    Objective Loss 0.204499                                        LR 0.000250    Time 0.017006    
2022-01-10 16:32:41,037 - Epoch: [146][  200/  500]    Overall Loss 0.205187    Objective Loss 0.205187                                        LR 0.000250    Time 0.016288    
2022-01-10 16:32:42,542 - Epoch: [146][  300/  500]    Overall Loss 0.208748    Objective Loss 0.208748                                        LR 0.000250    Time 0.015873    
2022-01-10 16:32:43,990 - Epoch: [146][  400/  500]    Overall Loss 0.207243    Objective Loss 0.207243                                        LR 0.000250    Time 0.015521    
2022-01-10 16:32:45,446 - Epoch: [146][  500/  500]    Overall Loss 0.207723    Objective Loss 0.207723    Top1 92.000000    Top5 99.000000    LR 0.000250    Time 0.015328    
2022-01-10 16:32:45,500 - --- validate (epoch=146)-----------
2022-01-10 16:32:45,501 - 10000 samples (100 per mini-batch)
2022-01-10 16:32:46,276 - Epoch: [146][  100/  100]    Loss 1.789372    Top1 62.740000    Top5 87.680000    
2022-01-10 16:32:46,327 - ==> Top1: 62.740    Top5: 87.680    Loss: 1.789

2022-01-10 16:32:46,329 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:32:46,329 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:32:46,356 - 

2022-01-10 16:32:46,357 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:32:47,964 - Epoch: [147][  100/  500]    Overall Loss 0.200729    Objective Loss 0.200729                                        LR 0.000250    Time 0.016064    
2022-01-10 16:32:49,395 - Epoch: [147][  200/  500]    Overall Loss 0.203394    Objective Loss 0.203394                                        LR 0.000250    Time 0.015182    
2022-01-10 16:32:50,833 - Epoch: [147][  300/  500]    Overall Loss 0.207373    Objective Loss 0.207373                                        LR 0.000250    Time 0.014910    
2022-01-10 16:32:52,277 - Epoch: [147][  400/  500]    Overall Loss 0.206535    Objective Loss 0.206535                                        LR 0.000250    Time 0.014793    
2022-01-10 16:32:53,732 - Epoch: [147][  500/  500]    Overall Loss 0.207263    Objective Loss 0.207263    Top1 96.000000    Top5 100.000000    LR 0.000250    Time 0.014742    
2022-01-10 16:32:53,772 - --- validate (epoch=147)-----------
2022-01-10 16:32:53,773 - 10000 samples (100 per mini-batch)
2022-01-10 16:32:54,503 - Epoch: [147][  100/  100]    Loss 1.803941    Top1 62.020000    Top5 87.340000    
2022-01-10 16:32:54,550 - ==> Top1: 62.020    Top5: 87.340    Loss: 1.804

2022-01-10 16:32:54,552 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:32:54,552 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:32:54,573 - 

2022-01-10 16:32:54,573 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:32:56,250 - Epoch: [148][  100/  500]    Overall Loss 0.183581    Objective Loss 0.183581                                        LR 0.000250    Time 0.016756    
2022-01-10 16:32:57,685 - Epoch: [148][  200/  500]    Overall Loss 0.188843    Objective Loss 0.188843                                        LR 0.000250    Time 0.015548    
2022-01-10 16:32:59,120 - Epoch: [148][  300/  500]    Overall Loss 0.192356    Objective Loss 0.192356                                        LR 0.000250    Time 0.015147    
2022-01-10 16:33:00,557 - Epoch: [148][  400/  500]    Overall Loss 0.195962    Objective Loss 0.195962                                        LR 0.000250    Time 0.014949    
2022-01-10 16:33:01,999 - Epoch: [148][  500/  500]    Overall Loss 0.198670    Objective Loss 0.198670    Top1 95.000000    Top5 100.000000    LR 0.000250    Time 0.014843    
2022-01-10 16:33:02,041 - --- validate (epoch=148)-----------
2022-01-10 16:33:02,041 - 10000 samples (100 per mini-batch)
2022-01-10 16:33:02,814 - Epoch: [148][  100/  100]    Loss 1.792551    Top1 62.350000    Top5 87.430000    
2022-01-10 16:33:02,855 - ==> Top1: 62.350    Top5: 87.430    Loss: 1.793

2022-01-10 16:33:02,857 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:33:02,857 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:33:02,885 - 

2022-01-10 16:33:02,885 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:33:04,600 - Epoch: [149][  100/  500]    Overall Loss 0.188606    Objective Loss 0.188606                                        LR 0.000250    Time 0.017137    
2022-01-10 16:33:06,036 - Epoch: [149][  200/  500]    Overall Loss 0.194597    Objective Loss 0.194597                                        LR 0.000250    Time 0.015743    
2022-01-10 16:33:07,450 - Epoch: [149][  300/  500]    Overall Loss 0.198313    Objective Loss 0.198313                                        LR 0.000250    Time 0.015205    
2022-01-10 16:33:08,863 - Epoch: [149][  400/  500]    Overall Loss 0.199668    Objective Loss 0.199668                                        LR 0.000250    Time 0.014937    
2022-01-10 16:33:10,283 - Epoch: [149][  500/  500]    Overall Loss 0.200847    Objective Loss 0.200847    Top1 91.500000    Top5 99.500000    LR 0.000250    Time 0.014786    
2022-01-10 16:33:10,329 - --- validate (epoch=149)-----------
2022-01-10 16:33:10,330 - 10000 samples (100 per mini-batch)
2022-01-10 16:33:11,100 - Epoch: [149][  100/  100]    Loss 1.801798    Top1 62.530000    Top5 87.430000    
2022-01-10 16:33:11,157 - ==> Top1: 62.530    Top5: 87.430    Loss: 1.802

2022-01-10 16:33:11,159 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:33:11,159 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:33:11,180 - 

2022-01-10 16:33:11,180 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:33:12,762 - Epoch: [150][  100/  500]    Overall Loss 0.187135    Objective Loss 0.187135                                        LR 0.000063    Time 0.015800    
2022-01-10 16:33:14,168 - Epoch: [150][  200/  500]    Overall Loss 0.186623    Objective Loss 0.186623                                        LR 0.000063    Time 0.014929    
2022-01-10 16:33:15,578 - Epoch: [150][  300/  500]    Overall Loss 0.185485    Objective Loss 0.185485                                        LR 0.000063    Time 0.014648    
2022-01-10 16:33:16,990 - Epoch: [150][  400/  500]    Overall Loss 0.185640    Objective Loss 0.185640                                        LR 0.000063    Time 0.014514    
2022-01-10 16:33:18,408 - Epoch: [150][  500/  500]    Overall Loss 0.184108    Objective Loss 0.184108    Top1 94.500000    Top5 100.000000    LR 0.000063    Time 0.014447    
2022-01-10 16:33:18,459 - --- validate (epoch=150)-----------
2022-01-10 16:33:18,460 - 10000 samples (100 per mini-batch)
2022-01-10 16:33:19,202 - Epoch: [150][  100/  100]    Loss 1.785251    Top1 62.740000    Top5 87.690000    
2022-01-10 16:33:19,249 - ==> Top1: 62.740    Top5: 87.690    Loss: 1.785

2022-01-10 16:33:19,251 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:33:19,251 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:33:19,278 - 

2022-01-10 16:33:19,278 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:33:20,916 - Epoch: [151][  100/  500]    Overall Loss 0.179526    Objective Loss 0.179526                                        LR 0.000063    Time 0.016369    
2022-01-10 16:33:22,352 - Epoch: [151][  200/  500]    Overall Loss 0.180039    Objective Loss 0.180039                                        LR 0.000063    Time 0.015360    
2022-01-10 16:33:23,776 - Epoch: [151][  300/  500]    Overall Loss 0.178381    Objective Loss 0.178381                                        LR 0.000063    Time 0.014982    
2022-01-10 16:33:25,193 - Epoch: [151][  400/  500]    Overall Loss 0.178414    Objective Loss 0.178414                                        LR 0.000063    Time 0.014777    
2022-01-10 16:33:26,629 - Epoch: [151][  500/  500]    Overall Loss 0.179926    Objective Loss 0.179926    Top1 96.000000    Top5 100.000000    LR 0.000063    Time 0.014692    
2022-01-10 16:33:26,667 - --- validate (epoch=151)-----------
2022-01-10 16:33:26,668 - 10000 samples (100 per mini-batch)
2022-01-10 16:33:27,443 - Epoch: [151][  100/  100]    Loss 1.784431    Top1 62.680000    Top5 87.670000    
2022-01-10 16:33:27,497 - ==> Top1: 62.680    Top5: 87.670    Loss: 1.784

2022-01-10 16:33:27,499 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:33:27,499 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:33:27,521 - 

2022-01-10 16:33:27,521 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:33:29,157 - Epoch: [152][  100/  500]    Overall Loss 0.173322    Objective Loss 0.173322                                        LR 0.000063    Time 0.016347    
2022-01-10 16:33:30,583 - Epoch: [152][  200/  500]    Overall Loss 0.172832    Objective Loss 0.172832                                        LR 0.000063    Time 0.015301    
2022-01-10 16:33:32,043 - Epoch: [152][  300/  500]    Overall Loss 0.176121    Objective Loss 0.176121                                        LR 0.000063    Time 0.015063    
2022-01-10 16:33:33,469 - Epoch: [152][  400/  500]    Overall Loss 0.173372    Objective Loss 0.173372                                        LR 0.000063    Time 0.014860    
2022-01-10 16:33:34,907 - Epoch: [152][  500/  500]    Overall Loss 0.173916    Objective Loss 0.173916    Top1 99.000000    Top5 100.000000    LR 0.000063    Time 0.014764    
2022-01-10 16:33:34,946 - --- validate (epoch=152)-----------
2022-01-10 16:33:34,946 - 10000 samples (100 per mini-batch)
2022-01-10 16:33:35,813 - Epoch: [152][  100/  100]    Loss 1.785945    Top1 62.700000    Top5 87.700000    
2022-01-10 16:33:35,852 - ==> Top1: 62.700    Top5: 87.700    Loss: 1.786

2022-01-10 16:33:35,853 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:33:35,853 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:33:35,881 - 

2022-01-10 16:33:35,881 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:33:37,489 - Epoch: [153][  100/  500]    Overall Loss 0.172999    Objective Loss 0.172999                                        LR 0.000063    Time 0.016063    
2022-01-10 16:33:38,937 - Epoch: [153][  200/  500]    Overall Loss 0.171855    Objective Loss 0.171855                                        LR 0.000063    Time 0.015270    
2022-01-10 16:33:40,374 - Epoch: [153][  300/  500]    Overall Loss 0.171303    Objective Loss 0.171303                                        LR 0.000063    Time 0.014966    
2022-01-10 16:33:41,824 - Epoch: [153][  400/  500]    Overall Loss 0.172933    Objective Loss 0.172933                                        LR 0.000063    Time 0.014846    
2022-01-10 16:33:43,281 - Epoch: [153][  500/  500]    Overall Loss 0.174492    Objective Loss 0.174492    Top1 94.500000    Top5 100.000000    LR 0.000063    Time 0.014790    
2022-01-10 16:33:43,324 - --- validate (epoch=153)-----------
2022-01-10 16:33:43,325 - 10000 samples (100 per mini-batch)
2022-01-10 16:33:44,078 - Epoch: [153][  100/  100]    Loss 1.782356    Top1 62.950000    Top5 87.860000    
2022-01-10 16:33:44,118 - ==> Top1: 62.950    Top5: 87.860    Loss: 1.782

2022-01-10 16:33:44,119 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:33:44,120 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:33:44,143 - 

2022-01-10 16:33:44,143 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:33:45,801 - Epoch: [154][  100/  500]    Overall Loss 0.174111    Objective Loss 0.174111                                        LR 0.000063    Time 0.016567    
2022-01-10 16:33:47,211 - Epoch: [154][  200/  500]    Overall Loss 0.173501    Objective Loss 0.173501                                        LR 0.000063    Time 0.015327    
2022-01-10 16:33:48,632 - Epoch: [154][  300/  500]    Overall Loss 0.170838    Objective Loss 0.170838                                        LR 0.000063    Time 0.014954    
2022-01-10 16:33:50,063 - Epoch: [154][  400/  500]    Overall Loss 0.172252    Objective Loss 0.172252                                        LR 0.000063    Time 0.014791    
2022-01-10 16:33:51,500 - Epoch: [154][  500/  500]    Overall Loss 0.172344    Objective Loss 0.172344    Top1 94.000000    Top5 100.000000    LR 0.000063    Time 0.014705    
2022-01-10 16:33:51,541 - --- validate (epoch=154)-----------
2022-01-10 16:33:51,541 - 10000 samples (100 per mini-batch)
2022-01-10 16:33:52,264 - Epoch: [154][  100/  100]    Loss 1.787683    Top1 62.730000    Top5 87.780000    
2022-01-10 16:33:52,308 - ==> Top1: 62.730    Top5: 87.780    Loss: 1.788

2022-01-10 16:33:52,310 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:33:52,310 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:33:52,338 - 

2022-01-10 16:33:52,338 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:33:53,942 - Epoch: [155][  100/  500]    Overall Loss 0.173245    Objective Loss 0.173245                                        LR 0.000063    Time 0.016027    
2022-01-10 16:33:55,425 - Epoch: [155][  200/  500]    Overall Loss 0.174253    Objective Loss 0.174253                                        LR 0.000063    Time 0.015423    
2022-01-10 16:33:56,866 - Epoch: [155][  300/  500]    Overall Loss 0.173542    Objective Loss 0.173542                                        LR 0.000063    Time 0.015084    
2022-01-10 16:33:58,299 - Epoch: [155][  400/  500]    Overall Loss 0.173956    Objective Loss 0.173956                                        LR 0.000063    Time 0.014894    
2022-01-10 16:33:59,731 - Epoch: [155][  500/  500]    Overall Loss 0.173740    Objective Loss 0.173740    Top1 96.500000    Top5 100.000000    LR 0.000063    Time 0.014778    
2022-01-10 16:33:59,784 - --- validate (epoch=155)-----------
2022-01-10 16:33:59,785 - 10000 samples (100 per mini-batch)
2022-01-10 16:34:00,586 - Epoch: [155][  100/  100]    Loss 1.795366    Top1 62.890000    Top5 87.700000    
2022-01-10 16:34:00,629 - ==> Top1: 62.890    Top5: 87.700    Loss: 1.795

2022-01-10 16:34:00,631 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:34:00,631 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:34:00,659 - 

2022-01-10 16:34:00,659 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:34:02,268 - Epoch: [156][  100/  500]    Overall Loss 0.174985    Objective Loss 0.174985                                        LR 0.000063    Time 0.016077    
2022-01-10 16:34:03,708 - Epoch: [156][  200/  500]    Overall Loss 0.176136    Objective Loss 0.176136                                        LR 0.000063    Time 0.015236    
2022-01-10 16:34:05,154 - Epoch: [156][  300/  500]    Overall Loss 0.175369    Objective Loss 0.175369                                        LR 0.000063    Time 0.014974    
2022-01-10 16:34:06,620 - Epoch: [156][  400/  500]    Overall Loss 0.174729    Objective Loss 0.174729                                        LR 0.000063    Time 0.014893    
2022-01-10 16:34:08,089 - Epoch: [156][  500/  500]    Overall Loss 0.173637    Objective Loss 0.173637    Top1 95.500000    Top5 100.000000    LR 0.000063    Time 0.014852    
2022-01-10 16:34:08,134 - --- validate (epoch=156)-----------
2022-01-10 16:34:08,135 - 10000 samples (100 per mini-batch)
2022-01-10 16:34:08,908 - Epoch: [156][  100/  100]    Loss 1.784345    Top1 63.150000    Top5 87.770000    
2022-01-10 16:34:08,951 - ==> Top1: 63.150    Top5: 87.770    Loss: 1.784

2022-01-10 16:34:08,953 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:34:08,953 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:34:08,980 - 

2022-01-10 16:34:08,980 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:34:10,677 - Epoch: [157][  100/  500]    Overall Loss 0.171613    Objective Loss 0.171613                                        LR 0.000063    Time 0.016955    
2022-01-10 16:34:12,123 - Epoch: [157][  200/  500]    Overall Loss 0.165472    Objective Loss 0.165472                                        LR 0.000063    Time 0.015703    
2022-01-10 16:34:13,554 - Epoch: [157][  300/  500]    Overall Loss 0.165903    Objective Loss 0.165903                                        LR 0.000063    Time 0.015236    
2022-01-10 16:34:14,984 - Epoch: [157][  400/  500]    Overall Loss 0.168347    Objective Loss 0.168347                                        LR 0.000063    Time 0.015000    
2022-01-10 16:34:16,422 - Epoch: [157][  500/  500]    Overall Loss 0.168495    Objective Loss 0.168495    Top1 96.000000    Top5 100.000000    LR 0.000063    Time 0.014874    
2022-01-10 16:34:16,473 - --- validate (epoch=157)-----------
2022-01-10 16:34:16,474 - 10000 samples (100 per mini-batch)
2022-01-10 16:34:17,252 - Epoch: [157][  100/  100]    Loss 1.795701    Top1 63.070000    Top5 87.660000    
2022-01-10 16:34:17,302 - ==> Top1: 63.070    Top5: 87.660    Loss: 1.796

2022-01-10 16:34:17,304 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:34:17,304 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:34:17,331 - 

2022-01-10 16:34:17,331 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:34:18,921 - Epoch: [158][  100/  500]    Overall Loss 0.178002    Objective Loss 0.178002                                        LR 0.000063    Time 0.015880    
2022-01-10 16:34:20,349 - Epoch: [158][  200/  500]    Overall Loss 0.171808    Objective Loss 0.171808                                        LR 0.000063    Time 0.015077    
2022-01-10 16:34:21,791 - Epoch: [158][  300/  500]    Overall Loss 0.168417    Objective Loss 0.168417                                        LR 0.000063    Time 0.014854    
2022-01-10 16:34:23,347 - Epoch: [158][  400/  500]    Overall Loss 0.169580    Objective Loss 0.169580                                        LR 0.000063    Time 0.015030    
2022-01-10 16:34:24,910 - Epoch: [158][  500/  500]    Overall Loss 0.168953    Objective Loss 0.168953    Top1 97.000000    Top5 100.000000    LR 0.000063    Time 0.015149    
2022-01-10 16:34:24,946 - --- validate (epoch=158)-----------
2022-01-10 16:34:24,946 - 10000 samples (100 per mini-batch)
2022-01-10 16:34:25,731 - Epoch: [158][  100/  100]    Loss 1.797496    Top1 62.890000    Top5 87.580000    
2022-01-10 16:34:25,785 - ==> Top1: 62.890    Top5: 87.580    Loss: 1.797

2022-01-10 16:34:25,787 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:34:25,787 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:34:25,815 - 

2022-01-10 16:34:25,815 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:34:27,408 - Epoch: [159][  100/  500]    Overall Loss 0.168148    Objective Loss 0.168148                                        LR 0.000063    Time 0.015916    
2022-01-10 16:34:28,865 - Epoch: [159][  200/  500]    Overall Loss 0.161885    Objective Loss 0.161885                                        LR 0.000063    Time 0.015240    
2022-01-10 16:34:30,328 - Epoch: [159][  300/  500]    Overall Loss 0.161392    Objective Loss 0.161392                                        LR 0.000063    Time 0.015033    
2022-01-10 16:34:31,766 - Epoch: [159][  400/  500]    Overall Loss 0.163876    Objective Loss 0.163876                                        LR 0.000063    Time 0.014868    
2022-01-10 16:34:33,215 - Epoch: [159][  500/  500]    Overall Loss 0.164734    Objective Loss 0.164734    Top1 94.000000    Top5 99.500000    LR 0.000063    Time 0.014791    
2022-01-10 16:34:33,256 - --- validate (epoch=159)-----------
2022-01-10 16:34:33,257 - 10000 samples (100 per mini-batch)
2022-01-10 16:34:34,012 - Epoch: [159][  100/  100]    Loss 1.796931    Top1 62.840000    Top5 87.690000    
2022-01-10 16:34:34,054 - ==> Top1: 62.840    Top5: 87.690    Loss: 1.797

2022-01-10 16:34:34,056 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:34:34,056 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:34:34,077 - 

2022-01-10 16:34:34,077 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:34:35,737 - Epoch: [160][  100/  500]    Overall Loss 0.161016    Objective Loss 0.161016                                        LR 0.000063    Time 0.016585    
2022-01-10 16:34:37,149 - Epoch: [160][  200/  500]    Overall Loss 0.164032    Objective Loss 0.164032                                        LR 0.000063    Time 0.015351    
2022-01-10 16:34:38,572 - Epoch: [160][  300/  500]    Overall Loss 0.165998    Objective Loss 0.165998                                        LR 0.000063    Time 0.014973    
2022-01-10 16:34:40,000 - Epoch: [160][  400/  500]    Overall Loss 0.165918    Objective Loss 0.165918                                        LR 0.000063    Time 0.014798    
2022-01-10 16:34:41,433 - Epoch: [160][  500/  500]    Overall Loss 0.166807    Objective Loss 0.166807    Top1 94.000000    Top5 100.000000    LR 0.000063    Time 0.014704    
2022-01-10 16:34:41,476 - --- validate (epoch=160)-----------
2022-01-10 16:34:41,476 - 10000 samples (100 per mini-batch)
2022-01-10 16:34:42,277 - Epoch: [160][  100/  100]    Loss 1.797159    Top1 62.920000    Top5 87.820000    
2022-01-10 16:34:42,326 - ==> Top1: 62.920    Top5: 87.820    Loss: 1.797

2022-01-10 16:34:42,328 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:34:42,328 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:34:42,355 - 

2022-01-10 16:34:42,355 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:34:43,981 - Epoch: [161][  100/  500]    Overall Loss 0.167591    Objective Loss 0.167591                                        LR 0.000063    Time 0.016248    
2022-01-10 16:34:45,394 - Epoch: [161][  200/  500]    Overall Loss 0.165717    Objective Loss 0.165717                                        LR 0.000063    Time 0.015186    
2022-01-10 16:34:46,805 - Epoch: [161][  300/  500]    Overall Loss 0.166363    Objective Loss 0.166363                                        LR 0.000063    Time 0.014823    
2022-01-10 16:34:48,219 - Epoch: [161][  400/  500]    Overall Loss 0.165186    Objective Loss 0.165186                                        LR 0.000063    Time 0.014650    
2022-01-10 16:34:49,637 - Epoch: [161][  500/  500]    Overall Loss 0.166295    Objective Loss 0.166295    Top1 95.500000    Top5 99.500000    LR 0.000063    Time 0.014555    
2022-01-10 16:34:49,688 - --- validate (epoch=161)-----------
2022-01-10 16:34:49,688 - 10000 samples (100 per mini-batch)
2022-01-10 16:34:50,513 - Epoch: [161][  100/  100]    Loss 1.798452    Top1 62.990000    Top5 87.630000    
2022-01-10 16:34:50,565 - ==> Top1: 62.990    Top5: 87.630    Loss: 1.798

2022-01-10 16:34:50,567 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:34:50,567 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:34:50,594 - 

2022-01-10 16:34:50,595 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:34:52,197 - Epoch: [162][  100/  500]    Overall Loss 0.164256    Objective Loss 0.164256                                        LR 0.000063    Time 0.016014    
2022-01-10 16:34:53,643 - Epoch: [162][  200/  500]    Overall Loss 0.163715    Objective Loss 0.163715                                        LR 0.000063    Time 0.015228    
2022-01-10 16:34:55,100 - Epoch: [162][  300/  500]    Overall Loss 0.163990    Objective Loss 0.163990                                        LR 0.000063    Time 0.015007    
2022-01-10 16:34:56,532 - Epoch: [162][  400/  500]    Overall Loss 0.163576    Objective Loss 0.163576                                        LR 0.000063    Time 0.014833    
2022-01-10 16:34:57,970 - Epoch: [162][  500/  500]    Overall Loss 0.163666    Objective Loss 0.163666    Top1 95.500000    Top5 100.000000    LR 0.000063    Time 0.014741    
2022-01-10 16:34:58,011 - --- validate (epoch=162)-----------
2022-01-10 16:34:58,012 - 10000 samples (100 per mini-batch)
2022-01-10 16:34:58,745 - Epoch: [162][  100/  100]    Loss 1.805521    Top1 62.730000    Top5 87.740000    
2022-01-10 16:34:58,802 - ==> Top1: 62.730    Top5: 87.740    Loss: 1.806

2022-01-10 16:34:58,804 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:34:58,804 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:34:58,832 - 

2022-01-10 16:34:58,832 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:35:00,517 - Epoch: [163][  100/  500]    Overall Loss 0.162338    Objective Loss 0.162338                                        LR 0.000063    Time 0.016836    
2022-01-10 16:35:02,006 - Epoch: [163][  200/  500]    Overall Loss 0.167061    Objective Loss 0.167061                                        LR 0.000063    Time 0.015860    
2022-01-10 16:35:03,579 - Epoch: [163][  300/  500]    Overall Loss 0.165761    Objective Loss 0.165761                                        LR 0.000063    Time 0.015814    
2022-01-10 16:35:05,036 - Epoch: [163][  400/  500]    Overall Loss 0.166695    Objective Loss 0.166695                                        LR 0.000063    Time 0.015501    
2022-01-10 16:35:06,481 - Epoch: [163][  500/  500]    Overall Loss 0.166223    Objective Loss 0.166223    Top1 94.000000    Top5 100.000000    LR 0.000063    Time 0.015289    
2022-01-10 16:35:06,533 - --- validate (epoch=163)-----------
2022-01-10 16:35:06,533 - 10000 samples (100 per mini-batch)
2022-01-10 16:35:07,280 - Epoch: [163][  100/  100]    Loss 1.809800    Top1 62.670000    Top5 87.820000    
2022-01-10 16:35:07,318 - ==> Top1: 62.670    Top5: 87.820    Loss: 1.810

2022-01-10 16:35:07,320 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:35:07,320 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:35:07,348 - 

2022-01-10 16:35:07,348 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:35:08,983 - Epoch: [164][  100/  500]    Overall Loss 0.164732    Objective Loss 0.164732                                        LR 0.000063    Time 0.016337    
2022-01-10 16:35:10,424 - Epoch: [164][  200/  500]    Overall Loss 0.159946    Objective Loss 0.159946                                        LR 0.000063    Time 0.015371    
2022-01-10 16:35:11,866 - Epoch: [164][  300/  500]    Overall Loss 0.162437    Objective Loss 0.162437                                        LR 0.000063    Time 0.015051    
2022-01-10 16:35:13,306 - Epoch: [164][  400/  500]    Overall Loss 0.161588    Objective Loss 0.161588                                        LR 0.000063    Time 0.014886    
2022-01-10 16:35:14,756 - Epoch: [164][  500/  500]    Overall Loss 0.162126    Objective Loss 0.162126    Top1 95.500000    Top5 100.000000    LR 0.000063    Time 0.014807    
2022-01-10 16:35:14,795 - --- validate (epoch=164)-----------
2022-01-10 16:35:14,795 - 10000 samples (100 per mini-batch)
2022-01-10 16:35:15,599 - Epoch: [164][  100/  100]    Loss 1.808429    Top1 62.770000    Top5 87.740000    
2022-01-10 16:35:15,645 - ==> Top1: 62.770    Top5: 87.740    Loss: 1.808

2022-01-10 16:35:15,647 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:35:15,647 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:35:15,675 - 

2022-01-10 16:35:15,675 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:35:17,296 - Epoch: [165][  100/  500]    Overall Loss 0.157042    Objective Loss 0.157042                                        LR 0.000063    Time 0.016199    
2022-01-10 16:35:18,744 - Epoch: [165][  200/  500]    Overall Loss 0.155010    Objective Loss 0.155010                                        LR 0.000063    Time 0.015333    
2022-01-10 16:35:20,187 - Epoch: [165][  300/  500]    Overall Loss 0.157443    Objective Loss 0.157443                                        LR 0.000063    Time 0.015030    
2022-01-10 16:35:21,597 - Epoch: [165][  400/  500]    Overall Loss 0.162803    Objective Loss 0.162803                                        LR 0.000063    Time 0.014797    
2022-01-10 16:35:23,015 - Epoch: [165][  500/  500]    Overall Loss 0.161589    Objective Loss 0.161589    Top1 96.500000    Top5 99.000000    LR 0.000063    Time 0.014670    
2022-01-10 16:35:23,066 - --- validate (epoch=165)-----------
2022-01-10 16:35:23,067 - 10000 samples (100 per mini-batch)
2022-01-10 16:35:23,802 - Epoch: [165][  100/  100]    Loss 1.804571    Top1 63.070000    Top5 87.790000    
2022-01-10 16:35:23,855 - ==> Top1: 63.070    Top5: 87.790    Loss: 1.805

2022-01-10 16:35:23,857 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:35:23,857 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:35:23,884 - 

2022-01-10 16:35:23,885 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:35:25,558 - Epoch: [166][  100/  500]    Overall Loss 0.158357    Objective Loss 0.158357                                        LR 0.000063    Time 0.016718    
2022-01-10 16:35:27,003 - Epoch: [166][  200/  500]    Overall Loss 0.159805    Objective Loss 0.159805                                        LR 0.000063    Time 0.015580    
2022-01-10 16:35:28,453 - Epoch: [166][  300/  500]    Overall Loss 0.159961    Objective Loss 0.159961                                        LR 0.000063    Time 0.015217    
2022-01-10 16:35:29,903 - Epoch: [166][  400/  500]    Overall Loss 0.160767    Objective Loss 0.160767                                        LR 0.000063    Time 0.015036    
2022-01-10 16:35:31,361 - Epoch: [166][  500/  500]    Overall Loss 0.160727    Objective Loss 0.160727    Top1 93.000000    Top5 100.000000    LR 0.000063    Time 0.014944    
2022-01-10 16:35:31,407 - --- validate (epoch=166)-----------
2022-01-10 16:35:31,407 - 10000 samples (100 per mini-batch)
2022-01-10 16:35:32,156 - Epoch: [166][  100/  100]    Loss 1.798274    Top1 62.910000    Top5 87.870000    
2022-01-10 16:35:32,204 - ==> Top1: 62.910    Top5: 87.870    Loss: 1.798

2022-01-10 16:35:32,206 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:35:32,206 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:35:32,233 - 

2022-01-10 16:35:32,234 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:35:33,915 - Epoch: [167][  100/  500]    Overall Loss 0.163555    Objective Loss 0.163555                                        LR 0.000063    Time 0.016802    
2022-01-10 16:35:35,342 - Epoch: [167][  200/  500]    Overall Loss 0.160813    Objective Loss 0.160813                                        LR 0.000063    Time 0.015531    
2022-01-10 16:35:36,768 - Epoch: [167][  300/  500]    Overall Loss 0.160507    Objective Loss 0.160507                                        LR 0.000063    Time 0.015106    
2022-01-10 16:35:38,194 - Epoch: [167][  400/  500]    Overall Loss 0.162205    Objective Loss 0.162205                                        LR 0.000063    Time 0.014891    
2022-01-10 16:35:39,630 - Epoch: [167][  500/  500]    Overall Loss 0.161772    Objective Loss 0.161772    Top1 96.000000    Top5 100.000000    LR 0.000063    Time 0.014784    
2022-01-10 16:35:39,676 - --- validate (epoch=167)-----------
2022-01-10 16:35:39,676 - 10000 samples (100 per mini-batch)
2022-01-10 16:35:40,489 - Epoch: [167][  100/  100]    Loss 1.812265    Top1 63.160000    Top5 87.670000    
2022-01-10 16:35:40,536 - ==> Top1: 63.160    Top5: 87.670    Loss: 1.812

2022-01-10 16:35:40,538 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:35:40,538 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:35:40,565 - 

2022-01-10 16:35:40,565 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:35:42,149 - Epoch: [168][  100/  500]    Overall Loss 0.157377    Objective Loss 0.157377                                        LR 0.000063    Time 0.015827    
2022-01-10 16:35:43,580 - Epoch: [168][  200/  500]    Overall Loss 0.158422    Objective Loss 0.158422                                        LR 0.000063    Time 0.015066    
2022-01-10 16:35:45,008 - Epoch: [168][  300/  500]    Overall Loss 0.158945    Objective Loss 0.158945                                        LR 0.000063    Time 0.014801    
2022-01-10 16:35:46,437 - Epoch: [168][  400/  500]    Overall Loss 0.159597    Objective Loss 0.159597                                        LR 0.000063    Time 0.014672    
2022-01-10 16:35:47,874 - Epoch: [168][  500/  500]    Overall Loss 0.162042    Objective Loss 0.162042    Top1 93.500000    Top5 99.500000    LR 0.000063    Time 0.014610    
2022-01-10 16:35:47,916 - --- validate (epoch=168)-----------
2022-01-10 16:35:47,916 - 10000 samples (100 per mini-batch)
2022-01-10 16:35:48,668 - Epoch: [168][  100/  100]    Loss 1.813039    Top1 63.120000    Top5 87.610000    
2022-01-10 16:35:48,722 - ==> Top1: 63.120    Top5: 87.610    Loss: 1.813

2022-01-10 16:35:48,724 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:35:48,724 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:35:48,751 - 

2022-01-10 16:35:48,751 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:35:50,419 - Epoch: [169][  100/  500]    Overall Loss 0.167592    Objective Loss 0.167592                                        LR 0.000063    Time 0.016668    
2022-01-10 16:35:51,852 - Epoch: [169][  200/  500]    Overall Loss 0.166337    Objective Loss 0.166337                                        LR 0.000063    Time 0.015494    
2022-01-10 16:35:53,285 - Epoch: [169][  300/  500]    Overall Loss 0.167466    Objective Loss 0.167466                                        LR 0.000063    Time 0.015103    
2022-01-10 16:35:54,743 - Epoch: [169][  400/  500]    Overall Loss 0.165547    Objective Loss 0.165547                                        LR 0.000063    Time 0.014971    
2022-01-10 16:35:56,212 - Epoch: [169][  500/  500]    Overall Loss 0.166625    Objective Loss 0.166625    Top1 91.000000    Top5 100.000000    LR 0.000063    Time 0.014914    
2022-01-10 16:35:56,253 - --- validate (epoch=169)-----------
2022-01-10 16:35:56,253 - 10000 samples (100 per mini-batch)
2022-01-10 16:35:56,994 - Epoch: [169][  100/  100]    Loss 1.813642    Top1 63.040000    Top5 87.890000    
2022-01-10 16:35:57,040 - ==> Top1: 63.040    Top5: 87.890    Loss: 1.814

2022-01-10 16:35:57,041 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:35:57,041 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:35:57,065 - 

2022-01-10 16:35:57,065 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:35:58,659 - Epoch: [170][  100/  500]    Overall Loss 0.161408    Objective Loss 0.161408                                        LR 0.000063    Time 0.015930    
2022-01-10 16:36:00,087 - Epoch: [170][  200/  500]    Overall Loss 0.164738    Objective Loss 0.164738                                        LR 0.000063    Time 0.015102    
2022-01-10 16:36:01,516 - Epoch: [170][  300/  500]    Overall Loss 0.161372    Objective Loss 0.161372                                        LR 0.000063    Time 0.014829    
2022-01-10 16:36:02,944 - Epoch: [170][  400/  500]    Overall Loss 0.159464    Objective Loss 0.159464                                        LR 0.000063    Time 0.014690    
2022-01-10 16:36:04,377 - Epoch: [170][  500/  500]    Overall Loss 0.161576    Objective Loss 0.161576    Top1 94.500000    Top5 100.000000    LR 0.000063    Time 0.014615    
2022-01-10 16:36:04,417 - --- validate (epoch=170)-----------
2022-01-10 16:36:04,417 - 10000 samples (100 per mini-batch)
2022-01-10 16:36:05,164 - Epoch: [170][  100/  100]    Loss 1.812135    Top1 62.900000    Top5 87.820000    
2022-01-10 16:36:05,210 - ==> Top1: 62.900    Top5: 87.820    Loss: 1.812

2022-01-10 16:36:05,212 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:36:05,212 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:36:05,239 - 

2022-01-10 16:36:05,240 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:36:06,907 - Epoch: [171][  100/  500]    Overall Loss 0.156079    Objective Loss 0.156079                                        LR 0.000063    Time 0.016658    
2022-01-10 16:36:08,349 - Epoch: [171][  200/  500]    Overall Loss 0.160026    Objective Loss 0.160026                                        LR 0.000063    Time 0.015537    
2022-01-10 16:36:09,836 - Epoch: [171][  300/  500]    Overall Loss 0.158927    Objective Loss 0.158927                                        LR 0.000063    Time 0.015313    
2022-01-10 16:36:11,272 - Epoch: [171][  400/  500]    Overall Loss 0.160664    Objective Loss 0.160664                                        LR 0.000063    Time 0.015070    
2022-01-10 16:36:12,747 - Epoch: [171][  500/  500]    Overall Loss 0.161654    Objective Loss 0.161654    Top1 96.500000    Top5 100.000000    LR 0.000063    Time 0.015006    
2022-01-10 16:36:12,799 - --- validate (epoch=171)-----------
2022-01-10 16:36:12,799 - 10000 samples (100 per mini-batch)
2022-01-10 16:36:13,533 - Epoch: [171][  100/  100]    Loss 1.812527    Top1 63.000000    Top5 87.790000    
2022-01-10 16:36:13,592 - ==> Top1: 63.000    Top5: 87.790    Loss: 1.813

2022-01-10 16:36:13,594 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:36:13,594 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:36:13,615 - 

2022-01-10 16:36:13,616 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:36:15,334 - Epoch: [172][  100/  500]    Overall Loss 0.165884    Objective Loss 0.165884                                        LR 0.000063    Time 0.017167    
2022-01-10 16:36:16,776 - Epoch: [172][  200/  500]    Overall Loss 0.161100    Objective Loss 0.161100                                        LR 0.000063    Time 0.015793    
2022-01-10 16:36:18,301 - Epoch: [172][  300/  500]    Overall Loss 0.161525    Objective Loss 0.161525                                        LR 0.000063    Time 0.015609    
2022-01-10 16:36:19,857 - Epoch: [172][  400/  500]    Overall Loss 0.164134    Objective Loss 0.164134                                        LR 0.000063    Time 0.015593    
2022-01-10 16:36:21,428 - Epoch: [172][  500/  500]    Overall Loss 0.163798    Objective Loss 0.163798    Top1 95.000000    Top5 99.500000    LR 0.000063    Time 0.015616    
2022-01-10 16:36:21,472 - --- validate (epoch=172)-----------
2022-01-10 16:36:21,472 - 10000 samples (100 per mini-batch)
2022-01-10 16:36:22,215 - Epoch: [172][  100/  100]    Loss 1.808637    Top1 62.950000    Top5 87.820000    
2022-01-10 16:36:22,264 - ==> Top1: 62.950    Top5: 87.820    Loss: 1.809

2022-01-10 16:36:22,266 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:36:22,266 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:36:22,289 - 

2022-01-10 16:36:22,289 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:36:23,885 - Epoch: [173][  100/  500]    Overall Loss 0.161487    Objective Loss 0.161487                                        LR 0.000063    Time 0.015946    
2022-01-10 16:36:25,327 - Epoch: [173][  200/  500]    Overall Loss 0.155065    Objective Loss 0.155065                                        LR 0.000063    Time 0.015176    
2022-01-10 16:36:26,764 - Epoch: [173][  300/  500]    Overall Loss 0.158340    Objective Loss 0.158340                                        LR 0.000063    Time 0.014905    
2022-01-10 16:36:28,205 - Epoch: [173][  400/  500]    Overall Loss 0.160809    Objective Loss 0.160809                                        LR 0.000063    Time 0.014780    
2022-01-10 16:36:29,656 - Epoch: [173][  500/  500]    Overall Loss 0.159863    Objective Loss 0.159863    Top1 93.000000    Top5 100.000000    LR 0.000063    Time 0.014725    
2022-01-10 16:36:29,707 - --- validate (epoch=173)-----------
2022-01-10 16:36:29,707 - 10000 samples (100 per mini-batch)
2022-01-10 16:36:30,444 - Epoch: [173][  100/  100]    Loss 1.810977    Top1 62.780000    Top5 87.840000    
2022-01-10 16:36:30,494 - ==> Top1: 62.780    Top5: 87.840    Loss: 1.811

2022-01-10 16:36:30,496 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:36:30,496 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:36:30,518 - 

2022-01-10 16:36:30,518 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:36:32,198 - Epoch: [174][  100/  500]    Overall Loss 0.159247    Objective Loss 0.159247                                        LR 0.000063    Time 0.016791    
2022-01-10 16:36:33,627 - Epoch: [174][  200/  500]    Overall Loss 0.158392    Objective Loss 0.158392                                        LR 0.000063    Time 0.015534    
2022-01-10 16:36:35,071 - Epoch: [174][  300/  500]    Overall Loss 0.158101    Objective Loss 0.158101                                        LR 0.000063    Time 0.015168    
2022-01-10 16:36:36,518 - Epoch: [174][  400/  500]    Overall Loss 0.157974    Objective Loss 0.157974                                        LR 0.000063    Time 0.014990    
2022-01-10 16:36:37,973 - Epoch: [174][  500/  500]    Overall Loss 0.157947    Objective Loss 0.157947    Top1 94.000000    Top5 100.000000    LR 0.000063    Time 0.014901    
2022-01-10 16:36:38,013 - --- validate (epoch=174)-----------
2022-01-10 16:36:38,013 - 10000 samples (100 per mini-batch)
2022-01-10 16:36:38,737 - Epoch: [174][  100/  100]    Loss 1.814726    Top1 62.560000    Top5 87.840000    
2022-01-10 16:36:38,778 - ==> Top1: 62.560    Top5: 87.840    Loss: 1.815

2022-01-10 16:36:38,780 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:36:38,780 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:36:38,808 - 

2022-01-10 16:36:38,808 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:36:40,515 - Epoch: [175][  100/  500]    Overall Loss 0.160947    Objective Loss 0.160947                                        LR 0.000063    Time 0.017058    
2022-01-10 16:36:41,951 - Epoch: [175][  200/  500]    Overall Loss 0.160008    Objective Loss 0.160008                                        LR 0.000063    Time 0.015701    
2022-01-10 16:36:43,385 - Epoch: [175][  300/  500]    Overall Loss 0.161331    Objective Loss 0.161331                                        LR 0.000063    Time 0.015246    
2022-01-10 16:36:44,819 - Epoch: [175][  400/  500]    Overall Loss 0.160557    Objective Loss 0.160557                                        LR 0.000063    Time 0.015016    
2022-01-10 16:36:46,260 - Epoch: [175][  500/  500]    Overall Loss 0.158965    Objective Loss 0.158965    Top1 95.000000    Top5 100.000000    LR 0.000063    Time 0.014894    
2022-01-10 16:36:46,315 - --- validate (epoch=175)-----------
2022-01-10 16:36:46,316 - 10000 samples (100 per mini-batch)
2022-01-10 16:36:47,067 - Epoch: [175][  100/  100]    Loss 1.812510    Top1 62.960000    Top5 87.780000    
2022-01-10 16:36:47,111 - ==> Top1: 62.960    Top5: 87.780    Loss: 1.813

2022-01-10 16:36:47,113 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:36:47,113 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:36:47,136 - 

2022-01-10 16:36:47,136 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:36:48,733 - Epoch: [176][  100/  500]    Overall Loss 0.158008    Objective Loss 0.158008                                        LR 0.000063    Time 0.015949    
2022-01-10 16:36:50,173 - Epoch: [176][  200/  500]    Overall Loss 0.160005    Objective Loss 0.160005                                        LR 0.000063    Time 0.015172    
2022-01-10 16:36:51,611 - Epoch: [176][  300/  500]    Overall Loss 0.160038    Objective Loss 0.160038                                        LR 0.000063    Time 0.014904    
2022-01-10 16:36:53,053 - Epoch: [176][  400/  500]    Overall Loss 0.161422    Objective Loss 0.161422                                        LR 0.000063    Time 0.014782    
2022-01-10 16:36:54,515 - Epoch: [176][  500/  500]    Overall Loss 0.161937    Objective Loss 0.161937    Top1 94.500000    Top5 100.000000    LR 0.000063    Time 0.014748    
2022-01-10 16:36:54,565 - --- validate (epoch=176)-----------
2022-01-10 16:36:54,565 - 10000 samples (100 per mini-batch)
2022-01-10 16:36:55,331 - Epoch: [176][  100/  100]    Loss 1.814738    Top1 62.950000    Top5 87.750000    
2022-01-10 16:36:55,373 - ==> Top1: 62.950    Top5: 87.750    Loss: 1.815

2022-01-10 16:36:55,375 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:36:55,375 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:36:55,402 - 

2022-01-10 16:36:55,402 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:36:57,125 - Epoch: [177][  100/  500]    Overall Loss 0.154776    Objective Loss 0.154776                                        LR 0.000063    Time 0.017214    
2022-01-10 16:36:58,575 - Epoch: [177][  200/  500]    Overall Loss 0.154472    Objective Loss 0.154472                                        LR 0.000063    Time 0.015850    
2022-01-10 16:37:00,153 - Epoch: [177][  300/  500]    Overall Loss 0.154801    Objective Loss 0.154801                                        LR 0.000063    Time 0.015826    
2022-01-10 16:37:01,730 - Epoch: [177][  400/  500]    Overall Loss 0.156197    Objective Loss 0.156197                                        LR 0.000063    Time 0.015809    
2022-01-10 16:37:03,305 - Epoch: [177][  500/  500]    Overall Loss 0.156266    Objective Loss 0.156266    Top1 95.000000    Top5 100.000000    LR 0.000063    Time 0.015795    
2022-01-10 16:37:03,352 - --- validate (epoch=177)-----------
2022-01-10 16:37:03,352 - 10000 samples (100 per mini-batch)
2022-01-10 16:37:04,128 - Epoch: [177][  100/  100]    Loss 1.818872    Top1 62.690000    Top5 87.710000    
2022-01-10 16:37:04,165 - ==> Top1: 62.690    Top5: 87.710    Loss: 1.819

2022-01-10 16:37:04,167 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:37:04,167 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:37:04,186 - 

2022-01-10 16:37:04,186 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:37:05,862 - Epoch: [178][  100/  500]    Overall Loss 0.158665    Objective Loss 0.158665                                        LR 0.000063    Time 0.016747    
2022-01-10 16:37:07,304 - Epoch: [178][  200/  500]    Overall Loss 0.159806    Objective Loss 0.159806                                        LR 0.000063    Time 0.015578    
2022-01-10 16:37:08,745 - Epoch: [178][  300/  500]    Overall Loss 0.160443    Objective Loss 0.160443                                        LR 0.000063    Time 0.015188    
2022-01-10 16:37:10,188 - Epoch: [178][  400/  500]    Overall Loss 0.158988    Objective Loss 0.158988                                        LR 0.000063    Time 0.014990    
2022-01-10 16:37:11,637 - Epoch: [178][  500/  500]    Overall Loss 0.160261    Objective Loss 0.160261    Top1 94.500000    Top5 100.000000    LR 0.000063    Time 0.014889    
2022-01-10 16:37:11,673 - --- validate (epoch=178)-----------
2022-01-10 16:37:11,674 - 10000 samples (100 per mini-batch)
2022-01-10 16:37:12,437 - Epoch: [178][  100/  100]    Loss 1.815039    Top1 63.010000    Top5 87.620000    
2022-01-10 16:37:12,483 - ==> Top1: 63.010    Top5: 87.620    Loss: 1.815

2022-01-10 16:37:12,485 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:37:12,485 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:37:12,513 - 

2022-01-10 16:37:12,513 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:37:14,120 - Epoch: [179][  100/  500]    Overall Loss 0.152563    Objective Loss 0.152563                                        LR 0.000063    Time 0.016052    
2022-01-10 16:37:15,567 - Epoch: [179][  200/  500]    Overall Loss 0.154517    Objective Loss 0.154517                                        LR 0.000063    Time 0.015260    
2022-01-10 16:37:17,018 - Epoch: [179][  300/  500]    Overall Loss 0.154185    Objective Loss 0.154185                                        LR 0.000063    Time 0.015006    
2022-01-10 16:37:18,468 - Epoch: [179][  400/  500]    Overall Loss 0.156635    Objective Loss 0.156635                                        LR 0.000063    Time 0.014879    
2022-01-10 16:37:19,939 - Epoch: [179][  500/  500]    Overall Loss 0.157236    Objective Loss 0.157236    Top1 93.500000    Top5 100.000000    LR 0.000063    Time 0.014842    
2022-01-10 16:37:19,987 - --- validate (epoch=179)-----------
2022-01-10 16:37:19,987 - 10000 samples (100 per mini-batch)
2022-01-10 16:37:20,806 - Epoch: [179][  100/  100]    Loss 1.823199    Top1 62.790000    Top5 87.630000    
2022-01-10 16:37:20,859 - ==> Top1: 62.790    Top5: 87.630    Loss: 1.823

2022-01-10 16:37:20,861 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:37:20,861 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:37:20,888 - 

2022-01-10 16:37:20,889 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:37:22,559 - Epoch: [180][  100/  500]    Overall Loss 0.158378    Objective Loss 0.158378                                        LR 0.000063    Time 0.016687    
2022-01-10 16:37:23,998 - Epoch: [180][  200/  500]    Overall Loss 0.158332    Objective Loss 0.158332                                        LR 0.000063    Time 0.015537    
2022-01-10 16:37:25,436 - Epoch: [180][  300/  500]    Overall Loss 0.156515    Objective Loss 0.156515                                        LR 0.000063    Time 0.015149    
2022-01-10 16:37:26,875 - Epoch: [180][  400/  500]    Overall Loss 0.155724    Objective Loss 0.155724                                        LR 0.000063    Time 0.014957    
2022-01-10 16:37:28,320 - Epoch: [180][  500/  500]    Overall Loss 0.156060    Objective Loss 0.156060    Top1 93.500000    Top5 100.000000    LR 0.000063    Time 0.014854    
2022-01-10 16:37:28,363 - --- validate (epoch=180)-----------
2022-01-10 16:37:28,363 - 10000 samples (100 per mini-batch)
2022-01-10 16:37:29,100 - Epoch: [180][  100/  100]    Loss 1.823873    Top1 62.810000    Top5 87.620000    
2022-01-10 16:37:29,138 - ==> Top1: 62.810    Top5: 87.620    Loss: 1.824

2022-01-10 16:37:29,140 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:37:29,140 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:37:29,168 - 

2022-01-10 16:37:29,168 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:37:30,847 - Epoch: [181][  100/  500]    Overall Loss 0.155757    Objective Loss 0.155757                                        LR 0.000063    Time 0.016781    
2022-01-10 16:37:32,286 - Epoch: [181][  200/  500]    Overall Loss 0.154925    Objective Loss 0.154925                                        LR 0.000063    Time 0.015578    
2022-01-10 16:37:33,729 - Epoch: [181][  300/  500]    Overall Loss 0.154672    Objective Loss 0.154672                                        LR 0.000063    Time 0.015193    
2022-01-10 16:37:35,170 - Epoch: [181][  400/  500]    Overall Loss 0.154137    Objective Loss 0.154137                                        LR 0.000063    Time 0.014998    
2022-01-10 16:37:36,623 - Epoch: [181][  500/  500]    Overall Loss 0.154582    Objective Loss 0.154582    Top1 98.500000    Top5 100.000000    LR 0.000063    Time 0.014902    
2022-01-10 16:37:36,669 - --- validate (epoch=181)-----------
2022-01-10 16:37:36,669 - 10000 samples (100 per mini-batch)
2022-01-10 16:37:37,415 - Epoch: [181][  100/  100]    Loss 1.826786    Top1 63.030000    Top5 87.690000    
2022-01-10 16:37:37,459 - ==> Top1: 63.030    Top5: 87.690    Loss: 1.827

2022-01-10 16:37:37,461 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:37:37,461 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:37:37,489 - 

2022-01-10 16:37:37,489 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:37:39,094 - Epoch: [182][  100/  500]    Overall Loss 0.162377    Objective Loss 0.162377                                        LR 0.000063    Time 0.016033    
2022-01-10 16:37:40,632 - Epoch: [182][  200/  500]    Overall Loss 0.155685    Objective Loss 0.155685                                        LR 0.000063    Time 0.015702    
2022-01-10 16:37:42,108 - Epoch: [182][  300/  500]    Overall Loss 0.155953    Objective Loss 0.155953                                        LR 0.000063    Time 0.015386    
2022-01-10 16:37:43,554 - Epoch: [182][  400/  500]    Overall Loss 0.155574    Objective Loss 0.155574                                        LR 0.000063    Time 0.015152    
2022-01-10 16:37:45,008 - Epoch: [182][  500/  500]    Overall Loss 0.154741    Objective Loss 0.154741    Top1 95.000000    Top5 100.000000    LR 0.000063    Time 0.015028    
2022-01-10 16:37:45,061 - --- validate (epoch=182)-----------
2022-01-10 16:37:45,062 - 10000 samples (100 per mini-batch)
2022-01-10 16:37:45,842 - Epoch: [182][  100/  100]    Loss 1.825618    Top1 62.860000    Top5 87.690000    
2022-01-10 16:37:45,886 - ==> Top1: 62.860    Top5: 87.690    Loss: 1.826

2022-01-10 16:37:45,888 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:37:45,888 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:37:45,916 - 

2022-01-10 16:37:45,916 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:37:47,618 - Epoch: [183][  100/  500]    Overall Loss 0.155739    Objective Loss 0.155739                                        LR 0.000063    Time 0.017008    
2022-01-10 16:37:49,065 - Epoch: [183][  200/  500]    Overall Loss 0.156987    Objective Loss 0.156987                                        LR 0.000063    Time 0.015734    
2022-01-10 16:37:50,509 - Epoch: [183][  300/  500]    Overall Loss 0.155766    Objective Loss 0.155766                                        LR 0.000063    Time 0.015298    
2022-01-10 16:37:51,957 - Epoch: [183][  400/  500]    Overall Loss 0.156645    Objective Loss 0.156645                                        LR 0.000063    Time 0.015093    
2022-01-10 16:37:53,414 - Epoch: [183][  500/  500]    Overall Loss 0.156957    Objective Loss 0.156957    Top1 97.500000    Top5 100.000000    LR 0.000063    Time 0.014987    
2022-01-10 16:37:53,452 - --- validate (epoch=183)-----------
2022-01-10 16:37:53,452 - 10000 samples (100 per mini-batch)
2022-01-10 16:37:54,249 - Epoch: [183][  100/  100]    Loss 1.819118    Top1 62.880000    Top5 87.690000    
2022-01-10 16:37:54,294 - ==> Top1: 62.880    Top5: 87.690    Loss: 1.819

2022-01-10 16:37:54,296 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:37:54,296 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:37:54,324 - 

2022-01-10 16:37:54,324 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:37:56,133 - Epoch: [184][  100/  500]    Overall Loss 0.154543    Objective Loss 0.154543                                        LR 0.000063    Time 0.018078    
2022-01-10 16:37:57,673 - Epoch: [184][  200/  500]    Overall Loss 0.150742    Objective Loss 0.150742                                        LR 0.000063    Time 0.016732    
2022-01-10 16:37:59,086 - Epoch: [184][  300/  500]    Overall Loss 0.151188    Objective Loss 0.151188                                        LR 0.000063    Time 0.015864    
2022-01-10 16:38:00,553 - Epoch: [184][  400/  500]    Overall Loss 0.154190    Objective Loss 0.154190                                        LR 0.000063    Time 0.015564    
2022-01-10 16:38:01,987 - Epoch: [184][  500/  500]    Overall Loss 0.153955    Objective Loss 0.153955    Top1 96.000000    Top5 100.000000    LR 0.000063    Time 0.015316    
2022-01-10 16:38:02,038 - --- validate (epoch=184)-----------
2022-01-10 16:38:02,038 - 10000 samples (100 per mini-batch)
2022-01-10 16:38:02,780 - Epoch: [184][  100/  100]    Loss 1.823443    Top1 63.000000    Top5 87.800000    
2022-01-10 16:38:02,831 - ==> Top1: 63.000    Top5: 87.800    Loss: 1.823

2022-01-10 16:38:02,833 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:38:02,833 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:38:02,861 - 

2022-01-10 16:38:02,861 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:38:04,477 - Epoch: [185][  100/  500]    Overall Loss 0.154953    Objective Loss 0.154953                                        LR 0.000063    Time 0.016143    
2022-01-10 16:38:05,937 - Epoch: [185][  200/  500]    Overall Loss 0.155082    Objective Loss 0.155082                                        LR 0.000063    Time 0.015368    
2022-01-10 16:38:07,514 - Epoch: [185][  300/  500]    Overall Loss 0.154428    Objective Loss 0.154428                                        LR 0.000063    Time 0.015498    
2022-01-10 16:38:09,066 - Epoch: [185][  400/  500]    Overall Loss 0.153573    Objective Loss 0.153573                                        LR 0.000063    Time 0.015502    
2022-01-10 16:38:10,512 - Epoch: [185][  500/  500]    Overall Loss 0.153828    Objective Loss 0.153828    Top1 94.000000    Top5 99.500000    LR 0.000063    Time 0.015291    
2022-01-10 16:38:10,558 - --- validate (epoch=185)-----------
2022-01-10 16:38:10,558 - 10000 samples (100 per mini-batch)
2022-01-10 16:38:11,304 - Epoch: [185][  100/  100]    Loss 1.828072    Top1 63.160000    Top5 87.610000    
2022-01-10 16:38:11,347 - ==> Top1: 63.160    Top5: 87.610    Loss: 1.828

2022-01-10 16:38:11,350 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:38:11,350 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:38:11,377 - 

2022-01-10 16:38:11,377 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:38:13,043 - Epoch: [186][  100/  500]    Overall Loss 0.156074    Objective Loss 0.156074                                        LR 0.000063    Time 0.016653    
2022-01-10 16:38:14,467 - Epoch: [186][  200/  500]    Overall Loss 0.156387    Objective Loss 0.156387                                        LR 0.000063    Time 0.015439    
2022-01-10 16:38:15,885 - Epoch: [186][  300/  500]    Overall Loss 0.154163    Objective Loss 0.154163                                        LR 0.000063    Time 0.015018    
2022-01-10 16:38:17,302 - Epoch: [186][  400/  500]    Overall Loss 0.154423    Objective Loss 0.154423                                        LR 0.000063    Time 0.014803    
2022-01-10 16:38:18,717 - Epoch: [186][  500/  500]    Overall Loss 0.154826    Objective Loss 0.154826    Top1 94.000000    Top5 99.500000    LR 0.000063    Time 0.014672    
2022-01-10 16:38:18,756 - --- validate (epoch=186)-----------
2022-01-10 16:38:18,756 - 10000 samples (100 per mini-batch)
2022-01-10 16:38:19,496 - Epoch: [186][  100/  100]    Loss 1.828803    Top1 62.890000    Top5 87.820000    
2022-01-10 16:38:19,541 - ==> Top1: 62.890    Top5: 87.820    Loss: 1.829

2022-01-10 16:38:19,543 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:38:19,543 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:38:19,570 - 

2022-01-10 16:38:19,570 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:38:21,328 - Epoch: [187][  100/  500]    Overall Loss 0.148129    Objective Loss 0.148129                                        LR 0.000063    Time 0.017558    
2022-01-10 16:38:22,888 - Epoch: [187][  200/  500]    Overall Loss 0.153666    Objective Loss 0.153666                                        LR 0.000063    Time 0.016579    
2022-01-10 16:38:24,452 - Epoch: [187][  300/  500]    Overall Loss 0.153684    Objective Loss 0.153684                                        LR 0.000063    Time 0.016261    
2022-01-10 16:38:26,016 - Epoch: [187][  400/  500]    Overall Loss 0.153261    Objective Loss 0.153261                                        LR 0.000063    Time 0.016103    
2022-01-10 16:38:27,577 - Epoch: [187][  500/  500]    Overall Loss 0.151828    Objective Loss 0.151828    Top1 94.000000    Top5 99.500000    LR 0.000063    Time 0.016004    
2022-01-10 16:38:27,626 - --- validate (epoch=187)-----------
2022-01-10 16:38:27,626 - 10000 samples (100 per mini-batch)
2022-01-10 16:38:28,379 - Epoch: [187][  100/  100]    Loss 1.830037    Top1 62.940000    Top5 87.710000    
2022-01-10 16:38:28,429 - ==> Top1: 62.940    Top5: 87.710    Loss: 1.830

2022-01-10 16:38:28,431 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:38:28,431 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:38:28,458 - 

2022-01-10 16:38:28,459 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:38:30,067 - Epoch: [188][  100/  500]    Overall Loss 0.157105    Objective Loss 0.157105                                        LR 0.000063    Time 0.016074    
2022-01-10 16:38:31,528 - Epoch: [188][  200/  500]    Overall Loss 0.158205    Objective Loss 0.158205                                        LR 0.000063    Time 0.015337    
2022-01-10 16:38:32,968 - Epoch: [188][  300/  500]    Overall Loss 0.155860    Objective Loss 0.155860                                        LR 0.000063    Time 0.015020    
2022-01-10 16:38:34,383 - Epoch: [188][  400/  500]    Overall Loss 0.151776    Objective Loss 0.151776                                        LR 0.000063    Time 0.014801    
2022-01-10 16:38:35,803 - Epoch: [188][  500/  500]    Overall Loss 0.153349    Objective Loss 0.153349    Top1 96.500000    Top5 100.000000    LR 0.000063    Time 0.014679    
2022-01-10 16:38:35,846 - --- validate (epoch=188)-----------
2022-01-10 16:38:35,847 - 10000 samples (100 per mini-batch)
2022-01-10 16:38:36,591 - Epoch: [188][  100/  100]    Loss 1.828827    Top1 63.060000    Top5 87.690000    
2022-01-10 16:38:36,646 - ==> Top1: 63.060    Top5: 87.690    Loss: 1.829

2022-01-10 16:38:36,648 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:38:36,648 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:38:36,675 - 

2022-01-10 16:38:36,676 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:38:38,367 - Epoch: [189][  100/  500]    Overall Loss 0.152578    Objective Loss 0.152578                                        LR 0.000063    Time 0.016899    
2022-01-10 16:38:39,837 - Epoch: [189][  200/  500]    Overall Loss 0.154535    Objective Loss 0.154535                                        LR 0.000063    Time 0.015797    
2022-01-10 16:38:41,308 - Epoch: [189][  300/  500]    Overall Loss 0.152984    Objective Loss 0.152984                                        LR 0.000063    Time 0.015430    
2022-01-10 16:38:42,753 - Epoch: [189][  400/  500]    Overall Loss 0.151890    Objective Loss 0.151890                                        LR 0.000063    Time 0.015183    
2022-01-10 16:38:44,205 - Epoch: [189][  500/  500]    Overall Loss 0.152490    Objective Loss 0.152490    Top1 98.000000    Top5 100.000000    LR 0.000063    Time 0.015050    
2022-01-10 16:38:44,250 - --- validate (epoch=189)-----------
2022-01-10 16:38:44,251 - 10000 samples (100 per mini-batch)
2022-01-10 16:38:45,000 - Epoch: [189][  100/  100]    Loss 1.833618    Top1 63.020000    Top5 87.710000    
2022-01-10 16:38:45,053 - ==> Top1: 63.020    Top5: 87.710    Loss: 1.834

2022-01-10 16:38:45,055 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:38:45,055 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:38:45,083 - 

2022-01-10 16:38:45,083 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:38:46,794 - Epoch: [190][  100/  500]    Overall Loss 0.147403    Objective Loss 0.147403                                        LR 0.000063    Time 0.017093    
2022-01-10 16:38:48,252 - Epoch: [190][  200/  500]    Overall Loss 0.150974    Objective Loss 0.150974                                        LR 0.000063    Time 0.015835    
2022-01-10 16:38:49,726 - Epoch: [190][  300/  500]    Overall Loss 0.151656    Objective Loss 0.151656                                        LR 0.000063    Time 0.015465    
2022-01-10 16:38:51,170 - Epoch: [190][  400/  500]    Overall Loss 0.153672    Objective Loss 0.153672                                        LR 0.000063    Time 0.015208    
2022-01-10 16:38:52,617 - Epoch: [190][  500/  500]    Overall Loss 0.155182    Objective Loss 0.155182    Top1 95.500000    Top5 100.000000    LR 0.000063    Time 0.015059    
2022-01-10 16:38:52,658 - --- validate (epoch=190)-----------
2022-01-10 16:38:52,658 - 10000 samples (100 per mini-batch)
2022-01-10 16:38:53,414 - Epoch: [190][  100/  100]    Loss 1.832609    Top1 62.690000    Top5 87.550000    
2022-01-10 16:38:53,460 - ==> Top1: 62.690    Top5: 87.550    Loss: 1.833

2022-01-10 16:38:53,462 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:38:53,462 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:38:53,489 - 

2022-01-10 16:38:53,490 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:38:55,101 - Epoch: [191][  100/  500]    Overall Loss 0.155768    Objective Loss 0.155768                                        LR 0.000063    Time 0.016096    
2022-01-10 16:38:56,540 - Epoch: [191][  200/  500]    Overall Loss 0.150849    Objective Loss 0.150849                                        LR 0.000063    Time 0.015243    
2022-01-10 16:38:57,978 - Epoch: [191][  300/  500]    Overall Loss 0.149830    Objective Loss 0.149830                                        LR 0.000063    Time 0.014951    
2022-01-10 16:38:59,416 - Epoch: [191][  400/  500]    Overall Loss 0.150107    Objective Loss 0.150107                                        LR 0.000063    Time 0.014807    
2022-01-10 16:39:00,865 - Epoch: [191][  500/  500]    Overall Loss 0.150374    Objective Loss 0.150374    Top1 94.500000    Top5 100.000000    LR 0.000063    Time 0.014741    
2022-01-10 16:39:00,908 - --- validate (epoch=191)-----------
2022-01-10 16:39:00,908 - 10000 samples (100 per mini-batch)
2022-01-10 16:39:01,653 - Epoch: [191][  100/  100]    Loss 1.840165    Top1 62.670000    Top5 87.550000    
2022-01-10 16:39:01,697 - ==> Top1: 62.670    Top5: 87.550    Loss: 1.840

2022-01-10 16:39:01,699 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:39:01,699 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:39:01,727 - 

2022-01-10 16:39:01,727 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:39:03,415 - Epoch: [192][  100/  500]    Overall Loss 0.152823    Objective Loss 0.152823                                        LR 0.000063    Time 0.016864    
2022-01-10 16:39:04,862 - Epoch: [192][  200/  500]    Overall Loss 0.152269    Objective Loss 0.152269                                        LR 0.000063    Time 0.015665    
2022-01-10 16:39:06,306 - Epoch: [192][  300/  500]    Overall Loss 0.153602    Objective Loss 0.153602                                        LR 0.000063    Time 0.015252    
2022-01-10 16:39:07,742 - Epoch: [192][  400/  500]    Overall Loss 0.152555    Objective Loss 0.152555                                        LR 0.000063    Time 0.015028    
2022-01-10 16:39:09,188 - Epoch: [192][  500/  500]    Overall Loss 0.152640    Objective Loss 0.152640    Top1 97.000000    Top5 100.000000    LR 0.000063    Time 0.014912    
2022-01-10 16:39:09,241 - --- validate (epoch=192)-----------
2022-01-10 16:39:09,242 - 10000 samples (100 per mini-batch)
2022-01-10 16:39:09,969 - Epoch: [192][  100/  100]    Loss 1.831926    Top1 63.120000    Top5 87.690000    
2022-01-10 16:39:10,014 - ==> Top1: 63.120    Top5: 87.690    Loss: 1.832

2022-01-10 16:39:10,015 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:39:10,016 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:39:10,043 - 

2022-01-10 16:39:10,043 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:39:11,654 - Epoch: [193][  100/  500]    Overall Loss 0.147806    Objective Loss 0.147806                                        LR 0.000063    Time 0.016092    
2022-01-10 16:39:13,101 - Epoch: [193][  200/  500]    Overall Loss 0.151228    Objective Loss 0.151228                                        LR 0.000063    Time 0.015279    
2022-01-10 16:39:14,547 - Epoch: [193][  300/  500]    Overall Loss 0.150178    Objective Loss 0.150178                                        LR 0.000063    Time 0.015001    
2022-01-10 16:39:15,991 - Epoch: [193][  400/  500]    Overall Loss 0.153125    Objective Loss 0.153125                                        LR 0.000063    Time 0.014860    
2022-01-10 16:39:17,448 - Epoch: [193][  500/  500]    Overall Loss 0.152508    Objective Loss 0.152508    Top1 97.500000    Top5 100.000000    LR 0.000063    Time 0.014801    
2022-01-10 16:39:17,495 - --- validate (epoch=193)-----------
2022-01-10 16:39:17,495 - 10000 samples (100 per mini-batch)
2022-01-10 16:39:18,285 - Epoch: [193][  100/  100]    Loss 1.834738    Top1 62.950000    Top5 87.520000    
2022-01-10 16:39:18,339 - ==> Top1: 62.950    Top5: 87.520    Loss: 1.835

2022-01-10 16:39:18,341 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:39:18,341 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:39:18,368 - 

2022-01-10 16:39:18,368 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:39:19,977 - Epoch: [194][  100/  500]    Overall Loss 0.148123    Objective Loss 0.148123                                        LR 0.000063    Time 0.016072    
2022-01-10 16:39:21,416 - Epoch: [194][  200/  500]    Overall Loss 0.153591    Objective Loss 0.153591                                        LR 0.000063    Time 0.015227    
2022-01-10 16:39:22,854 - Epoch: [194][  300/  500]    Overall Loss 0.151769    Objective Loss 0.151769                                        LR 0.000063    Time 0.014940    
2022-01-10 16:39:24,290 - Epoch: [194][  400/  500]    Overall Loss 0.151836    Objective Loss 0.151836                                        LR 0.000063    Time 0.014793    
2022-01-10 16:39:25,815 - Epoch: [194][  500/  500]    Overall Loss 0.151508    Objective Loss 0.151508    Top1 96.500000    Top5 100.000000    LR 0.000063    Time 0.014885    
2022-01-10 16:39:25,856 - --- validate (epoch=194)-----------
2022-01-10 16:39:25,856 - 10000 samples (100 per mini-batch)
2022-01-10 16:39:26,586 - Epoch: [194][  100/  100]    Loss 1.845167    Top1 62.610000    Top5 87.490000    
2022-01-10 16:39:26,631 - ==> Top1: 62.610    Top5: 87.490    Loss: 1.845

2022-01-10 16:39:26,632 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:39:26,633 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:39:26,660 - 

2022-01-10 16:39:26,660 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:39:28,336 - Epoch: [195][  100/  500]    Overall Loss 0.150682    Objective Loss 0.150682                                        LR 0.000063    Time 0.016741    
2022-01-10 16:39:29,790 - Epoch: [195][  200/  500]    Overall Loss 0.149739    Objective Loss 0.149739                                        LR 0.000063    Time 0.015637    
2022-01-10 16:39:31,262 - Epoch: [195][  300/  500]    Overall Loss 0.148092    Objective Loss 0.148092                                        LR 0.000063    Time 0.015330    
2022-01-10 16:39:32,733 - Epoch: [195][  400/  500]    Overall Loss 0.149135    Objective Loss 0.149135                                        LR 0.000063    Time 0.015174    
2022-01-10 16:39:34,181 - Epoch: [195][  500/  500]    Overall Loss 0.152038    Objective Loss 0.152038    Top1 95.000000    Top5 100.000000    LR 0.000063    Time 0.015032    
2022-01-10 16:39:34,226 - --- validate (epoch=195)-----------
2022-01-10 16:39:34,227 - 10000 samples (100 per mini-batch)
2022-01-10 16:39:34,981 - Epoch: [195][  100/  100]    Loss 1.838347    Top1 62.940000    Top5 87.620000    
2022-01-10 16:39:35,024 - ==> Top1: 62.940    Top5: 87.620    Loss: 1.838

2022-01-10 16:39:35,027 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:39:35,027 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:39:35,048 - 

2022-01-10 16:39:35,048 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:39:36,620 - Epoch: [196][  100/  500]    Overall Loss 0.143541    Objective Loss 0.143541                                        LR 0.000063    Time 0.015704    
2022-01-10 16:39:38,038 - Epoch: [196][  200/  500]    Overall Loss 0.149042    Objective Loss 0.149042                                        LR 0.000063    Time 0.014939    
2022-01-10 16:39:39,462 - Epoch: [196][  300/  500]    Overall Loss 0.150175    Objective Loss 0.150175                                        LR 0.000063    Time 0.014705    
2022-01-10 16:39:40,888 - Epoch: [196][  400/  500]    Overall Loss 0.150088    Objective Loss 0.150088                                        LR 0.000063    Time 0.014591    
2022-01-10 16:39:42,322 - Epoch: [196][  500/  500]    Overall Loss 0.152186    Objective Loss 0.152186    Top1 95.500000    Top5 100.000000    LR 0.000063    Time 0.014539    
2022-01-10 16:39:42,364 - --- validate (epoch=196)-----------
2022-01-10 16:39:42,364 - 10000 samples (100 per mini-batch)
2022-01-10 16:39:43,197 - Epoch: [196][  100/  100]    Loss 1.844133    Top1 62.530000    Top5 87.560000    
2022-01-10 16:39:43,241 - ==> Top1: 62.530    Top5: 87.560    Loss: 1.844

2022-01-10 16:39:43,243 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:39:43,243 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:39:43,270 - 

2022-01-10 16:39:43,271 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:39:44,856 - Epoch: [197][  100/  500]    Overall Loss 0.149545    Objective Loss 0.149545                                        LR 0.000063    Time 0.015847    
2022-01-10 16:39:46,288 - Epoch: [197][  200/  500]    Overall Loss 0.154053    Objective Loss 0.154053                                        LR 0.000063    Time 0.015074    
2022-01-10 16:39:47,722 - Epoch: [197][  300/  500]    Overall Loss 0.152083    Objective Loss 0.152083                                        LR 0.000063    Time 0.014829    
2022-01-10 16:39:49,152 - Epoch: [197][  400/  500]    Overall Loss 0.152265    Objective Loss 0.152265                                        LR 0.000063    Time 0.014694    
2022-01-10 16:39:50,591 - Epoch: [197][  500/  500]    Overall Loss 0.150683    Objective Loss 0.150683    Top1 97.500000    Top5 100.000000    LR 0.000063    Time 0.014632    
2022-01-10 16:39:50,629 - --- validate (epoch=197)-----------
2022-01-10 16:39:50,629 - 10000 samples (100 per mini-batch)
2022-01-10 16:39:51,366 - Epoch: [197][  100/  100]    Loss 1.848491    Top1 62.650000    Top5 87.690000    
2022-01-10 16:39:51,410 - ==> Top1: 62.650    Top5: 87.690    Loss: 1.848

2022-01-10 16:39:51,412 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:39:51,412 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:39:51,440 - 

2022-01-10 16:39:51,440 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:39:53,112 - Epoch: [198][  100/  500]    Overall Loss 0.146290    Objective Loss 0.146290                                        LR 0.000063    Time 0.016706    
2022-01-10 16:39:54,545 - Epoch: [198][  200/  500]    Overall Loss 0.146186    Objective Loss 0.146186                                        LR 0.000063    Time 0.015517    
2022-01-10 16:39:55,994 - Epoch: [198][  300/  500]    Overall Loss 0.146043    Objective Loss 0.146043                                        LR 0.000063    Time 0.015170    
2022-01-10 16:39:57,431 - Epoch: [198][  400/  500]    Overall Loss 0.145389    Objective Loss 0.145389                                        LR 0.000063    Time 0.014969    
2022-01-10 16:39:58,895 - Epoch: [198][  500/  500]    Overall Loss 0.148108    Objective Loss 0.148108    Top1 94.500000    Top5 99.500000    LR 0.000063    Time 0.014901    
2022-01-10 16:39:58,947 - --- validate (epoch=198)-----------
2022-01-10 16:39:58,947 - 10000 samples (100 per mini-batch)
2022-01-10 16:39:59,724 - Epoch: [198][  100/  100]    Loss 1.852176    Top1 62.730000    Top5 87.540000    
2022-01-10 16:39:59,771 - ==> Top1: 62.730    Top5: 87.540    Loss: 1.852

2022-01-10 16:39:59,773 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:39:59,773 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:39:59,800 - 

2022-01-10 16:39:59,801 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:40:01,515 - Epoch: [199][  100/  500]    Overall Loss 0.148567    Objective Loss 0.148567                                        LR 0.000063    Time 0.017128    
2022-01-10 16:40:02,957 - Epoch: [199][  200/  500]    Overall Loss 0.147580    Objective Loss 0.147580                                        LR 0.000063    Time 0.015770    
2022-01-10 16:40:04,391 - Epoch: [199][  300/  500]    Overall Loss 0.147942    Objective Loss 0.147942                                        LR 0.000063    Time 0.015290    
2022-01-10 16:40:05,813 - Epoch: [199][  400/  500]    Overall Loss 0.147643    Objective Loss 0.147643                                        LR 0.000063    Time 0.015021    
2022-01-10 16:40:07,276 - Epoch: [199][  500/  500]    Overall Loss 0.149393    Objective Loss 0.149393    Top1 94.000000    Top5 99.500000    LR 0.000063    Time 0.014942    
2022-01-10 16:40:07,317 - --- validate (epoch=199)-----------
2022-01-10 16:40:07,317 - 10000 samples (100 per mini-batch)
2022-01-10 16:40:08,125 - Epoch: [199][  100/  100]    Loss 1.848627    Top1 62.950000    Top5 87.680000    
2022-01-10 16:40:08,163 - ==> Top1: 62.950    Top5: 87.680    Loss: 1.849

2022-01-10 16:40:08,164 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:40:08,164 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:40:08,191 - 

2022-01-10 16:40:08,191 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:40:09,798 - Epoch: [200][  100/  500]    Overall Loss 0.149477    Objective Loss 0.149477                                        LR 0.000016    Time 0.016049    
2022-01-10 16:40:11,252 - Epoch: [200][  200/  500]    Overall Loss 0.147610    Objective Loss 0.147610                                        LR 0.000016    Time 0.015291    
2022-01-10 16:40:12,679 - Epoch: [200][  300/  500]    Overall Loss 0.149091    Objective Loss 0.149091                                        LR 0.000016    Time 0.014949    
2022-01-10 16:40:14,105 - Epoch: [200][  400/  500]    Overall Loss 0.148785    Objective Loss 0.148785                                        LR 0.000016    Time 0.014775    
2022-01-10 16:40:15,540 - Epoch: [200][  500/  500]    Overall Loss 0.148103    Objective Loss 0.148103    Top1 97.500000    Top5 100.000000    LR 0.000016    Time 0.014687    
2022-01-10 16:40:15,582 - --- validate (epoch=200)-----------
2022-01-10 16:40:15,582 - 10000 samples (100 per mini-batch)
2022-01-10 16:40:16,299 - Epoch: [200][  100/  100]    Loss 1.837481    Top1 63.170000    Top5 87.560000    
2022-01-10 16:40:16,344 - ==> Top1: 63.170    Top5: 87.560    Loss: 1.837

2022-01-10 16:40:16,346 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:40:16,346 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:40:16,373 - 

2022-01-10 16:40:16,373 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:40:18,042 - Epoch: [201][  100/  500]    Overall Loss 0.150351    Objective Loss 0.150351                                        LR 0.000016    Time 0.016669    
2022-01-10 16:40:19,472 - Epoch: [201][  200/  500]    Overall Loss 0.148625    Objective Loss 0.148625                                        LR 0.000016    Time 0.015482    
2022-01-10 16:40:20,904 - Epoch: [201][  300/  500]    Overall Loss 0.145477    Objective Loss 0.145477                                        LR 0.000016    Time 0.015092    
2022-01-10 16:40:22,343 - Epoch: [201][  400/  500]    Overall Loss 0.145383    Objective Loss 0.145383                                        LR 0.000016    Time 0.014915    
2022-01-10 16:40:23,783 - Epoch: [201][  500/  500]    Overall Loss 0.145750    Objective Loss 0.145750    Top1 95.500000    Top5 99.500000    LR 0.000016    Time 0.014810    
2022-01-10 16:40:23,825 - --- validate (epoch=201)-----------
2022-01-10 16:40:23,825 - 10000 samples (100 per mini-batch)
2022-01-10 16:40:24,575 - Epoch: [201][  100/  100]    Loss 1.844564    Top1 63.000000    Top5 87.450000    
2022-01-10 16:40:24,623 - ==> Top1: 63.000    Top5: 87.450    Loss: 1.845

2022-01-10 16:40:24,625 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:40:24,625 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:40:24,652 - 

2022-01-10 16:40:24,653 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:40:26,270 - Epoch: [202][  100/  500]    Overall Loss 0.151519    Objective Loss 0.151519                                        LR 0.000016    Time 0.016165    
2022-01-10 16:40:27,730 - Epoch: [202][  200/  500]    Overall Loss 0.144383    Objective Loss 0.144383                                        LR 0.000016    Time 0.015374    
2022-01-10 16:40:29,159 - Epoch: [202][  300/  500]    Overall Loss 0.144478    Objective Loss 0.144478                                        LR 0.000016    Time 0.015010    
2022-01-10 16:40:30,588 - Epoch: [202][  400/  500]    Overall Loss 0.143964    Objective Loss 0.143964                                        LR 0.000016    Time 0.014829    
2022-01-10 16:40:32,025 - Epoch: [202][  500/  500]    Overall Loss 0.143918    Objective Loss 0.143918    Top1 97.500000    Top5 100.000000    LR 0.000016    Time 0.014735    
2022-01-10 16:40:32,073 - --- validate (epoch=202)-----------
2022-01-10 16:40:32,074 - 10000 samples (100 per mini-batch)
2022-01-10 16:40:32,898 - Epoch: [202][  100/  100]    Loss 1.840876    Top1 63.040000    Top5 87.620000    
2022-01-10 16:40:32,938 - ==> Top1: 63.040    Top5: 87.620    Loss: 1.841

2022-01-10 16:40:32,940 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:40:32,940 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:40:32,968 - 

2022-01-10 16:40:32,968 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:40:34,653 - Epoch: [203][  100/  500]    Overall Loss 0.139932    Objective Loss 0.139932                                        LR 0.000016    Time 0.016831    
2022-01-10 16:40:36,227 - Epoch: [203][  200/  500]    Overall Loss 0.144110    Objective Loss 0.144110                                        LR 0.000016    Time 0.016282    
2022-01-10 16:40:37,804 - Epoch: [203][  300/  500]    Overall Loss 0.144531    Objective Loss 0.144531                                        LR 0.000016    Time 0.016110    
2022-01-10 16:40:39,340 - Epoch: [203][  400/  500]    Overall Loss 0.144857    Objective Loss 0.144857                                        LR 0.000016    Time 0.015919    
2022-01-10 16:40:40,916 - Epoch: [203][  500/  500]    Overall Loss 0.145443    Objective Loss 0.145443    Top1 94.500000    Top5 100.000000    LR 0.000016    Time 0.015886    
2022-01-10 16:40:40,957 - --- validate (epoch=203)-----------
2022-01-10 16:40:40,957 - 10000 samples (100 per mini-batch)
2022-01-10 16:40:41,707 - Epoch: [203][  100/  100]    Loss 1.847631    Top1 62.890000    Top5 87.460000    
2022-01-10 16:40:41,744 - ==> Top1: 62.890    Top5: 87.460    Loss: 1.848

2022-01-10 16:40:41,746 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:40:41,747 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:40:41,774 - 

2022-01-10 16:40:41,774 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:40:43,532 - Epoch: [204][  100/  500]    Overall Loss 0.150865    Objective Loss 0.150865                                        LR 0.000016    Time 0.017569    
2022-01-10 16:40:45,084 - Epoch: [204][  200/  500]    Overall Loss 0.145518    Objective Loss 0.145518                                        LR 0.000016    Time 0.016541    
2022-01-10 16:40:46,642 - Epoch: [204][  300/  500]    Overall Loss 0.144136    Objective Loss 0.144136                                        LR 0.000016    Time 0.016217    
2022-01-10 16:40:48,201 - Epoch: [204][  400/  500]    Overall Loss 0.144492    Objective Loss 0.144492                                        LR 0.000016    Time 0.016058    
2022-01-10 16:40:49,753 - Epoch: [204][  500/  500]    Overall Loss 0.143652    Objective Loss 0.143652    Top1 95.500000    Top5 100.000000    LR 0.000016    Time 0.015949    
2022-01-10 16:40:49,793 - --- validate (epoch=204)-----------
2022-01-10 16:40:49,793 - 10000 samples (100 per mini-batch)
2022-01-10 16:40:50,531 - Epoch: [204][  100/  100]    Loss 1.845355    Top1 62.800000    Top5 87.470000    
2022-01-10 16:40:50,576 - ==> Top1: 62.800    Top5: 87.470    Loss: 1.845

2022-01-10 16:40:50,578 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:40:50,578 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:40:50,606 - 

2022-01-10 16:40:50,606 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:40:52,184 - Epoch: [205][  100/  500]    Overall Loss 0.143609    Objective Loss 0.143609                                        LR 0.000016    Time 0.015762    
2022-01-10 16:40:53,604 - Epoch: [205][  200/  500]    Overall Loss 0.144165    Objective Loss 0.144165                                        LR 0.000016    Time 0.014979    
2022-01-10 16:40:55,039 - Epoch: [205][  300/  500]    Overall Loss 0.141319    Objective Loss 0.141319                                        LR 0.000016    Time 0.014765    
2022-01-10 16:40:56,477 - Epoch: [205][  400/  500]    Overall Loss 0.141948    Objective Loss 0.141948                                        LR 0.000016    Time 0.014667    
2022-01-10 16:40:57,923 - Epoch: [205][  500/  500]    Overall Loss 0.142401    Objective Loss 0.142401    Top1 96.000000    Top5 100.000000    LR 0.000016    Time 0.014625    
2022-01-10 16:40:57,968 - --- validate (epoch=205)-----------
2022-01-10 16:40:57,968 - 10000 samples (100 per mini-batch)
2022-01-10 16:40:58,788 - Epoch: [205][  100/  100]    Loss 1.847489    Top1 62.990000    Top5 87.500000    
2022-01-10 16:40:58,830 - ==> Top1: 62.990    Top5: 87.500    Loss: 1.847

2022-01-10 16:40:58,832 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:40:58,832 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:40:58,855 - 

2022-01-10 16:40:58,855 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:41:00,444 - Epoch: [206][  100/  500]    Overall Loss 0.143265    Objective Loss 0.143265                                        LR 0.000016    Time 0.015880    
2022-01-10 16:41:01,875 - Epoch: [206][  200/  500]    Overall Loss 0.139174    Objective Loss 0.139174                                        LR 0.000016    Time 0.015091    
2022-01-10 16:41:03,307 - Epoch: [206][  300/  500]    Overall Loss 0.142762    Objective Loss 0.142762                                        LR 0.000016    Time 0.014832    
2022-01-10 16:41:04,751 - Epoch: [206][  400/  500]    Overall Loss 0.142636    Objective Loss 0.142636                                        LR 0.000016    Time 0.014732    
2022-01-10 16:41:06,208 - Epoch: [206][  500/  500]    Overall Loss 0.143283    Objective Loss 0.143283    Top1 98.500000    Top5 99.500000    LR 0.000016    Time 0.014698    
2022-01-10 16:41:06,253 - --- validate (epoch=206)-----------
2022-01-10 16:41:06,254 - 10000 samples (100 per mini-batch)
2022-01-10 16:41:07,011 - Epoch: [206][  100/  100]    Loss 1.843137    Top1 62.760000    Top5 87.600000    
2022-01-10 16:41:07,060 - ==> Top1: 62.760    Top5: 87.600    Loss: 1.843

2022-01-10 16:41:07,062 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:41:07,062 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:41:07,090 - 

2022-01-10 16:41:07,090 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:41:08,763 - Epoch: [207][  100/  500]    Overall Loss 0.146960    Objective Loss 0.146960                                        LR 0.000016    Time 0.016712    
2022-01-10 16:41:10,184 - Epoch: [207][  200/  500]    Overall Loss 0.149590    Objective Loss 0.149590                                        LR 0.000016    Time 0.015460    
2022-01-10 16:41:11,610 - Epoch: [207][  300/  500]    Overall Loss 0.146679    Objective Loss 0.146679                                        LR 0.000016    Time 0.015057    
2022-01-10 16:41:13,034 - Epoch: [207][  400/  500]    Overall Loss 0.145416    Objective Loss 0.145416                                        LR 0.000016    Time 0.014849    
2022-01-10 16:41:14,466 - Epoch: [207][  500/  500]    Overall Loss 0.144995    Objective Loss 0.144995    Top1 97.500000    Top5 100.000000    LR 0.000016    Time 0.014744    
2022-01-10 16:41:14,513 - --- validate (epoch=207)-----------
2022-01-10 16:41:14,514 - 10000 samples (100 per mini-batch)
2022-01-10 16:41:15,313 - Epoch: [207][  100/  100]    Loss 1.846428    Top1 62.940000    Top5 87.660000    
2022-01-10 16:41:15,357 - ==> Top1: 62.940    Top5: 87.660    Loss: 1.846

2022-01-10 16:41:15,359 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:41:15,359 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:41:15,386 - 

2022-01-10 16:41:15,386 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:41:16,974 - Epoch: [208][  100/  500]    Overall Loss 0.147833    Objective Loss 0.147833                                        LR 0.000016    Time 0.015862    
2022-01-10 16:41:18,408 - Epoch: [208][  200/  500]    Overall Loss 0.144310    Objective Loss 0.144310                                        LR 0.000016    Time 0.015098    
2022-01-10 16:41:19,840 - Epoch: [208][  300/  500]    Overall Loss 0.141759    Objective Loss 0.141759                                        LR 0.000016    Time 0.014837    
2022-01-10 16:41:21,272 - Epoch: [208][  400/  500]    Overall Loss 0.142009    Objective Loss 0.142009                                        LR 0.000016    Time 0.014705    
2022-01-10 16:41:22,714 - Epoch: [208][  500/  500]    Overall Loss 0.141492    Objective Loss 0.141492    Top1 95.500000    Top5 100.000000    LR 0.000016    Time 0.014646    
2022-01-10 16:41:22,764 - --- validate (epoch=208)-----------
2022-01-10 16:41:22,764 - 10000 samples (100 per mini-batch)
2022-01-10 16:41:23,489 - Epoch: [208][  100/  100]    Loss 1.849986    Top1 62.790000    Top5 87.650000    
2022-01-10 16:41:23,540 - ==> Top1: 62.790    Top5: 87.650    Loss: 1.850

2022-01-10 16:41:23,542 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:41:23,542 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:41:23,569 - 

2022-01-10 16:41:23,569 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:41:25,270 - Epoch: [209][  100/  500]    Overall Loss 0.136026    Objective Loss 0.136026                                        LR 0.000016    Time 0.016994    
2022-01-10 16:41:26,670 - Epoch: [209][  200/  500]    Overall Loss 0.137222    Objective Loss 0.137222                                        LR 0.000016    Time 0.015496    
2022-01-10 16:41:28,076 - Epoch: [209][  300/  500]    Overall Loss 0.140130    Objective Loss 0.140130                                        LR 0.000016    Time 0.015013    
2022-01-10 16:41:29,633 - Epoch: [209][  400/  500]    Overall Loss 0.140573    Objective Loss 0.140573                                        LR 0.000016    Time 0.015150    
2022-01-10 16:41:31,154 - Epoch: [209][  500/  500]    Overall Loss 0.140885    Objective Loss 0.140885    Top1 94.000000    Top5 99.500000    LR 0.000016    Time 0.015162    
2022-01-10 16:41:31,208 - --- validate (epoch=209)-----------
2022-01-10 16:41:31,208 - 10000 samples (100 per mini-batch)
2022-01-10 16:41:32,041 - Epoch: [209][  100/  100]    Loss 1.847235    Top1 62.970000    Top5 87.550000    
2022-01-10 16:41:32,081 - ==> Top1: 62.970    Top5: 87.550    Loss: 1.847

2022-01-10 16:41:32,083 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:41:32,083 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:41:32,102 - 

2022-01-10 16:41:32,102 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:41:33,817 - Epoch: [210][  100/  500]    Overall Loss 0.148738    Objective Loss 0.148738                                        LR 0.000016    Time 0.017131    
2022-01-10 16:41:35,261 - Epoch: [210][  200/  500]    Overall Loss 0.148674    Objective Loss 0.148674                                        LR 0.000016    Time 0.015781    
2022-01-10 16:41:36,710 - Epoch: [210][  300/  500]    Overall Loss 0.149696    Objective Loss 0.149696                                        LR 0.000016    Time 0.015351    
2022-01-10 16:41:38,161 - Epoch: [210][  400/  500]    Overall Loss 0.144545    Objective Loss 0.144545                                        LR 0.000016    Time 0.015136    
2022-01-10 16:41:39,616 - Epoch: [210][  500/  500]    Overall Loss 0.143940    Objective Loss 0.143940    Top1 96.000000    Top5 99.500000    LR 0.000016    Time 0.015019    
2022-01-10 16:41:39,666 - --- validate (epoch=210)-----------
2022-01-10 16:41:39,666 - 10000 samples (100 per mini-batch)
2022-01-10 16:41:40,401 - Epoch: [210][  100/  100]    Loss 1.844981    Top1 62.930000    Top5 87.550000    
2022-01-10 16:41:40,435 - ==> Top1: 62.930    Top5: 87.550    Loss: 1.845

2022-01-10 16:41:40,437 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:41:40,437 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:41:40,456 - 

2022-01-10 16:41:40,456 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:41:42,148 - Epoch: [211][  100/  500]    Overall Loss 0.135382    Objective Loss 0.135382                                        LR 0.000016    Time 0.016902    
2022-01-10 16:41:43,726 - Epoch: [211][  200/  500]    Overall Loss 0.141820    Objective Loss 0.141820                                        LR 0.000016    Time 0.016336    
2022-01-10 16:41:45,308 - Epoch: [211][  300/  500]    Overall Loss 0.144195    Objective Loss 0.144195                                        LR 0.000016    Time 0.016161    
2022-01-10 16:41:46,886 - Epoch: [211][  400/  500]    Overall Loss 0.143698    Objective Loss 0.143698                                        LR 0.000016    Time 0.016063    
2022-01-10 16:41:48,456 - Epoch: [211][  500/  500]    Overall Loss 0.144331    Objective Loss 0.144331    Top1 96.500000    Top5 100.000000    LR 0.000016    Time 0.015989    
2022-01-10 16:41:48,499 - --- validate (epoch=211)-----------
2022-01-10 16:41:48,499 - 10000 samples (100 per mini-batch)
2022-01-10 16:41:49,253 - Epoch: [211][  100/  100]    Loss 1.846914    Top1 62.870000    Top5 87.480000    
2022-01-10 16:41:49,298 - ==> Top1: 62.870    Top5: 87.480    Loss: 1.847

2022-01-10 16:41:49,300 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:41:49,300 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:41:49,328 - 

2022-01-10 16:41:49,328 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:41:51,019 - Epoch: [212][  100/  500]    Overall Loss 0.140607    Objective Loss 0.140607                                        LR 0.000016    Time 0.016892    
2022-01-10 16:41:52,492 - Epoch: [212][  200/  500]    Overall Loss 0.140725    Objective Loss 0.140725                                        LR 0.000016    Time 0.015806    
2022-01-10 16:41:53,966 - Epoch: [212][  300/  500]    Overall Loss 0.141592    Objective Loss 0.141592                                        LR 0.000016    Time 0.015448    
2022-01-10 16:41:55,442 - Epoch: [212][  400/  500]    Overall Loss 0.141862    Objective Loss 0.141862                                        LR 0.000016    Time 0.015273    
2022-01-10 16:41:56,922 - Epoch: [212][  500/  500]    Overall Loss 0.141428    Objective Loss 0.141428    Top1 97.500000    Top5 99.500000    LR 0.000016    Time 0.015178    
2022-01-10 16:41:56,969 - --- validate (epoch=212)-----------
2022-01-10 16:41:56,969 - 10000 samples (100 per mini-batch)
2022-01-10 16:41:57,736 - Epoch: [212][  100/  100]    Loss 1.847029    Top1 62.970000    Top5 87.490000    
2022-01-10 16:41:57,774 - ==> Top1: 62.970    Top5: 87.490    Loss: 1.847

2022-01-10 16:41:57,776 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:41:57,776 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:41:57,803 - 

2022-01-10 16:41:57,803 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:41:59,503 - Epoch: [213][  100/  500]    Overall Loss 0.142402    Objective Loss 0.142402                                        LR 0.000016    Time 0.016980    
2022-01-10 16:42:01,068 - Epoch: [213][  200/  500]    Overall Loss 0.141731    Objective Loss 0.141731                                        LR 0.000016    Time 0.016314    
2022-01-10 16:42:02,572 - Epoch: [213][  300/  500]    Overall Loss 0.143505    Objective Loss 0.143505                                        LR 0.000016    Time 0.015886    
2022-01-10 16:42:04,023 - Epoch: [213][  400/  500]    Overall Loss 0.141581    Objective Loss 0.141581                                        LR 0.000016    Time 0.015539    
2022-01-10 16:42:05,484 - Epoch: [213][  500/  500]    Overall Loss 0.141477    Objective Loss 0.141477    Top1 95.000000    Top5 99.500000    LR 0.000016    Time 0.015351    
2022-01-10 16:42:05,533 - --- validate (epoch=213)-----------
2022-01-10 16:42:05,533 - 10000 samples (100 per mini-batch)
2022-01-10 16:42:06,262 - Epoch: [213][  100/  100]    Loss 1.849345    Top1 62.680000    Top5 87.610000    
2022-01-10 16:42:06,300 - ==> Top1: 62.680    Top5: 87.610    Loss: 1.849

2022-01-10 16:42:06,302 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:42:06,302 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:42:06,330 - 

2022-01-10 16:42:06,330 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:42:07,943 - Epoch: [214][  100/  500]    Overall Loss 0.145509    Objective Loss 0.145509                                        LR 0.000016    Time 0.016115    
2022-01-10 16:42:09,386 - Epoch: [214][  200/  500]    Overall Loss 0.144643    Objective Loss 0.144643                                        LR 0.000016    Time 0.015267    
2022-01-10 16:42:10,821 - Epoch: [214][  300/  500]    Overall Loss 0.144452    Objective Loss 0.144452                                        LR 0.000016    Time 0.014959    
2022-01-10 16:42:12,258 - Epoch: [214][  400/  500]    Overall Loss 0.144088    Objective Loss 0.144088                                        LR 0.000016    Time 0.014810    
2022-01-10 16:42:13,821 - Epoch: [214][  500/  500]    Overall Loss 0.143527    Objective Loss 0.143527    Top1 95.000000    Top5 100.000000    LR 0.000016    Time 0.014972    
2022-01-10 16:42:13,869 - --- validate (epoch=214)-----------
2022-01-10 16:42:13,869 - 10000 samples (100 per mini-batch)
2022-01-10 16:42:14,667 - Epoch: [214][  100/  100]    Loss 1.849609    Top1 63.040000    Top5 87.610000    
2022-01-10 16:42:14,705 - ==> Top1: 63.040    Top5: 87.610    Loss: 1.850

2022-01-10 16:42:14,707 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:42:14,707 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:42:14,734 - 

2022-01-10 16:42:14,734 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:42:16,414 - Epoch: [215][  100/  500]    Overall Loss 0.147135    Objective Loss 0.147135                                        LR 0.000016    Time 0.016777    
2022-01-10 16:42:17,840 - Epoch: [215][  200/  500]    Overall Loss 0.142430    Objective Loss 0.142430                                        LR 0.000016    Time 0.015516    
2022-01-10 16:42:19,252 - Epoch: [215][  300/  500]    Overall Loss 0.145040    Objective Loss 0.145040                                        LR 0.000016    Time 0.015049    
2022-01-10 16:42:20,672 - Epoch: [215][  400/  500]    Overall Loss 0.144432    Objective Loss 0.144432                                        LR 0.000016    Time 0.014833    
2022-01-10 16:42:22,093 - Epoch: [215][  500/  500]    Overall Loss 0.144695    Objective Loss 0.144695    Top1 97.500000    Top5 100.000000    LR 0.000016    Time 0.014707    
2022-01-10 16:42:22,145 - --- validate (epoch=215)-----------
2022-01-10 16:42:22,145 - 10000 samples (100 per mini-batch)
2022-01-10 16:42:22,894 - Epoch: [215][  100/  100]    Loss 1.850836    Top1 63.020000    Top5 87.540000    
2022-01-10 16:42:22,934 - ==> Top1: 63.020    Top5: 87.540    Loss: 1.851

2022-01-10 16:42:22,936 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:42:22,936 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:42:22,964 - 

2022-01-10 16:42:22,964 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:42:24,684 - Epoch: [216][  100/  500]    Overall Loss 0.144908    Objective Loss 0.144908                                        LR 0.000016    Time 0.017185    
2022-01-10 16:42:26,129 - Epoch: [216][  200/  500]    Overall Loss 0.140348    Objective Loss 0.140348                                        LR 0.000016    Time 0.015815    
2022-01-10 16:42:27,576 - Epoch: [216][  300/  500]    Overall Loss 0.140438    Objective Loss 0.140438                                        LR 0.000016    Time 0.015364    
2022-01-10 16:42:29,024 - Epoch: [216][  400/  500]    Overall Loss 0.141366    Objective Loss 0.141366                                        LR 0.000016    Time 0.015141    
2022-01-10 16:42:30,467 - Epoch: [216][  500/  500]    Overall Loss 0.142160    Objective Loss 0.142160    Top1 96.000000    Top5 100.000000    LR 0.000016    Time 0.014996    
2022-01-10 16:42:30,516 - --- validate (epoch=216)-----------
2022-01-10 16:42:30,516 - 10000 samples (100 per mini-batch)
2022-01-10 16:42:31,263 - Epoch: [216][  100/  100]    Loss 1.849738    Top1 63.020000    Top5 87.510000    
2022-01-10 16:42:31,310 - ==> Top1: 63.020    Top5: 87.510    Loss: 1.850

2022-01-10 16:42:31,312 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:42:31,312 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:42:31,333 - 

2022-01-10 16:42:31,333 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:42:32,975 - Epoch: [217][  100/  500]    Overall Loss 0.140433    Objective Loss 0.140433                                        LR 0.000016    Time 0.016399    
2022-01-10 16:42:34,415 - Epoch: [217][  200/  500]    Overall Loss 0.140157    Objective Loss 0.140157                                        LR 0.000016    Time 0.015396    
2022-01-10 16:42:35,849 - Epoch: [217][  300/  500]    Overall Loss 0.140989    Objective Loss 0.140989                                        LR 0.000016    Time 0.015041    
2022-01-10 16:42:37,281 - Epoch: [217][  400/  500]    Overall Loss 0.141792    Objective Loss 0.141792                                        LR 0.000016    Time 0.014860    
2022-01-10 16:42:38,721 - Epoch: [217][  500/  500]    Overall Loss 0.141613    Objective Loss 0.141613    Top1 96.500000    Top5 100.000000    LR 0.000016    Time 0.014767    
2022-01-10 16:42:38,763 - --- validate (epoch=217)-----------
2022-01-10 16:42:38,763 - 10000 samples (100 per mini-batch)
2022-01-10 16:42:39,523 - Epoch: [217][  100/  100]    Loss 1.850495    Top1 63.090000    Top5 87.570000    
2022-01-10 16:42:39,570 - ==> Top1: 63.090    Top5: 87.570    Loss: 1.850

2022-01-10 16:42:39,572 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:42:39,572 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:42:39,599 - 

2022-01-10 16:42:39,600 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:42:41,277 - Epoch: [218][  100/  500]    Overall Loss 0.136716    Objective Loss 0.136716                                        LR 0.000016    Time 0.016761    
2022-01-10 16:42:42,848 - Epoch: [218][  200/  500]    Overall Loss 0.142162    Objective Loss 0.142162                                        LR 0.000016    Time 0.016231    
2022-01-10 16:42:44,343 - Epoch: [218][  300/  500]    Overall Loss 0.142775    Objective Loss 0.142775                                        LR 0.000016    Time 0.015799    
2022-01-10 16:42:45,796 - Epoch: [218][  400/  500]    Overall Loss 0.142025    Objective Loss 0.142025                                        LR 0.000016    Time 0.015482    
2022-01-10 16:42:47,369 - Epoch: [218][  500/  500]    Overall Loss 0.142294    Objective Loss 0.142294    Top1 95.500000    Top5 100.000000    LR 0.000016    Time 0.015528    
2022-01-10 16:42:47,416 - --- validate (epoch=218)-----------
2022-01-10 16:42:47,417 - 10000 samples (100 per mini-batch)
2022-01-10 16:42:48,210 - Epoch: [218][  100/  100]    Loss 1.851542    Top1 62.960000    Top5 87.570000    
2022-01-10 16:42:48,255 - ==> Top1: 62.960    Top5: 87.570    Loss: 1.852

2022-01-10 16:42:48,257 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:42:48,257 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:42:48,285 - 

2022-01-10 16:42:48,285 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:42:49,967 - Epoch: [219][  100/  500]    Overall Loss 0.132721    Objective Loss 0.132721                                        LR 0.000016    Time 0.016805    
2022-01-10 16:42:51,406 - Epoch: [219][  200/  500]    Overall Loss 0.141740    Objective Loss 0.141740                                        LR 0.000016    Time 0.015595    
2022-01-10 16:42:52,926 - Epoch: [219][  300/  500]    Overall Loss 0.140861    Objective Loss 0.140861                                        LR 0.000016    Time 0.015461    
2022-01-10 16:42:54,485 - Epoch: [219][  400/  500]    Overall Loss 0.139454    Objective Loss 0.139454                                        LR 0.000016    Time 0.015491    
2022-01-10 16:42:56,054 - Epoch: [219][  500/  500]    Overall Loss 0.140644    Objective Loss 0.140644    Top1 95.500000    Top5 100.000000    LR 0.000016    Time 0.015530    
2022-01-10 16:42:56,097 - --- validate (epoch=219)-----------
2022-01-10 16:42:56,097 - 10000 samples (100 per mini-batch)
2022-01-10 16:42:56,905 - Epoch: [219][  100/  100]    Loss 1.847900    Top1 63.050000    Top5 87.460000    
2022-01-10 16:42:56,950 - ==> Top1: 63.050    Top5: 87.460    Loss: 1.848

2022-01-10 16:42:56,952 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:42:56,952 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:42:56,980 - 

2022-01-10 16:42:56,980 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:42:58,583 - Epoch: [220][  100/  500]    Overall Loss 0.141833    Objective Loss 0.141833                                        LR 0.000016    Time 0.016016    
2022-01-10 16:43:00,002 - Epoch: [220][  200/  500]    Overall Loss 0.146356    Objective Loss 0.146356                                        LR 0.000016    Time 0.015100    
2022-01-10 16:43:01,460 - Epoch: [220][  300/  500]    Overall Loss 0.143776    Objective Loss 0.143776                                        LR 0.000016    Time 0.014923    
2022-01-10 16:43:02,905 - Epoch: [220][  400/  500]    Overall Loss 0.143296    Objective Loss 0.143296                                        LR 0.000016    Time 0.014803    
2022-01-10 16:43:04,346 - Epoch: [220][  500/  500]    Overall Loss 0.141646    Objective Loss 0.141646    Top1 94.500000    Top5 100.000000    LR 0.000016    Time 0.014721    
2022-01-10 16:43:04,392 - --- validate (epoch=220)-----------
2022-01-10 16:43:04,392 - 10000 samples (100 per mini-batch)
2022-01-10 16:43:05,135 - Epoch: [220][  100/  100]    Loss 1.855222    Top1 62.990000    Top5 87.530000    
2022-01-10 16:43:05,172 - ==> Top1: 62.990    Top5: 87.530    Loss: 1.855

2022-01-10 16:43:05,174 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:43:05,174 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:43:05,202 - 

2022-01-10 16:43:05,202 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:43:06,897 - Epoch: [221][  100/  500]    Overall Loss 0.138180    Objective Loss 0.138180                                        LR 0.000016    Time 0.016931    
2022-01-10 16:43:08,330 - Epoch: [221][  200/  500]    Overall Loss 0.141023    Objective Loss 0.141023                                        LR 0.000016    Time 0.015628    
2022-01-10 16:43:09,765 - Epoch: [221][  300/  500]    Overall Loss 0.141851    Objective Loss 0.141851                                        LR 0.000016    Time 0.015200    
2022-01-10 16:43:11,195 - Epoch: [221][  400/  500]    Overall Loss 0.142683    Objective Loss 0.142683                                        LR 0.000016    Time 0.014971    
2022-01-10 16:43:12,631 - Epoch: [221][  500/  500]    Overall Loss 0.144007    Objective Loss 0.144007    Top1 95.000000    Top5 100.000000    LR 0.000016    Time 0.014848    
2022-01-10 16:43:12,670 - --- validate (epoch=221)-----------
2022-01-10 16:43:12,670 - 10000 samples (100 per mini-batch)
2022-01-10 16:43:13,436 - Epoch: [221][  100/  100]    Loss 1.851917    Top1 62.890000    Top5 87.520000    
2022-01-10 16:43:13,488 - ==> Top1: 62.890    Top5: 87.520    Loss: 1.852

2022-01-10 16:43:13,490 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:43:13,490 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:43:13,512 - 

2022-01-10 16:43:13,512 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:43:15,233 - Epoch: [222][  100/  500]    Overall Loss 0.147865    Objective Loss 0.147865                                        LR 0.000016    Time 0.017194    
2022-01-10 16:43:16,679 - Epoch: [222][  200/  500]    Overall Loss 0.147655    Objective Loss 0.147655                                        LR 0.000016    Time 0.015825    
2022-01-10 16:43:18,127 - Epoch: [222][  300/  500]    Overall Loss 0.147455    Objective Loss 0.147455                                        LR 0.000016    Time 0.015374    
2022-01-10 16:43:19,601 - Epoch: [222][  400/  500]    Overall Loss 0.145901    Objective Loss 0.145901                                        LR 0.000016    Time 0.015214    
2022-01-10 16:43:21,101 - Epoch: [222][  500/  500]    Overall Loss 0.146275    Objective Loss 0.146275    Top1 96.000000    Top5 100.000000    LR 0.000016    Time 0.015169    
2022-01-10 16:43:21,148 - --- validate (epoch=222)-----------
2022-01-10 16:43:21,148 - 10000 samples (100 per mini-batch)
2022-01-10 16:43:21,969 - Epoch: [222][  100/  100]    Loss 1.846605    Top1 62.940000    Top5 87.540000    
2022-01-10 16:43:22,006 - ==> Top1: 62.940    Top5: 87.540    Loss: 1.847

2022-01-10 16:43:22,008 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:43:22,008 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:43:22,035 - 

2022-01-10 16:43:22,036 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:43:23,670 - Epoch: [223][  100/  500]    Overall Loss 0.141691    Objective Loss 0.141691                                        LR 0.000016    Time 0.016331    
2022-01-10 16:43:25,119 - Epoch: [223][  200/  500]    Overall Loss 0.138482    Objective Loss 0.138482                                        LR 0.000016    Time 0.015408    
2022-01-10 16:43:26,569 - Epoch: [223][  300/  500]    Overall Loss 0.140958    Objective Loss 0.140958                                        LR 0.000016    Time 0.015100    
2022-01-10 16:43:28,015 - Epoch: [223][  400/  500]    Overall Loss 0.142050    Objective Loss 0.142050                                        LR 0.000016    Time 0.014940    
2022-01-10 16:43:29,470 - Epoch: [223][  500/  500]    Overall Loss 0.142548    Objective Loss 0.142548    Top1 96.500000    Top5 100.000000    LR 0.000016    Time 0.014859    
2022-01-10 16:43:29,521 - --- validate (epoch=223)-----------
2022-01-10 16:43:29,521 - 10000 samples (100 per mini-batch)
2022-01-10 16:43:30,298 - Epoch: [223][  100/  100]    Loss 1.851363    Top1 63.000000    Top5 87.570000    
2022-01-10 16:43:30,345 - ==> Top1: 63.000    Top5: 87.570    Loss: 1.851

2022-01-10 16:43:30,348 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:43:30,348 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:43:30,369 - 

2022-01-10 16:43:30,369 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:43:32,080 - Epoch: [224][  100/  500]    Overall Loss 0.147815    Objective Loss 0.147815                                        LR 0.000016    Time 0.017092    
2022-01-10 16:43:33,518 - Epoch: [224][  200/  500]    Overall Loss 0.143155    Objective Loss 0.143155                                        LR 0.000016    Time 0.015735    
2022-01-10 16:43:34,969 - Epoch: [224][  300/  500]    Overall Loss 0.144398    Objective Loss 0.144398                                        LR 0.000016    Time 0.015323    
2022-01-10 16:43:36,420 - Epoch: [224][  400/  500]    Overall Loss 0.141737    Objective Loss 0.141737                                        LR 0.000016    Time 0.015119    
2022-01-10 16:43:37,875 - Epoch: [224][  500/  500]    Overall Loss 0.140466    Objective Loss 0.140466    Top1 97.000000    Top5 100.000000    LR 0.000016    Time 0.015003    
2022-01-10 16:43:37,924 - --- validate (epoch=224)-----------
2022-01-10 16:43:37,925 - 10000 samples (100 per mini-batch)
2022-01-10 16:43:38,719 - Epoch: [224][  100/  100]    Loss 1.853125    Top1 62.840000    Top5 87.650000    
2022-01-10 16:43:38,755 - ==> Top1: 62.840    Top5: 87.650    Loss: 1.853

2022-01-10 16:43:38,757 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:43:38,757 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:43:38,784 - 

2022-01-10 16:43:38,785 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:43:40,433 - Epoch: [225][  100/  500]    Overall Loss 0.144785    Objective Loss 0.144785                                        LR 0.000016    Time 0.016471    
2022-01-10 16:43:41,988 - Epoch: [225][  200/  500]    Overall Loss 0.144474    Objective Loss 0.144474                                        LR 0.000016    Time 0.016008    
2022-01-10 16:43:43,447 - Epoch: [225][  300/  500]    Overall Loss 0.142985    Objective Loss 0.142985                                        LR 0.000016    Time 0.015530    
2022-01-10 16:43:44,873 - Epoch: [225][  400/  500]    Overall Loss 0.141166    Objective Loss 0.141166                                        LR 0.000016    Time 0.015212    
2022-01-10 16:43:46,306 - Epoch: [225][  500/  500]    Overall Loss 0.139802    Objective Loss 0.139802    Top1 97.500000    Top5 100.000000    LR 0.000016    Time 0.015033    
2022-01-10 16:43:46,357 - --- validate (epoch=225)-----------
2022-01-10 16:43:46,357 - 10000 samples (100 per mini-batch)
2022-01-10 16:43:47,171 - Epoch: [225][  100/  100]    Loss 1.849910    Top1 62.920000    Top5 87.640000    
2022-01-10 16:43:47,222 - ==> Top1: 62.920    Top5: 87.640    Loss: 1.850

2022-01-10 16:43:47,224 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:43:47,224 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:43:47,244 - 

2022-01-10 16:43:47,245 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:43:48,836 - Epoch: [226][  100/  500]    Overall Loss 0.141991    Objective Loss 0.141991                                        LR 0.000016    Time 0.015899    
2022-01-10 16:43:50,329 - Epoch: [226][  200/  500]    Overall Loss 0.140371    Objective Loss 0.140371                                        LR 0.000016    Time 0.015410    
2022-01-10 16:43:51,765 - Epoch: [226][  300/  500]    Overall Loss 0.141309    Objective Loss 0.141309                                        LR 0.000016    Time 0.015056    
2022-01-10 16:43:53,200 - Epoch: [226][  400/  500]    Overall Loss 0.139251    Objective Loss 0.139251                                        LR 0.000016    Time 0.014879    
2022-01-10 16:43:54,644 - Epoch: [226][  500/  500]    Overall Loss 0.139102    Objective Loss 0.139102    Top1 94.500000    Top5 100.000000    LR 0.000016    Time 0.014789    
2022-01-10 16:43:54,699 - --- validate (epoch=226)-----------
2022-01-10 16:43:54,699 - 10000 samples (100 per mini-batch)
2022-01-10 16:43:55,461 - Epoch: [226][  100/  100]    Loss 1.850426    Top1 62.990000    Top5 87.590000    
2022-01-10 16:43:55,504 - ==> Top1: 62.990    Top5: 87.590    Loss: 1.850

2022-01-10 16:43:55,506 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:43:55,506 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:43:55,534 - 

2022-01-10 16:43:55,534 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:43:57,226 - Epoch: [227][  100/  500]    Overall Loss 0.144444    Objective Loss 0.144444                                        LR 0.000016    Time 0.016905    
2022-01-10 16:43:58,674 - Epoch: [227][  200/  500]    Overall Loss 0.141879    Objective Loss 0.141879                                        LR 0.000016    Time 0.015687    
2022-01-10 16:44:00,121 - Epoch: [227][  300/  500]    Overall Loss 0.142140    Objective Loss 0.142140                                        LR 0.000016    Time 0.015280    
2022-01-10 16:44:01,607 - Epoch: [227][  400/  500]    Overall Loss 0.143073    Objective Loss 0.143073                                        LR 0.000016    Time 0.015172    
2022-01-10 16:44:03,110 - Epoch: [227][  500/  500]    Overall Loss 0.141141    Objective Loss 0.141141    Top1 97.500000    Top5 100.000000    LR 0.000016    Time 0.015142    
2022-01-10 16:44:03,158 - --- validate (epoch=227)-----------
2022-01-10 16:44:03,158 - 10000 samples (100 per mini-batch)
2022-01-10 16:44:03,921 - Epoch: [227][  100/  100]    Loss 1.848302    Top1 63.030000    Top5 87.610000    
2022-01-10 16:44:03,962 - ==> Top1: 63.030    Top5: 87.610    Loss: 1.848

2022-01-10 16:44:03,964 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:44:03,964 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:44:03,985 - 

2022-01-10 16:44:03,985 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:44:05,607 - Epoch: [228][  100/  500]    Overall Loss 0.140901    Objective Loss 0.140901                                        LR 0.000016    Time 0.016208    
2022-01-10 16:44:07,073 - Epoch: [228][  200/  500]    Overall Loss 0.143686    Objective Loss 0.143686                                        LR 0.000016    Time 0.015428    
2022-01-10 16:44:08,537 - Epoch: [228][  300/  500]    Overall Loss 0.143826    Objective Loss 0.143826                                        LR 0.000016    Time 0.015162    
2022-01-10 16:44:10,001 - Epoch: [228][  400/  500]    Overall Loss 0.144294    Objective Loss 0.144294                                        LR 0.000016    Time 0.015029    
2022-01-10 16:44:11,461 - Epoch: [228][  500/  500]    Overall Loss 0.143738    Objective Loss 0.143738    Top1 96.000000    Top5 100.000000    LR 0.000016    Time 0.014942    
2022-01-10 16:44:11,510 - --- validate (epoch=228)-----------
2022-01-10 16:44:11,510 - 10000 samples (100 per mini-batch)
2022-01-10 16:44:12,337 - Epoch: [228][  100/  100]    Loss 1.849836    Top1 63.000000    Top5 87.630000    
2022-01-10 16:44:12,392 - ==> Top1: 63.000    Top5: 87.630    Loss: 1.850

2022-01-10 16:44:12,394 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:44:12,394 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:44:12,422 - 

2022-01-10 16:44:12,422 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:44:14,041 - Epoch: [229][  100/  500]    Overall Loss 0.143436    Objective Loss 0.143436                                        LR 0.000016    Time 0.016179    
2022-01-10 16:44:15,487 - Epoch: [229][  200/  500]    Overall Loss 0.142855    Objective Loss 0.142855                                        LR 0.000016    Time 0.015315    
2022-01-10 16:44:16,934 - Epoch: [229][  300/  500]    Overall Loss 0.143830    Objective Loss 0.143830                                        LR 0.000016    Time 0.015030    
2022-01-10 16:44:18,376 - Epoch: [229][  400/  500]    Overall Loss 0.145642    Objective Loss 0.145642                                        LR 0.000016    Time 0.014874    
2022-01-10 16:44:19,840 - Epoch: [229][  500/  500]    Overall Loss 0.145848    Objective Loss 0.145848    Top1 97.000000    Top5 100.000000    LR 0.000016    Time 0.014826    
2022-01-10 16:44:19,883 - --- validate (epoch=229)-----------
2022-01-10 16:44:19,883 - 10000 samples (100 per mini-batch)
2022-01-10 16:44:20,608 - Epoch: [229][  100/  100]    Loss 1.852419    Top1 62.850000    Top5 87.570000    
2022-01-10 16:44:20,653 - ==> Top1: 62.850    Top5: 87.570    Loss: 1.852

2022-01-10 16:44:20,655 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:44:20,655 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:44:20,683 - 

2022-01-10 16:44:20,683 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:44:22,379 - Epoch: [230][  100/  500]    Overall Loss 0.134255    Objective Loss 0.134255                                        LR 0.000016    Time 0.016949    
2022-01-10 16:44:23,808 - Epoch: [230][  200/  500]    Overall Loss 0.140798    Objective Loss 0.140798                                        LR 0.000016    Time 0.015615    
2022-01-10 16:44:25,239 - Epoch: [230][  300/  500]    Overall Loss 0.138043    Objective Loss 0.138043                                        LR 0.000016    Time 0.015178    
2022-01-10 16:44:26,671 - Epoch: [230][  400/  500]    Overall Loss 0.138223    Objective Loss 0.138223                                        LR 0.000016    Time 0.014960    
2022-01-10 16:44:28,109 - Epoch: [230][  500/  500]    Overall Loss 0.138973    Objective Loss 0.138973    Top1 97.000000    Top5 100.000000    LR 0.000016    Time 0.014842    
2022-01-10 16:44:28,156 - --- validate (epoch=230)-----------
2022-01-10 16:44:28,157 - 10000 samples (100 per mini-batch)
2022-01-10 16:44:28,926 - Epoch: [230][  100/  100]    Loss 1.846157    Top1 63.070000    Top5 87.570000    
2022-01-10 16:44:28,965 - ==> Top1: 63.070    Top5: 87.570    Loss: 1.846

2022-01-10 16:44:28,967 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:44:28,967 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:44:28,995 - 

2022-01-10 16:44:28,995 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:44:30,594 - Epoch: [231][  100/  500]    Overall Loss 0.141797    Objective Loss 0.141797                                        LR 0.000016    Time 0.015972    
2022-01-10 16:44:32,030 - Epoch: [231][  200/  500]    Overall Loss 0.141226    Objective Loss 0.141226                                        LR 0.000016    Time 0.015165    
2022-01-10 16:44:33,477 - Epoch: [231][  300/  500]    Overall Loss 0.140333    Objective Loss 0.140333                                        LR 0.000016    Time 0.014930    
2022-01-10 16:44:34,939 - Epoch: [231][  400/  500]    Overall Loss 0.140120    Objective Loss 0.140120                                        LR 0.000016    Time 0.014849    
2022-01-10 16:44:36,375 - Epoch: [231][  500/  500]    Overall Loss 0.141793    Objective Loss 0.141793    Top1 96.000000    Top5 100.000000    LR 0.000016    Time 0.014751    
2022-01-10 16:44:36,415 - --- validate (epoch=231)-----------
2022-01-10 16:44:36,415 - 10000 samples (100 per mini-batch)
2022-01-10 16:44:37,237 - Epoch: [231][  100/  100]    Loss 1.849762    Top1 63.080000    Top5 87.500000    
2022-01-10 16:44:37,281 - ==> Top1: 63.080    Top5: 87.500    Loss: 1.850

2022-01-10 16:44:37,283 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:44:37,283 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:44:37,310 - 

2022-01-10 16:44:37,310 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:44:38,898 - Epoch: [232][  100/  500]    Overall Loss 0.137394    Objective Loss 0.137394                                        LR 0.000016    Time 0.015865    
2022-01-10 16:44:40,317 - Epoch: [232][  200/  500]    Overall Loss 0.136832    Objective Loss 0.136832                                        LR 0.000016    Time 0.015021    
2022-01-10 16:44:41,732 - Epoch: [232][  300/  500]    Overall Loss 0.139044    Objective Loss 0.139044                                        LR 0.000016    Time 0.014730    
2022-01-10 16:44:43,151 - Epoch: [232][  400/  500]    Overall Loss 0.138635    Objective Loss 0.138635                                        LR 0.000016    Time 0.014593    
2022-01-10 16:44:44,590 - Epoch: [232][  500/  500]    Overall Loss 0.137412    Objective Loss 0.137412    Top1 96.000000    Top5 100.000000    LR 0.000016    Time 0.014550    
2022-01-10 16:44:44,641 - --- validate (epoch=232)-----------
2022-01-10 16:44:44,642 - 10000 samples (100 per mini-batch)
2022-01-10 16:44:45,392 - Epoch: [232][  100/  100]    Loss 1.847801    Top1 63.150000    Top5 87.640000    
2022-01-10 16:44:45,431 - ==> Top1: 63.150    Top5: 87.640    Loss: 1.848

2022-01-10 16:44:45,433 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:44:45,433 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:44:45,460 - 

2022-01-10 16:44:45,460 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:44:47,137 - Epoch: [233][  100/  500]    Overall Loss 0.142441    Objective Loss 0.142441                                        LR 0.000016    Time 0.016755    
2022-01-10 16:44:48,616 - Epoch: [233][  200/  500]    Overall Loss 0.140937    Objective Loss 0.140937                                        LR 0.000016    Time 0.015765    
2022-01-10 16:44:50,092 - Epoch: [233][  300/  500]    Overall Loss 0.143221    Objective Loss 0.143221                                        LR 0.000016    Time 0.015429    
2022-01-10 16:44:51,577 - Epoch: [233][  400/  500]    Overall Loss 0.142123    Objective Loss 0.142123                                        LR 0.000016    Time 0.015281    
2022-01-10 16:44:53,134 - Epoch: [233][  500/  500]    Overall Loss 0.140742    Objective Loss 0.140742    Top1 96.000000    Top5 100.000000    LR 0.000016    Time 0.015338    
2022-01-10 16:44:53,188 - --- validate (epoch=233)-----------
2022-01-10 16:44:53,188 - 10000 samples (100 per mini-batch)
2022-01-10 16:44:54,007 - Epoch: [233][  100/  100]    Loss 1.848175    Top1 63.150000    Top5 87.700000    
2022-01-10 16:44:54,053 - ==> Top1: 63.150    Top5: 87.700    Loss: 1.848

2022-01-10 16:44:54,055 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:44:54,055 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:44:54,083 - 

2022-01-10 16:44:54,083 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:44:55,685 - Epoch: [234][  100/  500]    Overall Loss 0.149517    Objective Loss 0.149517                                        LR 0.000016    Time 0.015996    
2022-01-10 16:44:57,129 - Epoch: [234][  200/  500]    Overall Loss 0.142502    Objective Loss 0.142502                                        LR 0.000016    Time 0.015216    
2022-01-10 16:44:58,584 - Epoch: [234][  300/  500]    Overall Loss 0.139998    Objective Loss 0.139998                                        LR 0.000016    Time 0.014990    
2022-01-10 16:45:00,036 - Epoch: [234][  400/  500]    Overall Loss 0.139799    Objective Loss 0.139799                                        LR 0.000016    Time 0.014870    
2022-01-10 16:45:01,498 - Epoch: [234][  500/  500]    Overall Loss 0.139584    Objective Loss 0.139584    Top1 97.000000    Top5 100.000000    LR 0.000016    Time 0.014819    
2022-01-10 16:45:01,543 - --- validate (epoch=234)-----------
2022-01-10 16:45:01,543 - 10000 samples (100 per mini-batch)
2022-01-10 16:45:02,384 - Epoch: [234][  100/  100]    Loss 1.852639    Top1 62.900000    Top5 87.730000    
2022-01-10 16:45:02,431 - ==> Top1: 62.900    Top5: 87.730    Loss: 1.853

2022-01-10 16:45:02,433 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:45:02,433 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:45:02,525 - 

2022-01-10 16:45:02,525 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:45:04,172 - Epoch: [235][  100/  500]    Overall Loss 0.147364    Objective Loss 0.147364                                        LR 0.000016    Time 0.016448    
2022-01-10 16:45:05,757 - Epoch: [235][  200/  500]    Overall Loss 0.144804    Objective Loss 0.144804                                        LR 0.000016    Time 0.016144    
2022-01-10 16:45:07,249 - Epoch: [235][  300/  500]    Overall Loss 0.142979    Objective Loss 0.142979                                        LR 0.000016    Time 0.015734    
2022-01-10 16:45:08,710 - Epoch: [235][  400/  500]    Overall Loss 0.142334    Objective Loss 0.142334                                        LR 0.000016    Time 0.015450    
2022-01-10 16:45:10,179 - Epoch: [235][  500/  500]    Overall Loss 0.141863    Objective Loss 0.141863    Top1 96.500000    Top5 100.000000    LR 0.000016    Time 0.015298    
2022-01-10 16:45:10,224 - --- validate (epoch=235)-----------
2022-01-10 16:45:10,224 - 10000 samples (100 per mini-batch)
2022-01-10 16:45:11,041 - Epoch: [235][  100/  100]    Loss 1.850604    Top1 62.890000    Top5 87.530000    
2022-01-10 16:45:11,088 - ==> Top1: 62.890    Top5: 87.530    Loss: 1.851

2022-01-10 16:45:11,090 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:45:11,090 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:45:11,118 - 

2022-01-10 16:45:11,118 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:45:12,873 - Epoch: [236][  100/  500]    Overall Loss 0.135853    Objective Loss 0.135853                                        LR 0.000016    Time 0.017532    
2022-01-10 16:45:14,461 - Epoch: [236][  200/  500]    Overall Loss 0.135147    Objective Loss 0.135147                                        LR 0.000016    Time 0.016702    
2022-01-10 16:45:16,049 - Epoch: [236][  300/  500]    Overall Loss 0.137837    Objective Loss 0.137837                                        LR 0.000016    Time 0.016427    
2022-01-10 16:45:17,618 - Epoch: [236][  400/  500]    Overall Loss 0.137354    Objective Loss 0.137354                                        LR 0.000016    Time 0.016238    
2022-01-10 16:45:19,083 - Epoch: [236][  500/  500]    Overall Loss 0.138172    Objective Loss 0.138172    Top1 94.000000    Top5 100.000000    LR 0.000016    Time 0.015919    
2022-01-10 16:45:19,134 - --- validate (epoch=236)-----------
2022-01-10 16:45:19,134 - 10000 samples (100 per mini-batch)
2022-01-10 16:45:19,893 - Epoch: [236][  100/  100]    Loss 1.847480    Top1 63.040000    Top5 87.630000    
2022-01-10 16:45:19,934 - ==> Top1: 63.040    Top5: 87.630    Loss: 1.847

2022-01-10 16:45:19,936 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:45:19,936 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:45:19,955 - 

2022-01-10 16:45:19,955 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:45:21,573 - Epoch: [237][  100/  500]    Overall Loss 0.138023    Objective Loss 0.138023                                        LR 0.000016    Time 0.016168    
2022-01-10 16:45:23,035 - Epoch: [237][  200/  500]    Overall Loss 0.138682    Objective Loss 0.138682                                        LR 0.000016    Time 0.015390    
2022-01-10 16:45:24,607 - Epoch: [237][  300/  500]    Overall Loss 0.140392    Objective Loss 0.140392                                        LR 0.000016    Time 0.015495    
2022-01-10 16:45:26,167 - Epoch: [237][  400/  500]    Overall Loss 0.139980    Objective Loss 0.139980                                        LR 0.000016    Time 0.015519    
2022-01-10 16:45:27,731 - Epoch: [237][  500/  500]    Overall Loss 0.140556    Objective Loss 0.140556    Top1 96.000000    Top5 99.500000    LR 0.000016    Time 0.015542    
2022-01-10 16:45:27,774 - --- validate (epoch=237)-----------
2022-01-10 16:45:27,774 - 10000 samples (100 per mini-batch)
2022-01-10 16:45:28,513 - Epoch: [237][  100/  100]    Loss 1.852067    Top1 62.780000    Top5 87.540000    
2022-01-10 16:45:28,565 - ==> Top1: 62.780    Top5: 87.540    Loss: 1.852

2022-01-10 16:45:28,567 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:45:28,567 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:45:28,595 - 

2022-01-10 16:45:28,595 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:45:30,265 - Epoch: [238][  100/  500]    Overall Loss 0.136354    Objective Loss 0.136354                                        LR 0.000016    Time 0.016686    
2022-01-10 16:45:31,713 - Epoch: [238][  200/  500]    Overall Loss 0.138718    Objective Loss 0.138718                                        LR 0.000016    Time 0.015577    
2022-01-10 16:45:33,160 - Epoch: [238][  300/  500]    Overall Loss 0.139897    Objective Loss 0.139897                                        LR 0.000016    Time 0.015206    
2022-01-10 16:45:34,608 - Epoch: [238][  400/  500]    Overall Loss 0.140434    Objective Loss 0.140434                                        LR 0.000016    Time 0.015021    
2022-01-10 16:45:36,052 - Epoch: [238][  500/  500]    Overall Loss 0.141178    Objective Loss 0.141178    Top1 97.500000    Top5 100.000000    LR 0.000016    Time 0.014905    
2022-01-10 16:45:36,098 - --- validate (epoch=238)-----------
2022-01-10 16:45:36,098 - 10000 samples (100 per mini-batch)
2022-01-10 16:45:36,822 - Epoch: [238][  100/  100]    Loss 1.852982    Top1 62.810000    Top5 87.610000    
2022-01-10 16:45:36,867 - ==> Top1: 62.810    Top5: 87.610    Loss: 1.853

2022-01-10 16:45:36,869 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:45:36,869 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:45:36,896 - 

2022-01-10 16:45:36,896 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:45:38,568 - Epoch: [239][  100/  500]    Overall Loss 0.140705    Objective Loss 0.140705                                        LR 0.000016    Time 0.016705    
2022-01-10 16:45:40,051 - Epoch: [239][  200/  500]    Overall Loss 0.141678    Objective Loss 0.141678                                        LR 0.000016    Time 0.015762    
2022-01-10 16:45:41,633 - Epoch: [239][  300/  500]    Overall Loss 0.141559    Objective Loss 0.141559                                        LR 0.000016    Time 0.015780    
2022-01-10 16:45:43,213 - Epoch: [239][  400/  500]    Overall Loss 0.142169    Objective Loss 0.142169                                        LR 0.000016    Time 0.015783    
2022-01-10 16:45:44,724 - Epoch: [239][  500/  500]    Overall Loss 0.141242    Objective Loss 0.141242    Top1 98.000000    Top5 99.500000    LR 0.000016    Time 0.015647    
2022-01-10 16:45:44,768 - --- validate (epoch=239)-----------
2022-01-10 16:45:44,768 - 10000 samples (100 per mini-batch)
2022-01-10 16:45:45,523 - Epoch: [239][  100/  100]    Loss 1.859864    Top1 62.900000    Top5 87.450000    
2022-01-10 16:45:45,562 - ==> Top1: 62.900    Top5: 87.450    Loss: 1.860

2022-01-10 16:45:45,564 - ==> Best [Top1: 63.200   Top5: 88.210   Sparsity:0.00   Params: 725128 on epoch: 106]
2022-01-10 16:45:45,564 - Saving checkpoint to: logs/2022.01.10-161214/checkpoint.pth.tar
2022-01-10 16:45:45,608 - 

2022-01-10 16:45:45,609 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:45:47,885 - Epoch: [240][  100/  500]    Overall Loss 4.352842    Objective Loss 4.352842                                        LR 0.000016    Time 0.022750    
2022-01-10 16:45:49,980 - Epoch: [240][  200/  500]    Overall Loss 3.755356    Objective Loss 3.755356                                        LR 0.000016    Time 0.021846    
2022-01-10 16:45:52,070 - Epoch: [240][  300/  500]    Overall Loss 3.328321    Objective Loss 3.328321                                        LR 0.000016    Time 0.021529    
2022-01-10 16:45:54,164 - Epoch: [240][  400/  500]    Overall Loss 3.046548    Objective Loss 3.046548                                        LR 0.000016    Time 0.021378    
2022-01-10 16:45:56,286 - Epoch: [240][  500/  500]    Overall Loss 2.843867    Objective Loss 2.843867    Top1 61.000000    Top5 89.500000    LR 0.000016    Time 0.021346    
2022-01-10 16:45:56,326 - --- validate (epoch=240)-----------
2022-01-10 16:45:56,326 - 10000 samples (100 per mini-batch)
2022-01-10 16:45:57,764 - Epoch: [240][  100/  100]    Loss 2.178156    Top1 48.720000    Top5 78.970000    
2022-01-10 16:45:57,809 - ==> Top1: 48.720    Top5: 78.970    Loss: 2.178

2022-01-10 16:45:57,810 - ==> Best [Top1: 48.720   Top5: 78.970   Sparsity:0.00   Params: 725128 on epoch: 240]
2022-01-10 16:45:57,810 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:45:57,829 - 

2022-01-10 16:45:57,829 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:46:00,160 - Epoch: [241][  100/  500]    Overall Loss 1.947003    Objective Loss 1.947003                                        LR 0.000016    Time 0.023297    
2022-01-10 16:46:02,261 - Epoch: [241][  200/  500]    Overall Loss 1.901632    Objective Loss 1.901632                                        LR 0.000016    Time 0.022146    
2022-01-10 16:46:04,373 - Epoch: [241][  300/  500]    Overall Loss 1.866469    Objective Loss 1.866469                                        LR 0.000016    Time 0.021802    
2022-01-10 16:46:06,483 - Epoch: [241][  400/  500]    Overall Loss 1.840613    Objective Loss 1.840613                                        LR 0.000016    Time 0.021623    
2022-01-10 16:46:08,589 - Epoch: [241][  500/  500]    Overall Loss 1.816818    Objective Loss 1.816818    Top1 61.500000    Top5 90.000000    LR 0.000016    Time 0.021509    
2022-01-10 16:46:08,637 - --- validate (epoch=241)-----------
2022-01-10 16:46:08,637 - 10000 samples (100 per mini-batch)
2022-01-10 16:46:10,086 - Epoch: [241][  100/  100]    Loss 1.958512    Top1 52.950000    Top5 82.200000    
2022-01-10 16:46:10,141 - ==> Top1: 52.950    Top5: 82.200    Loss: 1.959

2022-01-10 16:46:10,143 - ==> Best [Top1: 52.950   Top5: 82.200   Sparsity:0.00   Params: 725128 on epoch: 241]
2022-01-10 16:46:10,143 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:46:10,175 - 

2022-01-10 16:46:10,175 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:46:12,516 - Epoch: [242][  100/  500]    Overall Loss 1.680792    Objective Loss 1.680792                                        LR 0.000016    Time 0.023395    
2022-01-10 16:46:14,597 - Epoch: [242][  200/  500]    Overall Loss 1.667234    Objective Loss 1.667234                                        LR 0.000016    Time 0.022103    
2022-01-10 16:46:16,683 - Epoch: [242][  300/  500]    Overall Loss 1.654016    Objective Loss 1.654016                                        LR 0.000016    Time 0.021684    
2022-01-10 16:46:18,773 - Epoch: [242][  400/  500]    Overall Loss 1.647783    Objective Loss 1.647783                                        LR 0.000016    Time 0.021485    
2022-01-10 16:46:20,876 - Epoch: [242][  500/  500]    Overall Loss 1.638963    Objective Loss 1.638963    Top1 69.500000    Top5 95.500000    LR 0.000016    Time 0.021393    
2022-01-10 16:46:20,928 - --- validate (epoch=242)-----------
2022-01-10 16:46:20,928 - 10000 samples (100 per mini-batch)
2022-01-10 16:46:22,351 - Epoch: [242][  100/  100]    Loss 1.867156    Top1 54.900000    Top5 83.610000    
2022-01-10 16:46:22,393 - ==> Top1: 54.900    Top5: 83.610    Loss: 1.867

2022-01-10 16:46:22,394 - ==> Best [Top1: 54.900   Top5: 83.610   Sparsity:0.00   Params: 725128 on epoch: 242]
2022-01-10 16:46:22,395 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:46:22,427 - 

2022-01-10 16:46:22,428 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:46:24,708 - Epoch: [243][  100/  500]    Overall Loss 1.582409    Objective Loss 1.582409                                        LR 0.000016    Time 0.022791    
2022-01-10 16:46:26,801 - Epoch: [243][  200/  500]    Overall Loss 1.577210    Objective Loss 1.577210                                        LR 0.000016    Time 0.021857    
2022-01-10 16:46:28,907 - Epoch: [243][  300/  500]    Overall Loss 1.567911    Objective Loss 1.567911                                        LR 0.000016    Time 0.021588    
2022-01-10 16:46:31,020 - Epoch: [243][  400/  500]    Overall Loss 1.558165    Objective Loss 1.558165                                        LR 0.000016    Time 0.021473    
2022-01-10 16:46:33,139 - Epoch: [243][  500/  500]    Overall Loss 1.551605    Objective Loss 1.551605    Top1 69.500000    Top5 94.500000    LR 0.000016    Time 0.021413    
2022-01-10 16:46:33,189 - --- validate (epoch=243)-----------
2022-01-10 16:46:33,189 - 10000 samples (100 per mini-batch)
2022-01-10 16:46:34,594 - Epoch: [243][  100/  100]    Loss 1.827047    Top1 55.970000    Top5 84.170000    
2022-01-10 16:46:34,636 - ==> Top1: 55.970    Top5: 84.170    Loss: 1.827

2022-01-10 16:46:34,638 - ==> Best [Top1: 55.970   Top5: 84.170   Sparsity:0.00   Params: 725128 on epoch: 243]
2022-01-10 16:46:34,638 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:46:34,670 - 

2022-01-10 16:46:34,670 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:46:37,020 - Epoch: [244][  100/  500]    Overall Loss 1.524898    Objective Loss 1.524898                                        LR 0.000016    Time 0.023487    
2022-01-10 16:46:39,116 - Epoch: [244][  200/  500]    Overall Loss 1.525867    Objective Loss 1.525867                                        LR 0.000016    Time 0.022220    
2022-01-10 16:46:41,204 - Epoch: [244][  300/  500]    Overall Loss 1.508639    Objective Loss 1.508639                                        LR 0.000016    Time 0.021772    
2022-01-10 16:46:43,294 - Epoch: [244][  400/  500]    Overall Loss 1.505045    Objective Loss 1.505045                                        LR 0.000016    Time 0.021550    
2022-01-10 16:46:45,393 - Epoch: [244][  500/  500]    Overall Loss 1.496276    Objective Loss 1.496276    Top1 63.500000    Top5 95.000000    LR 0.000016    Time 0.021436    
2022-01-10 16:46:45,440 - --- validate (epoch=244)-----------
2022-01-10 16:46:45,440 - 10000 samples (100 per mini-batch)
2022-01-10 16:46:46,950 - Epoch: [244][  100/  100]    Loss 1.765642    Top1 56.890000    Top5 84.700000    
2022-01-10 16:46:46,993 - ==> Top1: 56.890    Top5: 84.700    Loss: 1.766

2022-01-10 16:46:46,994 - ==> Best [Top1: 56.890   Top5: 84.700   Sparsity:0.00   Params: 725128 on epoch: 244]
2022-01-10 16:46:46,994 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:46:47,026 - 

2022-01-10 16:46:47,026 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:46:49,434 - Epoch: [245][  100/  500]    Overall Loss 1.476523    Objective Loss 1.476523                                        LR 0.000016    Time 0.024067    
2022-01-10 16:46:51,522 - Epoch: [245][  200/  500]    Overall Loss 1.462525    Objective Loss 1.462525                                        LR 0.000016    Time 0.022470    
2022-01-10 16:46:53,645 - Epoch: [245][  300/  500]    Overall Loss 1.454701    Objective Loss 1.454701                                        LR 0.000016    Time 0.022051    
2022-01-10 16:46:55,779 - Epoch: [245][  400/  500]    Overall Loss 1.452816    Objective Loss 1.452816                                        LR 0.000016    Time 0.021873    
2022-01-10 16:46:57,915 - Epoch: [245][  500/  500]    Overall Loss 1.450302    Objective Loss 1.450302    Top1 65.000000    Top5 91.500000    LR 0.000016    Time 0.021768    
2022-01-10 16:46:57,965 - --- validate (epoch=245)-----------
2022-01-10 16:46:57,965 - 10000 samples (100 per mini-batch)
2022-01-10 16:46:59,427 - Epoch: [245][  100/  100]    Loss 1.783699    Top1 56.240000    Top5 84.110000    
2022-01-10 16:46:59,466 - ==> Top1: 56.240    Top5: 84.110    Loss: 1.784

2022-01-10 16:46:59,467 - ==> Best [Top1: 56.890   Top5: 84.700   Sparsity:0.00   Params: 725128 on epoch: 244]
2022-01-10 16:46:59,467 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:46:59,492 - 

2022-01-10 16:46:59,492 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:47:01,734 - Epoch: [246][  100/  500]    Overall Loss 1.433939    Objective Loss 1.433939                                        LR 0.000016    Time 0.022410    
2022-01-10 16:47:03,827 - Epoch: [246][  200/  500]    Overall Loss 1.424964    Objective Loss 1.424964                                        LR 0.000016    Time 0.021665    
2022-01-10 16:47:05,943 - Epoch: [246][  300/  500]    Overall Loss 1.425058    Objective Loss 1.425058                                        LR 0.000016    Time 0.021492    
2022-01-10 16:47:08,064 - Epoch: [246][  400/  500]    Overall Loss 1.419759    Objective Loss 1.419759                                        LR 0.000016    Time 0.021421    
2022-01-10 16:47:10,198 - Epoch: [246][  500/  500]    Overall Loss 1.419395    Objective Loss 1.419395    Top1 76.500000    Top5 95.000000    LR 0.000016    Time 0.021402    
2022-01-10 16:47:10,243 - --- validate (epoch=246)-----------
2022-01-10 16:47:10,243 - 10000 samples (100 per mini-batch)
2022-01-10 16:47:11,693 - Epoch: [246][  100/  100]    Loss 1.742978    Top1 57.070000    Top5 85.000000    
2022-01-10 16:47:11,738 - ==> Top1: 57.070    Top5: 85.000    Loss: 1.743

2022-01-10 16:47:11,740 - ==> Best [Top1: 57.070   Top5: 85.000   Sparsity:0.00   Params: 725128 on epoch: 246]
2022-01-10 16:47:11,740 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:47:11,772 - 

2022-01-10 16:47:11,772 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:47:14,139 - Epoch: [247][  100/  500]    Overall Loss 1.397218    Objective Loss 1.397218                                        LR 0.000016    Time 0.023649    
2022-01-10 16:47:16,304 - Epoch: [247][  200/  500]    Overall Loss 1.401614    Objective Loss 1.401614                                        LR 0.000016    Time 0.022644    
2022-01-10 16:47:18,450 - Epoch: [247][  300/  500]    Overall Loss 1.402774    Objective Loss 1.402774                                        LR 0.000016    Time 0.022246    
2022-01-10 16:47:20,547 - Epoch: [247][  400/  500]    Overall Loss 1.398466    Objective Loss 1.398466                                        LR 0.000016    Time 0.021925    
2022-01-10 16:47:22,628 - Epoch: [247][  500/  500]    Overall Loss 1.398272    Objective Loss 1.398272    Top1 67.000000    Top5 94.500000    LR 0.000016    Time 0.021701    
2022-01-10 16:47:22,681 - --- validate (epoch=247)-----------
2022-01-10 16:47:22,681 - 10000 samples (100 per mini-batch)
2022-01-10 16:47:24,105 - Epoch: [247][  100/  100]    Loss 1.745609    Top1 57.190000    Top5 84.700000    
2022-01-10 16:47:24,151 - ==> Top1: 57.190    Top5: 84.700    Loss: 1.746

2022-01-10 16:47:24,152 - ==> Best [Top1: 57.190   Top5: 84.700   Sparsity:0.00   Params: 725128 on epoch: 247]
2022-01-10 16:47:24,152 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:47:24,184 - 

2022-01-10 16:47:24,184 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:47:26,471 - Epoch: [248][  100/  500]    Overall Loss 1.396522    Objective Loss 1.396522                                        LR 0.000016    Time 0.022860    
2022-01-10 16:47:28,521 - Epoch: [248][  200/  500]    Overall Loss 1.382355    Objective Loss 1.382355                                        LR 0.000016    Time 0.021673    
2022-01-10 16:47:30,588 - Epoch: [248][  300/  500]    Overall Loss 1.375953    Objective Loss 1.375953                                        LR 0.000016    Time 0.021336    
2022-01-10 16:47:32,698 - Epoch: [248][  400/  500]    Overall Loss 1.372140    Objective Loss 1.372140                                        LR 0.000016    Time 0.021275    
2022-01-10 16:47:34,827 - Epoch: [248][  500/  500]    Overall Loss 1.370612    Objective Loss 1.370612    Top1 63.000000    Top5 92.000000    LR 0.000016    Time 0.021276    
2022-01-10 16:47:34,884 - --- validate (epoch=248)-----------
2022-01-10 16:47:34,884 - 10000 samples (100 per mini-batch)
2022-01-10 16:47:36,441 - Epoch: [248][  100/  100]    Loss 1.717518    Top1 57.270000    Top5 85.080000    
2022-01-10 16:47:36,486 - ==> Top1: 57.270    Top5: 85.080    Loss: 1.718

2022-01-10 16:47:36,488 - ==> Best [Top1: 57.270   Top5: 85.080   Sparsity:0.00   Params: 725128 on epoch: 248]
2022-01-10 16:47:36,488 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:47:36,520 - 

2022-01-10 16:47:36,520 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:47:38,775 - Epoch: [249][  100/  500]    Overall Loss 1.336741    Objective Loss 1.336741                                        LR 0.000016    Time 0.022529    
2022-01-10 16:47:40,917 - Epoch: [249][  200/  500]    Overall Loss 1.346510    Objective Loss 1.346510                                        LR 0.000016    Time 0.021970    
2022-01-10 16:47:43,074 - Epoch: [249][  300/  500]    Overall Loss 1.353167    Objective Loss 1.353167                                        LR 0.000016    Time 0.021833    
2022-01-10 16:47:45,201 - Epoch: [249][  400/  500]    Overall Loss 1.350335    Objective Loss 1.350335                                        LR 0.000016    Time 0.021691    
2022-01-10 16:47:47,328 - Epoch: [249][  500/  500]    Overall Loss 1.349863    Objective Loss 1.349863    Top1 64.500000    Top5 94.000000    LR 0.000016    Time 0.021604    
2022-01-10 16:47:47,379 - --- validate (epoch=249)-----------
2022-01-10 16:47:47,379 - 10000 samples (100 per mini-batch)
2022-01-10 16:47:48,834 - Epoch: [249][  100/  100]    Loss 1.731442    Top1 56.850000    Top5 84.460000    
2022-01-10 16:47:48,886 - ==> Top1: 56.850    Top5: 84.460    Loss: 1.731

2022-01-10 16:47:48,888 - ==> Best [Top1: 57.270   Top5: 85.080   Sparsity:0.00   Params: 725128 on epoch: 248]
2022-01-10 16:47:48,888 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:47:48,913 - 

2022-01-10 16:47:48,913 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:47:51,267 - Epoch: [250][  100/  500]    Overall Loss 1.333770    Objective Loss 1.333770                                        LR 0.000016    Time 0.023518    
2022-01-10 16:47:53,367 - Epoch: [250][  200/  500]    Overall Loss 1.339183    Objective Loss 1.339183                                        LR 0.000016    Time 0.022258    
2022-01-10 16:47:55,518 - Epoch: [250][  300/  500]    Overall Loss 1.334585    Objective Loss 1.334585                                        LR 0.000016    Time 0.022004    
2022-01-10 16:47:57,649 - Epoch: [250][  400/  500]    Overall Loss 1.337889    Objective Loss 1.337889                                        LR 0.000016    Time 0.021828    
2022-01-10 16:47:59,775 - Epoch: [250][  500/  500]    Overall Loss 1.336201    Objective Loss 1.336201    Top1 72.500000    Top5 93.000000    LR 0.000016    Time 0.021713    
2022-01-10 16:47:59,822 - --- validate (epoch=250)-----------
2022-01-10 16:47:59,822 - 10000 samples (100 per mini-batch)
2022-01-10 16:48:01,271 - Epoch: [250][  100/  100]    Loss 1.691720    Top1 57.850000    Top5 84.720000    
2022-01-10 16:48:01,321 - ==> Top1: 57.850    Top5: 84.720    Loss: 1.692

2022-01-10 16:48:01,322 - ==> Best [Top1: 57.850   Top5: 84.720   Sparsity:0.00   Params: 725128 on epoch: 250]
2022-01-10 16:48:01,322 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:48:01,354 - 

2022-01-10 16:48:01,354 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:48:03,747 - Epoch: [251][  100/  500]    Overall Loss 1.327397    Objective Loss 1.327397                                        LR 0.000016    Time 0.023913    
2022-01-10 16:48:05,867 - Epoch: [251][  200/  500]    Overall Loss 1.319580    Objective Loss 1.319580                                        LR 0.000016    Time 0.022553    
2022-01-10 16:48:08,002 - Epoch: [251][  300/  500]    Overall Loss 1.326061    Objective Loss 1.326061                                        LR 0.000016    Time 0.022148    
2022-01-10 16:48:10,217 - Epoch: [251][  400/  500]    Overall Loss 1.325439    Objective Loss 1.325439                                        LR 0.000016    Time 0.022146    
2022-01-10 16:48:12,340 - Epoch: [251][  500/  500]    Overall Loss 1.322584    Objective Loss 1.322584    Top1 78.500000    Top5 98.000000    LR 0.000016    Time 0.021961    
2022-01-10 16:48:12,394 - --- validate (epoch=251)-----------
2022-01-10 16:48:12,394 - 10000 samples (100 per mini-batch)
2022-01-10 16:48:13,817 - Epoch: [251][  100/  100]    Loss 1.656067    Top1 59.010000    Top5 85.570000    
2022-01-10 16:48:13,864 - ==> Top1: 59.010    Top5: 85.570    Loss: 1.656

2022-01-10 16:48:13,865 - ==> Best [Top1: 59.010   Top5: 85.570   Sparsity:0.00   Params: 725128 on epoch: 251]
2022-01-10 16:48:13,865 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:48:13,897 - 

2022-01-10 16:48:13,897 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:48:16,175 - Epoch: [252][  100/  500]    Overall Loss 1.315848    Objective Loss 1.315848                                        LR 0.000016    Time 0.022760    
2022-01-10 16:48:18,296 - Epoch: [252][  200/  500]    Overall Loss 1.319521    Objective Loss 1.319521                                        LR 0.000016    Time 0.021979    
2022-01-10 16:48:20,423 - Epoch: [252][  300/  500]    Overall Loss 1.315943    Objective Loss 1.315943                                        LR 0.000016    Time 0.021740    
2022-01-10 16:48:22,509 - Epoch: [252][  400/  500]    Overall Loss 1.313189    Objective Loss 1.313189                                        LR 0.000016    Time 0.021519    
2022-01-10 16:48:24,628 - Epoch: [252][  500/  500]    Overall Loss 1.311238    Objective Loss 1.311238    Top1 69.500000    Top5 93.500000    LR 0.000016    Time 0.021451    
2022-01-10 16:48:24,671 - --- validate (epoch=252)-----------
2022-01-10 16:48:24,671 - 10000 samples (100 per mini-batch)
2022-01-10 16:48:26,102 - Epoch: [252][  100/  100]    Loss 1.652278    Top1 58.400000    Top5 85.510000    
2022-01-10 16:48:26,146 - ==> Top1: 58.400    Top5: 85.510    Loss: 1.652

2022-01-10 16:48:26,148 - ==> Best [Top1: 59.010   Top5: 85.570   Sparsity:0.00   Params: 725128 on epoch: 251]
2022-01-10 16:48:26,148 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:48:26,173 - 

2022-01-10 16:48:26,173 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:48:28,528 - Epoch: [253][  100/  500]    Overall Loss 1.303779    Objective Loss 1.303779                                        LR 0.000016    Time 0.023533    
2022-01-10 16:48:30,617 - Epoch: [253][  200/  500]    Overall Loss 1.297514    Objective Loss 1.297514                                        LR 0.000016    Time 0.022206    
2022-01-10 16:48:32,705 - Epoch: [253][  300/  500]    Overall Loss 1.302892    Objective Loss 1.302892                                        LR 0.000016    Time 0.021761    
2022-01-10 16:48:34,796 - Epoch: [253][  400/  500]    Overall Loss 1.298446    Objective Loss 1.298446                                        LR 0.000016    Time 0.021546    
2022-01-10 16:48:36,895 - Epoch: [253][  500/  500]    Overall Loss 1.298671    Objective Loss 1.298671    Top1 73.500000    Top5 94.000000    LR 0.000016    Time 0.021433    
2022-01-10 16:48:36,949 - --- validate (epoch=253)-----------
2022-01-10 16:48:36,949 - 10000 samples (100 per mini-batch)
2022-01-10 16:48:38,403 - Epoch: [253][  100/  100]    Loss 1.685965    Top1 57.360000    Top5 85.110000    
2022-01-10 16:48:38,442 - ==> Top1: 57.360    Top5: 85.110    Loss: 1.686

2022-01-10 16:48:38,444 - ==> Best [Top1: 59.010   Top5: 85.570   Sparsity:0.00   Params: 725128 on epoch: 251]
2022-01-10 16:48:38,444 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:48:38,469 - 

2022-01-10 16:48:38,469 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:48:40,805 - Epoch: [254][  100/  500]    Overall Loss 1.283222    Objective Loss 1.283222                                        LR 0.000016    Time 0.023339    
2022-01-10 16:48:42,904 - Epoch: [254][  200/  500]    Overall Loss 1.286842    Objective Loss 1.286842                                        LR 0.000016    Time 0.022161    
2022-01-10 16:48:45,013 - Epoch: [254][  300/  500]    Overall Loss 1.283950    Objective Loss 1.283950                                        LR 0.000016    Time 0.021802    
2022-01-10 16:48:47,115 - Epoch: [254][  400/  500]    Overall Loss 1.288886    Objective Loss 1.288886                                        LR 0.000016    Time 0.021604    
2022-01-10 16:48:49,204 - Epoch: [254][  500/  500]    Overall Loss 1.287151    Objective Loss 1.287151    Top1 72.500000    Top5 92.500000    LR 0.000016    Time 0.021460    
2022-01-10 16:48:49,257 - --- validate (epoch=254)-----------
2022-01-10 16:48:49,257 - 10000 samples (100 per mini-batch)
2022-01-10 16:48:50,701 - Epoch: [254][  100/  100]    Loss 1.666328    Top1 57.840000    Top5 85.260000    
2022-01-10 16:48:50,746 - ==> Top1: 57.840    Top5: 85.260    Loss: 1.666

2022-01-10 16:48:50,748 - ==> Best [Top1: 59.010   Top5: 85.570   Sparsity:0.00   Params: 725128 on epoch: 251]
2022-01-10 16:48:50,748 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:48:50,772 - 

2022-01-10 16:48:50,772 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:48:53,029 - Epoch: [255][  100/  500]    Overall Loss 1.301808    Objective Loss 1.301808                                        LR 0.000016    Time 0.022548    
2022-01-10 16:48:55,089 - Epoch: [255][  200/  500]    Overall Loss 1.295419    Objective Loss 1.295419                                        LR 0.000016    Time 0.021569    
2022-01-10 16:48:57,146 - Epoch: [255][  300/  500]    Overall Loss 1.286873    Objective Loss 1.286873                                        LR 0.000016    Time 0.021236    
2022-01-10 16:48:59,207 - Epoch: [255][  400/  500]    Overall Loss 1.288670    Objective Loss 1.288670                                        LR 0.000016    Time 0.021076    
2022-01-10 16:49:01,273 - Epoch: [255][  500/  500]    Overall Loss 1.282677    Objective Loss 1.282677    Top1 72.500000    Top5 96.000000    LR 0.000016    Time 0.020991    
2022-01-10 16:49:01,326 - --- validate (epoch=255)-----------
2022-01-10 16:49:01,326 - 10000 samples (100 per mini-batch)
2022-01-10 16:49:02,824 - Epoch: [255][  100/  100]    Loss 1.636013    Top1 59.000000    Top5 85.510000    
2022-01-10 16:49:02,880 - ==> Top1: 59.000    Top5: 85.510    Loss: 1.636

2022-01-10 16:49:02,881 - ==> Best [Top1: 59.010   Top5: 85.570   Sparsity:0.00   Params: 725128 on epoch: 251]
2022-01-10 16:49:02,881 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:49:02,906 - 

2022-01-10 16:49:02,906 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:49:05,281 - Epoch: [256][  100/  500]    Overall Loss 1.263175    Objective Loss 1.263175                                        LR 0.000016    Time 0.023730    
2022-01-10 16:49:07,412 - Epoch: [256][  200/  500]    Overall Loss 1.270218    Objective Loss 1.270218                                        LR 0.000016    Time 0.022518    
2022-01-10 16:49:09,537 - Epoch: [256][  300/  500]    Overall Loss 1.265321    Objective Loss 1.265321                                        LR 0.000016    Time 0.022090    
2022-01-10 16:49:11,662 - Epoch: [256][  400/  500]    Overall Loss 1.265854    Objective Loss 1.265854                                        LR 0.000016    Time 0.021879    
2022-01-10 16:49:13,781 - Epoch: [256][  500/  500]    Overall Loss 1.269099    Objective Loss 1.269099    Top1 67.500000    Top5 92.000000    LR 0.000016    Time 0.021738    
2022-01-10 16:49:13,835 - --- validate (epoch=256)-----------
2022-01-10 16:49:13,835 - 10000 samples (100 per mini-batch)
2022-01-10 16:49:15,289 - Epoch: [256][  100/  100]    Loss 1.619931    Top1 58.570000    Top5 86.030000    
2022-01-10 16:49:15,334 - ==> Top1: 58.570    Top5: 86.030    Loss: 1.620

2022-01-10 16:49:15,336 - ==> Best [Top1: 59.010   Top5: 85.570   Sparsity:0.00   Params: 725128 on epoch: 251]
2022-01-10 16:49:15,336 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:49:15,361 - 

2022-01-10 16:49:15,361 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:49:17,656 - Epoch: [257][  100/  500]    Overall Loss 1.259678    Objective Loss 1.259678                                        LR 0.000016    Time 0.022934    
2022-01-10 16:49:19,819 - Epoch: [257][  200/  500]    Overall Loss 1.257020    Objective Loss 1.257020                                        LR 0.000016    Time 0.022277    
2022-01-10 16:49:22,038 - Epoch: [257][  300/  500]    Overall Loss 1.259506    Objective Loss 1.259506                                        LR 0.000016    Time 0.022246    
2022-01-10 16:49:24,207 - Epoch: [257][  400/  500]    Overall Loss 1.257677    Objective Loss 1.257677                                        LR 0.000016    Time 0.022102    
2022-01-10 16:49:26,392 - Epoch: [257][  500/  500]    Overall Loss 1.262319    Objective Loss 1.262319    Top1 72.500000    Top5 95.000000    LR 0.000016    Time 0.022051    
2022-01-10 16:49:26,431 - --- validate (epoch=257)-----------
2022-01-10 16:49:26,431 - 10000 samples (100 per mini-batch)
2022-01-10 16:49:27,971 - Epoch: [257][  100/  100]    Loss 1.651129    Top1 58.190000    Top5 85.350000    
2022-01-10 16:49:28,017 - ==> Top1: 58.190    Top5: 85.350    Loss: 1.651

2022-01-10 16:49:28,019 - ==> Best [Top1: 59.010   Top5: 85.570   Sparsity:0.00   Params: 725128 on epoch: 251]
2022-01-10 16:49:28,019 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:49:28,045 - 

2022-01-10 16:49:28,045 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:49:30,376 - Epoch: [258][  100/  500]    Overall Loss 1.243817    Objective Loss 1.243817                                        LR 0.000016    Time 0.023297    
2022-01-10 16:49:32,516 - Epoch: [258][  200/  500]    Overall Loss 1.247326    Objective Loss 1.247326                                        LR 0.000016    Time 0.022344    
2022-01-10 16:49:34,651 - Epoch: [258][  300/  500]    Overall Loss 1.255362    Objective Loss 1.255362                                        LR 0.000016    Time 0.022011    
2022-01-10 16:49:36,791 - Epoch: [258][  400/  500]    Overall Loss 1.251300    Objective Loss 1.251300                                        LR 0.000016    Time 0.021856    
2022-01-10 16:49:38,943 - Epoch: [258][  500/  500]    Overall Loss 1.253391    Objective Loss 1.253391    Top1 71.000000    Top5 95.500000    LR 0.000016    Time 0.021787    
2022-01-10 16:49:38,992 - --- validate (epoch=258)-----------
2022-01-10 16:49:38,992 - 10000 samples (100 per mini-batch)
2022-01-10 16:49:40,523 - Epoch: [258][  100/  100]    Loss 1.670726    Top1 56.690000    Top5 84.540000    
2022-01-10 16:49:40,575 - ==> Top1: 56.690    Top5: 84.540    Loss: 1.671

2022-01-10 16:49:40,577 - ==> Best [Top1: 59.010   Top5: 85.570   Sparsity:0.00   Params: 725128 on epoch: 251]
2022-01-10 16:49:40,577 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:49:40,602 - 

2022-01-10 16:49:40,602 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:49:42,987 - Epoch: [259][  100/  500]    Overall Loss 1.233856    Objective Loss 1.233856                                        LR 0.000016    Time 0.023831    
2022-01-10 16:49:45,119 - Epoch: [259][  200/  500]    Overall Loss 1.243704    Objective Loss 1.243704                                        LR 0.000016    Time 0.022572    
2022-01-10 16:49:47,243 - Epoch: [259][  300/  500]    Overall Loss 1.242849    Objective Loss 1.242849                                        LR 0.000016    Time 0.022126    
2022-01-10 16:49:49,370 - Epoch: [259][  400/  500]    Overall Loss 1.246147    Objective Loss 1.246147                                        LR 0.000016    Time 0.021909    
2022-01-10 16:49:51,508 - Epoch: [259][  500/  500]    Overall Loss 1.244826    Objective Loss 1.244826    Top1 70.500000    Top5 91.500000    LR 0.000016    Time 0.021802    
2022-01-10 16:49:51,559 - --- validate (epoch=259)-----------
2022-01-10 16:49:51,559 - 10000 samples (100 per mini-batch)
2022-01-10 16:49:53,104 - Epoch: [259][  100/  100]    Loss 1.641936    Top1 57.760000    Top5 85.540000    
2022-01-10 16:49:53,153 - ==> Top1: 57.760    Top5: 85.540    Loss: 1.642

2022-01-10 16:49:53,154 - ==> Best [Top1: 59.010   Top5: 85.570   Sparsity:0.00   Params: 725128 on epoch: 251]
2022-01-10 16:49:53,155 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:49:53,180 - 

2022-01-10 16:49:53,180 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:49:55,491 - Epoch: [260][  100/  500]    Overall Loss 1.224570    Objective Loss 1.224570                                        LR 0.000016    Time 0.023091    
2022-01-10 16:49:57,642 - Epoch: [260][  200/  500]    Overall Loss 1.218982    Objective Loss 1.218982                                        LR 0.000016    Time 0.022299    
2022-01-10 16:49:59,781 - Epoch: [260][  300/  500]    Overall Loss 1.226975    Objective Loss 1.226975                                        LR 0.000016    Time 0.021992    
2022-01-10 16:50:01,890 - Epoch: [260][  400/  500]    Overall Loss 1.226629    Objective Loss 1.226629                                        LR 0.000016    Time 0.021765    
2022-01-10 16:50:03,979 - Epoch: [260][  500/  500]    Overall Loss 1.232820    Objective Loss 1.232820    Top1 70.000000    Top5 96.000000    LR 0.000016    Time 0.021587    
2022-01-10 16:50:04,032 - --- validate (epoch=260)-----------
2022-01-10 16:50:04,032 - 10000 samples (100 per mini-batch)
2022-01-10 16:50:05,534 - Epoch: [260][  100/  100]    Loss 1.665947    Top1 57.540000    Top5 84.650000    
2022-01-10 16:50:05,588 - ==> Top1: 57.540    Top5: 84.650    Loss: 1.666

2022-01-10 16:50:05,589 - ==> Best [Top1: 59.010   Top5: 85.570   Sparsity:0.00   Params: 725128 on epoch: 251]
2022-01-10 16:50:05,589 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:50:05,608 - 

2022-01-10 16:50:05,608 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:50:07,881 - Epoch: [261][  100/  500]    Overall Loss 1.225212    Objective Loss 1.225212                                        LR 0.000016    Time 0.022720    
2022-01-10 16:50:10,002 - Epoch: [261][  200/  500]    Overall Loss 1.219922    Objective Loss 1.219922                                        LR 0.000016    Time 0.021960    
2022-01-10 16:50:12,121 - Epoch: [261][  300/  500]    Overall Loss 1.220349    Objective Loss 1.220349                                        LR 0.000016    Time 0.021702    
2022-01-10 16:50:14,223 - Epoch: [261][  400/  500]    Overall Loss 1.228624    Objective Loss 1.228624                                        LR 0.000016    Time 0.021528    
2022-01-10 16:50:16,338 - Epoch: [261][  500/  500]    Overall Loss 1.227905    Objective Loss 1.227905    Top1 70.000000    Top5 92.500000    LR 0.000016    Time 0.021451    
2022-01-10 16:50:16,391 - --- validate (epoch=261)-----------
2022-01-10 16:50:16,391 - 10000 samples (100 per mini-batch)
2022-01-10 16:50:17,835 - Epoch: [261][  100/  100]    Loss 1.652433    Top1 57.630000    Top5 85.300000    
2022-01-10 16:50:17,886 - ==> Top1: 57.630    Top5: 85.300    Loss: 1.652

2022-01-10 16:50:17,888 - ==> Best [Top1: 59.010   Top5: 85.570   Sparsity:0.00   Params: 725128 on epoch: 251]
2022-01-10 16:50:17,888 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:50:17,913 - 

2022-01-10 16:50:17,913 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:50:20,315 - Epoch: [262][  100/  500]    Overall Loss 1.231361    Objective Loss 1.231361                                        LR 0.000016    Time 0.024003    
2022-01-10 16:50:22,441 - Epoch: [262][  200/  500]    Overall Loss 1.222242    Objective Loss 1.222242                                        LR 0.000016    Time 0.022624    
2022-01-10 16:50:24,619 - Epoch: [262][  300/  500]    Overall Loss 1.222224    Objective Loss 1.222224                                        LR 0.000016    Time 0.022338    
2022-01-10 16:50:26,773 - Epoch: [262][  400/  500]    Overall Loss 1.229707    Objective Loss 1.229707                                        LR 0.000016    Time 0.022137    
2022-01-10 16:50:28,881 - Epoch: [262][  500/  500]    Overall Loss 1.225178    Objective Loss 1.225178    Top1 69.000000    Top5 95.000000    LR 0.000016    Time 0.021924    
2022-01-10 16:50:28,926 - --- validate (epoch=262)-----------
2022-01-10 16:50:28,926 - 10000 samples (100 per mini-batch)
2022-01-10 16:50:30,502 - Epoch: [262][  100/  100]    Loss 1.580528    Top1 59.410000    Top5 86.070000    
2022-01-10 16:50:30,541 - ==> Top1: 59.410    Top5: 86.070    Loss: 1.581

2022-01-10 16:50:30,542 - ==> Best [Top1: 59.410   Top5: 86.070   Sparsity:0.00   Params: 725128 on epoch: 262]
2022-01-10 16:50:30,542 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:50:30,574 - 

2022-01-10 16:50:30,574 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:50:32,862 - Epoch: [263][  100/  500]    Overall Loss 1.211707    Objective Loss 1.211707                                        LR 0.000016    Time 0.022864    
2022-01-10 16:50:34,979 - Epoch: [263][  200/  500]    Overall Loss 1.206902    Objective Loss 1.206902                                        LR 0.000016    Time 0.022013    
2022-01-10 16:50:37,104 - Epoch: [263][  300/  500]    Overall Loss 1.213403    Objective Loss 1.213403                                        LR 0.000016    Time 0.021757    
2022-01-10 16:50:39,227 - Epoch: [263][  400/  500]    Overall Loss 1.210942    Objective Loss 1.210942                                        LR 0.000016    Time 0.021620    
2022-01-10 16:50:41,339 - Epoch: [263][  500/  500]    Overall Loss 1.212201    Objective Loss 1.212201    Top1 72.500000    Top5 95.500000    LR 0.000016    Time 0.021519    
2022-01-10 16:50:41,387 - --- validate (epoch=263)-----------
2022-01-10 16:50:41,387 - 10000 samples (100 per mini-batch)
2022-01-10 16:50:42,840 - Epoch: [263][  100/  100]    Loss 1.618445    Top1 58.510000    Top5 85.340000    
2022-01-10 16:50:42,886 - ==> Top1: 58.510    Top5: 85.340    Loss: 1.618

2022-01-10 16:50:42,888 - ==> Best [Top1: 59.410   Top5: 86.070   Sparsity:0.00   Params: 725128 on epoch: 262]
2022-01-10 16:50:42,888 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:50:42,913 - 

2022-01-10 16:50:42,914 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:50:45,282 - Epoch: [264][  100/  500]    Overall Loss 1.198228    Objective Loss 1.198228                                        LR 0.000016    Time 0.023669    
2022-01-10 16:50:47,408 - Epoch: [264][  200/  500]    Overall Loss 1.201660    Objective Loss 1.201660                                        LR 0.000016    Time 0.022463    
2022-01-10 16:50:49,509 - Epoch: [264][  300/  500]    Overall Loss 1.209096    Objective Loss 1.209096                                        LR 0.000016    Time 0.021973    
2022-01-10 16:50:51,602 - Epoch: [264][  400/  500]    Overall Loss 1.205883    Objective Loss 1.205883                                        LR 0.000016    Time 0.021711    
2022-01-10 16:50:53,704 - Epoch: [264][  500/  500]    Overall Loss 1.202522    Objective Loss 1.202522    Top1 73.500000    Top5 93.000000    LR 0.000016    Time 0.021571    
2022-01-10 16:50:53,757 - --- validate (epoch=264)-----------
2022-01-10 16:50:53,757 - 10000 samples (100 per mini-batch)
2022-01-10 16:50:55,178 - Epoch: [264][  100/  100]    Loss 1.595712    Top1 58.840000    Top5 85.920000    
2022-01-10 16:50:55,227 - ==> Top1: 58.840    Top5: 85.920    Loss: 1.596

2022-01-10 16:50:55,229 - ==> Best [Top1: 59.410   Top5: 86.070   Sparsity:0.00   Params: 725128 on epoch: 262]
2022-01-10 16:50:55,229 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:50:55,253 - 

2022-01-10 16:50:55,253 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:50:57,619 - Epoch: [265][  100/  500]    Overall Loss 1.200686    Objective Loss 1.200686                                        LR 0.000016    Time 0.023640    
2022-01-10 16:50:59,751 - Epoch: [265][  200/  500]    Overall Loss 1.209926    Objective Loss 1.209926                                        LR 0.000016    Time 0.022478    
2022-01-10 16:51:01,904 - Epoch: [265][  300/  500]    Overall Loss 1.204762    Objective Loss 1.204762                                        LR 0.000016    Time 0.022157    
2022-01-10 16:51:04,052 - Epoch: [265][  400/  500]    Overall Loss 1.203324    Objective Loss 1.203324                                        LR 0.000016    Time 0.021984    
2022-01-10 16:51:06,173 - Epoch: [265][  500/  500]    Overall Loss 1.200067    Objective Loss 1.200067    Top1 69.000000    Top5 91.500000    LR 0.000016    Time 0.021828    
2022-01-10 16:51:06,212 - --- validate (epoch=265)-----------
2022-01-10 16:51:06,212 - 10000 samples (100 per mini-batch)
2022-01-10 16:51:07,649 - Epoch: [265][  100/  100]    Loss 1.605794    Top1 58.780000    Top5 86.270000    
2022-01-10 16:51:07,697 - ==> Top1: 58.780    Top5: 86.270    Loss: 1.606

2022-01-10 16:51:07,698 - ==> Best [Top1: 59.410   Top5: 86.070   Sparsity:0.00   Params: 725128 on epoch: 262]
2022-01-10 16:51:07,698 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:51:07,724 - 

2022-01-10 16:51:07,724 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:51:10,050 - Epoch: [266][  100/  500]    Overall Loss 1.192932    Objective Loss 1.192932                                        LR 0.000016    Time 0.023248    
2022-01-10 16:51:12,153 - Epoch: [266][  200/  500]    Overall Loss 1.181653    Objective Loss 1.181653                                        LR 0.000016    Time 0.022133    
2022-01-10 16:51:14,244 - Epoch: [266][  300/  500]    Overall Loss 1.196550    Objective Loss 1.196550                                        LR 0.000016    Time 0.021723    
2022-01-10 16:51:16,342 - Epoch: [266][  400/  500]    Overall Loss 1.194478    Objective Loss 1.194478                                        LR 0.000016    Time 0.021535    
2022-01-10 16:51:18,451 - Epoch: [266][  500/  500]    Overall Loss 1.195876    Objective Loss 1.195876    Top1 77.500000    Top5 97.500000    LR 0.000016    Time 0.021444    
2022-01-10 16:51:18,497 - --- validate (epoch=266)-----------
2022-01-10 16:51:18,497 - 10000 samples (100 per mini-batch)
2022-01-10 16:51:19,932 - Epoch: [266][  100/  100]    Loss 1.597745    Top1 59.360000    Top5 85.470000    
2022-01-10 16:51:19,976 - ==> Top1: 59.360    Top5: 85.470    Loss: 1.598

2022-01-10 16:51:19,978 - ==> Best [Top1: 59.410   Top5: 86.070   Sparsity:0.00   Params: 725128 on epoch: 262]
2022-01-10 16:51:19,978 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:51:20,004 - 

2022-01-10 16:51:20,004 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:51:22,376 - Epoch: [267][  100/  500]    Overall Loss 1.190998    Objective Loss 1.190998                                        LR 0.000016    Time 0.023710    
2022-01-10 16:51:24,477 - Epoch: [267][  200/  500]    Overall Loss 1.192258    Objective Loss 1.192258                                        LR 0.000016    Time 0.022357    
2022-01-10 16:51:26,604 - Epoch: [267][  300/  500]    Overall Loss 1.193297    Objective Loss 1.193297                                        LR 0.000016    Time 0.021991    
2022-01-10 16:51:28,715 - Epoch: [267][  400/  500]    Overall Loss 1.189235    Objective Loss 1.189235                                        LR 0.000016    Time 0.021769    
2022-01-10 16:51:30,829 - Epoch: [267][  500/  500]    Overall Loss 1.189053    Objective Loss 1.189053    Top1 77.500000    Top5 97.000000    LR 0.000016    Time 0.021641    
2022-01-10 16:51:30,879 - --- validate (epoch=267)-----------
2022-01-10 16:51:30,879 - 10000 samples (100 per mini-batch)
2022-01-10 16:51:32,318 - Epoch: [267][  100/  100]    Loss 1.576771    Top1 59.550000    Top5 86.080000    
2022-01-10 16:51:32,374 - ==> Top1: 59.550    Top5: 86.080    Loss: 1.577

2022-01-10 16:51:32,376 - ==> Best [Top1: 59.550   Top5: 86.080   Sparsity:0.00   Params: 725128 on epoch: 267]
2022-01-10 16:51:32,376 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:51:32,408 - 

2022-01-10 16:51:32,408 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:51:34,766 - Epoch: [268][  100/  500]    Overall Loss 1.171642    Objective Loss 1.171642                                        LR 0.000016    Time 0.023567    
2022-01-10 16:51:36,864 - Epoch: [268][  200/  500]    Overall Loss 1.171952    Objective Loss 1.171952                                        LR 0.000016    Time 0.022268    
2022-01-10 16:51:38,962 - Epoch: [268][  300/  500]    Overall Loss 1.171379    Objective Loss 1.171379                                        LR 0.000016    Time 0.021835    
2022-01-10 16:51:41,099 - Epoch: [268][  400/  500]    Overall Loss 1.174081    Objective Loss 1.174081                                        LR 0.000016    Time 0.021716    
2022-01-10 16:51:43,233 - Epoch: [268][  500/  500]    Overall Loss 1.175155    Objective Loss 1.175155    Top1 72.000000    Top5 94.000000    LR 0.000016    Time 0.021640    
2022-01-10 16:51:43,285 - --- validate (epoch=268)-----------
2022-01-10 16:51:43,285 - 10000 samples (100 per mini-batch)
2022-01-10 16:51:44,767 - Epoch: [268][  100/  100]    Loss 1.600769    Top1 58.300000    Top5 85.490000    
2022-01-10 16:51:44,813 - ==> Top1: 58.300    Top5: 85.490    Loss: 1.601

2022-01-10 16:51:44,815 - ==> Best [Top1: 59.550   Top5: 86.080   Sparsity:0.00   Params: 725128 on epoch: 267]
2022-01-10 16:51:44,815 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:51:44,840 - 

2022-01-10 16:51:44,841 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:51:47,170 - Epoch: [269][  100/  500]    Overall Loss 1.174046    Objective Loss 1.174046                                        LR 0.000016    Time 0.023281    
2022-01-10 16:51:49,314 - Epoch: [269][  200/  500]    Overall Loss 1.187213    Objective Loss 1.187213                                        LR 0.000016    Time 0.022356    
2022-01-10 16:51:51,470 - Epoch: [269][  300/  500]    Overall Loss 1.185201    Objective Loss 1.185201                                        LR 0.000016    Time 0.022087    
2022-01-10 16:51:53,613 - Epoch: [269][  400/  500]    Overall Loss 1.180312    Objective Loss 1.180312                                        LR 0.000016    Time 0.021919    
2022-01-10 16:51:55,755 - Epoch: [269][  500/  500]    Overall Loss 1.180577    Objective Loss 1.180577    Top1 76.500000    Top5 96.500000    LR 0.000016    Time 0.021817    
2022-01-10 16:51:55,800 - --- validate (epoch=269)-----------
2022-01-10 16:51:55,801 - 10000 samples (100 per mini-batch)
2022-01-10 16:51:57,258 - Epoch: [269][  100/  100]    Loss 1.618736    Top1 57.840000    Top5 85.020000    
2022-01-10 16:51:57,303 - ==> Top1: 57.840    Top5: 85.020    Loss: 1.619

2022-01-10 16:51:57,305 - ==> Best [Top1: 59.550   Top5: 86.080   Sparsity:0.00   Params: 725128 on epoch: 267]
2022-01-10 16:51:57,305 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:51:57,330 - 

2022-01-10 16:51:57,330 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:51:59,714 - Epoch: [270][  100/  500]    Overall Loss 1.166582    Objective Loss 1.166582                                        LR 0.000016    Time 0.023825    
2022-01-10 16:52:01,827 - Epoch: [270][  200/  500]    Overall Loss 1.161569    Objective Loss 1.161569                                        LR 0.000016    Time 0.022475    
2022-01-10 16:52:03,941 - Epoch: [270][  300/  500]    Overall Loss 1.164566    Objective Loss 1.164566                                        LR 0.000016    Time 0.022027    
2022-01-10 16:52:06,036 - Epoch: [270][  400/  500]    Overall Loss 1.170111    Objective Loss 1.170111                                        LR 0.000016    Time 0.021753    
2022-01-10 16:52:08,134 - Epoch: [270][  500/  500]    Overall Loss 1.174293    Objective Loss 1.174293    Top1 72.500000    Top5 92.000000    LR 0.000016    Time 0.021598    
2022-01-10 16:52:08,178 - --- validate (epoch=270)-----------
2022-01-10 16:52:08,179 - 10000 samples (100 per mini-batch)
2022-01-10 16:52:09,628 - Epoch: [270][  100/  100]    Loss 1.600830    Top1 59.080000    Top5 85.080000    
2022-01-10 16:52:09,673 - ==> Top1: 59.080    Top5: 85.080    Loss: 1.601

2022-01-10 16:52:09,674 - ==> Best [Top1: 59.550   Top5: 86.080   Sparsity:0.00   Params: 725128 on epoch: 267]
2022-01-10 16:52:09,674 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:52:09,700 - 

2022-01-10 16:52:09,700 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:52:12,051 - Epoch: [271][  100/  500]    Overall Loss 1.164628    Objective Loss 1.164628                                        LR 0.000016    Time 0.023496    
2022-01-10 16:52:14,144 - Epoch: [271][  200/  500]    Overall Loss 1.157372    Objective Loss 1.157372                                        LR 0.000016    Time 0.022209    
2022-01-10 16:52:16,236 - Epoch: [271][  300/  500]    Overall Loss 1.166172    Objective Loss 1.166172                                        LR 0.000016    Time 0.021776    
2022-01-10 16:52:18,334 - Epoch: [271][  400/  500]    Overall Loss 1.168269    Objective Loss 1.168269                                        LR 0.000016    Time 0.021575    
2022-01-10 16:52:20,441 - Epoch: [271][  500/  500]    Overall Loss 1.169447    Objective Loss 1.169447    Top1 68.500000    Top5 94.500000    LR 0.000016    Time 0.021472    
2022-01-10 16:52:20,491 - --- validate (epoch=271)-----------
2022-01-10 16:52:20,491 - 10000 samples (100 per mini-batch)
2022-01-10 16:52:21,933 - Epoch: [271][  100/  100]    Loss 1.581745    Top1 59.310000    Top5 85.810000    
2022-01-10 16:52:21,977 - ==> Top1: 59.310    Top5: 85.810    Loss: 1.582

2022-01-10 16:52:21,979 - ==> Best [Top1: 59.550   Top5: 86.080   Sparsity:0.00   Params: 725128 on epoch: 267]
2022-01-10 16:52:21,979 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:52:22,004 - 

2022-01-10 16:52:22,004 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:52:24,276 - Epoch: [272][  100/  500]    Overall Loss 1.171923    Objective Loss 1.171923                                        LR 0.000016    Time 0.022709    
2022-01-10 16:52:26,408 - Epoch: [272][  200/  500]    Overall Loss 1.176875    Objective Loss 1.176875                                        LR 0.000016    Time 0.022008    
2022-01-10 16:52:28,559 - Epoch: [272][  300/  500]    Overall Loss 1.167776    Objective Loss 1.167776                                        LR 0.000016    Time 0.021839    
2022-01-10 16:52:30,709 - Epoch: [272][  400/  500]    Overall Loss 1.173158    Objective Loss 1.173158                                        LR 0.000016    Time 0.021751    
2022-01-10 16:52:32,847 - Epoch: [272][  500/  500]    Overall Loss 1.171476    Objective Loss 1.171476    Top1 74.000000    Top5 95.500000    LR 0.000016    Time 0.021676    
2022-01-10 16:52:32,894 - --- validate (epoch=272)-----------
2022-01-10 16:52:32,895 - 10000 samples (100 per mini-batch)
2022-01-10 16:52:34,352 - Epoch: [272][  100/  100]    Loss 1.588616    Top1 59.160000    Top5 85.520000    
2022-01-10 16:52:34,403 - ==> Top1: 59.160    Top5: 85.520    Loss: 1.589

2022-01-10 16:52:34,405 - ==> Best [Top1: 59.550   Top5: 86.080   Sparsity:0.00   Params: 725128 on epoch: 267]
2022-01-10 16:52:34,405 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:52:34,430 - 

2022-01-10 16:52:34,430 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:52:36,836 - Epoch: [273][  100/  500]    Overall Loss 1.152836    Objective Loss 1.152836                                        LR 0.000016    Time 0.024044    
2022-01-10 16:52:38,952 - Epoch: [273][  200/  500]    Overall Loss 1.155143    Objective Loss 1.155143                                        LR 0.000016    Time 0.022596    
2022-01-10 16:52:41,032 - Epoch: [273][  300/  500]    Overall Loss 1.151737    Objective Loss 1.151737                                        LR 0.000016    Time 0.021996    
2022-01-10 16:52:43,116 - Epoch: [273][  400/  500]    Overall Loss 1.155456    Objective Loss 1.155456                                        LR 0.000016    Time 0.021704    
2022-01-10 16:52:45,209 - Epoch: [273][  500/  500]    Overall Loss 1.159112    Objective Loss 1.159112    Top1 72.000000    Top5 92.500000    LR 0.000016    Time 0.021548    
2022-01-10 16:52:45,255 - --- validate (epoch=273)-----------
2022-01-10 16:52:45,256 - 10000 samples (100 per mini-batch)
2022-01-10 16:52:46,703 - Epoch: [273][  100/  100]    Loss 1.574273    Top1 58.690000    Top5 85.760000    
2022-01-10 16:52:46,745 - ==> Top1: 58.690    Top5: 85.760    Loss: 1.574

2022-01-10 16:52:46,747 - ==> Best [Top1: 59.550   Top5: 86.080   Sparsity:0.00   Params: 725128 on epoch: 267]
2022-01-10 16:52:46,747 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:52:46,771 - 

2022-01-10 16:52:46,772 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:52:49,038 - Epoch: [274][  100/  500]    Overall Loss 1.146418    Objective Loss 1.146418                                        LR 0.000016    Time 0.022653    
2022-01-10 16:52:51,138 - Epoch: [274][  200/  500]    Overall Loss 1.156089    Objective Loss 1.156089                                        LR 0.000016    Time 0.021822    
2022-01-10 16:52:53,243 - Epoch: [274][  300/  500]    Overall Loss 1.162203    Objective Loss 1.162203                                        LR 0.000016    Time 0.021561    
2022-01-10 16:52:55,338 - Epoch: [274][  400/  500]    Overall Loss 1.161909    Objective Loss 1.161909                                        LR 0.000016    Time 0.021406    
2022-01-10 16:52:57,435 - Epoch: [274][  500/  500]    Overall Loss 1.163050    Objective Loss 1.163050    Top1 74.000000    Top5 95.500000    LR 0.000016    Time 0.021317    
2022-01-10 16:52:57,489 - --- validate (epoch=274)-----------
2022-01-10 16:52:57,489 - 10000 samples (100 per mini-batch)
2022-01-10 16:52:59,080 - Epoch: [274][  100/  100]    Loss 1.590106    Top1 58.780000    Top5 85.850000    
2022-01-10 16:52:59,122 - ==> Top1: 58.780    Top5: 85.850    Loss: 1.590

2022-01-10 16:52:59,123 - ==> Best [Top1: 59.550   Top5: 86.080   Sparsity:0.00   Params: 725128 on epoch: 267]
2022-01-10 16:52:59,124 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:52:59,148 - 

2022-01-10 16:52:59,148 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:53:01,410 - Epoch: [275][  100/  500]    Overall Loss 1.134590    Objective Loss 1.134590                                        LR 0.000016    Time 0.022597    
2022-01-10 16:53:03,535 - Epoch: [275][  200/  500]    Overall Loss 1.154230    Objective Loss 1.154230                                        LR 0.000016    Time 0.021922    
2022-01-10 16:53:05,681 - Epoch: [275][  300/  500]    Overall Loss 1.154575    Objective Loss 1.154575                                        LR 0.000016    Time 0.021766    
2022-01-10 16:53:07,782 - Epoch: [275][  400/  500]    Overall Loss 1.156759    Objective Loss 1.156759                                        LR 0.000016    Time 0.021573    
2022-01-10 16:53:09,888 - Epoch: [275][  500/  500]    Overall Loss 1.153308    Objective Loss 1.153308    Top1 73.500000    Top5 94.500000    LR 0.000016    Time 0.021470    
2022-01-10 16:53:09,934 - --- validate (epoch=275)-----------
2022-01-10 16:53:09,934 - 10000 samples (100 per mini-batch)
2022-01-10 16:53:11,374 - Epoch: [275][  100/  100]    Loss 1.611368    Top1 58.160000    Top5 85.040000    
2022-01-10 16:53:11,427 - ==> Top1: 58.160    Top5: 85.040    Loss: 1.611

2022-01-10 16:53:11,429 - ==> Best [Top1: 59.550   Top5: 86.080   Sparsity:0.00   Params: 725128 on epoch: 267]
2022-01-10 16:53:11,429 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:53:11,454 - 

2022-01-10 16:53:11,454 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:53:13,775 - Epoch: [276][  100/  500]    Overall Loss 1.148182    Objective Loss 1.148182                                        LR 0.000016    Time 0.023197    
2022-01-10 16:53:15,865 - Epoch: [276][  200/  500]    Overall Loss 1.154157    Objective Loss 1.154157                                        LR 0.000016    Time 0.022042    
2022-01-10 16:53:17,965 - Epoch: [276][  300/  500]    Overall Loss 1.156412    Objective Loss 1.156412                                        LR 0.000016    Time 0.021694    
2022-01-10 16:53:20,054 - Epoch: [276][  400/  500]    Overall Loss 1.159257    Objective Loss 1.159257                                        LR 0.000016    Time 0.021491    
2022-01-10 16:53:22,141 - Epoch: [276][  500/  500]    Overall Loss 1.154815    Objective Loss 1.154815    Top1 68.500000    Top5 98.000000    LR 0.000016    Time 0.021364    
2022-01-10 16:53:22,194 - --- validate (epoch=276)-----------
2022-01-10 16:53:22,194 - 10000 samples (100 per mini-batch)
2022-01-10 16:53:23,628 - Epoch: [276][  100/  100]    Loss 1.627325    Top1 57.860000    Top5 84.750000    
2022-01-10 16:53:23,683 - ==> Top1: 57.860    Top5: 84.750    Loss: 1.627

2022-01-10 16:53:23,685 - ==> Best [Top1: 59.550   Top5: 86.080   Sparsity:0.00   Params: 725128 on epoch: 267]
2022-01-10 16:53:23,685 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:53:23,710 - 

2022-01-10 16:53:23,710 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:53:26,002 - Epoch: [277][  100/  500]    Overall Loss 1.145567    Objective Loss 1.145567                                        LR 0.000016    Time 0.022904    
2022-01-10 16:53:28,081 - Epoch: [277][  200/  500]    Overall Loss 1.152446    Objective Loss 1.152446                                        LR 0.000016    Time 0.021846    
2022-01-10 16:53:30,156 - Epoch: [277][  300/  500]    Overall Loss 1.149774    Objective Loss 1.149774                                        LR 0.000016    Time 0.021477    
2022-01-10 16:53:32,233 - Epoch: [277][  400/  500]    Overall Loss 1.156272    Objective Loss 1.156272                                        LR 0.000016    Time 0.021297    
2022-01-10 16:53:34,317 - Epoch: [277][  500/  500]    Overall Loss 1.154893    Objective Loss 1.154893    Top1 68.500000    Top5 93.000000    LR 0.000016    Time 0.021204    
2022-01-10 16:53:34,359 - --- validate (epoch=277)-----------
2022-01-10 16:53:34,359 - 10000 samples (100 per mini-batch)
2022-01-10 16:53:35,825 - Epoch: [277][  100/  100]    Loss 1.585256    Top1 58.500000    Top5 85.660000    
2022-01-10 16:53:35,868 - ==> Top1: 58.500    Top5: 85.660    Loss: 1.585

2022-01-10 16:53:35,870 - ==> Best [Top1: 59.550   Top5: 86.080   Sparsity:0.00   Params: 725128 on epoch: 267]
2022-01-10 16:53:35,870 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:53:35,956 - 

2022-01-10 16:53:35,956 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:53:38,229 - Epoch: [278][  100/  500]    Overall Loss 1.147758    Objective Loss 1.147758                                        LR 0.000016    Time 0.022712    
2022-01-10 16:53:40,328 - Epoch: [278][  200/  500]    Overall Loss 1.136405    Objective Loss 1.136405                                        LR 0.000016    Time 0.021846    
2022-01-10 16:53:42,428 - Epoch: [278][  300/  500]    Overall Loss 1.137448    Objective Loss 1.137448                                        LR 0.000016    Time 0.021561    
2022-01-10 16:53:44,531 - Epoch: [278][  400/  500]    Overall Loss 1.140462    Objective Loss 1.140462                                        LR 0.000016    Time 0.021426    
2022-01-10 16:53:46,638 - Epoch: [278][  500/  500]    Overall Loss 1.142426    Objective Loss 1.142426    Top1 68.500000    Top5 91.500000    LR 0.000016    Time 0.021353    
2022-01-10 16:53:46,688 - --- validate (epoch=278)-----------
2022-01-10 16:53:46,688 - 10000 samples (100 per mini-batch)
2022-01-10 16:53:48,249 - Epoch: [278][  100/  100]    Loss 1.582042    Top1 58.460000    Top5 85.470000    
2022-01-10 16:53:48,299 - ==> Top1: 58.460    Top5: 85.470    Loss: 1.582

2022-01-10 16:53:48,301 - ==> Best [Top1: 59.550   Top5: 86.080   Sparsity:0.00   Params: 725128 on epoch: 267]
2022-01-10 16:53:48,301 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:53:48,326 - 

2022-01-10 16:53:48,327 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:53:50,662 - Epoch: [279][  100/  500]    Overall Loss 1.136012    Objective Loss 1.136012                                        LR 0.000016    Time 0.023341    
2022-01-10 16:53:52,762 - Epoch: [279][  200/  500]    Overall Loss 1.135653    Objective Loss 1.135653                                        LR 0.000016    Time 0.022165    
2022-01-10 16:53:54,861 - Epoch: [279][  300/  500]    Overall Loss 1.137587    Objective Loss 1.137587                                        LR 0.000016    Time 0.021769    
2022-01-10 16:53:56,988 - Epoch: [279][  400/  500]    Overall Loss 1.142520    Objective Loss 1.142520                                        LR 0.000016    Time 0.021644    
2022-01-10 16:53:59,111 - Epoch: [279][  500/  500]    Overall Loss 1.141051    Objective Loss 1.141051    Top1 75.000000    Top5 95.500000    LR 0.000016    Time 0.021558    
2022-01-10 16:53:59,162 - --- validate (epoch=279)-----------
2022-01-10 16:53:59,163 - 10000 samples (100 per mini-batch)
2022-01-10 16:54:00,610 - Epoch: [279][  100/  100]    Loss 1.595760    Top1 58.610000    Top5 85.520000    
2022-01-10 16:54:00,665 - ==> Top1: 58.610    Top5: 85.520    Loss: 1.596

2022-01-10 16:54:00,666 - ==> Best [Top1: 59.550   Top5: 86.080   Sparsity:0.00   Params: 725128 on epoch: 267]
2022-01-10 16:54:00,666 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:54:00,685 - 

2022-01-10 16:54:00,685 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:54:02,948 - Epoch: [280][  100/  500]    Overall Loss 1.136724    Objective Loss 1.136724                                        LR 0.000016    Time 0.022611    
2022-01-10 16:54:05,040 - Epoch: [280][  200/  500]    Overall Loss 1.137772    Objective Loss 1.137772                                        LR 0.000016    Time 0.021760    
2022-01-10 16:54:07,129 - Epoch: [280][  300/  500]    Overall Loss 1.141475    Objective Loss 1.141475                                        LR 0.000016    Time 0.021467    
2022-01-10 16:54:09,218 - Epoch: [280][  400/  500]    Overall Loss 1.141244    Objective Loss 1.141244                                        LR 0.000016    Time 0.021322    
2022-01-10 16:54:11,320 - Epoch: [280][  500/  500]    Overall Loss 1.138981    Objective Loss 1.138981    Top1 75.500000    Top5 98.000000    LR 0.000016    Time 0.021259    
2022-01-10 16:54:11,371 - --- validate (epoch=280)-----------
2022-01-10 16:54:11,371 - 10000 samples (100 per mini-batch)
2022-01-10 16:54:12,799 - Epoch: [280][  100/  100]    Loss 1.525788    Top1 60.160000    Top5 86.510000    
2022-01-10 16:54:12,847 - ==> Top1: 60.160    Top5: 86.510    Loss: 1.526

2022-01-10 16:54:12,849 - ==> Best [Top1: 60.160   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 280]
2022-01-10 16:54:12,849 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:54:12,880 - 

2022-01-10 16:54:12,881 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:54:15,227 - Epoch: [281][  100/  500]    Overall Loss 1.115819    Objective Loss 1.115819                                        LR 0.000016    Time 0.023452    
2022-01-10 16:54:17,346 - Epoch: [281][  200/  500]    Overall Loss 1.120628    Objective Loss 1.120628                                        LR 0.000016    Time 0.022317    
2022-01-10 16:54:19,456 - Epoch: [281][  300/  500]    Overall Loss 1.119123    Objective Loss 1.119123                                        LR 0.000016    Time 0.021908    
2022-01-10 16:54:21,566 - Epoch: [281][  400/  500]    Overall Loss 1.123883    Objective Loss 1.123883                                        LR 0.000016    Time 0.021704    
2022-01-10 16:54:23,700 - Epoch: [281][  500/  500]    Overall Loss 1.129000    Objective Loss 1.129000    Top1 72.500000    Top5 96.000000    LR 0.000016    Time 0.021630    
2022-01-10 16:54:23,751 - --- validate (epoch=281)-----------
2022-01-10 16:54:23,751 - 10000 samples (100 per mini-batch)
2022-01-10 16:54:25,198 - Epoch: [281][  100/  100]    Loss 1.619102    Top1 57.660000    Top5 84.570000    
2022-01-10 16:54:25,250 - ==> Top1: 57.660    Top5: 84.570    Loss: 1.619

2022-01-10 16:54:25,252 - ==> Best [Top1: 60.160   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 280]
2022-01-10 16:54:25,252 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:54:25,277 - 

2022-01-10 16:54:25,277 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:54:27,705 - Epoch: [282][  100/  500]    Overall Loss 1.125384    Objective Loss 1.125384                                        LR 0.000016    Time 0.024268    
2022-01-10 16:54:29,823 - Epoch: [282][  200/  500]    Overall Loss 1.128920    Objective Loss 1.128920                                        LR 0.000016    Time 0.022720    
2022-01-10 16:54:31,971 - Epoch: [282][  300/  500]    Overall Loss 1.122698    Objective Loss 1.122698                                        LR 0.000016    Time 0.022301    
2022-01-10 16:54:34,022 - Epoch: [282][  400/  500]    Overall Loss 1.128684    Objective Loss 1.128684                                        LR 0.000016    Time 0.021853    
2022-01-10 16:54:36,156 - Epoch: [282][  500/  500]    Overall Loss 1.131047    Objective Loss 1.131047    Top1 72.000000    Top5 97.000000    LR 0.000016    Time 0.021747    
2022-01-10 16:54:36,204 - --- validate (epoch=282)-----------
2022-01-10 16:54:36,204 - 10000 samples (100 per mini-batch)
2022-01-10 16:54:37,636 - Epoch: [282][  100/  100]    Loss 1.550436    Top1 59.030000    Top5 85.900000    
2022-01-10 16:54:37,682 - ==> Top1: 59.030    Top5: 85.900    Loss: 1.550

2022-01-10 16:54:37,684 - ==> Best [Top1: 60.160   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 280]
2022-01-10 16:54:37,684 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:54:37,708 - 

2022-01-10 16:54:37,709 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:54:39,931 - Epoch: [283][  100/  500]    Overall Loss 1.110502    Objective Loss 1.110502                                        LR 0.000016    Time 0.022208    
2022-01-10 16:54:42,042 - Epoch: [283][  200/  500]    Overall Loss 1.114943    Objective Loss 1.114943                                        LR 0.000016    Time 0.021654    
2022-01-10 16:54:44,143 - Epoch: [283][  300/  500]    Overall Loss 1.120134    Objective Loss 1.120134                                        LR 0.000016    Time 0.021436    
2022-01-10 16:54:46,243 - Epoch: [283][  400/  500]    Overall Loss 1.123785    Objective Loss 1.123785                                        LR 0.000016    Time 0.021325    
2022-01-10 16:54:48,350 - Epoch: [283][  500/  500]    Overall Loss 1.120534    Objective Loss 1.120534    Top1 79.000000    Top5 95.000000    LR 0.000016    Time 0.021272    
2022-01-10 16:54:48,409 - --- validate (epoch=283)-----------
2022-01-10 16:54:48,409 - 10000 samples (100 per mini-batch)
2022-01-10 16:54:49,853 - Epoch: [283][  100/  100]    Loss 1.590971    Top1 58.240000    Top5 85.410000    
2022-01-10 16:54:49,903 - ==> Top1: 58.240    Top5: 85.410    Loss: 1.591

2022-01-10 16:54:49,904 - ==> Best [Top1: 60.160   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 280]
2022-01-10 16:54:49,904 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:54:49,926 - 

2022-01-10 16:54:49,926 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:54:52,266 - Epoch: [284][  100/  500]    Overall Loss 1.117063    Objective Loss 1.117063                                        LR 0.000016    Time 0.023384    
2022-01-10 16:54:54,360 - Epoch: [284][  200/  500]    Overall Loss 1.122964    Objective Loss 1.122964                                        LR 0.000016    Time 0.022160    
2022-01-10 16:54:56,454 - Epoch: [284][  300/  500]    Overall Loss 1.122019    Objective Loss 1.122019                                        LR 0.000016    Time 0.021748    
2022-01-10 16:54:58,553 - Epoch: [284][  400/  500]    Overall Loss 1.126682    Objective Loss 1.126682                                        LR 0.000016    Time 0.021557    
2022-01-10 16:55:00,661 - Epoch: [284][  500/  500]    Overall Loss 1.125148    Objective Loss 1.125148    Top1 76.000000    Top5 97.000000    LR 0.000016    Time 0.021459    
2022-01-10 16:55:00,709 - --- validate (epoch=284)-----------
2022-01-10 16:55:00,710 - 10000 samples (100 per mini-batch)
2022-01-10 16:55:02,147 - Epoch: [284][  100/  100]    Loss 1.542486    Top1 59.870000    Top5 85.990000    
2022-01-10 16:55:02,198 - ==> Top1: 59.870    Top5: 85.990    Loss: 1.542

2022-01-10 16:55:02,199 - ==> Best [Top1: 60.160   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 280]
2022-01-10 16:55:02,200 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:55:02,225 - 

2022-01-10 16:55:02,225 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:55:04,607 - Epoch: [285][  100/  500]    Overall Loss 1.117524    Objective Loss 1.117524                                        LR 0.000016    Time 0.023805    
2022-01-10 16:55:06,743 - Epoch: [285][  200/  500]    Overall Loss 1.105586    Objective Loss 1.105586                                        LR 0.000016    Time 0.022578    
2022-01-10 16:55:08,866 - Epoch: [285][  300/  500]    Overall Loss 1.108423    Objective Loss 1.108423                                        LR 0.000016    Time 0.022124    
2022-01-10 16:55:11,033 - Epoch: [285][  400/  500]    Overall Loss 1.117878    Objective Loss 1.117878                                        LR 0.000016    Time 0.022010    
2022-01-10 16:55:13,188 - Epoch: [285][  500/  500]    Overall Loss 1.118161    Objective Loss 1.118161    Top1 73.000000    Top5 97.000000    LR 0.000016    Time 0.021916    
2022-01-10 16:55:13,233 - --- validate (epoch=285)-----------
2022-01-10 16:55:13,233 - 10000 samples (100 per mini-batch)
2022-01-10 16:55:14,766 - Epoch: [285][  100/  100]    Loss 1.527676    Top1 59.610000    Top5 86.250000    
2022-01-10 16:55:14,811 - ==> Top1: 59.610    Top5: 86.250    Loss: 1.528

2022-01-10 16:55:14,812 - ==> Best [Top1: 60.160   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 280]
2022-01-10 16:55:14,812 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:55:14,837 - 

2022-01-10 16:55:14,837 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:55:17,111 - Epoch: [286][  100/  500]    Overall Loss 1.114929    Objective Loss 1.114929                                        LR 0.000016    Time 0.022722    
2022-01-10 16:55:19,184 - Epoch: [286][  200/  500]    Overall Loss 1.118214    Objective Loss 1.118214                                        LR 0.000016    Time 0.021724    
2022-01-10 16:55:21,246 - Epoch: [286][  300/  500]    Overall Loss 1.121057    Objective Loss 1.121057                                        LR 0.000016    Time 0.021351    
2022-01-10 16:55:23,328 - Epoch: [286][  400/  500]    Overall Loss 1.122863    Objective Loss 1.122863                                        LR 0.000016    Time 0.021218    
2022-01-10 16:55:25,412 - Epoch: [286][  500/  500]    Overall Loss 1.121328    Objective Loss 1.121328    Top1 75.000000    Top5 96.500000    LR 0.000016    Time 0.021140    
2022-01-10 16:55:25,461 - --- validate (epoch=286)-----------
2022-01-10 16:55:25,462 - 10000 samples (100 per mini-batch)
2022-01-10 16:55:26,994 - Epoch: [286][  100/  100]    Loss 1.529458    Top1 59.390000    Top5 86.390000    
2022-01-10 16:55:27,036 - ==> Top1: 59.390    Top5: 86.390    Loss: 1.529

2022-01-10 16:55:27,038 - ==> Best [Top1: 60.160   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 280]
2022-01-10 16:55:27,038 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:55:27,063 - 

2022-01-10 16:55:27,063 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:55:29,423 - Epoch: [287][  100/  500]    Overall Loss 1.101655    Objective Loss 1.101655                                        LR 0.000016    Time 0.023588    
2022-01-10 16:55:31,513 - Epoch: [287][  200/  500]    Overall Loss 1.108405    Objective Loss 1.108405                                        LR 0.000016    Time 0.022238    
2022-01-10 16:55:33,597 - Epoch: [287][  300/  500]    Overall Loss 1.113601    Objective Loss 1.113601                                        LR 0.000016    Time 0.021770    
2022-01-10 16:55:35,677 - Epoch: [287][  400/  500]    Overall Loss 1.114443    Objective Loss 1.114443                                        LR 0.000016    Time 0.021524    
2022-01-10 16:55:37,772 - Epoch: [287][  500/  500]    Overall Loss 1.113719    Objective Loss 1.113719    Top1 74.500000    Top5 94.500000    LR 0.000016    Time 0.021408    
2022-01-10 16:55:37,816 - --- validate (epoch=287)-----------
2022-01-10 16:55:37,816 - 10000 samples (100 per mini-batch)
2022-01-10 16:55:39,274 - Epoch: [287][  100/  100]    Loss 1.565378    Top1 59.150000    Top5 85.560000    
2022-01-10 16:55:39,321 - ==> Top1: 59.150    Top5: 85.560    Loss: 1.565

2022-01-10 16:55:39,322 - ==> Best [Top1: 60.160   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 280]
2022-01-10 16:55:39,322 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:55:39,347 - 

2022-01-10 16:55:39,347 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:55:41,693 - Epoch: [288][  100/  500]    Overall Loss 1.086570    Objective Loss 1.086570                                        LR 0.000016    Time 0.023447    
2022-01-10 16:55:43,802 - Epoch: [288][  200/  500]    Overall Loss 1.088929    Objective Loss 1.088929                                        LR 0.000016    Time 0.022263    
2022-01-10 16:55:45,885 - Epoch: [288][  300/  500]    Overall Loss 1.101474    Objective Loss 1.101474                                        LR 0.000016    Time 0.021783    
2022-01-10 16:55:47,971 - Epoch: [288][  400/  500]    Overall Loss 1.102455    Objective Loss 1.102455                                        LR 0.000016    Time 0.021551    
2022-01-10 16:55:50,099 - Epoch: [288][  500/  500]    Overall Loss 1.102826    Objective Loss 1.102826    Top1 72.500000    Top5 95.500000    LR 0.000016    Time 0.021495    
2022-01-10 16:55:50,153 - --- validate (epoch=288)-----------
2022-01-10 16:55:50,153 - 10000 samples (100 per mini-batch)
2022-01-10 16:55:51,598 - Epoch: [288][  100/  100]    Loss 1.544122    Top1 59.740000    Top5 86.140000    
2022-01-10 16:55:51,642 - ==> Top1: 59.740    Top5: 86.140    Loss: 1.544

2022-01-10 16:55:51,644 - ==> Best [Top1: 60.160   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 280]
2022-01-10 16:55:51,644 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:55:51,669 - 

2022-01-10 16:55:51,669 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:55:53,963 - Epoch: [289][  100/  500]    Overall Loss 1.095312    Objective Loss 1.095312                                        LR 0.000016    Time 0.022922    
2022-01-10 16:55:56,072 - Epoch: [289][  200/  500]    Overall Loss 1.090179    Objective Loss 1.090179                                        LR 0.000016    Time 0.022002    
2022-01-10 16:55:58,202 - Epoch: [289][  300/  500]    Overall Loss 1.095149    Objective Loss 1.095149                                        LR 0.000016    Time 0.021764    
2022-01-10 16:56:00,327 - Epoch: [289][  400/  500]    Overall Loss 1.106520    Objective Loss 1.106520                                        LR 0.000016    Time 0.021632    
2022-01-10 16:56:02,445 - Epoch: [289][  500/  500]    Overall Loss 1.104252    Objective Loss 1.104252    Top1 76.000000    Top5 96.500000    LR 0.000016    Time 0.021540    
2022-01-10 16:56:02,492 - --- validate (epoch=289)-----------
2022-01-10 16:56:02,492 - 10000 samples (100 per mini-batch)
2022-01-10 16:56:03,931 - Epoch: [289][  100/  100]    Loss 1.605762    Top1 57.840000    Top5 84.910000    
2022-01-10 16:56:03,984 - ==> Top1: 57.840    Top5: 84.910    Loss: 1.606

2022-01-10 16:56:03,986 - ==> Best [Top1: 60.160   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 280]
2022-01-10 16:56:03,986 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:56:04,007 - 

2022-01-10 16:56:04,007 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:56:06,352 - Epoch: [290][  100/  500]    Overall Loss 1.097282    Objective Loss 1.097282                                        LR 0.000016    Time 0.023433    
2022-01-10 16:56:08,481 - Epoch: [290][  200/  500]    Overall Loss 1.101183    Objective Loss 1.101183                                        LR 0.000016    Time 0.022358    
2022-01-10 16:56:10,591 - Epoch: [290][  300/  500]    Overall Loss 1.097674    Objective Loss 1.097674                                        LR 0.000016    Time 0.021934    
2022-01-10 16:56:12,698 - Epoch: [290][  400/  500]    Overall Loss 1.096014    Objective Loss 1.096014                                        LR 0.000016    Time 0.021717    
2022-01-10 16:56:14,805 - Epoch: [290][  500/  500]    Overall Loss 1.098939    Objective Loss 1.098939    Top1 72.500000    Top5 97.500000    LR 0.000016    Time 0.021586    
2022-01-10 16:56:14,860 - --- validate (epoch=290)-----------
2022-01-10 16:56:14,861 - 10000 samples (100 per mini-batch)
2022-01-10 16:56:16,308 - Epoch: [290][  100/  100]    Loss 1.555286    Top1 59.330000    Top5 85.580000    
2022-01-10 16:56:16,353 - ==> Top1: 59.330    Top5: 85.580    Loss: 1.555

2022-01-10 16:56:16,355 - ==> Best [Top1: 60.160   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 280]
2022-01-10 16:56:16,355 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:56:16,380 - 

2022-01-10 16:56:16,381 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:56:18,657 - Epoch: [291][  100/  500]    Overall Loss 1.104538    Objective Loss 1.104538                                        LR 0.000016    Time 0.022748    
2022-01-10 16:56:20,780 - Epoch: [291][  200/  500]    Overall Loss 1.101521    Objective Loss 1.101521                                        LR 0.000016    Time 0.021984    
2022-01-10 16:56:22,924 - Epoch: [291][  300/  500]    Overall Loss 1.095682    Objective Loss 1.095682                                        LR 0.000016    Time 0.021801    
2022-01-10 16:56:25,033 - Epoch: [291][  400/  500]    Overall Loss 1.099536    Objective Loss 1.099536                                        LR 0.000016    Time 0.021622    
2022-01-10 16:56:27,121 - Epoch: [291][  500/  500]    Overall Loss 1.098643    Objective Loss 1.098643    Top1 75.500000    Top5 96.000000    LR 0.000016    Time 0.021471    
2022-01-10 16:56:27,174 - --- validate (epoch=291)-----------
2022-01-10 16:56:27,175 - 10000 samples (100 per mini-batch)
2022-01-10 16:56:28,701 - Epoch: [291][  100/  100]    Loss 1.603249    Top1 58.090000    Top5 84.670000    
2022-01-10 16:56:28,753 - ==> Top1: 58.090    Top5: 84.670    Loss: 1.603

2022-01-10 16:56:28,754 - ==> Best [Top1: 60.160   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 280]
2022-01-10 16:56:28,754 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:56:28,780 - 

2022-01-10 16:56:28,780 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:56:31,035 - Epoch: [292][  100/  500]    Overall Loss 1.103923    Objective Loss 1.103923                                        LR 0.000016    Time 0.022531    
2022-01-10 16:56:33,111 - Epoch: [292][  200/  500]    Overall Loss 1.095521    Objective Loss 1.095521                                        LR 0.000016    Time 0.021640    
2022-01-10 16:56:35,181 - Epoch: [292][  300/  500]    Overall Loss 1.087621    Objective Loss 1.087621                                        LR 0.000016    Time 0.021324    
2022-01-10 16:56:37,288 - Epoch: [292][  400/  500]    Overall Loss 1.093084    Objective Loss 1.093084                                        LR 0.000016    Time 0.021259    
2022-01-10 16:56:39,418 - Epoch: [292][  500/  500]    Overall Loss 1.092139    Objective Loss 1.092139    Top1 72.500000    Top5 98.000000    LR 0.000016    Time 0.021266    
2022-01-10 16:56:39,467 - --- validate (epoch=292)-----------
2022-01-10 16:56:39,467 - 10000 samples (100 per mini-batch)
2022-01-10 16:56:40,918 - Epoch: [292][  100/  100]    Loss 1.521556    Top1 60.060000    Top5 86.430000    
2022-01-10 16:56:40,970 - ==> Top1: 60.060    Top5: 86.430    Loss: 1.522

2022-01-10 16:56:40,972 - ==> Best [Top1: 60.160   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 280]
2022-01-10 16:56:40,972 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:56:40,997 - 

2022-01-10 16:56:40,997 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:56:43,339 - Epoch: [293][  100/  500]    Overall Loss 1.098681    Objective Loss 1.098681                                        LR 0.000016    Time 0.023407    
2022-01-10 16:56:45,469 - Epoch: [293][  200/  500]    Overall Loss 1.082042    Objective Loss 1.082042                                        LR 0.000016    Time 0.022346    
2022-01-10 16:56:47,619 - Epoch: [293][  300/  500]    Overall Loss 1.086504    Objective Loss 1.086504                                        LR 0.000016    Time 0.022061    
2022-01-10 16:56:49,757 - Epoch: [293][  400/  500]    Overall Loss 1.090501    Objective Loss 1.090501                                        LR 0.000016    Time 0.021888    
2022-01-10 16:56:51,882 - Epoch: [293][  500/  500]    Overall Loss 1.090787    Objective Loss 1.090787    Top1 72.500000    Top5 96.500000    LR 0.000016    Time 0.021758    
2022-01-10 16:56:51,933 - --- validate (epoch=293)-----------
2022-01-10 16:56:51,933 - 10000 samples (100 per mini-batch)
2022-01-10 16:56:53,542 - Epoch: [293][  100/  100]    Loss 1.554475    Top1 58.900000    Top5 85.610000    
2022-01-10 16:56:53,580 - ==> Top1: 58.900    Top5: 85.610    Loss: 1.554

2022-01-10 16:56:53,582 - ==> Best [Top1: 60.160   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 280]
2022-01-10 16:56:53,582 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:56:53,607 - 

2022-01-10 16:56:53,608 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:56:55,875 - Epoch: [294][  100/  500]    Overall Loss 1.065964    Objective Loss 1.065964                                        LR 0.000016    Time 0.022656    
2022-01-10 16:56:57,991 - Epoch: [294][  200/  500]    Overall Loss 1.075976    Objective Loss 1.075976                                        LR 0.000016    Time 0.021906    
2022-01-10 16:57:00,092 - Epoch: [294][  300/  500]    Overall Loss 1.085813    Objective Loss 1.085813                                        LR 0.000016    Time 0.021604    
2022-01-10 16:57:02,193 - Epoch: [294][  400/  500]    Overall Loss 1.092731    Objective Loss 1.092731                                        LR 0.000016    Time 0.021452    
2022-01-10 16:57:04,322 - Epoch: [294][  500/  500]    Overall Loss 1.092340    Objective Loss 1.092340    Top1 73.500000    Top5 95.500000    LR 0.000016    Time 0.021419    
2022-01-10 16:57:04,371 - --- validate (epoch=294)-----------
2022-01-10 16:57:04,372 - 10000 samples (100 per mini-batch)
2022-01-10 16:57:05,889 - Epoch: [294][  100/  100]    Loss 1.557918    Top1 58.710000    Top5 85.510000    
2022-01-10 16:57:05,933 - ==> Top1: 58.710    Top5: 85.510    Loss: 1.558

2022-01-10 16:57:05,934 - ==> Best [Top1: 60.160   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 280]
2022-01-10 16:57:05,934 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:57:05,959 - 

2022-01-10 16:57:05,960 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:57:08,231 - Epoch: [295][  100/  500]    Overall Loss 1.075309    Objective Loss 1.075309                                        LR 0.000016    Time 0.022703    
2022-01-10 16:57:10,362 - Epoch: [295][  200/  500]    Overall Loss 1.092599    Objective Loss 1.092599                                        LR 0.000016    Time 0.021998    
2022-01-10 16:57:12,445 - Epoch: [295][  300/  500]    Overall Loss 1.092310    Objective Loss 1.092310                                        LR 0.000016    Time 0.021608    
2022-01-10 16:57:14,531 - Epoch: [295][  400/  500]    Overall Loss 1.089091    Objective Loss 1.089091                                        LR 0.000016    Time 0.021418    
2022-01-10 16:57:16,630 - Epoch: [295][  500/  500]    Overall Loss 1.090992    Objective Loss 1.090992    Top1 74.500000    Top5 97.000000    LR 0.000016    Time 0.021331    
2022-01-10 16:57:16,684 - --- validate (epoch=295)-----------
2022-01-10 16:57:16,684 - 10000 samples (100 per mini-batch)
2022-01-10 16:57:18,135 - Epoch: [295][  100/  100]    Loss 1.571021    Top1 58.600000    Top5 85.220000    
2022-01-10 16:57:18,175 - ==> Top1: 58.600    Top5: 85.220    Loss: 1.571

2022-01-10 16:57:18,177 - ==> Best [Top1: 60.160   Top5: 86.510   Sparsity:0.00   Params: 725128 on epoch: 280]
2022-01-10 16:57:18,177 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:57:18,202 - 

2022-01-10 16:57:18,202 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:57:20,492 - Epoch: [296][  100/  500]    Overall Loss 1.089477    Objective Loss 1.089477                                        LR 0.000016    Time 0.022885    
2022-01-10 16:57:22,574 - Epoch: [296][  200/  500]    Overall Loss 1.094105    Objective Loss 1.094105                                        LR 0.000016    Time 0.021852    
2022-01-10 16:57:24,697 - Epoch: [296][  300/  500]    Overall Loss 1.090429    Objective Loss 1.090429                                        LR 0.000016    Time 0.021640    
2022-01-10 16:57:26,830 - Epoch: [296][  400/  500]    Overall Loss 1.087182    Objective Loss 1.087182                                        LR 0.000016    Time 0.021559    
2022-01-10 16:57:28,961 - Epoch: [296][  500/  500]    Overall Loss 1.090376    Objective Loss 1.090376    Top1 75.500000    Top5 96.500000    LR 0.000016    Time 0.021507    
2022-01-10 16:57:29,010 - --- validate (epoch=296)-----------
2022-01-10 16:57:29,010 - 10000 samples (100 per mini-batch)
2022-01-10 16:57:30,445 - Epoch: [296][  100/  100]    Loss 1.525709    Top1 60.300000    Top5 86.230000    
2022-01-10 16:57:30,490 - ==> Top1: 60.300    Top5: 86.230    Loss: 1.526

2022-01-10 16:57:30,491 - ==> Best [Top1: 60.300   Top5: 86.230   Sparsity:0.00   Params: 725128 on epoch: 296]
2022-01-10 16:57:30,491 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:57:30,523 - 

2022-01-10 16:57:30,524 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:57:32,820 - Epoch: [297][  100/  500]    Overall Loss 1.071610    Objective Loss 1.071610                                        LR 0.000016    Time 0.022947    
2022-01-10 16:57:34,900 - Epoch: [297][  200/  500]    Overall Loss 1.078080    Objective Loss 1.078080                                        LR 0.000016    Time 0.021869    
2022-01-10 16:57:36,994 - Epoch: [297][  300/  500]    Overall Loss 1.076779    Objective Loss 1.076779                                        LR 0.000016    Time 0.021556    
2022-01-10 16:57:39,092 - Epoch: [297][  400/  500]    Overall Loss 1.083819    Objective Loss 1.083819                                        LR 0.000016    Time 0.021409    
2022-01-10 16:57:41,196 - Epoch: [297][  500/  500]    Overall Loss 1.088145    Objective Loss 1.088145    Top1 71.000000    Top5 94.500000    LR 0.000016    Time 0.021333    
2022-01-10 16:57:41,241 - --- validate (epoch=297)-----------
2022-01-10 16:57:41,241 - 10000 samples (100 per mini-batch)
2022-01-10 16:57:42,826 - Epoch: [297][  100/  100]    Loss 1.551101    Top1 59.210000    Top5 85.640000    
2022-01-10 16:57:42,881 - ==> Top1: 59.210    Top5: 85.640    Loss: 1.551

2022-01-10 16:57:42,882 - ==> Best [Top1: 60.300   Top5: 86.230   Sparsity:0.00   Params: 725128 on epoch: 296]
2022-01-10 16:57:42,882 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:57:42,903 - 

2022-01-10 16:57:42,903 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:57:45,140 - Epoch: [298][  100/  500]    Overall Loss 1.080529    Objective Loss 1.080529                                        LR 0.000016    Time 0.022360    
2022-01-10 16:57:47,196 - Epoch: [298][  200/  500]    Overall Loss 1.077157    Objective Loss 1.077157                                        LR 0.000016    Time 0.021453    
2022-01-10 16:57:49,253 - Epoch: [298][  300/  500]    Overall Loss 1.080972    Objective Loss 1.080972                                        LR 0.000016    Time 0.021157    
2022-01-10 16:57:51,312 - Epoch: [298][  400/  500]    Overall Loss 1.079258    Objective Loss 1.079258                                        LR 0.000016    Time 0.021014    
2022-01-10 16:57:53,453 - Epoch: [298][  500/  500]    Overall Loss 1.085056    Objective Loss 1.085056    Top1 69.500000    Top5 93.000000    LR 0.000016    Time 0.021090    
2022-01-10 16:57:53,506 - --- validate (epoch=298)-----------
2022-01-10 16:57:53,506 - 10000 samples (100 per mini-batch)
2022-01-10 16:57:54,962 - Epoch: [298][  100/  100]    Loss 1.542363    Top1 59.280000    Top5 85.920000    
2022-01-10 16:57:55,006 - ==> Top1: 59.280    Top5: 85.920    Loss: 1.542

2022-01-10 16:57:55,007 - ==> Best [Top1: 60.300   Top5: 86.230   Sparsity:0.00   Params: 725128 on epoch: 296]
2022-01-10 16:57:55,008 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:57:55,028 - 

2022-01-10 16:57:55,028 - Training epoch: 50000 samples (100 per mini-batch)
2022-01-10 16:57:57,372 - Epoch: [299][  100/  500]    Overall Loss 1.066296    Objective Loss 1.066296                                        LR 0.000016    Time 0.023422    
2022-01-10 16:57:59,477 - Epoch: [299][  200/  500]    Overall Loss 1.081407    Objective Loss 1.081407                                        LR 0.000016    Time 0.022231    
2022-01-10 16:58:01,570 - Epoch: [299][  300/  500]    Overall Loss 1.082852    Objective Loss 1.082852                                        LR 0.000016    Time 0.021795    
2022-01-10 16:58:03,677 - Epoch: [299][  400/  500]    Overall Loss 1.088256    Objective Loss 1.088256                                        LR 0.000016    Time 0.021611    
2022-01-10 16:58:05,820 - Epoch: [299][  500/  500]    Overall Loss 1.085278    Objective Loss 1.085278    Top1 75.000000    Top5 96.500000    LR 0.000016    Time 0.021575    
2022-01-10 16:58:05,872 - --- validate (epoch=299)-----------
2022-01-10 16:58:05,873 - 10000 samples (100 per mini-batch)
2022-01-10 16:58:07,337 - Epoch: [299][  100/  100]    Loss 1.531109    Top1 59.740000    Top5 86.250000    
2022-01-10 16:58:07,385 - ==> Top1: 59.740    Top5: 86.250    Loss: 1.531

2022-01-10 16:58:07,387 - ==> Best [Top1: 60.300   Top5: 86.230   Sparsity:0.00   Params: 725128 on epoch: 296]
2022-01-10 16:58:07,387 - Saving checkpoint to: logs/2022.01.10-161214/qat_checkpoint.pth.tar
2022-01-10 16:58:07,412 - --- test ---------------------
2022-01-10 16:58:07,412 - 10000 samples (100 per mini-batch)
2022-01-10 16:58:08,871 - Test: [  100/  100]    Loss 1.531109    Top1 59.740000    Top5 86.250000    
2022-01-10 16:58:08,924 - ==> Top1: 59.740    Top5: 86.250    Loss: 1.531

2022-01-10 16:58:08,927 - 
2022-01-10 16:58:08,928 - Log file for this run: /home/gorkemulkar/Workspace/Python/AI8X_GitHub/ai8x-training/logs/2022.01.10-161214/2022.01.10-161214.log
