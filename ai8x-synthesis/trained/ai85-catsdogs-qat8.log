2023-09-11 13:41:09,898 - Log file for this run: /home/ermanokman/repos/ai8x-training/logs/2023.09.11-134109/2023.09.11-134109.log
2023-09-11 13:41:15,218 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2023-09-11 13:41:15,218 - Optimizer Args: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2023-09-11 13:41:15,304 - Dataset sizes:
	training=18000
	validation=2000
	test=5000
2023-09-11 13:41:15,304 - Reading compression schedule from: policies/schedule-catsdogs-new.yaml
2023-09-11 13:41:15,306 - 

2023-09-11 13:41:15,306 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:41:24,743 - Epoch: [0][   10/   71]    Overall Loss 0.699847    Objective Loss 0.699847                                        LR 0.001000    Time 0.943677    
2023-09-11 13:41:26,804 - Epoch: [0][   20/   71]    Overall Loss 0.691583    Objective Loss 0.691583                                        LR 0.001000    Time 0.574843    
2023-09-11 13:41:29,820 - Epoch: [0][   30/   71]    Overall Loss 0.686328    Objective Loss 0.686328                                        LR 0.001000    Time 0.483771    
2023-09-11 13:41:32,306 - Epoch: [0][   40/   71]    Overall Loss 0.681851    Objective Loss 0.681851                                        LR 0.001000    Time 0.424962    
2023-09-11 13:41:35,332 - Epoch: [0][   50/   71]    Overall Loss 0.678799    Objective Loss 0.678799                                        LR 0.001000    Time 0.400490    
2023-09-11 13:41:37,722 - Epoch: [0][   60/   71]    Overall Loss 0.675177    Objective Loss 0.675177                                        LR 0.001000    Time 0.373559    
2023-09-11 13:41:40,330 - Epoch: [0][   70/   71]    Overall Loss 0.672030    Objective Loss 0.672030    Top1 62.890625    LR 0.001000    Time 0.357459    
2023-09-11 13:41:40,387 - Epoch: [0][   71/   71]    Overall Loss 0.671505    Objective Loss 0.671505    Top1 62.797619    LR 0.001000    Time 0.353223    
2023-09-11 13:41:40,455 - --- validate (epoch=0)-----------
2023-09-11 13:41:40,456 - 2000 samples (256 per mini-batch)
2023-09-11 13:41:42,933 - Epoch: [0][    8/    8]    Loss 0.651870    Top1 63.200000    
2023-09-11 13:41:43,019 - ==> Top1: 63.200    Loss: 0.652

2023-09-11 13:41:43,027 - ==> Confusion:
[[838 147]
 [589 426]]

2023-09-11 13:41:43,028 - ==> Best [Top1: 63.200   Sparsity:0.00   Params: 57776 on epoch: 0]
2023-09-11 13:41:43,029 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:41:43,033 - 

2023-09-11 13:41:43,033 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:41:46,504 - Epoch: [1][   10/   71]    Overall Loss 0.643949    Objective Loss 0.643949                                        LR 0.001000    Time 0.347086    
2023-09-11 13:41:48,389 - Epoch: [1][   20/   71]    Overall Loss 0.644591    Objective Loss 0.644591                                        LR 0.001000    Time 0.267755    
2023-09-11 13:41:50,986 - Epoch: [1][   30/   71]    Overall Loss 0.642382    Objective Loss 0.642382                                        LR 0.001000    Time 0.265060    
2023-09-11 13:41:53,113 - Epoch: [1][   40/   71]    Overall Loss 0.641840    Objective Loss 0.641840                                        LR 0.001000    Time 0.251956    
2023-09-11 13:41:55,560 - Epoch: [1][   50/   71]    Overall Loss 0.635971    Objective Loss 0.635971                                        LR 0.001000    Time 0.250498    
2023-09-11 13:41:57,808 - Epoch: [1][   60/   71]    Overall Loss 0.634133    Objective Loss 0.634133                                        LR 0.001000    Time 0.246217    
2023-09-11 13:41:59,983 - Epoch: [1][   70/   71]    Overall Loss 0.630999    Objective Loss 0.630999    Top1 64.453125    LR 0.001000    Time 0.242112    
2023-09-11 13:42:00,039 - Epoch: [1][   71/   71]    Overall Loss 0.630654    Objective Loss 0.630654    Top1 64.583333    LR 0.001000    Time 0.239482    
2023-09-11 13:42:00,130 - --- validate (epoch=1)-----------
2023-09-11 13:42:00,131 - 2000 samples (256 per mini-batch)
2023-09-11 13:42:02,634 - Epoch: [1][    8/    8]    Loss 0.592152    Top1 69.600000    
2023-09-11 13:42:02,716 - ==> Top1: 69.600    Loss: 0.592

2023-09-11 13:42:02,716 - ==> Confusion:
[[705 280]
 [328 687]]

2023-09-11 13:42:02,731 - ==> Best [Top1: 69.600   Sparsity:0.00   Params: 57776 on epoch: 1]
2023-09-11 13:42:02,731 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:42:02,734 - 

2023-09-11 13:42:02,734 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:42:05,732 - Epoch: [2][   10/   71]    Overall Loss 0.601056    Objective Loss 0.601056                                        LR 0.001000    Time 0.299777    
2023-09-11 13:42:08,256 - Epoch: [2][   20/   71]    Overall Loss 0.592658    Objective Loss 0.592658                                        LR 0.001000    Time 0.276101    
2023-09-11 13:42:10,221 - Epoch: [2][   30/   71]    Overall Loss 0.593226    Objective Loss 0.593226                                        LR 0.001000    Time 0.249553    
2023-09-11 13:42:12,810 - Epoch: [2][   40/   71]    Overall Loss 0.588757    Objective Loss 0.588757                                        LR 0.001000    Time 0.251865    
2023-09-11 13:42:14,801 - Epoch: [2][   50/   71]    Overall Loss 0.588544    Objective Loss 0.588544                                        LR 0.001000    Time 0.241307    
2023-09-11 13:42:17,533 - Epoch: [2][   60/   71]    Overall Loss 0.584145    Objective Loss 0.584145                                        LR 0.001000    Time 0.246631    
2023-09-11 13:42:19,849 - Epoch: [2][   70/   71]    Overall Loss 0.579406    Objective Loss 0.579406    Top1 73.828125    LR 0.001000    Time 0.244471    
2023-09-11 13:42:19,903 - Epoch: [2][   71/   71]    Overall Loss 0.576867    Objective Loss 0.576867    Top1 75.297619    LR 0.001000    Time 0.241793    
2023-09-11 13:42:19,993 - --- validate (epoch=2)-----------
2023-09-11 13:42:19,993 - 2000 samples (256 per mini-batch)
2023-09-11 13:42:22,774 - Epoch: [2][    8/    8]    Loss 0.580504    Top1 69.300000    
2023-09-11 13:42:22,875 - ==> Top1: 69.300    Loss: 0.581

2023-09-11 13:42:22,875 - ==> Confusion:
[[532 453]
 [161 854]]

2023-09-11 13:42:22,891 - ==> Best [Top1: 69.600   Sparsity:0.00   Params: 57776 on epoch: 1]
2023-09-11 13:42:22,891 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:42:22,893 - 

2023-09-11 13:42:22,893 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:42:26,686 - Epoch: [3][   10/   71]    Overall Loss 0.562311    Objective Loss 0.562311                                        LR 0.001000    Time 0.379196    
2023-09-11 13:42:29,037 - Epoch: [3][   20/   71]    Overall Loss 0.567811    Objective Loss 0.567811                                        LR 0.001000    Time 0.307147    
2023-09-11 13:42:31,599 - Epoch: [3][   30/   71]    Overall Loss 0.560860    Objective Loss 0.560860                                        LR 0.001000    Time 0.290158    
2023-09-11 13:42:33,591 - Epoch: [3][   40/   71]    Overall Loss 0.552764    Objective Loss 0.552764                                        LR 0.001000    Time 0.267408    
2023-09-11 13:42:36,620 - Epoch: [3][   50/   71]    Overall Loss 0.549047    Objective Loss 0.549047                                        LR 0.001000    Time 0.274494    
2023-09-11 13:42:38,573 - Epoch: [3][   60/   71]    Overall Loss 0.544536    Objective Loss 0.544536                                        LR 0.001000    Time 0.261289    
2023-09-11 13:42:40,813 - Epoch: [3][   70/   71]    Overall Loss 0.541454    Objective Loss 0.541454    Top1 71.484375    LR 0.001000    Time 0.255952    
2023-09-11 13:42:40,858 - Epoch: [3][   71/   71]    Overall Loss 0.541515    Objective Loss 0.541515    Top1 71.726190    LR 0.001000    Time 0.252983    
2023-09-11 13:42:40,936 - --- validate (epoch=3)-----------
2023-09-11 13:42:40,936 - 2000 samples (256 per mini-batch)
2023-09-11 13:42:43,634 - Epoch: [3][    8/    8]    Loss 0.506979    Top1 75.700000    
2023-09-11 13:42:43,727 - ==> Top1: 75.700    Loss: 0.507

2023-09-11 13:42:43,728 - ==> Confusion:
[[777 208]
 [278 737]]

2023-09-11 13:42:43,743 - ==> Best [Top1: 75.700   Sparsity:0.00   Params: 57776 on epoch: 3]
2023-09-11 13:42:43,743 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:42:43,746 - 

2023-09-11 13:42:43,746 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:42:46,709 - Epoch: [4][   10/   71]    Overall Loss 0.510656    Objective Loss 0.510656                                        LR 0.001000    Time 0.296287    
2023-09-11 13:42:48,733 - Epoch: [4][   20/   71]    Overall Loss 0.509729    Objective Loss 0.509729                                        LR 0.001000    Time 0.249319    
2023-09-11 13:42:52,149 - Epoch: [4][   30/   71]    Overall Loss 0.510293    Objective Loss 0.510293                                        LR 0.001000    Time 0.280066    
2023-09-11 13:42:53,993 - Epoch: [4][   40/   71]    Overall Loss 0.513419    Objective Loss 0.513419                                        LR 0.001000    Time 0.256145    
2023-09-11 13:42:56,704 - Epoch: [4][   50/   71]    Overall Loss 0.511901    Objective Loss 0.511901                                        LR 0.001000    Time 0.259121    
2023-09-11 13:42:58,738 - Epoch: [4][   60/   71]    Overall Loss 0.509342    Objective Loss 0.509342                                        LR 0.001000    Time 0.249823    
2023-09-11 13:43:01,037 - Epoch: [4][   70/   71]    Overall Loss 0.506468    Objective Loss 0.506468    Top1 78.125000    LR 0.001000    Time 0.246978    
2023-09-11 13:43:01,093 - Epoch: [4][   71/   71]    Overall Loss 0.506386    Objective Loss 0.506386    Top1 76.488095    LR 0.001000    Time 0.244287    
2023-09-11 13:43:01,189 - --- validate (epoch=4)-----------
2023-09-11 13:43:01,189 - 2000 samples (256 per mini-batch)
2023-09-11 13:43:03,524 - Epoch: [4][    8/    8]    Loss 0.471281    Top1 76.950000    
2023-09-11 13:43:03,616 - ==> Top1: 76.950    Loss: 0.471

2023-09-11 13:43:03,617 - ==> Confusion:
[[750 235]
 [226 789]]

2023-09-11 13:43:03,633 - ==> Best [Top1: 76.950   Sparsity:0.00   Params: 57776 on epoch: 4]
2023-09-11 13:43:03,633 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:43:03,635 - 

2023-09-11 13:43:03,635 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:43:07,712 - Epoch: [5][   10/   71]    Overall Loss 0.481360    Objective Loss 0.481360                                        LR 0.001000    Time 0.407623    
2023-09-11 13:43:09,572 - Epoch: [5][   20/   71]    Overall Loss 0.476039    Objective Loss 0.476039                                        LR 0.001000    Time 0.296815    
2023-09-11 13:43:12,136 - Epoch: [5][   30/   71]    Overall Loss 0.470085    Objective Loss 0.470085                                        LR 0.001000    Time 0.283310    
2023-09-11 13:43:14,144 - Epoch: [5][   40/   71]    Overall Loss 0.476820    Objective Loss 0.476820                                        LR 0.001000    Time 0.262687    
2023-09-11 13:43:17,208 - Epoch: [5][   50/   71]    Overall Loss 0.474682    Objective Loss 0.474682                                        LR 0.001000    Time 0.271425    
2023-09-11 13:43:19,284 - Epoch: [5][   60/   71]    Overall Loss 0.473813    Objective Loss 0.473813                                        LR 0.001000    Time 0.260781    
2023-09-11 13:43:21,593 - Epoch: [5][   70/   71]    Overall Loss 0.473163    Objective Loss 0.473163    Top1 80.468750    LR 0.001000    Time 0.256506    
2023-09-11 13:43:21,651 - Epoch: [5][   71/   71]    Overall Loss 0.472495    Objective Loss 0.472495    Top1 80.654762    LR 0.001000    Time 0.253711    
2023-09-11 13:43:21,743 - --- validate (epoch=5)-----------
2023-09-11 13:43:21,743 - 2000 samples (256 per mini-batch)
2023-09-11 13:43:23,961 - Epoch: [5][    8/    8]    Loss 0.461314    Top1 77.700000    
2023-09-11 13:43:24,054 - ==> Top1: 77.700    Loss: 0.461

2023-09-11 13:43:24,055 - ==> Confusion:
[[702 283]
 [163 852]]

2023-09-11 13:43:24,057 - ==> Best [Top1: 77.700   Sparsity:0.00   Params: 57776 on epoch: 5]
2023-09-11 13:43:24,057 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:43:24,059 - 

2023-09-11 13:43:24,060 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:43:27,962 - Epoch: [6][   10/   71]    Overall Loss 0.464840    Objective Loss 0.464840                                        LR 0.001000    Time 0.390198    
2023-09-11 13:43:30,148 - Epoch: [6][   20/   71]    Overall Loss 0.460022    Objective Loss 0.460022                                        LR 0.001000    Time 0.304375    
2023-09-11 13:43:32,800 - Epoch: [6][   30/   71]    Overall Loss 0.453277    Objective Loss 0.453277                                        LR 0.001000    Time 0.291312    
2023-09-11 13:43:34,814 - Epoch: [6][   40/   71]    Overall Loss 0.458294    Objective Loss 0.458294                                        LR 0.001000    Time 0.268811    
2023-09-11 13:43:37,435 - Epoch: [6][   50/   71]    Overall Loss 0.458526    Objective Loss 0.458526                                        LR 0.001000    Time 0.267482    
2023-09-11 13:43:39,416 - Epoch: [6][   60/   71]    Overall Loss 0.458614    Objective Loss 0.458614                                        LR 0.001000    Time 0.255905    
2023-09-11 13:43:41,744 - Epoch: [6][   70/   71]    Overall Loss 0.456109    Objective Loss 0.456109    Top1 78.125000    LR 0.001000    Time 0.252605    
2023-09-11 13:43:41,803 - Epoch: [6][   71/   71]    Overall Loss 0.456300    Objective Loss 0.456300    Top1 77.083333    LR 0.001000    Time 0.249876    
2023-09-11 13:43:41,895 - --- validate (epoch=6)-----------
2023-09-11 13:43:41,895 - 2000 samples (256 per mini-batch)
2023-09-11 13:43:44,136 - Epoch: [6][    8/    8]    Loss 0.429514    Top1 79.850000    
2023-09-11 13:43:44,223 - ==> Top1: 79.850    Loss: 0.430

2023-09-11 13:43:44,223 - ==> Confusion:
[[783 202]
 [201 814]]

2023-09-11 13:43:44,238 - ==> Best [Top1: 79.850   Sparsity:0.00   Params: 57776 on epoch: 6]
2023-09-11 13:43:44,238 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:43:44,243 - 

2023-09-11 13:43:44,244 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:43:47,964 - Epoch: [7][   10/   71]    Overall Loss 0.421811    Objective Loss 0.421811                                        LR 0.001000    Time 0.371958    
2023-09-11 13:43:49,797 - Epoch: [7][   20/   71]    Overall Loss 0.424821    Objective Loss 0.424821                                        LR 0.001000    Time 0.277642    
2023-09-11 13:43:52,431 - Epoch: [7][   30/   71]    Overall Loss 0.418515    Objective Loss 0.418515                                        LR 0.001000    Time 0.272881    
2023-09-11 13:43:54,355 - Epoch: [7][   40/   71]    Overall Loss 0.412479    Objective Loss 0.412479                                        LR 0.001000    Time 0.252756    
2023-09-11 13:43:57,025 - Epoch: [7][   50/   71]    Overall Loss 0.427500    Objective Loss 0.427500                                        LR 0.001000    Time 0.255589    
2023-09-11 13:43:58,952 - Epoch: [7][   60/   71]    Overall Loss 0.426962    Objective Loss 0.426962                                        LR 0.001000    Time 0.245105    
2023-09-11 13:44:01,259 - Epoch: [7][   70/   71]    Overall Loss 0.429144    Objective Loss 0.429144    Top1 78.125000    LR 0.001000    Time 0.243041    
2023-09-11 13:44:01,315 - Epoch: [7][   71/   71]    Overall Loss 0.429795    Objective Loss 0.429795    Top1 77.380952    LR 0.001000    Time 0.240411    
2023-09-11 13:44:01,402 - --- validate (epoch=7)-----------
2023-09-11 13:44:01,402 - 2000 samples (256 per mini-batch)
2023-09-11 13:44:04,211 - Epoch: [7][    8/    8]    Loss 0.426213    Top1 80.100000    
2023-09-11 13:44:04,308 - ==> Top1: 80.100    Loss: 0.426

2023-09-11 13:44:04,308 - ==> Confusion:
[[826 159]
 [239 776]]

2023-09-11 13:44:04,311 - ==> Best [Top1: 80.100   Sparsity:0.00   Params: 57776 on epoch: 7]
2023-09-11 13:44:04,312 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:44:04,314 - 

2023-09-11 13:44:04,314 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:44:07,397 - Epoch: [8][   10/   71]    Overall Loss 0.428434    Objective Loss 0.428434                                        LR 0.001000    Time 0.308250    
2023-09-11 13:44:09,545 - Epoch: [8][   20/   71]    Overall Loss 0.415466    Objective Loss 0.415466                                        LR 0.001000    Time 0.261479    
2023-09-11 13:44:12,356 - Epoch: [8][   30/   71]    Overall Loss 0.419250    Objective Loss 0.419250                                        LR 0.001000    Time 0.268008    
2023-09-11 13:44:14,690 - Epoch: [8][   40/   71]    Overall Loss 0.417764    Objective Loss 0.417764                                        LR 0.001000    Time 0.259343    
2023-09-11 13:44:17,835 - Epoch: [8][   50/   71]    Overall Loss 0.419137    Objective Loss 0.419137                                        LR 0.001000    Time 0.270382    
2023-09-11 13:44:19,822 - Epoch: [8][   60/   71]    Overall Loss 0.418678    Objective Loss 0.418678                                        LR 0.001000    Time 0.258423    
2023-09-11 13:44:22,041 - Epoch: [8][   70/   71]    Overall Loss 0.414687    Objective Loss 0.414687    Top1 82.812500    LR 0.001000    Time 0.253198    
2023-09-11 13:44:22,077 - Epoch: [8][   71/   71]    Overall Loss 0.414823    Objective Loss 0.414823    Top1 81.250000    LR 0.001000    Time 0.250149    
2023-09-11 13:44:22,162 - --- validate (epoch=8)-----------
2023-09-11 13:44:22,162 - 2000 samples (256 per mini-batch)
2023-09-11 13:44:24,584 - Epoch: [8][    8/    8]    Loss 0.446891    Top1 77.900000    
2023-09-11 13:44:24,668 - ==> Top1: 77.900    Loss: 0.447

2023-09-11 13:44:24,669 - ==> Confusion:
[[887  98]
 [344 671]]

2023-09-11 13:44:24,684 - ==> Best [Top1: 80.100   Sparsity:0.00   Params: 57776 on epoch: 7]
2023-09-11 13:44:24,684 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:44:24,688 - 

2023-09-11 13:44:24,689 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:44:28,540 - Epoch: [9][   10/   71]    Overall Loss 0.412560    Objective Loss 0.412560                                        LR 0.001000    Time 0.385065    
2023-09-11 13:44:30,406 - Epoch: [9][   20/   71]    Overall Loss 0.417186    Objective Loss 0.417186                                        LR 0.001000    Time 0.285822    
2023-09-11 13:44:32,974 - Epoch: [9][   30/   71]    Overall Loss 0.411344    Objective Loss 0.411344                                        LR 0.001000    Time 0.276124    
2023-09-11 13:44:35,000 - Epoch: [9][   40/   71]    Overall Loss 0.407738    Objective Loss 0.407738                                        LR 0.001000    Time 0.257741    
2023-09-11 13:44:37,647 - Epoch: [9][   50/   71]    Overall Loss 0.400353    Objective Loss 0.400353                                        LR 0.001000    Time 0.259120    
2023-09-11 13:44:39,581 - Epoch: [9][   60/   71]    Overall Loss 0.398903    Objective Loss 0.398903                                        LR 0.001000    Time 0.248170    
2023-09-11 13:44:41,838 - Epoch: [9][   70/   71]    Overall Loss 0.399365    Objective Loss 0.399365    Top1 81.250000    LR 0.001000    Time 0.244949    
2023-09-11 13:44:41,894 - Epoch: [9][   71/   71]    Overall Loss 0.398988    Objective Loss 0.398988    Top1 82.440476    LR 0.001000    Time 0.242288    
2023-09-11 13:44:41,986 - --- validate (epoch=9)-----------
2023-09-11 13:44:41,986 - 2000 samples (256 per mini-batch)
2023-09-11 13:44:44,304 - Epoch: [9][    8/    8]    Loss 0.375745    Top1 83.050000    
2023-09-11 13:44:44,398 - ==> Top1: 83.050    Loss: 0.376

2023-09-11 13:44:44,398 - ==> Confusion:
[[815 170]
 [169 846]]

2023-09-11 13:44:44,404 - ==> Best [Top1: 83.050   Sparsity:0.00   Params: 57776 on epoch: 9]
2023-09-11 13:44:44,404 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:44:44,407 - 

2023-09-11 13:44:44,407 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:44:47,445 - Epoch: [10][   10/   71]    Overall Loss 0.390573    Objective Loss 0.390573                                        LR 0.001000    Time 0.303756    
2023-09-11 13:44:49,437 - Epoch: [10][   20/   71]    Overall Loss 0.389078    Objective Loss 0.389078                                        LR 0.001000    Time 0.251462    
2023-09-11 13:44:52,037 - Epoch: [10][   30/   71]    Overall Loss 0.399523    Objective Loss 0.399523                                        LR 0.001000    Time 0.254308    
2023-09-11 13:44:55,105 - Epoch: [10][   40/   71]    Overall Loss 0.397142    Objective Loss 0.397142                                        LR 0.001000    Time 0.267423    
2023-09-11 13:44:57,136 - Epoch: [10][   50/   71]    Overall Loss 0.392307    Objective Loss 0.392307                                        LR 0.001000    Time 0.254549    
2023-09-11 13:44:59,742 - Epoch: [10][   60/   71]    Overall Loss 0.394839    Objective Loss 0.394839                                        LR 0.001000    Time 0.255538    
2023-09-11 13:45:01,482 - Epoch: [10][   70/   71]    Overall Loss 0.393819    Objective Loss 0.393819    Top1 85.156250    LR 0.001000    Time 0.243887    
2023-09-11 13:45:01,531 - Epoch: [10][   71/   71]    Overall Loss 0.393722    Objective Loss 0.393722    Top1 82.738095    LR 0.001000    Time 0.241152    
2023-09-11 13:45:01,623 - --- validate (epoch=10)-----------
2023-09-11 13:45:01,623 - 2000 samples (256 per mini-batch)
2023-09-11 13:45:03,900 - Epoch: [10][    8/    8]    Loss 0.386369    Top1 81.350000    
2023-09-11 13:45:03,994 - ==> Top1: 81.350    Loss: 0.386

2023-09-11 13:45:03,994 - ==> Confusion:
[[830 155]
 [218 797]]

2023-09-11 13:45:04,010 - ==> Best [Top1: 83.050   Sparsity:0.00   Params: 57776 on epoch: 9]
2023-09-11 13:45:04,010 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:45:04,012 - 

2023-09-11 13:45:04,012 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:45:06,981 - Epoch: [11][   10/   71]    Overall Loss 0.359529    Objective Loss 0.359529                                        LR 0.001000    Time 0.296802    
2023-09-11 13:45:09,502 - Epoch: [11][   20/   71]    Overall Loss 0.361109    Objective Loss 0.361109                                        LR 0.001000    Time 0.274459    
2023-09-11 13:45:11,377 - Epoch: [11][   30/   71]    Overall Loss 0.363438    Objective Loss 0.363438                                        LR 0.001000    Time 0.245447    
2023-09-11 13:45:13,962 - Epoch: [11][   40/   71]    Overall Loss 0.363700    Objective Loss 0.363700                                        LR 0.001000    Time 0.248698    
2023-09-11 13:45:15,940 - Epoch: [11][   50/   71]    Overall Loss 0.364780    Objective Loss 0.364780                                        LR 0.001000    Time 0.238522    
2023-09-11 13:45:18,553 - Epoch: [11][   60/   71]    Overall Loss 0.365858    Objective Loss 0.365858                                        LR 0.001000    Time 0.242303    
2023-09-11 13:45:20,351 - Epoch: [11][   70/   71]    Overall Loss 0.369157    Objective Loss 0.369157    Top1 81.250000    LR 0.001000    Time 0.233367    
2023-09-11 13:45:20,409 - Epoch: [11][   71/   71]    Overall Loss 0.369398    Objective Loss 0.369398    Top1 80.654762    LR 0.001000    Time 0.230901    
2023-09-11 13:45:20,500 - --- validate (epoch=11)-----------
2023-09-11 13:45:20,500 - 2000 samples (256 per mini-batch)
2023-09-11 13:45:23,340 - Epoch: [11][    8/    8]    Loss 0.392797    Top1 82.250000    
2023-09-11 13:45:23,432 - ==> Top1: 82.250    Loss: 0.393

2023-09-11 13:45:23,432 - ==> Confusion:
[[903  82]
 [273 742]]

2023-09-11 13:45:23,449 - ==> Best [Top1: 83.050   Sparsity:0.00   Params: 57776 on epoch: 9]
2023-09-11 13:45:23,449 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:45:23,451 - 

2023-09-11 13:45:23,451 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:45:26,429 - Epoch: [12][   10/   71]    Overall Loss 0.343490    Objective Loss 0.343490                                        LR 0.001000    Time 0.297701    
2023-09-11 13:45:28,411 - Epoch: [12][   20/   71]    Overall Loss 0.358670    Objective Loss 0.358670                                        LR 0.001000    Time 0.247952    
2023-09-11 13:45:31,007 - Epoch: [12][   30/   71]    Overall Loss 0.360095    Objective Loss 0.360095                                        LR 0.001000    Time 0.251827    
2023-09-11 13:45:32,893 - Epoch: [12][   40/   71]    Overall Loss 0.370311    Objective Loss 0.370311                                        LR 0.001000    Time 0.236011    
2023-09-11 13:45:35,515 - Epoch: [12][   50/   71]    Overall Loss 0.370859    Objective Loss 0.370859                                        LR 0.001000    Time 0.241231    
2023-09-11 13:45:37,948 - Epoch: [12][   60/   71]    Overall Loss 0.372756    Objective Loss 0.372756                                        LR 0.001000    Time 0.241584    
2023-09-11 13:45:39,879 - Epoch: [12][   70/   71]    Overall Loss 0.373521    Objective Loss 0.373521    Top1 85.546875    LR 0.001000    Time 0.234648    
2023-09-11 13:45:39,936 - Epoch: [12][   71/   71]    Overall Loss 0.373402    Objective Loss 0.373402    Top1 84.821429    LR 0.001000    Time 0.232138    
2023-09-11 13:45:40,026 - --- validate (epoch=12)-----------
2023-09-11 13:45:40,026 - 2000 samples (256 per mini-batch)
2023-09-11 13:45:42,177 - Epoch: [12][    8/    8]    Loss 0.360241    Top1 83.750000    
2023-09-11 13:45:42,267 - ==> Top1: 83.750    Loss: 0.360

2023-09-11 13:45:42,268 - ==> Confusion:
[[792 193]
 [132 883]]

2023-09-11 13:45:42,283 - ==> Best [Top1: 83.750   Sparsity:0.00   Params: 57776 on epoch: 12]
2023-09-11 13:45:42,283 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:45:42,286 - 

2023-09-11 13:45:42,286 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:45:45,311 - Epoch: [13][   10/   71]    Overall Loss 0.350654    Objective Loss 0.350654                                        LR 0.001000    Time 0.302441    
2023-09-11 13:45:47,337 - Epoch: [13][   20/   71]    Overall Loss 0.344415    Objective Loss 0.344415                                        LR 0.001000    Time 0.252499    
2023-09-11 13:45:49,906 - Epoch: [13][   30/   71]    Overall Loss 0.339721    Objective Loss 0.339721                                        LR 0.001000    Time 0.253950    
2023-09-11 13:45:51,853 - Epoch: [13][   40/   71]    Overall Loss 0.342982    Objective Loss 0.342982                                        LR 0.001000    Time 0.239141    
2023-09-11 13:45:54,432 - Epoch: [13][   50/   71]    Overall Loss 0.349146    Objective Loss 0.349146                                        LR 0.001000    Time 0.242880    
2023-09-11 13:45:57,278 - Epoch: [13][   60/   71]    Overall Loss 0.346913    Objective Loss 0.346913                                        LR 0.001000    Time 0.249826    
2023-09-11 13:45:59,081 - Epoch: [13][   70/   71]    Overall Loss 0.347331    Objective Loss 0.347331    Top1 77.734375    LR 0.001000    Time 0.239887    
2023-09-11 13:45:59,140 - Epoch: [13][   71/   71]    Overall Loss 0.348029    Objective Loss 0.348029    Top1 79.166667    LR 0.001000    Time 0.237337    
2023-09-11 13:45:59,233 - --- validate (epoch=13)-----------
2023-09-11 13:45:59,233 - 2000 samples (256 per mini-batch)
2023-09-11 13:46:01,423 - Epoch: [13][    8/    8]    Loss 0.346512    Top1 84.400000    
2023-09-11 13:46:01,516 - ==> Top1: 84.400    Loss: 0.347

2023-09-11 13:46:01,516 - ==> Confusion:
[[864 121]
 [191 824]]

2023-09-11 13:46:01,530 - ==> Best [Top1: 84.400   Sparsity:0.00   Params: 57776 on epoch: 13]
2023-09-11 13:46:01,530 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:46:01,533 - 

2023-09-11 13:46:01,533 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:46:04,619 - Epoch: [14][   10/   71]    Overall Loss 0.342434    Objective Loss 0.342434                                        LR 0.001000    Time 0.308546    
2023-09-11 13:46:06,492 - Epoch: [14][   20/   71]    Overall Loss 0.343459    Objective Loss 0.343459                                        LR 0.001000    Time 0.247890    
2023-09-11 13:46:09,077 - Epoch: [14][   30/   71]    Overall Loss 0.346110    Objective Loss 0.346110                                        LR 0.001000    Time 0.251431    
2023-09-11 13:46:11,015 - Epoch: [14][   40/   71]    Overall Loss 0.341886    Objective Loss 0.341886                                        LR 0.001000    Time 0.237006    
2023-09-11 13:46:13,636 - Epoch: [14][   50/   71]    Overall Loss 0.343969    Objective Loss 0.343969                                        LR 0.001000    Time 0.242014    
2023-09-11 13:46:15,628 - Epoch: [14][   60/   71]    Overall Loss 0.341847    Objective Loss 0.341847                                        LR 0.001000    Time 0.234882    
2023-09-11 13:46:17,955 - Epoch: [14][   70/   71]    Overall Loss 0.343307    Objective Loss 0.343307    Top1 83.593750    LR 0.001000    Time 0.234561    
2023-09-11 13:46:18,009 - Epoch: [14][   71/   71]    Overall Loss 0.343631    Objective Loss 0.343631    Top1 83.630952    LR 0.001000    Time 0.232014    
2023-09-11 13:46:18,101 - --- validate (epoch=14)-----------
2023-09-11 13:46:18,101 - 2000 samples (256 per mini-batch)
2023-09-11 13:46:20,917 - Epoch: [14][    8/    8]    Loss 0.385165    Top1 82.350000    
2023-09-11 13:46:21,015 - ==> Top1: 82.350    Loss: 0.385

2023-09-11 13:46:21,015 - ==> Confusion:
[[888  97]
 [256 759]]

2023-09-11 13:46:21,031 - ==> Best [Top1: 84.400   Sparsity:0.00   Params: 57776 on epoch: 13]
2023-09-11 13:46:21,031 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:46:21,034 - 

2023-09-11 13:46:21,034 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:46:24,541 - Epoch: [15][   10/   71]    Overall Loss 0.346086    Objective Loss 0.346086                                        LR 0.001000    Time 0.350661    
2023-09-11 13:46:26,450 - Epoch: [15][   20/   71]    Overall Loss 0.344210    Objective Loss 0.344210                                        LR 0.001000    Time 0.270766    
2023-09-11 13:46:29,061 - Epoch: [15][   30/   71]    Overall Loss 0.339610    Objective Loss 0.339610                                        LR 0.001000    Time 0.267537    
2023-09-11 13:46:31,089 - Epoch: [15][   40/   71]    Overall Loss 0.338289    Objective Loss 0.338289                                        LR 0.001000    Time 0.251359    
2023-09-11 13:46:33,676 - Epoch: [15][   50/   71]    Overall Loss 0.335131    Objective Loss 0.335131                                        LR 0.001000    Time 0.252802    
2023-09-11 13:46:35,598 - Epoch: [15][   60/   71]    Overall Loss 0.334381    Objective Loss 0.334381                                        LR 0.001000    Time 0.242700    
2023-09-11 13:46:37,960 - Epoch: [15][   70/   71]    Overall Loss 0.334742    Objective Loss 0.334742    Top1 85.937500    LR 0.001000    Time 0.241767    
2023-09-11 13:46:38,014 - Epoch: [15][   71/   71]    Overall Loss 0.337589    Objective Loss 0.337589    Top1 82.738095    LR 0.001000    Time 0.239122    
2023-09-11 13:46:38,098 - --- validate (epoch=15)-----------
2023-09-11 13:46:38,098 - 2000 samples (256 per mini-batch)
2023-09-11 13:46:40,321 - Epoch: [15][    8/    8]    Loss 0.321565    Top1 86.500000    
2023-09-11 13:46:40,401 - ==> Top1: 86.500    Loss: 0.322

2023-09-11 13:46:40,401 - ==> Confusion:
[[847 138]
 [132 883]]

2023-09-11 13:46:40,415 - ==> Best [Top1: 86.500   Sparsity:0.00   Params: 57776 on epoch: 15]
2023-09-11 13:46:40,416 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:46:40,418 - 

2023-09-11 13:46:40,418 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:46:43,510 - Epoch: [16][   10/   71]    Overall Loss 0.308856    Objective Loss 0.308856                                        LR 0.001000    Time 0.309109    
2023-09-11 13:46:45,361 - Epoch: [16][   20/   71]    Overall Loss 0.308568    Objective Loss 0.308568                                        LR 0.001000    Time 0.247098    
2023-09-11 13:46:48,065 - Epoch: [16][   30/   71]    Overall Loss 0.314104    Objective Loss 0.314104                                        LR 0.001000    Time 0.254848    
2023-09-11 13:46:49,994 - Epoch: [16][   40/   71]    Overall Loss 0.323229    Objective Loss 0.323229                                        LR 0.001000    Time 0.239338    
2023-09-11 13:46:52,747 - Epoch: [16][   50/   71]    Overall Loss 0.324753    Objective Loss 0.324753                                        LR 0.001000    Time 0.246535    
2023-09-11 13:46:54,748 - Epoch: [16][   60/   71]    Overall Loss 0.322716    Objective Loss 0.322716                                        LR 0.001000    Time 0.238794    
2023-09-11 13:46:57,201 - Epoch: [16][   70/   71]    Overall Loss 0.324338    Objective Loss 0.324338    Top1 84.765625    LR 0.001000    Time 0.239635    
2023-09-11 13:46:57,238 - Epoch: [16][   71/   71]    Overall Loss 0.323968    Objective Loss 0.323968    Top1 85.714286    LR 0.001000    Time 0.236776    
2023-09-11 13:46:57,334 - --- validate (epoch=16)-----------
2023-09-11 13:46:57,334 - 2000 samples (256 per mini-batch)
2023-09-11 13:47:00,118 - Epoch: [16][    8/    8]    Loss 0.375500    Top1 82.950000    
2023-09-11 13:47:00,215 - ==> Top1: 82.950    Loss: 0.375

2023-09-11 13:47:00,215 - ==> Confusion:
[[935  50]
 [291 724]]

2023-09-11 13:47:00,230 - ==> Best [Top1: 86.500   Sparsity:0.00   Params: 57776 on epoch: 15]
2023-09-11 13:47:00,230 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:47:00,232 - 

2023-09-11 13:47:00,232 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:47:03,279 - Epoch: [17][   10/   71]    Overall Loss 0.337823    Objective Loss 0.337823                                        LR 0.001000    Time 0.304633    
2023-09-11 13:47:05,177 - Epoch: [17][   20/   71]    Overall Loss 0.337905    Objective Loss 0.337905                                        LR 0.001000    Time 0.247203    
2023-09-11 13:47:07,844 - Epoch: [17][   30/   71]    Overall Loss 0.331820    Objective Loss 0.331820                                        LR 0.001000    Time 0.253682    
2023-09-11 13:47:09,835 - Epoch: [17][   40/   71]    Overall Loss 0.323893    Objective Loss 0.323893                                        LR 0.001000    Time 0.240035    
2023-09-11 13:47:12,613 - Epoch: [17][   50/   71]    Overall Loss 0.320550    Objective Loss 0.320550                                        LR 0.001000    Time 0.247582    
2023-09-11 13:47:14,558 - Epoch: [17][   60/   71]    Overall Loss 0.317630    Objective Loss 0.317630                                        LR 0.001000    Time 0.238727    
2023-09-11 13:47:16,938 - Epoch: [17][   70/   71]    Overall Loss 0.315687    Objective Loss 0.315687    Top1 84.375000    LR 0.001000    Time 0.238620    
2023-09-11 13:47:16,988 - Epoch: [17][   71/   71]    Overall Loss 0.315508    Objective Loss 0.315508    Top1 85.119048    LR 0.001000    Time 0.235950    
2023-09-11 13:47:17,088 - --- validate (epoch=17)-----------
2023-09-11 13:47:17,088 - 2000 samples (256 per mini-batch)
2023-09-11 13:47:19,349 - Epoch: [17][    8/    8]    Loss 0.328981    Top1 86.050000    
2023-09-11 13:47:19,443 - ==> Top1: 86.050    Loss: 0.329

2023-09-11 13:47:19,443 - ==> Confusion:
[[783 202]
 [ 77 938]]

2023-09-11 13:47:19,458 - ==> Best [Top1: 86.500   Sparsity:0.00   Params: 57776 on epoch: 15]
2023-09-11 13:47:19,459 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:47:19,461 - 

2023-09-11 13:47:19,461 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:47:22,484 - Epoch: [18][   10/   71]    Overall Loss 0.308845    Objective Loss 0.308845                                        LR 0.001000    Time 0.302244    
2023-09-11 13:47:24,975 - Epoch: [18][   20/   71]    Overall Loss 0.304315    Objective Loss 0.304315                                        LR 0.001000    Time 0.275671    
2023-09-11 13:47:26,970 - Epoch: [18][   30/   71]    Overall Loss 0.304256    Objective Loss 0.304256                                        LR 0.001000    Time 0.250262    
2023-09-11 13:47:29,604 - Epoch: [18][   40/   71]    Overall Loss 0.300606    Objective Loss 0.300606                                        LR 0.001000    Time 0.253548    
2023-09-11 13:47:32,663 - Epoch: [18][   50/   71]    Overall Loss 0.299598    Objective Loss 0.299598                                        LR 0.001000    Time 0.264007    
2023-09-11 13:47:34,988 - Epoch: [18][   60/   71]    Overall Loss 0.298097    Objective Loss 0.298097                                        LR 0.001000    Time 0.258746    
2023-09-11 13:47:36,940 - Epoch: [18][   70/   71]    Overall Loss 0.298756    Objective Loss 0.298756    Top1 87.890625    LR 0.001000    Time 0.249660    
2023-09-11 13:47:36,996 - Epoch: [18][   71/   71]    Overall Loss 0.298696    Objective Loss 0.298696    Top1 86.904762    LR 0.001000    Time 0.246936    
2023-09-11 13:47:37,086 - --- validate (epoch=18)-----------
2023-09-11 13:47:37,086 - 2000 samples (256 per mini-batch)
2023-09-11 13:47:39,443 - Epoch: [18][    8/    8]    Loss 0.330081    Top1 84.850000    
2023-09-11 13:47:39,535 - ==> Top1: 84.850    Loss: 0.330

2023-09-11 13:47:39,536 - ==> Confusion:
[[908  77]
 [226 789]]

2023-09-11 13:47:39,551 - ==> Best [Top1: 86.500   Sparsity:0.00   Params: 57776 on epoch: 15]
2023-09-11 13:47:39,551 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:47:39,553 - 

2023-09-11 13:47:39,553 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:47:42,909 - Epoch: [19][   10/   71]    Overall Loss 0.299029    Objective Loss 0.299029                                        LR 0.001000    Time 0.335538    
2023-09-11 13:47:44,935 - Epoch: [19][   20/   71]    Overall Loss 0.301830    Objective Loss 0.301830                                        LR 0.001000    Time 0.269058    
2023-09-11 13:47:47,535 - Epoch: [19][   30/   71]    Overall Loss 0.305319    Objective Loss 0.305319                                        LR 0.001000    Time 0.266015    
2023-09-11 13:47:49,410 - Epoch: [19][   40/   71]    Overall Loss 0.309273    Objective Loss 0.309273                                        LR 0.001000    Time 0.246374    
2023-09-11 13:47:52,209 - Epoch: [19][   50/   71]    Overall Loss 0.307461    Objective Loss 0.307461                                        LR 0.001000    Time 0.253072    
2023-09-11 13:47:54,664 - Epoch: [19][   60/   71]    Overall Loss 0.302684    Objective Loss 0.302684                                        LR 0.001000    Time 0.251801    
2023-09-11 13:47:56,934 - Epoch: [19][   70/   71]    Overall Loss 0.304184    Objective Loss 0.304184    Top1 87.109375    LR 0.001000    Time 0.248259    
2023-09-11 13:47:56,988 - Epoch: [19][   71/   71]    Overall Loss 0.304446    Objective Loss 0.304446    Top1 86.309524    LR 0.001000    Time 0.245519    
2023-09-11 13:47:57,074 - --- validate (epoch=19)-----------
2023-09-11 13:47:57,075 - 2000 samples (256 per mini-batch)
2023-09-11 13:48:00,023 - Epoch: [19][    8/    8]    Loss 0.392467    Top1 81.850000    
2023-09-11 13:48:00,114 - ==> Top1: 81.850    Loss: 0.392

2023-09-11 13:48:00,114 - ==> Confusion:
[[945  40]
 [323 692]]

2023-09-11 13:48:00,130 - ==> Best [Top1: 86.500   Sparsity:0.00   Params: 57776 on epoch: 15]
2023-09-11 13:48:00,131 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:48:00,133 - 

2023-09-11 13:48:00,133 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:48:03,333 - Epoch: [20][   10/   71]    Overall Loss 0.308596    Objective Loss 0.308596                                        LR 0.000500    Time 0.320001    
2023-09-11 13:48:05,200 - Epoch: [20][   20/   71]    Overall Loss 0.295294    Objective Loss 0.295294                                        LR 0.000500    Time 0.253300    
2023-09-11 13:48:07,883 - Epoch: [20][   30/   71]    Overall Loss 0.286377    Objective Loss 0.286377                                        LR 0.000500    Time 0.258288    
2023-09-11 13:48:09,825 - Epoch: [20][   40/   71]    Overall Loss 0.284384    Objective Loss 0.284384                                        LR 0.000500    Time 0.242272    
2023-09-11 13:48:12,595 - Epoch: [20][   50/   71]    Overall Loss 0.288698    Objective Loss 0.288698                                        LR 0.000500    Time 0.249210    
2023-09-11 13:48:14,652 - Epoch: [20][   60/   71]    Overall Loss 0.287108    Objective Loss 0.287108                                        LR 0.000500    Time 0.241949    
2023-09-11 13:48:16,858 - Epoch: [20][   70/   71]    Overall Loss 0.285438    Objective Loss 0.285438    Top1 90.234375    LR 0.000500    Time 0.238892    
2023-09-11 13:48:16,914 - Epoch: [20][   71/   71]    Overall Loss 0.284864    Objective Loss 0.284864    Top1 90.476190    LR 0.000500    Time 0.236312    
2023-09-11 13:48:17,002 - --- validate (epoch=20)-----------
2023-09-11 13:48:17,003 - 2000 samples (256 per mini-batch)
2023-09-11 13:48:19,684 - Epoch: [20][    8/    8]    Loss 0.288315    Top1 87.700000    
2023-09-11 13:48:19,784 - ==> Top1: 87.700    Loss: 0.288

2023-09-11 13:48:19,784 - ==> Confusion:
[[883 102]
 [144 871]]

2023-09-11 13:48:19,799 - ==> Best [Top1: 87.700   Sparsity:0.00   Params: 57776 on epoch: 20]
2023-09-11 13:48:19,800 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:48:19,802 - 

2023-09-11 13:48:19,802 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:48:23,064 - Epoch: [21][   10/   71]    Overall Loss 0.285752    Objective Loss 0.285752                                        LR 0.000500    Time 0.326112    
2023-09-11 13:48:25,962 - Epoch: [21][   20/   71]    Overall Loss 0.289914    Objective Loss 0.289914                                        LR 0.000500    Time 0.307928    
2023-09-11 13:48:27,858 - Epoch: [21][   30/   71]    Overall Loss 0.292561    Objective Loss 0.292561                                        LR 0.000500    Time 0.268493    
2023-09-11 13:48:30,551 - Epoch: [21][   40/   71]    Overall Loss 0.289862    Objective Loss 0.289862                                        LR 0.000500    Time 0.268689    
2023-09-11 13:48:32,744 - Epoch: [21][   50/   71]    Overall Loss 0.286270    Objective Loss 0.286270                                        LR 0.000500    Time 0.258804    
2023-09-11 13:48:35,379 - Epoch: [21][   60/   71]    Overall Loss 0.283386    Objective Loss 0.283386                                        LR 0.000500    Time 0.259570    
2023-09-11 13:48:37,168 - Epoch: [21][   70/   71]    Overall Loss 0.280368    Objective Loss 0.280368    Top1 87.500000    LR 0.000500    Time 0.248042    
2023-09-11 13:48:37,223 - Epoch: [21][   71/   71]    Overall Loss 0.281661    Objective Loss 0.281661    Top1 87.500000    LR 0.000500    Time 0.245322    
2023-09-11 13:48:37,319 - --- validate (epoch=21)-----------
2023-09-11 13:48:37,319 - 2000 samples (256 per mini-batch)
2023-09-11 13:48:40,108 - Epoch: [21][    8/    8]    Loss 0.273386    Top1 88.700000    
2023-09-11 13:48:40,201 - ==> Top1: 88.700    Loss: 0.273

2023-09-11 13:48:40,202 - ==> Confusion:
[[849 136]
 [ 90 925]]

2023-09-11 13:48:40,202 - ==> Best [Top1: 88.700   Sparsity:0.00   Params: 57776 on epoch: 21]
2023-09-11 13:48:40,202 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:48:40,205 - 

2023-09-11 13:48:40,205 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:48:44,144 - Epoch: [22][   10/   71]    Overall Loss 0.272255    Objective Loss 0.272255                                        LR 0.000500    Time 0.393836    
2023-09-11 13:48:46,056 - Epoch: [22][   20/   71]    Overall Loss 0.273082    Objective Loss 0.273082                                        LR 0.000500    Time 0.292494    
2023-09-11 13:48:48,715 - Epoch: [22][   30/   71]    Overall Loss 0.269281    Objective Loss 0.269281                                        LR 0.000500    Time 0.283605    
2023-09-11 13:48:50,683 - Epoch: [22][   40/   71]    Overall Loss 0.266524    Objective Loss 0.266524                                        LR 0.000500    Time 0.261902    
2023-09-11 13:48:53,314 - Epoch: [22][   50/   71]    Overall Loss 0.268067    Objective Loss 0.268067                                        LR 0.000500    Time 0.262135    
2023-09-11 13:48:55,339 - Epoch: [22][   60/   71]    Overall Loss 0.272788    Objective Loss 0.272788                                        LR 0.000500    Time 0.252188    
2023-09-11 13:48:57,649 - Epoch: [22][   70/   71]    Overall Loss 0.277193    Objective Loss 0.277193    Top1 90.625000    LR 0.000500    Time 0.249164    
2023-09-11 13:48:57,706 - Epoch: [22][   71/   71]    Overall Loss 0.277597    Objective Loss 0.277597    Top1 89.583333    LR 0.000500    Time 0.246457    
2023-09-11 13:48:57,798 - --- validate (epoch=22)-----------
2023-09-11 13:48:57,798 - 2000 samples (256 per mini-batch)
2023-09-11 13:49:00,140 - Epoch: [22][    8/    8]    Loss 0.268274    Top1 88.550000    
2023-09-11 13:49:00,235 - ==> Top1: 88.550    Loss: 0.268

2023-09-11 13:49:00,236 - ==> Confusion:
[[878 107]
 [122 893]]

2023-09-11 13:49:00,238 - ==> Best [Top1: 88.700   Sparsity:0.00   Params: 57776 on epoch: 21]
2023-09-11 13:49:00,238 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:49:00,240 - 

2023-09-11 13:49:00,240 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:49:03,252 - Epoch: [23][   10/   71]    Overall Loss 0.266420    Objective Loss 0.266420                                        LR 0.000500    Time 0.301163    
2023-09-11 13:49:05,196 - Epoch: [23][   20/   71]    Overall Loss 0.266742    Objective Loss 0.266742                                        LR 0.000500    Time 0.247729    
2023-09-11 13:49:07,967 - Epoch: [23][   30/   71]    Overall Loss 0.268304    Objective Loss 0.268304                                        LR 0.000500    Time 0.257534    
2023-09-11 13:49:09,937 - Epoch: [23][   40/   71]    Overall Loss 0.266549    Objective Loss 0.266549                                        LR 0.000500    Time 0.242395    
2023-09-11 13:49:12,617 - Epoch: [23][   50/   71]    Overall Loss 0.267717    Objective Loss 0.267717                                        LR 0.000500    Time 0.247504    
2023-09-11 13:49:14,717 - Epoch: [23][   60/   71]    Overall Loss 0.266442    Objective Loss 0.266442                                        LR 0.000500    Time 0.241240    
2023-09-11 13:49:16,882 - Epoch: [23][   70/   71]    Overall Loss 0.265819    Objective Loss 0.265819    Top1 89.453125    LR 0.000500    Time 0.237710    
2023-09-11 13:49:16,936 - Epoch: [23][   71/   71]    Overall Loss 0.265769    Objective Loss 0.265769    Top1 89.583333    LR 0.000500    Time 0.235113    
2023-09-11 13:49:17,027 - --- validate (epoch=23)-----------
2023-09-11 13:49:17,027 - 2000 samples (256 per mini-batch)
2023-09-11 13:49:19,615 - Epoch: [23][    8/    8]    Loss 0.273119    Top1 88.500000    
2023-09-11 13:49:19,696 - ==> Top1: 88.500    Loss: 0.273

2023-09-11 13:49:19,697 - ==> Confusion:
[[872 113]
 [117 898]]

2023-09-11 13:49:19,699 - ==> Best [Top1: 88.700   Sparsity:0.00   Params: 57776 on epoch: 21]
2023-09-11 13:49:19,699 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:49:19,701 - 

2023-09-11 13:49:19,701 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:49:22,920 - Epoch: [24][   10/   71]    Overall Loss 0.256910    Objective Loss 0.256910                                        LR 0.000500    Time 0.321854    
2023-09-11 13:49:25,424 - Epoch: [24][   20/   71]    Overall Loss 0.266019    Objective Loss 0.266019                                        LR 0.000500    Time 0.286129    
2023-09-11 13:49:27,352 - Epoch: [24][   30/   71]    Overall Loss 0.269425    Objective Loss 0.269425                                        LR 0.000500    Time 0.254983    
2023-09-11 13:49:29,940 - Epoch: [24][   40/   71]    Overall Loss 0.275635    Objective Loss 0.275635                                        LR 0.000500    Time 0.255942    
2023-09-11 13:49:31,996 - Epoch: [24][   50/   71]    Overall Loss 0.272881    Objective Loss 0.272881                                        LR 0.000500    Time 0.245869    
2023-09-11 13:49:34,562 - Epoch: [24][   60/   71]    Overall Loss 0.272661    Objective Loss 0.272661                                        LR 0.000500    Time 0.247636    
2023-09-11 13:49:36,344 - Epoch: [24][   70/   71]    Overall Loss 0.269014    Objective Loss 0.269014    Top1 90.234375    LR 0.000500    Time 0.237720    
2023-09-11 13:49:36,398 - Epoch: [24][   71/   71]    Overall Loss 0.269085    Objective Loss 0.269085    Top1 90.178571    LR 0.000500    Time 0.235127    
2023-09-11 13:49:36,488 - --- validate (epoch=24)-----------
2023-09-11 13:49:36,489 - 2000 samples (256 per mini-batch)
2023-09-11 13:49:39,285 - Epoch: [24][    8/    8]    Loss 0.262121    Top1 88.650000    
2023-09-11 13:49:39,379 - ==> Top1: 88.650    Loss: 0.262

2023-09-11 13:49:39,379 - ==> Confusion:
[[865 120]
 [107 908]]

2023-09-11 13:49:39,394 - ==> Best [Top1: 88.700   Sparsity:0.00   Params: 57776 on epoch: 21]
2023-09-11 13:49:39,394 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:49:39,397 - 

2023-09-11 13:49:39,397 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:49:42,347 - Epoch: [25][   10/   71]    Overall Loss 0.268170    Objective Loss 0.268170                                        LR 0.000500    Time 0.294933    
2023-09-11 13:49:44,277 - Epoch: [25][   20/   71]    Overall Loss 0.259584    Objective Loss 0.259584                                        LR 0.000500    Time 0.243975    
2023-09-11 13:49:46,867 - Epoch: [25][   30/   71]    Overall Loss 0.260530    Objective Loss 0.260530                                        LR 0.000500    Time 0.248964    
2023-09-11 13:49:48,907 - Epoch: [25][   40/   71]    Overall Loss 0.257426    Objective Loss 0.257426                                        LR 0.000500    Time 0.237719    
2023-09-11 13:49:51,556 - Epoch: [25][   50/   71]    Overall Loss 0.258607    Objective Loss 0.258607                                        LR 0.000500    Time 0.243148    
2023-09-11 13:49:53,634 - Epoch: [25][   60/   71]    Overall Loss 0.259880    Objective Loss 0.259880                                        LR 0.000500    Time 0.237244    
2023-09-11 13:49:56,017 - Epoch: [25][   70/   71]    Overall Loss 0.259897    Objective Loss 0.259897    Top1 88.281250    LR 0.000500    Time 0.237394    
2023-09-11 13:49:56,074 - Epoch: [25][   71/   71]    Overall Loss 0.260832    Objective Loss 0.260832    Top1 87.500000    LR 0.000500    Time 0.234855    
2023-09-11 13:49:56,167 - --- validate (epoch=25)-----------
2023-09-11 13:49:56,167 - 2000 samples (256 per mini-batch)
2023-09-11 13:49:58,976 - Epoch: [25][    8/    8]    Loss 0.252398    Top1 89.300000    
2023-09-11 13:49:59,065 - ==> Top1: 89.300    Loss: 0.252

2023-09-11 13:49:59,066 - ==> Confusion:
[[882 103]
 [111 904]]

2023-09-11 13:49:59,082 - ==> Best [Top1: 89.300   Sparsity:0.00   Params: 57776 on epoch: 25]
2023-09-11 13:49:59,082 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:49:59,085 - 

2023-09-11 13:49:59,085 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:50:02,124 - Epoch: [26][   10/   71]    Overall Loss 0.258358    Objective Loss 0.258358                                        LR 0.000500    Time 0.303874    
2023-09-11 13:50:04,809 - Epoch: [26][   20/   71]    Overall Loss 0.249143    Objective Loss 0.249143                                        LR 0.000500    Time 0.286156    
2023-09-11 13:50:07,807 - Epoch: [26][   30/   71]    Overall Loss 0.248127    Objective Loss 0.248127                                        LR 0.000500    Time 0.290708    
2023-09-11 13:50:09,725 - Epoch: [26][   40/   71]    Overall Loss 0.251466    Objective Loss 0.251466                                        LR 0.000500    Time 0.265960    
2023-09-11 13:50:12,375 - Epoch: [26][   50/   71]    Overall Loss 0.252845    Objective Loss 0.252845                                        LR 0.000500    Time 0.265757    
2023-09-11 13:50:14,255 - Epoch: [26][   60/   71]    Overall Loss 0.255276    Objective Loss 0.255276                                        LR 0.000500    Time 0.252796    
2023-09-11 13:50:16,834 - Epoch: [26][   70/   71]    Overall Loss 0.255538    Objective Loss 0.255538    Top1 93.359375    LR 0.000500    Time 0.253517    
2023-09-11 13:50:16,885 - Epoch: [26][   71/   71]    Overall Loss 0.254752    Objective Loss 0.254752    Top1 93.452381    LR 0.000500    Time 0.250670    
2023-09-11 13:50:16,982 - --- validate (epoch=26)-----------
2023-09-11 13:50:16,983 - 2000 samples (256 per mini-batch)
2023-09-11 13:50:19,275 - Epoch: [26][    8/    8]    Loss 0.290556    Top1 87.500000    
2023-09-11 13:50:19,370 - ==> Top1: 87.500    Loss: 0.291

2023-09-11 13:50:19,370 - ==> Confusion:
[[786 199]
 [ 51 964]]

2023-09-11 13:50:19,371 - ==> Best [Top1: 89.300   Sparsity:0.00   Params: 57776 on epoch: 25]
2023-09-11 13:50:19,371 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:50:19,374 - 

2023-09-11 13:50:19,374 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:50:22,461 - Epoch: [27][   10/   71]    Overall Loss 0.275978    Objective Loss 0.275978                                        LR 0.000500    Time 0.308697    
2023-09-11 13:50:24,397 - Epoch: [27][   20/   71]    Overall Loss 0.264666    Objective Loss 0.264666                                        LR 0.000500    Time 0.251099    
2023-09-11 13:50:26,946 - Epoch: [27][   30/   71]    Overall Loss 0.259205    Objective Loss 0.259205                                        LR 0.000500    Time 0.252380    
2023-09-11 13:50:29,226 - Epoch: [27][   40/   71]    Overall Loss 0.261822    Objective Loss 0.261822                                        LR 0.000500    Time 0.246269    
2023-09-11 13:50:31,730 - Epoch: [27][   50/   71]    Overall Loss 0.259050    Objective Loss 0.259050                                        LR 0.000500    Time 0.247097    
2023-09-11 13:50:33,979 - Epoch: [27][   60/   71]    Overall Loss 0.256715    Objective Loss 0.256715                                        LR 0.000500    Time 0.243391    
2023-09-11 13:50:35,997 - Epoch: [27][   70/   71]    Overall Loss 0.254551    Objective Loss 0.254551    Top1 88.281250    LR 0.000500    Time 0.237439    
2023-09-11 13:50:36,054 - Epoch: [27][   71/   71]    Overall Loss 0.253893    Objective Loss 0.253893    Top1 89.285714    LR 0.000500    Time 0.234894    
2023-09-11 13:50:36,144 - --- validate (epoch=27)-----------
2023-09-11 13:50:36,144 - 2000 samples (256 per mini-batch)
2023-09-11 13:50:38,861 - Epoch: [27][    8/    8]    Loss 0.280413    Top1 87.450000    
2023-09-11 13:50:38,965 - ==> Top1: 87.450    Loss: 0.280

2023-09-11 13:50:38,965 - ==> Confusion:
[[918  67]
 [184 831]]

2023-09-11 13:50:38,980 - ==> Best [Top1: 89.300   Sparsity:0.00   Params: 57776 on epoch: 25]
2023-09-11 13:50:38,980 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:50:38,984 - 

2023-09-11 13:50:38,984 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:50:42,397 - Epoch: [28][   10/   71]    Overall Loss 0.229270    Objective Loss 0.229270                                        LR 0.000500    Time 0.341166    
2023-09-11 13:50:44,994 - Epoch: [28][   20/   71]    Overall Loss 0.249809    Objective Loss 0.249809                                        LR 0.000500    Time 0.300419    
2023-09-11 13:50:47,261 - Epoch: [28][   30/   71]    Overall Loss 0.249062    Objective Loss 0.249062                                        LR 0.000500    Time 0.275836    
2023-09-11 13:50:50,010 - Epoch: [28][   40/   71]    Overall Loss 0.241849    Objective Loss 0.241849                                        LR 0.000500    Time 0.275586    
2023-09-11 13:50:52,046 - Epoch: [28][   50/   71]    Overall Loss 0.238061    Objective Loss 0.238061                                        LR 0.000500    Time 0.261187    
2023-09-11 13:50:54,779 - Epoch: [28][   60/   71]    Overall Loss 0.242507    Objective Loss 0.242507                                        LR 0.000500    Time 0.263205    
2023-09-11 13:50:56,506 - Epoch: [28][   70/   71]    Overall Loss 0.244726    Objective Loss 0.244726    Top1 87.500000    LR 0.000500    Time 0.250276    
2023-09-11 13:50:56,530 - Epoch: [28][   71/   71]    Overall Loss 0.246453    Objective Loss 0.246453    Top1 86.309524    LR 0.000500    Time 0.247075    
2023-09-11 13:50:56,624 - --- validate (epoch=28)-----------
2023-09-11 13:50:56,624 - 2000 samples (256 per mini-batch)
2023-09-11 13:50:58,771 - Epoch: [28][    8/    8]    Loss 0.276021    Top1 86.950000    
2023-09-11 13:50:58,865 - ==> Top1: 86.950    Loss: 0.276

2023-09-11 13:50:58,865 - ==> Confusion:
[[882 103]
 [158 857]]

2023-09-11 13:50:58,879 - ==> Best [Top1: 89.300   Sparsity:0.00   Params: 57776 on epoch: 25]
2023-09-11 13:50:58,879 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:50:58,882 - 

2023-09-11 13:50:58,882 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:51:02,007 - Epoch: [29][   10/   71]    Overall Loss 0.238283    Objective Loss 0.238283                                        LR 0.000500    Time 0.312507    
2023-09-11 13:51:03,945 - Epoch: [29][   20/   71]    Overall Loss 0.247203    Objective Loss 0.247203                                        LR 0.000500    Time 0.253129    
2023-09-11 13:51:06,535 - Epoch: [29][   30/   71]    Overall Loss 0.247636    Objective Loss 0.247636                                        LR 0.000500    Time 0.255064    
2023-09-11 13:51:08,686 - Epoch: [29][   40/   71]    Overall Loss 0.251349    Objective Loss 0.251349                                        LR 0.000500    Time 0.245058    
2023-09-11 13:51:11,367 - Epoch: [29][   50/   71]    Overall Loss 0.249663    Objective Loss 0.249663                                        LR 0.000500    Time 0.249674    
2023-09-11 13:51:13,487 - Epoch: [29][   60/   71]    Overall Loss 0.248195    Objective Loss 0.248195                                        LR 0.000500    Time 0.243387    
2023-09-11 13:51:15,720 - Epoch: [29][   70/   71]    Overall Loss 0.246869    Objective Loss 0.246869    Top1 88.671875    LR 0.000500    Time 0.240504    
2023-09-11 13:51:15,773 - Epoch: [29][   71/   71]    Overall Loss 0.246330    Objective Loss 0.246330    Top1 89.285714    LR 0.000500    Time 0.237869    
2023-09-11 13:51:15,863 - --- validate (epoch=29)-----------
2023-09-11 13:51:15,863 - 2000 samples (256 per mini-batch)
2023-09-11 13:51:18,356 - Epoch: [29][    8/    8]    Loss 0.273313    Top1 87.650000    
2023-09-11 13:51:18,449 - ==> Top1: 87.650    Loss: 0.273

2023-09-11 13:51:18,450 - ==> Confusion:
[[896  89]
 [158 857]]

2023-09-11 13:51:18,465 - ==> Best [Top1: 89.300   Sparsity:0.00   Params: 57776 on epoch: 25]
2023-09-11 13:51:18,465 - Saving checkpoint to: logs/2023.09.11-134109/checkpoint.pth.tar
2023-09-11 13:51:18,476 - 

2023-09-11 13:51:18,476 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:51:22,066 - Epoch: [30][   10/   71]    Overall Loss 0.397721    Objective Loss 0.397721                                        LR 0.000500    Time 0.358927    
2023-09-11 13:51:24,067 - Epoch: [30][   20/   71]    Overall Loss 0.371947    Objective Loss 0.371947                                        LR 0.000500    Time 0.279498    
2023-09-11 13:51:26,793 - Epoch: [30][   30/   71]    Overall Loss 0.347872    Objective Loss 0.347872                                        LR 0.000500    Time 0.277167    
2023-09-11 13:51:28,743 - Epoch: [30][   40/   71]    Overall Loss 0.335985    Objective Loss 0.335985                                        LR 0.000500    Time 0.256616    
2023-09-11 13:51:31,377 - Epoch: [30][   50/   71]    Overall Loss 0.321244    Objective Loss 0.321244                                        LR 0.000500    Time 0.257976    
2023-09-11 13:51:33,609 - Epoch: [30][   60/   71]    Overall Loss 0.310839    Objective Loss 0.310839                                        LR 0.000500    Time 0.252173    
2023-09-11 13:51:35,956 - Epoch: [30][   70/   71]    Overall Loss 0.301708    Objective Loss 0.301708    Top1 91.406250    LR 0.000500    Time 0.249681    
2023-09-11 13:51:36,033 - Epoch: [30][   71/   71]    Overall Loss 0.299712    Objective Loss 0.299712    Top1 91.071429    LR 0.000500    Time 0.247239    
2023-09-11 13:51:36,125 - --- validate (epoch=30)-----------
2023-09-11 13:51:36,125 - 2000 samples (256 per mini-batch)
2023-09-11 13:51:38,837 - Epoch: [30][    8/    8]    Loss 0.310722    Top1 87.150000    
2023-09-11 13:51:38,928 - ==> Top1: 87.150    Loss: 0.311

2023-09-11 13:51:38,928 - ==> Confusion:
[[917  68]
 [189 826]]

2023-09-11 13:51:38,945 - ==> Best [Top1: 87.150   Sparsity:0.00   Params: 57776 on epoch: 30]
2023-09-11 13:51:38,945 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:51:38,949 - 

2023-09-11 13:51:38,949 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:51:42,626 - Epoch: [31][   10/   71]    Overall Loss 0.279034    Objective Loss 0.279034                                        LR 0.000500    Time 0.367688    
2023-09-11 13:51:44,782 - Epoch: [31][   20/   71]    Overall Loss 0.264054    Objective Loss 0.264054                                        LR 0.000500    Time 0.291612    
2023-09-11 13:51:47,206 - Epoch: [31][   30/   71]    Overall Loss 0.255587    Objective Loss 0.255587                                        LR 0.000500    Time 0.275182    
2023-09-11 13:51:49,202 - Epoch: [31][   40/   71]    Overall Loss 0.250531    Objective Loss 0.250531                                        LR 0.000500    Time 0.256278    
2023-09-11 13:51:52,008 - Epoch: [31][   50/   71]    Overall Loss 0.251510    Objective Loss 0.251510                                        LR 0.000500    Time 0.261148    
2023-09-11 13:51:53,961 - Epoch: [31][   60/   71]    Overall Loss 0.252924    Objective Loss 0.252924                                        LR 0.000500    Time 0.250160    
2023-09-11 13:51:56,175 - Epoch: [31][   70/   71]    Overall Loss 0.252713    Objective Loss 0.252713    Top1 88.671875    LR 0.000500    Time 0.246057    
2023-09-11 13:51:56,240 - Epoch: [31][   71/   71]    Overall Loss 0.254634    Objective Loss 0.254634    Top1 87.797619    LR 0.000500    Time 0.243504    
2023-09-11 13:51:56,343 - --- validate (epoch=31)-----------
2023-09-11 13:51:56,343 - 2000 samples (256 per mini-batch)
2023-09-11 13:51:58,989 - Epoch: [31][    8/    8]    Loss 0.273137    Top1 88.750000    
2023-09-11 13:51:59,077 - ==> Top1: 88.750    Loss: 0.273

2023-09-11 13:51:59,077 - ==> Confusion:
[[849 136]
 [ 89 926]]

2023-09-11 13:51:59,094 - ==> Best [Top1: 88.750   Sparsity:0.00   Params: 57776 on epoch: 31]
2023-09-11 13:51:59,094 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:51:59,099 - 

2023-09-11 13:51:59,099 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:52:02,669 - Epoch: [32][   10/   71]    Overall Loss 0.231534    Objective Loss 0.231534                                        LR 0.000500    Time 0.356925    
2023-09-11 13:52:04,638 - Epoch: [32][   20/   71]    Overall Loss 0.234719    Objective Loss 0.234719                                        LR 0.000500    Time 0.276899    
2023-09-11 13:52:07,512 - Epoch: [32][   30/   71]    Overall Loss 0.232148    Objective Loss 0.232148                                        LR 0.000500    Time 0.280413    
2023-09-11 13:52:10,032 - Epoch: [32][   40/   71]    Overall Loss 0.232311    Objective Loss 0.232311                                        LR 0.000500    Time 0.273288    
2023-09-11 13:52:12,556 - Epoch: [32][   50/   71]    Overall Loss 0.237155    Objective Loss 0.237155                                        LR 0.000500    Time 0.269103    
2023-09-11 13:52:15,289 - Epoch: [32][   60/   71]    Overall Loss 0.236672    Objective Loss 0.236672                                        LR 0.000500    Time 0.269793    
2023-09-11 13:52:17,110 - Epoch: [32][   70/   71]    Overall Loss 0.241288    Objective Loss 0.241288    Top1 87.500000    LR 0.000500    Time 0.257271    
2023-09-11 13:52:17,183 - Epoch: [32][   71/   71]    Overall Loss 0.242420    Objective Loss 0.242420    Top1 87.500000    LR 0.000500    Time 0.254666    
2023-09-11 13:52:17,278 - --- validate (epoch=32)-----------
2023-09-11 13:52:17,279 - 2000 samples (256 per mini-batch)
2023-09-11 13:52:20,289 - Epoch: [32][    8/    8]    Loss 0.254586    Top1 89.000000    
2023-09-11 13:52:20,382 - ==> Top1: 89.000    Loss: 0.255

2023-09-11 13:52:20,382 - ==> Confusion:
[[852 133]
 [ 87 928]]

2023-09-11 13:52:20,384 - ==> Best [Top1: 89.000   Sparsity:0.00   Params: 57776 on epoch: 32]
2023-09-11 13:52:20,384 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:52:20,389 - 

2023-09-11 13:52:20,389 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:52:23,506 - Epoch: [33][   10/   71]    Overall Loss 0.245684    Objective Loss 0.245684                                        LR 0.000500    Time 0.311613    
2023-09-11 13:52:25,412 - Epoch: [33][   20/   71]    Overall Loss 0.253854    Objective Loss 0.253854                                        LR 0.000500    Time 0.251118    
2023-09-11 13:52:27,918 - Epoch: [33][   30/   71]    Overall Loss 0.248718    Objective Loss 0.248718                                        LR 0.000500    Time 0.250932    
2023-09-11 13:52:29,792 - Epoch: [33][   40/   71]    Overall Loss 0.253200    Objective Loss 0.253200                                        LR 0.000500    Time 0.235037    
2023-09-11 13:52:32,455 - Epoch: [33][   50/   71]    Overall Loss 0.248377    Objective Loss 0.248377                                        LR 0.000500    Time 0.241286    
2023-09-11 13:52:34,449 - Epoch: [33][   60/   71]    Overall Loss 0.247736    Objective Loss 0.247736                                        LR 0.000500    Time 0.234286    
2023-09-11 13:52:36,756 - Epoch: [33][   70/   71]    Overall Loss 0.245271    Objective Loss 0.245271    Top1 89.843750    LR 0.000500    Time 0.233769    
2023-09-11 13:52:36,828 - Epoch: [33][   71/   71]    Overall Loss 0.244977    Objective Loss 0.244977    Top1 89.285714    LR 0.000500    Time 0.231494    
2023-09-11 13:52:36,919 - --- validate (epoch=33)-----------
2023-09-11 13:52:36,919 - 2000 samples (256 per mini-batch)
2023-09-11 13:52:39,177 - Epoch: [33][    8/    8]    Loss 0.273379    Top1 87.750000    
2023-09-11 13:52:39,266 - ==> Top1: 87.750    Loss: 0.273

2023-09-11 13:52:39,266 - ==> Confusion:
[[895  90]
 [155 860]]

2023-09-11 13:52:39,267 - ==> Best [Top1: 89.000   Sparsity:0.00   Params: 57776 on epoch: 32]
2023-09-11 13:52:39,267 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:52:39,271 - 

2023-09-11 13:52:39,272 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:52:42,336 - Epoch: [34][   10/   71]    Overall Loss 0.236575    Objective Loss 0.236575                                        LR 0.000500    Time 0.306340    
2023-09-11 13:52:44,300 - Epoch: [34][   20/   71]    Overall Loss 0.237157    Objective Loss 0.237157                                        LR 0.000500    Time 0.251385    
2023-09-11 13:52:46,795 - Epoch: [34][   30/   71]    Overall Loss 0.240246    Objective Loss 0.240246                                        LR 0.000500    Time 0.250755    
2023-09-11 13:52:48,774 - Epoch: [34][   40/   71]    Overall Loss 0.241606    Objective Loss 0.241606                                        LR 0.000500    Time 0.237522    
2023-09-11 13:52:51,917 - Epoch: [34][   50/   71]    Overall Loss 0.239094    Objective Loss 0.239094                                        LR 0.000500    Time 0.252876    
2023-09-11 13:52:54,797 - Epoch: [34][   60/   71]    Overall Loss 0.237985    Objective Loss 0.237985                                        LR 0.000500    Time 0.258725    
2023-09-11 13:52:56,585 - Epoch: [34][   70/   71]    Overall Loss 0.238643    Objective Loss 0.238643    Top1 90.625000    LR 0.000500    Time 0.247298    
2023-09-11 13:52:56,666 - Epoch: [34][   71/   71]    Overall Loss 0.239103    Objective Loss 0.239103    Top1 90.178571    LR 0.000500    Time 0.244956    
2023-09-11 13:52:56,766 - --- validate (epoch=34)-----------
2023-09-11 13:52:56,767 - 2000 samples (256 per mini-batch)
2023-09-11 13:52:59,038 - Epoch: [34][    8/    8]    Loss 0.272697    Top1 88.200000    
2023-09-11 13:52:59,130 - ==> Top1: 88.200    Loss: 0.273

2023-09-11 13:52:59,130 - ==> Confusion:
[[900  85]
 [151 864]]

2023-09-11 13:52:59,147 - ==> Best [Top1: 89.000   Sparsity:0.00   Params: 57776 on epoch: 32]
2023-09-11 13:52:59,147 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:52:59,149 - 

2023-09-11 13:52:59,149 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:53:02,859 - Epoch: [35][   10/   71]    Overall Loss 0.233743    Objective Loss 0.233743                                        LR 0.000500    Time 0.370936    
2023-09-11 13:53:04,968 - Epoch: [35][   20/   71]    Overall Loss 0.229295    Objective Loss 0.229295                                        LR 0.000500    Time 0.290896    
2023-09-11 13:53:07,460 - Epoch: [35][   30/   71]    Overall Loss 0.235792    Objective Loss 0.235792                                        LR 0.000500    Time 0.276964    
2023-09-11 13:53:09,372 - Epoch: [35][   40/   71]    Overall Loss 0.235366    Objective Loss 0.235366                                        LR 0.000500    Time 0.255520    
2023-09-11 13:53:11,896 - Epoch: [35][   50/   71]    Overall Loss 0.236015    Objective Loss 0.236015                                        LR 0.000500    Time 0.254886    
2023-09-11 13:53:13,905 - Epoch: [35][   60/   71]    Overall Loss 0.236580    Objective Loss 0.236580                                        LR 0.000500    Time 0.245887    
2023-09-11 13:53:16,283 - Epoch: [35][   70/   71]    Overall Loss 0.238078    Objective Loss 0.238078    Top1 92.187500    LR 0.000500    Time 0.244735    
2023-09-11 13:53:16,349 - Epoch: [35][   71/   71]    Overall Loss 0.236958    Objective Loss 0.236958    Top1 92.857143    LR 0.000500    Time 0.242218    
2023-09-11 13:53:16,439 - --- validate (epoch=35)-----------
2023-09-11 13:53:16,439 - 2000 samples (256 per mini-batch)
2023-09-11 13:53:18,760 - Epoch: [35][    8/    8]    Loss 0.260876    Top1 89.850000    
2023-09-11 13:53:18,853 - ==> Top1: 89.850    Loss: 0.261

2023-09-11 13:53:18,853 - ==> Confusion:
[[865 120]
 [ 83 932]]

2023-09-11 13:53:18,855 - ==> Best [Top1: 89.850   Sparsity:0.00   Params: 57776 on epoch: 35]
2023-09-11 13:53:18,855 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:53:18,858 - 

2023-09-11 13:53:18,858 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:53:23,107 - Epoch: [36][   10/   71]    Overall Loss 0.248064    Objective Loss 0.248064                                        LR 0.000500    Time 0.424829    
2023-09-11 13:53:25,032 - Epoch: [36][   20/   71]    Overall Loss 0.239090    Objective Loss 0.239090                                        LR 0.000500    Time 0.308663    
2023-09-11 13:53:27,531 - Epoch: [36][   30/   71]    Overall Loss 0.237363    Objective Loss 0.237363                                        LR 0.000500    Time 0.289053    
2023-09-11 13:53:30,057 - Epoch: [36][   40/   71]    Overall Loss 0.239426    Objective Loss 0.239426                                        LR 0.000500    Time 0.279942    
2023-09-11 13:53:32,687 - Epoch: [36][   50/   71]    Overall Loss 0.238887    Objective Loss 0.238887                                        LR 0.000500    Time 0.276543    
2023-09-11 13:53:34,637 - Epoch: [36][   60/   71]    Overall Loss 0.237315    Objective Loss 0.237315                                        LR 0.000500    Time 0.262947    
2023-09-11 13:53:37,091 - Epoch: [36][   70/   71]    Overall Loss 0.236573    Objective Loss 0.236573    Top1 89.453125    LR 0.000500    Time 0.260444    
2023-09-11 13:53:37,168 - Epoch: [36][   71/   71]    Overall Loss 0.238585    Objective Loss 0.238585    Top1 88.392857    LR 0.000500    Time 0.257845    
2023-09-11 13:53:37,260 - --- validate (epoch=36)-----------
2023-09-11 13:53:37,260 - 2000 samples (256 per mini-batch)
2023-09-11 13:53:40,143 - Epoch: [36][    8/    8]    Loss 0.241535    Top1 89.700000    
2023-09-11 13:53:40,247 - ==> Top1: 89.700    Loss: 0.242

2023-09-11 13:53:40,247 - ==> Confusion:
[[874 111]
 [ 95 920]]

2023-09-11 13:53:40,262 - ==> Best [Top1: 89.850   Sparsity:0.00   Params: 57776 on epoch: 35]
2023-09-11 13:53:40,262 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:53:40,265 - 

2023-09-11 13:53:40,265 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:53:44,555 - Epoch: [37][   10/   71]    Overall Loss 0.222455    Objective Loss 0.222455                                        LR 0.000500    Time 0.428889    
2023-09-11 13:53:47,207 - Epoch: [37][   20/   71]    Overall Loss 0.221432    Objective Loss 0.221432                                        LR 0.000500    Time 0.347014    
2023-09-11 13:53:50,533 - Epoch: [37][   30/   71]    Overall Loss 0.221274    Objective Loss 0.221274                                        LR 0.000500    Time 0.342227    
2023-09-11 13:53:52,482 - Epoch: [37][   40/   71]    Overall Loss 0.224032    Objective Loss 0.224032                                        LR 0.000500    Time 0.305385    
2023-09-11 13:53:55,172 - Epoch: [37][   50/   71]    Overall Loss 0.226442    Objective Loss 0.226442                                        LR 0.000500    Time 0.298090    
2023-09-11 13:53:57,270 - Epoch: [37][   60/   71]    Overall Loss 0.223323    Objective Loss 0.223323                                        LR 0.000500    Time 0.283372    
2023-09-11 13:53:59,722 - Epoch: [37][   70/   71]    Overall Loss 0.224561    Objective Loss 0.224561    Top1 91.406250    LR 0.000500    Time 0.277910    
2023-09-11 13:53:59,797 - Epoch: [37][   71/   71]    Overall Loss 0.223947    Objective Loss 0.223947    Top1 91.666667    LR 0.000500    Time 0.275058    
2023-09-11 13:53:59,892 - --- validate (epoch=37)-----------
2023-09-11 13:53:59,893 - 2000 samples (256 per mini-batch)
2023-09-11 13:54:03,024 - Epoch: [37][    8/    8]    Loss 0.254704    Top1 89.150000    
2023-09-11 13:54:03,138 - ==> Top1: 89.150    Loss: 0.255

2023-09-11 13:54:03,139 - ==> Confusion:
[[871 114]
 [103 912]]

2023-09-11 13:54:03,153 - ==> Best [Top1: 89.850   Sparsity:0.00   Params: 57776 on epoch: 35]
2023-09-11 13:54:03,153 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:54:03,156 - 

2023-09-11 13:54:03,156 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:54:07,215 - Epoch: [38][   10/   71]    Overall Loss 0.238176    Objective Loss 0.238176                                        LR 0.000500    Time 0.405859    
2023-09-11 13:54:09,194 - Epoch: [38][   20/   71]    Overall Loss 0.243503    Objective Loss 0.243503                                        LR 0.000500    Time 0.301846    
2023-09-11 13:54:11,801 - Epoch: [38][   30/   71]    Overall Loss 0.235106    Objective Loss 0.235106                                        LR 0.000500    Time 0.288123    
2023-09-11 13:54:13,797 - Epoch: [38][   40/   71]    Overall Loss 0.232520    Objective Loss 0.232520                                        LR 0.000500    Time 0.265985    
2023-09-11 13:54:17,049 - Epoch: [38][   50/   71]    Overall Loss 0.227965    Objective Loss 0.227965                                        LR 0.000500    Time 0.277808    
2023-09-11 13:54:19,082 - Epoch: [38][   60/   71]    Overall Loss 0.226445    Objective Loss 0.226445                                        LR 0.000500    Time 0.265399    
2023-09-11 13:54:21,425 - Epoch: [38][   70/   71]    Overall Loss 0.228459    Objective Loss 0.228459    Top1 91.015625    LR 0.000500    Time 0.260950    
2023-09-11 13:54:21,557 - Epoch: [38][   71/   71]    Overall Loss 0.229903    Objective Loss 0.229903    Top1 88.690476    LR 0.000500    Time 0.259133    
2023-09-11 13:54:21,650 - --- validate (epoch=38)-----------
2023-09-11 13:54:21,651 - 2000 samples (256 per mini-batch)
2023-09-11 13:54:24,794 - Epoch: [38][    8/    8]    Loss 0.277877    Top1 87.350000    
2023-09-11 13:54:24,890 - ==> Top1: 87.350    Loss: 0.278

2023-09-11 13:54:24,890 - ==> Confusion:
[[925  60]
 [193 822]]

2023-09-11 13:54:24,906 - ==> Best [Top1: 89.850   Sparsity:0.00   Params: 57776 on epoch: 35]
2023-09-11 13:54:24,906 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:54:24,910 - 

2023-09-11 13:54:24,911 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:54:29,274 - Epoch: [39][   10/   71]    Overall Loss 0.220278    Objective Loss 0.220278                                        LR 0.000500    Time 0.436293    
2023-09-11 13:54:31,370 - Epoch: [39][   20/   71]    Overall Loss 0.221846    Objective Loss 0.221846                                        LR 0.000500    Time 0.322915    
2023-09-11 13:54:33,936 - Epoch: [39][   30/   71]    Overall Loss 0.224514    Objective Loss 0.224514                                        LR 0.000500    Time 0.300794    
2023-09-11 13:54:36,525 - Epoch: [39][   40/   71]    Overall Loss 0.226222    Objective Loss 0.226222                                        LR 0.000500    Time 0.290324    
2023-09-11 13:54:39,639 - Epoch: [39][   50/   71]    Overall Loss 0.226752    Objective Loss 0.226752                                        LR 0.000500    Time 0.294528    
2023-09-11 13:54:42,206 - Epoch: [39][   60/   71]    Overall Loss 0.225107    Objective Loss 0.225107                                        LR 0.000500    Time 0.288227    
2023-09-11 13:54:44,115 - Epoch: [39][   70/   71]    Overall Loss 0.225106    Objective Loss 0.225106    Top1 92.187500    LR 0.000500    Time 0.274316    
2023-09-11 13:54:44,192 - Epoch: [39][   71/   71]    Overall Loss 0.225915    Objective Loss 0.225915    Top1 91.369048    LR 0.000500    Time 0.271531    
2023-09-11 13:54:44,281 - --- validate (epoch=39)-----------
2023-09-11 13:54:44,282 - 2000 samples (256 per mini-batch)
2023-09-11 13:54:47,392 - Epoch: [39][    8/    8]    Loss 0.242747    Top1 89.050000    
2023-09-11 13:54:47,489 - ==> Top1: 89.050    Loss: 0.243

2023-09-11 13:54:47,489 - ==> Confusion:
[[852 133]
 [ 86 929]]

2023-09-11 13:54:47,495 - ==> Best [Top1: 89.850   Sparsity:0.00   Params: 57776 on epoch: 35]
2023-09-11 13:54:47,496 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:54:47,500 - 

2023-09-11 13:54:47,500 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:54:51,821 - Epoch: [40][   10/   71]    Overall Loss 0.210109    Objective Loss 0.210109                                        LR 0.000500    Time 0.432029    
2023-09-11 13:54:53,817 - Epoch: [40][   20/   71]    Overall Loss 0.231170    Objective Loss 0.231170                                        LR 0.000500    Time 0.315805    
2023-09-11 13:54:56,357 - Epoch: [40][   30/   71]    Overall Loss 0.230750    Objective Loss 0.230750                                        LR 0.000500    Time 0.295181    
2023-09-11 13:54:58,337 - Epoch: [40][   40/   71]    Overall Loss 0.230324    Objective Loss 0.230324                                        LR 0.000500    Time 0.270890    
2023-09-11 13:55:01,197 - Epoch: [40][   50/   71]    Overall Loss 0.231605    Objective Loss 0.231605                                        LR 0.000500    Time 0.273905    
2023-09-11 13:55:03,393 - Epoch: [40][   60/   71]    Overall Loss 0.227965    Objective Loss 0.227965                                        LR 0.000500    Time 0.264847    
2023-09-11 13:55:05,962 - Epoch: [40][   70/   71]    Overall Loss 0.225636    Objective Loss 0.225636    Top1 93.750000    LR 0.000500    Time 0.263698    
2023-09-11 13:55:06,043 - Epoch: [40][   71/   71]    Overall Loss 0.225788    Objective Loss 0.225788    Top1 92.559524    LR 0.000500    Time 0.261133    
2023-09-11 13:55:06,141 - --- validate (epoch=40)-----------
2023-09-11 13:55:06,141 - 2000 samples (256 per mini-batch)
2023-09-11 13:55:09,008 - Epoch: [40][    8/    8]    Loss 0.245391    Top1 89.550000    
2023-09-11 13:55:09,102 - ==> Top1: 89.550    Loss: 0.245

2023-09-11 13:55:09,102 - ==> Confusion:
[[893  92]
 [117 898]]

2023-09-11 13:55:09,106 - ==> Best [Top1: 89.850   Sparsity:0.00   Params: 57776 on epoch: 35]
2023-09-11 13:55:09,106 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:55:09,108 - 

2023-09-11 13:55:09,108 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:55:13,466 - Epoch: [41][   10/   71]    Overall Loss 0.244858    Objective Loss 0.244858                                        LR 0.000500    Time 0.435663    
2023-09-11 13:55:15,471 - Epoch: [41][   20/   71]    Overall Loss 0.247248    Objective Loss 0.247248                                        LR 0.000500    Time 0.318068    
2023-09-11 13:55:17,976 - Epoch: [41][   30/   71]    Overall Loss 0.239012    Objective Loss 0.239012                                        LR 0.000500    Time 0.295561    
2023-09-11 13:55:19,970 - Epoch: [41][   40/   71]    Overall Loss 0.241582    Objective Loss 0.241582                                        LR 0.000500    Time 0.271490    
2023-09-11 13:55:22,661 - Epoch: [41][   50/   71]    Overall Loss 0.234720    Objective Loss 0.234720                                        LR 0.000500    Time 0.271006    
2023-09-11 13:55:24,749 - Epoch: [41][   60/   71]    Overall Loss 0.237387    Objective Loss 0.237387                                        LR 0.000500    Time 0.260642    
2023-09-11 13:55:26,960 - Epoch: [41][   70/   71]    Overall Loss 0.234289    Objective Loss 0.234289    Top1 91.406250    LR 0.000500    Time 0.254981    
2023-09-11 13:55:27,058 - Epoch: [41][   71/   71]    Overall Loss 0.233733    Objective Loss 0.233733    Top1 91.369048    LR 0.000500    Time 0.252779    
2023-09-11 13:55:27,157 - --- validate (epoch=41)-----------
2023-09-11 13:55:27,157 - 2000 samples (256 per mini-batch)
2023-09-11 13:55:30,091 - Epoch: [41][    8/    8]    Loss 0.254824    Top1 88.750000    
2023-09-11 13:55:30,185 - ==> Top1: 88.750    Loss: 0.255

2023-09-11 13:55:30,185 - ==> Confusion:
[[907  78]
 [147 868]]

2023-09-11 13:55:30,201 - ==> Best [Top1: 89.850   Sparsity:0.00   Params: 57776 on epoch: 35]
2023-09-11 13:55:30,201 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:55:30,203 - 

2023-09-11 13:55:30,204 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:55:35,362 - Epoch: [42][   10/   71]    Overall Loss 0.225509    Objective Loss 0.225509                                        LR 0.000500    Time 0.515822    
2023-09-11 13:55:37,444 - Epoch: [42][   20/   71]    Overall Loss 0.222333    Objective Loss 0.222333                                        LR 0.000500    Time 0.361967    
2023-09-11 13:55:40,379 - Epoch: [42][   30/   71]    Overall Loss 0.219916    Objective Loss 0.219916                                        LR 0.000500    Time 0.339152    
2023-09-11 13:55:42,372 - Epoch: [42][   40/   71]    Overall Loss 0.221381    Objective Loss 0.221381                                        LR 0.000500    Time 0.304178    
2023-09-11 13:55:44,945 - Epoch: [42][   50/   71]    Overall Loss 0.222749    Objective Loss 0.222749                                        LR 0.000500    Time 0.294788    
2023-09-11 13:55:47,458 - Epoch: [42][   60/   71]    Overall Loss 0.225481    Objective Loss 0.225481                                        LR 0.000500    Time 0.287535    
2023-09-11 13:55:49,348 - Epoch: [42][   70/   71]    Overall Loss 0.222290    Objective Loss 0.222290    Top1 94.921875    LR 0.000500    Time 0.273457    
2023-09-11 13:55:49,464 - Epoch: [42][   71/   71]    Overall Loss 0.222086    Objective Loss 0.222086    Top1 94.345238    LR 0.000500    Time 0.271236    
2023-09-11 13:55:49,560 - --- validate (epoch=42)-----------
2023-09-11 13:55:49,561 - 2000 samples (256 per mini-batch)
2023-09-11 13:55:52,173 - Epoch: [42][    8/    8]    Loss 0.250978    Top1 89.600000    
2023-09-11 13:55:52,269 - ==> Top1: 89.600    Loss: 0.251

2023-09-11 13:55:52,270 - ==> Confusion:
[[842 143]
 [ 65 950]]

2023-09-11 13:55:52,271 - ==> Best [Top1: 89.850   Sparsity:0.00   Params: 57776 on epoch: 35]
2023-09-11 13:55:52,271 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:55:52,274 - 

2023-09-11 13:55:52,274 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:55:56,311 - Epoch: [43][   10/   71]    Overall Loss 0.218615    Objective Loss 0.218615                                        LR 0.000500    Time 0.403702    
2023-09-11 13:55:58,331 - Epoch: [43][   20/   71]    Overall Loss 0.222600    Objective Loss 0.222600                                        LR 0.000500    Time 0.302823    
2023-09-11 13:56:00,780 - Epoch: [43][   30/   71]    Overall Loss 0.223415    Objective Loss 0.223415                                        LR 0.000500    Time 0.283517    
2023-09-11 13:56:02,923 - Epoch: [43][   40/   71]    Overall Loss 0.227194    Objective Loss 0.227194                                        LR 0.000500    Time 0.266188    
2023-09-11 13:56:05,424 - Epoch: [43][   50/   71]    Overall Loss 0.224221    Objective Loss 0.224221                                        LR 0.000500    Time 0.262959    
2023-09-11 13:56:07,512 - Epoch: [43][   60/   71]    Overall Loss 0.224461    Objective Loss 0.224461                                        LR 0.000500    Time 0.253926    
2023-09-11 13:56:09,723 - Epoch: [43][   70/   71]    Overall Loss 0.223292    Objective Loss 0.223292    Top1 92.187500    LR 0.000500    Time 0.249231    
2023-09-11 13:56:09,837 - Epoch: [43][   71/   71]    Overall Loss 0.222594    Objective Loss 0.222594    Top1 92.857143    LR 0.000500    Time 0.247331    
2023-09-11 13:56:09,938 - --- validate (epoch=43)-----------
2023-09-11 13:56:09,938 - 2000 samples (256 per mini-batch)
2023-09-11 13:56:12,937 - Epoch: [43][    8/    8]    Loss 0.233740    Top1 90.250000    
2023-09-11 13:56:13,039 - ==> Top1: 90.250    Loss: 0.234

2023-09-11 13:56:13,039 - ==> Confusion:
[[890  95]
 [100 915]]

2023-09-11 13:56:13,043 - ==> Best [Top1: 90.250   Sparsity:0.00   Params: 57776 on epoch: 43]
2023-09-11 13:56:13,043 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:56:13,046 - 

2023-09-11 13:56:13,046 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:56:16,899 - Epoch: [44][   10/   71]    Overall Loss 0.228020    Objective Loss 0.228020                                        LR 0.000500    Time 0.385251    
2023-09-11 13:56:18,921 - Epoch: [44][   20/   71]    Overall Loss 0.237893    Objective Loss 0.237893                                        LR 0.000500    Time 0.293713    
2023-09-11 13:56:21,383 - Epoch: [44][   30/   71]    Overall Loss 0.233888    Objective Loss 0.233888                                        LR 0.000500    Time 0.277857    
2023-09-11 13:56:23,525 - Epoch: [44][   40/   71]    Overall Loss 0.229429    Objective Loss 0.229429                                        LR 0.000500    Time 0.261932    
2023-09-11 13:56:26,079 - Epoch: [44][   50/   71]    Overall Loss 0.224992    Objective Loss 0.224992                                        LR 0.000500    Time 0.260615    
2023-09-11 13:56:28,079 - Epoch: [44][   60/   71]    Overall Loss 0.222105    Objective Loss 0.222105                                        LR 0.000500    Time 0.250510    
2023-09-11 13:56:30,352 - Epoch: [44][   70/   71]    Overall Loss 0.225423    Objective Loss 0.225423    Top1 88.281250    LR 0.000500    Time 0.247186    
2023-09-11 13:56:30,433 - Epoch: [44][   71/   71]    Overall Loss 0.226879    Objective Loss 0.226879    Top1 88.392857    LR 0.000500    Time 0.244842    
2023-09-11 13:56:30,519 - --- validate (epoch=44)-----------
2023-09-11 13:56:30,519 - 2000 samples (256 per mini-batch)
2023-09-11 13:56:33,802 - Epoch: [44][    8/    8]    Loss 0.232204    Top1 90.300000    
2023-09-11 13:56:33,893 - ==> Top1: 90.300    Loss: 0.232

2023-09-11 13:56:33,894 - ==> Confusion:
[[873 112]
 [ 82 933]]

2023-09-11 13:56:33,909 - ==> Best [Top1: 90.300   Sparsity:0.00   Params: 57776 on epoch: 44]
2023-09-11 13:56:33,909 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:56:33,915 - 

2023-09-11 13:56:33,915 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:56:37,558 - Epoch: [45][   10/   71]    Overall Loss 0.223586    Objective Loss 0.223586                                        LR 0.000500    Time 0.364232    
2023-09-11 13:56:39,547 - Epoch: [45][   20/   71]    Overall Loss 0.228267    Objective Loss 0.228267                                        LR 0.000500    Time 0.281586    
2023-09-11 13:56:42,251 - Epoch: [45][   30/   71]    Overall Loss 0.222988    Objective Loss 0.222988                                        LR 0.000500    Time 0.277841    
2023-09-11 13:56:44,238 - Epoch: [45][   40/   71]    Overall Loss 0.221006    Objective Loss 0.221006                                        LR 0.000500    Time 0.258049    
2023-09-11 13:56:46,853 - Epoch: [45][   50/   71]    Overall Loss 0.221705    Objective Loss 0.221705                                        LR 0.000500    Time 0.258736    
2023-09-11 13:56:50,305 - Epoch: [45][   60/   71]    Overall Loss 0.221461    Objective Loss 0.221461                                        LR 0.000500    Time 0.273129    
2023-09-11 13:56:52,130 - Epoch: [45][   70/   71]    Overall Loss 0.218854    Objective Loss 0.218854    Top1 92.968750    LR 0.000500    Time 0.260187    
2023-09-11 13:56:52,211 - Epoch: [45][   71/   71]    Overall Loss 0.219334    Objective Loss 0.219334    Top1 91.964286    LR 0.000500    Time 0.257659    
2023-09-11 13:56:52,303 - --- validate (epoch=45)-----------
2023-09-11 13:56:52,303 - 2000 samples (256 per mini-batch)
2023-09-11 13:56:55,479 - Epoch: [45][    8/    8]    Loss 0.237121    Top1 89.850000    
2023-09-11 13:56:55,575 - ==> Top1: 89.850    Loss: 0.237

2023-09-11 13:56:55,575 - ==> Confusion:
[[890  95]
 [108 907]]

2023-09-11 13:56:55,591 - ==> Best [Top1: 90.300   Sparsity:0.00   Params: 57776 on epoch: 44]
2023-09-11 13:56:55,591 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:56:55,594 - 

2023-09-11 13:56:55,594 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:56:59,802 - Epoch: [46][   10/   71]    Overall Loss 0.207894    Objective Loss 0.207894                                        LR 0.000500    Time 0.420766    
2023-09-11 13:57:01,869 - Epoch: [46][   20/   71]    Overall Loss 0.216227    Objective Loss 0.216227                                        LR 0.000500    Time 0.313733    
2023-09-11 13:57:04,309 - Epoch: [46][   30/   71]    Overall Loss 0.215699    Objective Loss 0.215699                                        LR 0.000500    Time 0.290484    
2023-09-11 13:57:06,345 - Epoch: [46][   40/   71]    Overall Loss 0.216100    Objective Loss 0.216100                                        LR 0.000500    Time 0.268748    
2023-09-11 13:57:08,895 - Epoch: [46][   50/   71]    Overall Loss 0.215635    Objective Loss 0.215635                                        LR 0.000500    Time 0.265986    
2023-09-11 13:57:11,518 - Epoch: [46][   60/   71]    Overall Loss 0.217759    Objective Loss 0.217759                                        LR 0.000500    Time 0.265376    
2023-09-11 13:57:13,781 - Epoch: [46][   70/   71]    Overall Loss 0.217998    Objective Loss 0.217998    Top1 90.234375    LR 0.000500    Time 0.259785    
2023-09-11 13:57:13,851 - Epoch: [46][   71/   71]    Overall Loss 0.219428    Objective Loss 0.219428    Top1 88.392857    LR 0.000500    Time 0.257103    
2023-09-11 13:57:13,944 - --- validate (epoch=46)-----------
2023-09-11 13:57:13,945 - 2000 samples (256 per mini-batch)
2023-09-11 13:57:17,119 - Epoch: [46][    8/    8]    Loss 0.231850    Top1 89.850000    
2023-09-11 13:57:17,222 - ==> Top1: 89.850    Loss: 0.232

2023-09-11 13:57:17,222 - ==> Confusion:
[[866 119]
 [ 84 931]]

2023-09-11 13:57:17,239 - ==> Best [Top1: 90.300   Sparsity:0.00   Params: 57776 on epoch: 44]
2023-09-11 13:57:17,239 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:57:17,241 - 

2023-09-11 13:57:17,241 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:57:21,656 - Epoch: [47][   10/   71]    Overall Loss 0.232497    Objective Loss 0.232497                                        LR 0.000500    Time 0.441400    
2023-09-11 13:57:23,721 - Epoch: [47][   20/   71]    Overall Loss 0.220834    Objective Loss 0.220834                                        LR 0.000500    Time 0.323949    
2023-09-11 13:57:26,649 - Epoch: [47][   30/   71]    Overall Loss 0.218803    Objective Loss 0.218803                                        LR 0.000500    Time 0.313565    
2023-09-11 13:57:29,885 - Epoch: [47][   40/   71]    Overall Loss 0.213198    Objective Loss 0.213198                                        LR 0.000500    Time 0.316063    
2023-09-11 13:57:31,937 - Epoch: [47][   50/   71]    Overall Loss 0.212029    Objective Loss 0.212029                                        LR 0.000500    Time 0.293882    
2023-09-11 13:57:34,511 - Epoch: [47][   60/   71]    Overall Loss 0.216507    Objective Loss 0.216507                                        LR 0.000500    Time 0.287790    
2023-09-11 13:57:36,453 - Epoch: [47][   70/   71]    Overall Loss 0.217801    Objective Loss 0.217801    Top1 90.234375    LR 0.000500    Time 0.274419    
2023-09-11 13:57:36,527 - Epoch: [47][   71/   71]    Overall Loss 0.217838    Objective Loss 0.217838    Top1 90.476190    LR 0.000500    Time 0.271592    
2023-09-11 13:57:36,631 - --- validate (epoch=47)-----------
2023-09-11 13:57:36,631 - 2000 samples (256 per mini-batch)
2023-09-11 13:57:39,007 - Epoch: [47][    8/    8]    Loss 0.242589    Top1 89.600000    
2023-09-11 13:57:39,108 - ==> Top1: 89.600    Loss: 0.243

2023-09-11 13:57:39,108 - ==> Confusion:
[[861 124]
 [ 84 931]]

2023-09-11 13:57:39,123 - ==> Best [Top1: 90.300   Sparsity:0.00   Params: 57776 on epoch: 44]
2023-09-11 13:57:39,123 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:57:39,125 - 

2023-09-11 13:57:39,125 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:57:43,610 - Epoch: [48][   10/   71]    Overall Loss 0.216998    Objective Loss 0.216998                                        LR 0.000500    Time 0.448359    
2023-09-11 13:57:45,622 - Epoch: [48][   20/   71]    Overall Loss 0.215957    Objective Loss 0.215957                                        LR 0.000500    Time 0.324772    
2023-09-11 13:57:48,133 - Epoch: [48][   30/   71]    Overall Loss 0.209637    Objective Loss 0.209637                                        LR 0.000500    Time 0.300222    
2023-09-11 13:57:50,258 - Epoch: [48][   40/   71]    Overall Loss 0.210412    Objective Loss 0.210412                                        LR 0.000500    Time 0.278280    
2023-09-11 13:57:52,853 - Epoch: [48][   50/   71]    Overall Loss 0.213087    Objective Loss 0.213087                                        LR 0.000500    Time 0.274512    
2023-09-11 13:57:54,901 - Epoch: [48][   60/   71]    Overall Loss 0.215046    Objective Loss 0.215046                                        LR 0.000500    Time 0.262882    
2023-09-11 13:57:57,180 - Epoch: [48][   70/   71]    Overall Loss 0.213777    Objective Loss 0.213777    Top1 88.671875    LR 0.000500    Time 0.257884    
2023-09-11 13:57:57,261 - Epoch: [48][   71/   71]    Overall Loss 0.212789    Objective Loss 0.212789    Top1 90.773810    LR 0.000500    Time 0.255393    
2023-09-11 13:57:57,360 - --- validate (epoch=48)-----------
2023-09-11 13:57:57,360 - 2000 samples (256 per mini-batch)
2023-09-11 13:58:00,304 - Epoch: [48][    8/    8]    Loss 0.228272    Top1 90.250000    
2023-09-11 13:58:00,389 - ==> Top1: 90.250    Loss: 0.228

2023-09-11 13:58:00,390 - ==> Confusion:
[[867 118]
 [ 77 938]]

2023-09-11 13:58:00,406 - ==> Best [Top1: 90.300   Sparsity:0.00   Params: 57776 on epoch: 44]
2023-09-11 13:58:00,406 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:58:00,410 - 

2023-09-11 13:58:00,411 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:58:04,706 - Epoch: [49][   10/   71]    Overall Loss 0.223845    Objective Loss 0.223845                                        LR 0.000500    Time 0.429526    
2023-09-11 13:58:07,106 - Epoch: [49][   20/   71]    Overall Loss 0.222394    Objective Loss 0.222394                                        LR 0.000500    Time 0.334738    
2023-09-11 13:58:09,561 - Epoch: [49][   30/   71]    Overall Loss 0.220999    Objective Loss 0.220999                                        LR 0.000500    Time 0.304986    
2023-09-11 13:58:11,636 - Epoch: [49][   40/   71]    Overall Loss 0.212968    Objective Loss 0.212968                                        LR 0.000500    Time 0.280603    
2023-09-11 13:58:14,208 - Epoch: [49][   50/   71]    Overall Loss 0.211852    Objective Loss 0.211852                                        LR 0.000500    Time 0.275904    
2023-09-11 13:58:16,463 - Epoch: [49][   60/   71]    Overall Loss 0.210759    Objective Loss 0.210759                                        LR 0.000500    Time 0.267502    
2023-09-11 13:58:18,918 - Epoch: [49][   70/   71]    Overall Loss 0.210280    Objective Loss 0.210280    Top1 91.015625    LR 0.000500    Time 0.264360    
2023-09-11 13:58:18,997 - Epoch: [49][   71/   71]    Overall Loss 0.210721    Objective Loss 0.210721    Top1 91.071429    LR 0.000500    Time 0.261742    
2023-09-11 13:58:19,088 - --- validate (epoch=49)-----------
2023-09-11 13:58:19,088 - 2000 samples (256 per mini-batch)
2023-09-11 13:58:22,095 - Epoch: [49][    8/    8]    Loss 0.245969    Top1 89.600000    
2023-09-11 13:58:22,205 - ==> Top1: 89.600    Loss: 0.246

2023-09-11 13:58:22,206 - ==> Confusion:
[[928  57]
 [151 864]]

2023-09-11 13:58:22,208 - ==> Best [Top1: 90.300   Sparsity:0.00   Params: 57776 on epoch: 44]
2023-09-11 13:58:22,208 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:58:22,212 - 

2023-09-11 13:58:22,212 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:58:25,756 - Epoch: [50][   10/   71]    Overall Loss 0.210106    Objective Loss 0.210106                                        LR 0.000250    Time 0.354360    
2023-09-11 13:58:27,914 - Epoch: [50][   20/   71]    Overall Loss 0.214831    Objective Loss 0.214831                                        LR 0.000250    Time 0.285012    
2023-09-11 13:58:31,666 - Epoch: [50][   30/   71]    Overall Loss 0.209015    Objective Loss 0.209015                                        LR 0.000250    Time 0.315064    
2023-09-11 13:58:33,586 - Epoch: [50][   40/   71]    Overall Loss 0.205012    Objective Loss 0.205012                                        LR 0.000250    Time 0.284304    
2023-09-11 13:58:37,576 - Epoch: [50][   50/   71]    Overall Loss 0.201809    Objective Loss 0.201809                                        LR 0.000250    Time 0.307238    
2023-09-11 13:58:39,610 - Epoch: [50][   60/   71]    Overall Loss 0.200139    Objective Loss 0.200139                                        LR 0.000250    Time 0.289928    
2023-09-11 13:58:42,348 - Epoch: [50][   70/   71]    Overall Loss 0.201868    Objective Loss 0.201868    Top1 89.843750    LR 0.000250    Time 0.287613    
2023-09-11 13:58:42,448 - Epoch: [50][   71/   71]    Overall Loss 0.201710    Objective Loss 0.201710    Top1 90.476190    LR 0.000250    Time 0.284969    
2023-09-11 13:58:42,545 - --- validate (epoch=50)-----------
2023-09-11 13:58:42,545 - 2000 samples (256 per mini-batch)
2023-09-11 13:58:45,611 - Epoch: [50][    8/    8]    Loss 0.245072    Top1 89.900000    
2023-09-11 13:58:45,698 - ==> Top1: 89.900    Loss: 0.245

2023-09-11 13:58:45,699 - ==> Confusion:
[[910  75]
 [127 888]]

2023-09-11 13:58:45,715 - ==> Best [Top1: 90.300   Sparsity:0.00   Params: 57776 on epoch: 44]
2023-09-11 13:58:45,715 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:58:45,718 - 

2023-09-11 13:58:45,718 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:58:49,998 - Epoch: [51][   10/   71]    Overall Loss 0.205489    Objective Loss 0.205489                                        LR 0.000250    Time 0.427959    
2023-09-11 13:58:51,949 - Epoch: [51][   20/   71]    Overall Loss 0.198528    Objective Loss 0.198528                                        LR 0.000250    Time 0.311514    
2023-09-11 13:58:54,717 - Epoch: [51][   30/   71]    Overall Loss 0.195208    Objective Loss 0.195208                                        LR 0.000250    Time 0.299933    
2023-09-11 13:58:56,721 - Epoch: [51][   40/   71]    Overall Loss 0.194314    Objective Loss 0.194314                                        LR 0.000250    Time 0.275045    
2023-09-11 13:58:59,437 - Epoch: [51][   50/   71]    Overall Loss 0.194771    Objective Loss 0.194771                                        LR 0.000250    Time 0.274342    
2023-09-11 13:59:01,952 - Epoch: [51][   60/   71]    Overall Loss 0.195812    Objective Loss 0.195812                                        LR 0.000250    Time 0.270536    
2023-09-11 13:59:03,809 - Epoch: [51][   70/   71]    Overall Loss 0.194190    Objective Loss 0.194190    Top1 95.312500    LR 0.000250    Time 0.258404    
2023-09-11 13:59:03,918 - Epoch: [51][   71/   71]    Overall Loss 0.193488    Objective Loss 0.193488    Top1 94.940476    LR 0.000250    Time 0.256303    
2023-09-11 13:59:04,008 - --- validate (epoch=51)-----------
2023-09-11 13:59:04,009 - 2000 samples (256 per mini-batch)
2023-09-11 13:59:07,207 - Epoch: [51][    8/    8]    Loss 0.221582    Top1 90.000000    
2023-09-11 13:59:07,301 - ==> Top1: 90.000    Loss: 0.222

2023-09-11 13:59:07,301 - ==> Confusion:
[[886  99]
 [101 914]]

2023-09-11 13:59:07,317 - ==> Best [Top1: 90.300   Sparsity:0.00   Params: 57776 on epoch: 44]
2023-09-11 13:59:07,317 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:59:07,321 - 

2023-09-11 13:59:07,321 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:59:11,116 - Epoch: [52][   10/   71]    Overall Loss 0.210399    Objective Loss 0.210399                                        LR 0.000250    Time 0.379486    
2023-09-11 13:59:14,139 - Epoch: [52][   20/   71]    Overall Loss 0.203752    Objective Loss 0.203752                                        LR 0.000250    Time 0.340880    
2023-09-11 13:59:16,673 - Epoch: [52][   30/   71]    Overall Loss 0.206223    Objective Loss 0.206223                                        LR 0.000250    Time 0.311702    
2023-09-11 13:59:19,368 - Epoch: [52][   40/   71]    Overall Loss 0.209780    Objective Loss 0.209780                                        LR 0.000250    Time 0.301153    
2023-09-11 13:59:22,508 - Epoch: [52][   50/   71]    Overall Loss 0.204946    Objective Loss 0.204946                                        LR 0.000250    Time 0.303713    
2023-09-11 13:59:24,555 - Epoch: [52][   60/   71]    Overall Loss 0.203254    Objective Loss 0.203254                                        LR 0.000250    Time 0.287206    
2023-09-11 13:59:26,721 - Epoch: [52][   70/   71]    Overall Loss 0.203954    Objective Loss 0.203954    Top1 92.187500    LR 0.000250    Time 0.277109    
2023-09-11 13:59:26,830 - Epoch: [52][   71/   71]    Overall Loss 0.203238    Objective Loss 0.203238    Top1 92.261905    LR 0.000250    Time 0.274746    
2023-09-11 13:59:26,932 - --- validate (epoch=52)-----------
2023-09-11 13:59:26,933 - 2000 samples (256 per mini-batch)
2023-09-11 13:59:29,726 - Epoch: [52][    8/    8]    Loss 0.216090    Top1 91.300000    
2023-09-11 13:59:29,828 - ==> Top1: 91.300    Loss: 0.216

2023-09-11 13:59:29,829 - ==> Confusion:
[[871 114]
 [ 60 955]]

2023-09-11 13:59:29,843 - ==> Best [Top1: 91.300   Sparsity:0.00   Params: 57776 on epoch: 52]
2023-09-11 13:59:29,844 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:59:29,846 - 

2023-09-11 13:59:29,846 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:59:33,436 - Epoch: [53][   10/   71]    Overall Loss 0.191331    Objective Loss 0.191331                                        LR 0.000250    Time 0.358950    
2023-09-11 13:59:37,085 - Epoch: [53][   20/   71]    Overall Loss 0.187278    Objective Loss 0.187278                                        LR 0.000250    Time 0.361867    
2023-09-11 13:59:39,095 - Epoch: [53][   30/   71]    Overall Loss 0.195590    Objective Loss 0.195590                                        LR 0.000250    Time 0.308243    
2023-09-11 13:59:42,228 - Epoch: [53][   40/   71]    Overall Loss 0.195112    Objective Loss 0.195112                                        LR 0.000250    Time 0.309505    
2023-09-11 13:59:44,727 - Epoch: [53][   50/   71]    Overall Loss 0.195962    Objective Loss 0.195962                                        LR 0.000250    Time 0.297570    
2023-09-11 13:59:48,458 - Epoch: [53][   60/   71]    Overall Loss 0.200068    Objective Loss 0.200068                                        LR 0.000250    Time 0.310156    
2023-09-11 13:59:50,283 - Epoch: [53][   70/   71]    Overall Loss 0.199378    Objective Loss 0.199378    Top1 91.015625    LR 0.000250    Time 0.291924    
2023-09-11 13:59:50,375 - Epoch: [53][   71/   71]    Overall Loss 0.198956    Objective Loss 0.198956    Top1 91.369048    LR 0.000250    Time 0.289100    
2023-09-11 13:59:50,472 - --- validate (epoch=53)-----------
2023-09-11 13:59:50,472 - 2000 samples (256 per mini-batch)
2023-09-11 13:59:53,523 - Epoch: [53][    8/    8]    Loss 0.214612    Top1 91.950000    
2023-09-11 13:59:53,614 - ==> Top1: 91.950    Loss: 0.215

2023-09-11 13:59:53,614 - ==> Confusion:
[[882 103]
 [ 58 957]]

2023-09-11 13:59:53,631 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 13:59:53,631 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 13:59:53,633 - 

2023-09-11 13:59:53,634 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 13:59:58,056 - Epoch: [54][   10/   71]    Overall Loss 0.192717    Objective Loss 0.192717                                        LR 0.000250    Time 0.442237    
2023-09-11 14:00:00,074 - Epoch: [54][   20/   71]    Overall Loss 0.205282    Objective Loss 0.205282                                        LR 0.000250    Time 0.321961    
2023-09-11 14:00:02,519 - Epoch: [54][   30/   71]    Overall Loss 0.200948    Objective Loss 0.200948                                        LR 0.000250    Time 0.296144    
2023-09-11 14:00:04,605 - Epoch: [54][   40/   71]    Overall Loss 0.197583    Objective Loss 0.197583                                        LR 0.000250    Time 0.274235    
2023-09-11 14:00:07,441 - Epoch: [54][   50/   71]    Overall Loss 0.196830    Objective Loss 0.196830                                        LR 0.000250    Time 0.276117    
2023-09-11 14:00:09,456 - Epoch: [54][   60/   71]    Overall Loss 0.195745    Objective Loss 0.195745                                        LR 0.000250    Time 0.263675    
2023-09-11 14:00:11,733 - Epoch: [54][   70/   71]    Overall Loss 0.195905    Objective Loss 0.195905    Top1 92.187500    LR 0.000250    Time 0.258531    
2023-09-11 14:00:11,809 - Epoch: [54][   71/   71]    Overall Loss 0.195195    Objective Loss 0.195195    Top1 92.559524    LR 0.000250    Time 0.255959    
2023-09-11 14:00:11,901 - --- validate (epoch=54)-----------
2023-09-11 14:00:11,901 - 2000 samples (256 per mini-batch)
2023-09-11 14:00:15,192 - Epoch: [54][    8/    8]    Loss 0.222322    Top1 90.600000    
2023-09-11 14:00:15,270 - ==> Top1: 90.600    Loss: 0.222

2023-09-11 14:00:15,270 - ==> Confusion:
[[874 111]
 [ 77 938]]

2023-09-11 14:00:15,285 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:00:15,285 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:00:15,289 - 

2023-09-11 14:00:15,290 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:00:19,551 - Epoch: [55][   10/   71]    Overall Loss 0.181343    Objective Loss 0.181343                                        LR 0.000250    Time 0.426042    
2023-09-11 14:00:22,387 - Epoch: [55][   20/   71]    Overall Loss 0.189583    Objective Loss 0.189583                                        LR 0.000250    Time 0.354811    
2023-09-11 14:00:25,094 - Epoch: [55][   30/   71]    Overall Loss 0.190053    Objective Loss 0.190053                                        LR 0.000250    Time 0.326791    
2023-09-11 14:00:28,215 - Epoch: [55][   40/   71]    Overall Loss 0.193305    Objective Loss 0.193305                                        LR 0.000250    Time 0.323089    
2023-09-11 14:00:30,802 - Epoch: [55][   50/   71]    Overall Loss 0.194918    Objective Loss 0.194918                                        LR 0.000250    Time 0.310222    
2023-09-11 14:00:32,895 - Epoch: [55][   60/   71]    Overall Loss 0.195689    Objective Loss 0.195689                                        LR 0.000250    Time 0.293386    
2023-09-11 14:00:35,874 - Epoch: [55][   70/   71]    Overall Loss 0.197085    Objective Loss 0.197085    Top1 92.968750    LR 0.000250    Time 0.294034    
2023-09-11 14:00:35,955 - Epoch: [55][   71/   71]    Overall Loss 0.196309    Objective Loss 0.196309    Top1 93.452381    LR 0.000250    Time 0.291030    
2023-09-11 14:00:36,049 - --- validate (epoch=55)-----------
2023-09-11 14:00:36,049 - 2000 samples (256 per mini-batch)
2023-09-11 14:00:39,148 - Epoch: [55][    8/    8]    Loss 0.227342    Top1 91.000000    
2023-09-11 14:00:39,239 - ==> Top1: 91.000    Loss: 0.227

2023-09-11 14:00:39,239 - ==> Confusion:
[[888  97]
 [ 83 932]]

2023-09-11 14:00:39,254 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:00:39,254 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:00:39,259 - 

2023-09-11 14:00:39,259 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:00:43,506 - Epoch: [56][   10/   71]    Overall Loss 0.188112    Objective Loss 0.188112                                        LR 0.000250    Time 0.424646    
2023-09-11 14:00:45,396 - Epoch: [56][   20/   71]    Overall Loss 0.194265    Objective Loss 0.194265                                        LR 0.000250    Time 0.306801    
2023-09-11 14:00:47,928 - Epoch: [56][   30/   71]    Overall Loss 0.187184    Objective Loss 0.187184                                        LR 0.000250    Time 0.288910    
2023-09-11 14:00:49,941 - Epoch: [56][   40/   71]    Overall Loss 0.187814    Objective Loss 0.187814                                        LR 0.000250    Time 0.267013    
2023-09-11 14:00:52,538 - Epoch: [56][   50/   71]    Overall Loss 0.190618    Objective Loss 0.190618                                        LR 0.000250    Time 0.265543    
2023-09-11 14:00:54,614 - Epoch: [56][   60/   71]    Overall Loss 0.192400    Objective Loss 0.192400                                        LR 0.000250    Time 0.255884    
2023-09-11 14:00:56,898 - Epoch: [56][   70/   71]    Overall Loss 0.193970    Objective Loss 0.193970    Top1 91.406250    LR 0.000250    Time 0.251943    
2023-09-11 14:00:56,978 - Epoch: [56][   71/   71]    Overall Loss 0.193919    Objective Loss 0.193919    Top1 91.666667    LR 0.000250    Time 0.249520    
2023-09-11 14:00:57,086 - --- validate (epoch=56)-----------
2023-09-11 14:00:57,086 - 2000 samples (256 per mini-batch)
2023-09-11 14:01:00,042 - Epoch: [56][    8/    8]    Loss 0.215892    Top1 91.200000    
2023-09-11 14:01:00,131 - ==> Top1: 91.200    Loss: 0.216

2023-09-11 14:01:00,131 - ==> Confusion:
[[868 117]
 [ 59 956]]

2023-09-11 14:01:00,147 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:01:00,147 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:01:00,152 - 

2023-09-11 14:01:00,152 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:01:04,500 - Epoch: [57][   10/   71]    Overall Loss 0.193119    Objective Loss 0.193119                                        LR 0.000250    Time 0.434778    
2023-09-11 14:01:07,792 - Epoch: [57][   20/   71]    Overall Loss 0.198759    Objective Loss 0.198759                                        LR 0.000250    Time 0.381965    
2023-09-11 14:01:09,723 - Epoch: [57][   30/   71]    Overall Loss 0.196323    Objective Loss 0.196323                                        LR 0.000250    Time 0.318996    
2023-09-11 14:01:12,277 - Epoch: [57][   40/   71]    Overall Loss 0.193858    Objective Loss 0.193858                                        LR 0.000250    Time 0.303091    
2023-09-11 14:01:14,336 - Epoch: [57][   50/   71]    Overall Loss 0.197934    Objective Loss 0.197934                                        LR 0.000250    Time 0.283645    
2023-09-11 14:01:17,535 - Epoch: [57][   60/   71]    Overall Loss 0.196668    Objective Loss 0.196668                                        LR 0.000250    Time 0.289682    
2023-09-11 14:01:19,452 - Epoch: [57][   70/   71]    Overall Loss 0.197166    Objective Loss 0.197166    Top1 94.531250    LR 0.000250    Time 0.275683    
2023-09-11 14:01:19,528 - Epoch: [57][   71/   71]    Overall Loss 0.197062    Objective Loss 0.197062    Top1 93.750000    LR 0.000250    Time 0.272869    
2023-09-11 14:01:19,616 - --- validate (epoch=57)-----------
2023-09-11 14:01:19,616 - 2000 samples (256 per mini-batch)
2023-09-11 14:01:22,600 - Epoch: [57][    8/    8]    Loss 0.224364    Top1 90.100000    
2023-09-11 14:01:22,696 - ==> Top1: 90.100    Loss: 0.224

2023-09-11 14:01:22,697 - ==> Confusion:
[[903  82]
 [116 899]]

2023-09-11 14:01:22,711 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:01:22,712 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:01:22,716 - 

2023-09-11 14:01:22,716 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:01:27,180 - Epoch: [58][   10/   71]    Overall Loss 0.185140    Objective Loss 0.185140                                        LR 0.000250    Time 0.446288    
2023-09-11 14:01:29,316 - Epoch: [58][   20/   71]    Overall Loss 0.190309    Objective Loss 0.190309                                        LR 0.000250    Time 0.329956    
2023-09-11 14:01:32,131 - Epoch: [58][   30/   71]    Overall Loss 0.189229    Objective Loss 0.189229                                        LR 0.000250    Time 0.313793    
2023-09-11 14:01:34,131 - Epoch: [58][   40/   71]    Overall Loss 0.187631    Objective Loss 0.187631                                        LR 0.000250    Time 0.285317    
2023-09-11 14:01:36,772 - Epoch: [58][   50/   71]    Overall Loss 0.190303    Objective Loss 0.190303                                        LR 0.000250    Time 0.281087    
2023-09-11 14:01:39,015 - Epoch: [58][   60/   71]    Overall Loss 0.192195    Objective Loss 0.192195                                        LR 0.000250    Time 0.271606    
2023-09-11 14:01:41,299 - Epoch: [58][   70/   71]    Overall Loss 0.189657    Objective Loss 0.189657    Top1 89.843750    LR 0.000250    Time 0.265434    
2023-09-11 14:01:41,385 - Epoch: [58][   71/   71]    Overall Loss 0.189134    Objective Loss 0.189134    Top1 90.476190    LR 0.000250    Time 0.262905    
2023-09-11 14:01:41,470 - --- validate (epoch=58)-----------
2023-09-11 14:01:41,470 - 2000 samples (256 per mini-batch)
2023-09-11 14:01:44,677 - Epoch: [58][    8/    8]    Loss 0.210598    Top1 91.000000    
2023-09-11 14:01:44,776 - ==> Top1: 91.000    Loss: 0.211

2023-09-11 14:01:44,776 - ==> Confusion:
[[890  95]
 [ 85 930]]

2023-09-11 14:01:44,787 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:01:44,787 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:01:44,790 - 

2023-09-11 14:01:44,790 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:01:48,656 - Epoch: [59][   10/   71]    Overall Loss 0.181874    Objective Loss 0.181874                                        LR 0.000250    Time 0.386534    
2023-09-11 14:01:50,820 - Epoch: [59][   20/   71]    Overall Loss 0.191208    Objective Loss 0.191208                                        LR 0.000250    Time 0.301481    
2023-09-11 14:01:54,057 - Epoch: [59][   30/   71]    Overall Loss 0.194623    Objective Loss 0.194623                                        LR 0.000250    Time 0.308873    
2023-09-11 14:01:57,028 - Epoch: [59][   40/   71]    Overall Loss 0.196966    Objective Loss 0.196966                                        LR 0.000250    Time 0.305922    
2023-09-11 14:01:59,165 - Epoch: [59][   50/   71]    Overall Loss 0.195761    Objective Loss 0.195761                                        LR 0.000250    Time 0.287476    
2023-09-11 14:02:01,963 - Epoch: [59][   60/   71]    Overall Loss 0.193718    Objective Loss 0.193718                                        LR 0.000250    Time 0.286193    
2023-09-11 14:02:04,633 - Epoch: [59][   70/   71]    Overall Loss 0.191707    Objective Loss 0.191707    Top1 92.968750    LR 0.000250    Time 0.283442    
2023-09-11 14:02:04,726 - Epoch: [59][   71/   71]    Overall Loss 0.192098    Objective Loss 0.192098    Top1 92.857143    LR 0.000250    Time 0.280754    
2023-09-11 14:02:04,814 - --- validate (epoch=59)-----------
2023-09-11 14:02:04,814 - 2000 samples (256 per mini-batch)
2023-09-11 14:02:07,984 - Epoch: [59][    8/    8]    Loss 0.227912    Top1 90.300000    
2023-09-11 14:02:08,082 - ==> Top1: 90.300    Loss: 0.228

2023-09-11 14:02:08,083 - ==> Confusion:
[[918  67]
 [127 888]]

2023-09-11 14:02:08,089 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:02:08,089 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:02:08,093 - 

2023-09-11 14:02:08,093 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:02:11,911 - Epoch: [60][   10/   71]    Overall Loss 0.196799    Objective Loss 0.196799                                        LR 0.000250    Time 0.381696    
2023-09-11 14:02:14,250 - Epoch: [60][   20/   71]    Overall Loss 0.195302    Objective Loss 0.195302                                        LR 0.000250    Time 0.307801    
2023-09-11 14:02:16,597 - Epoch: [60][   30/   71]    Overall Loss 0.196358    Objective Loss 0.196358                                        LR 0.000250    Time 0.283420    
2023-09-11 14:02:18,776 - Epoch: [60][   40/   71]    Overall Loss 0.193358    Objective Loss 0.193358                                        LR 0.000250    Time 0.267028    
2023-09-11 14:02:21,727 - Epoch: [60][   50/   71]    Overall Loss 0.192883    Objective Loss 0.192883                                        LR 0.000250    Time 0.272638    
2023-09-11 14:02:23,797 - Epoch: [60][   60/   71]    Overall Loss 0.192364    Objective Loss 0.192364                                        LR 0.000250    Time 0.261680    
2023-09-11 14:02:26,313 - Epoch: [60][   70/   71]    Overall Loss 0.189845    Objective Loss 0.189845    Top1 91.015625    LR 0.000250    Time 0.260239    
2023-09-11 14:02:26,390 - Epoch: [60][   71/   71]    Overall Loss 0.189881    Objective Loss 0.189881    Top1 91.369048    LR 0.000250    Time 0.257655    
2023-09-11 14:02:26,487 - --- validate (epoch=60)-----------
2023-09-11 14:02:26,487 - 2000 samples (256 per mini-batch)
2023-09-11 14:02:29,678 - Epoch: [60][    8/    8]    Loss 0.230776    Top1 90.300000    
2023-09-11 14:02:29,774 - ==> Top1: 90.300    Loss: 0.231

2023-09-11 14:02:29,774 - ==> Confusion:
[[901  84]
 [110 905]]

2023-09-11 14:02:29,789 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:02:29,790 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:02:29,794 - 

2023-09-11 14:02:29,794 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:02:33,312 - Epoch: [61][   10/   71]    Overall Loss 0.185980    Objective Loss 0.185980                                        LR 0.000250    Time 0.351710    
2023-09-11 14:02:36,736 - Epoch: [61][   20/   71]    Overall Loss 0.189905    Objective Loss 0.189905                                        LR 0.000250    Time 0.347048    
2023-09-11 14:02:39,852 - Epoch: [61][   30/   71]    Overall Loss 0.191552    Objective Loss 0.191552                                        LR 0.000250    Time 0.335223    
2023-09-11 14:02:42,526 - Epoch: [61][   40/   71]    Overall Loss 0.192173    Objective Loss 0.192173                                        LR 0.000250    Time 0.318258    
2023-09-11 14:02:45,231 - Epoch: [61][   50/   71]    Overall Loss 0.188376    Objective Loss 0.188376                                        LR 0.000250    Time 0.308695    
2023-09-11 14:02:47,599 - Epoch: [61][   60/   71]    Overall Loss 0.188209    Objective Loss 0.188209                                        LR 0.000250    Time 0.296708    
2023-09-11 14:02:50,259 - Epoch: [61][   70/   71]    Overall Loss 0.189804    Objective Loss 0.189804    Top1 91.406250    LR 0.000250    Time 0.292310    
2023-09-11 14:02:50,364 - Epoch: [61][   71/   71]    Overall Loss 0.188648    Objective Loss 0.188648    Top1 93.154762    LR 0.000250    Time 0.289671    
2023-09-11 14:02:50,469 - --- validate (epoch=61)-----------
2023-09-11 14:02:50,469 - 2000 samples (256 per mini-batch)
2023-09-11 14:02:53,661 - Epoch: [61][    8/    8]    Loss 0.217436    Top1 90.450000    
2023-09-11 14:02:53,755 - ==> Top1: 90.450    Loss: 0.217

2023-09-11 14:02:53,755 - ==> Confusion:
[[889  96]
 [ 95 920]]

2023-09-11 14:02:53,771 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:02:53,771 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:02:53,774 - 

2023-09-11 14:02:53,774 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:02:58,019 - Epoch: [62][   10/   71]    Overall Loss 0.176550    Objective Loss 0.176550                                        LR 0.000250    Time 0.424490    
2023-09-11 14:03:00,083 - Epoch: [62][   20/   71]    Overall Loss 0.184256    Objective Loss 0.184256                                        LR 0.000250    Time 0.315451    
2023-09-11 14:03:02,722 - Epoch: [62][   30/   71]    Overall Loss 0.187185    Objective Loss 0.187185                                        LR 0.000250    Time 0.298230    
2023-09-11 14:03:05,033 - Epoch: [62][   40/   71]    Overall Loss 0.189391    Objective Loss 0.189391                                        LR 0.000250    Time 0.281460    
2023-09-11 14:03:08,259 - Epoch: [62][   50/   71]    Overall Loss 0.189735    Objective Loss 0.189735                                        LR 0.000250    Time 0.289663    
2023-09-11 14:03:10,897 - Epoch: [62][   60/   71]    Overall Loss 0.190345    Objective Loss 0.190345                                        LR 0.000250    Time 0.285361    
2023-09-11 14:03:13,536 - Epoch: [62][   70/   71]    Overall Loss 0.192127    Objective Loss 0.192127    Top1 90.234375    LR 0.000250    Time 0.282289    
2023-09-11 14:03:13,584 - Epoch: [62][   71/   71]    Overall Loss 0.192368    Objective Loss 0.192368    Top1 90.476190    LR 0.000250    Time 0.278983    
2023-09-11 14:03:13,677 - --- validate (epoch=62)-----------
2023-09-11 14:03:13,677 - 2000 samples (256 per mini-batch)
2023-09-11 14:03:16,782 - Epoch: [62][    8/    8]    Loss 0.213627    Top1 91.300000    
2023-09-11 14:03:16,878 - ==> Top1: 91.300    Loss: 0.214

2023-09-11 14:03:16,878 - ==> Confusion:
[[918  67]
 [107 908]]

2023-09-11 14:03:16,893 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:03:16,893 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:03:16,896 - 

2023-09-11 14:03:16,896 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:03:21,585 - Epoch: [63][   10/   71]    Overall Loss 0.192886    Objective Loss 0.192886                                        LR 0.000250    Time 0.468886    
2023-09-11 14:03:23,561 - Epoch: [63][   20/   71]    Overall Loss 0.200825    Objective Loss 0.200825                                        LR 0.000250    Time 0.333210    
2023-09-11 14:03:26,568 - Epoch: [63][   30/   71]    Overall Loss 0.199846    Objective Loss 0.199846                                        LR 0.000250    Time 0.322354    
2023-09-11 14:03:28,598 - Epoch: [63][   40/   71]    Overall Loss 0.195114    Objective Loss 0.195114                                        LR 0.000250    Time 0.292505    
2023-09-11 14:03:31,504 - Epoch: [63][   50/   71]    Overall Loss 0.197300    Objective Loss 0.197300                                        LR 0.000250    Time 0.292130    
2023-09-11 14:03:35,354 - Epoch: [63][   60/   71]    Overall Loss 0.196549    Objective Loss 0.196549                                        LR 0.000250    Time 0.307599    
2023-09-11 14:03:37,242 - Epoch: [63][   70/   71]    Overall Loss 0.194097    Objective Loss 0.194097    Top1 92.968750    LR 0.000250    Time 0.290625    
2023-09-11 14:03:37,363 - Epoch: [63][   71/   71]    Overall Loss 0.193604    Objective Loss 0.193604    Top1 93.154762    LR 0.000250    Time 0.288230    
2023-09-11 14:03:37,457 - --- validate (epoch=63)-----------
2023-09-11 14:03:37,457 - 2000 samples (256 per mini-batch)
2023-09-11 14:03:40,319 - Epoch: [63][    8/    8]    Loss 0.214302    Top1 90.650000    
2023-09-11 14:03:40,411 - ==> Top1: 90.650    Loss: 0.214

2023-09-11 14:03:40,411 - ==> Confusion:
[[879 106]
 [ 81 934]]

2023-09-11 14:03:40,427 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:03:40,427 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:03:40,429 - 

2023-09-11 14:03:40,429 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:03:43,974 - Epoch: [64][   10/   71]    Overall Loss 0.178083    Objective Loss 0.178083                                        LR 0.000250    Time 0.354384    
2023-09-11 14:03:46,494 - Epoch: [64][   20/   71]    Overall Loss 0.190776    Objective Loss 0.190776                                        LR 0.000250    Time 0.303183    
2023-09-11 14:03:49,460 - Epoch: [64][   30/   71]    Overall Loss 0.192779    Objective Loss 0.192779                                        LR 0.000250    Time 0.300996    
2023-09-11 14:03:51,928 - Epoch: [64][   40/   71]    Overall Loss 0.194145    Objective Loss 0.194145                                        LR 0.000250    Time 0.287430    
2023-09-11 14:03:55,028 - Epoch: [64][   50/   71]    Overall Loss 0.195180    Objective Loss 0.195180                                        LR 0.000250    Time 0.291932    
2023-09-11 14:03:57,366 - Epoch: [64][   60/   71]    Overall Loss 0.196967    Objective Loss 0.196967                                        LR 0.000250    Time 0.282236    
2023-09-11 14:04:00,185 - Epoch: [64][   70/   71]    Overall Loss 0.194432    Objective Loss 0.194432    Top1 94.531250    LR 0.000250    Time 0.282189    
2023-09-11 14:04:00,264 - Epoch: [64][   71/   71]    Overall Loss 0.193364    Objective Loss 0.193364    Top1 94.642857    LR 0.000250    Time 0.279333    
2023-09-11 14:04:00,359 - --- validate (epoch=64)-----------
2023-09-11 14:04:00,359 - 2000 samples (256 per mini-batch)
2023-09-11 14:04:03,475 - Epoch: [64][    8/    8]    Loss 0.218896    Top1 90.700000    
2023-09-11 14:04:03,571 - ==> Top1: 90.700    Loss: 0.219

2023-09-11 14:04:03,571 - ==> Confusion:
[[903  82]
 [104 911]]

2023-09-11 14:04:03,586 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:04:03,586 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:04:03,590 - 

2023-09-11 14:04:03,591 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:04:07,365 - Epoch: [65][   10/   71]    Overall Loss 0.197273    Objective Loss 0.197273                                        LR 0.000250    Time 0.377425    
2023-09-11 14:04:09,511 - Epoch: [65][   20/   71]    Overall Loss 0.193656    Objective Loss 0.193656                                        LR 0.000250    Time 0.295990    
2023-09-11 14:04:12,290 - Epoch: [65][   30/   71]    Overall Loss 0.192070    Objective Loss 0.192070                                        LR 0.000250    Time 0.289922    
2023-09-11 14:04:15,092 - Epoch: [65][   40/   71]    Overall Loss 0.187438    Objective Loss 0.187438                                        LR 0.000250    Time 0.287499    
2023-09-11 14:04:18,183 - Epoch: [65][   50/   71]    Overall Loss 0.186559    Objective Loss 0.186559                                        LR 0.000250    Time 0.291808    
2023-09-11 14:04:20,842 - Epoch: [65][   60/   71]    Overall Loss 0.184826    Objective Loss 0.184826                                        LR 0.000250    Time 0.287490    
2023-09-11 14:04:24,052 - Epoch: [65][   70/   71]    Overall Loss 0.184689    Objective Loss 0.184689    Top1 94.531250    LR 0.000250    Time 0.292276    
2023-09-11 14:04:24,155 - Epoch: [65][   71/   71]    Overall Loss 0.184547    Objective Loss 0.184547    Top1 94.642857    LR 0.000250    Time 0.289608    
2023-09-11 14:04:24,247 - --- validate (epoch=65)-----------
2023-09-11 14:04:24,247 - 2000 samples (256 per mini-batch)
2023-09-11 14:04:26,748 - Epoch: [65][    8/    8]    Loss 0.221507    Top1 90.200000    
2023-09-11 14:04:26,843 - ==> Top1: 90.200    Loss: 0.222

2023-09-11 14:04:26,843 - ==> Confusion:
[[875 110]
 [ 86 929]]

2023-09-11 14:04:26,860 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:04:26,860 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:04:26,862 - 

2023-09-11 14:04:26,862 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:04:31,177 - Epoch: [66][   10/   71]    Overall Loss 0.185086    Objective Loss 0.185086                                        LR 0.000250    Time 0.431414    
2023-09-11 14:04:33,167 - Epoch: [66][   20/   71]    Overall Loss 0.188280    Objective Loss 0.188280                                        LR 0.000250    Time 0.315179    
2023-09-11 14:04:36,052 - Epoch: [66][   30/   71]    Overall Loss 0.191589    Objective Loss 0.191589                                        LR 0.000250    Time 0.306291    
2023-09-11 14:04:38,174 - Epoch: [66][   40/   71]    Overall Loss 0.188799    Objective Loss 0.188799                                        LR 0.000250    Time 0.282753    
2023-09-11 14:04:41,067 - Epoch: [66][   50/   71]    Overall Loss 0.191133    Objective Loss 0.191133                                        LR 0.000250    Time 0.284062    
2023-09-11 14:04:43,273 - Epoch: [66][   60/   71]    Overall Loss 0.190130    Objective Loss 0.190130                                        LR 0.000250    Time 0.273475    
2023-09-11 14:04:45,628 - Epoch: [66][   70/   71]    Overall Loss 0.192004    Objective Loss 0.192004    Top1 92.968750    LR 0.000250    Time 0.268048    
2023-09-11 14:04:45,706 - Epoch: [66][   71/   71]    Overall Loss 0.191559    Objective Loss 0.191559    Top1 92.857143    LR 0.000250    Time 0.265362    
2023-09-11 14:04:45,794 - --- validate (epoch=66)-----------
2023-09-11 14:04:45,794 - 2000 samples (256 per mini-batch)
2023-09-11 14:04:48,218 - Epoch: [66][    8/    8]    Loss 0.210423    Top1 90.900000    
2023-09-11 14:04:48,318 - ==> Top1: 90.900    Loss: 0.210

2023-09-11 14:04:48,318 - ==> Confusion:
[[879 106]
 [ 76 939]]

2023-09-11 14:04:48,334 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:04:48,334 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:04:48,339 - 

2023-09-11 14:04:48,339 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:04:52,976 - Epoch: [67][   10/   71]    Overall Loss 0.172711    Objective Loss 0.172711                                        LR 0.000250    Time 0.463609    
2023-09-11 14:04:55,342 - Epoch: [67][   20/   71]    Overall Loss 0.182210    Objective Loss 0.182210                                        LR 0.000250    Time 0.350121    
2023-09-11 14:04:57,869 - Epoch: [67][   30/   71]    Overall Loss 0.187123    Objective Loss 0.187123                                        LR 0.000250    Time 0.317634    
2023-09-11 14:05:00,616 - Epoch: [67][   40/   71]    Overall Loss 0.183886    Objective Loss 0.183886                                        LR 0.000250    Time 0.306884    
2023-09-11 14:05:03,485 - Epoch: [67][   50/   71]    Overall Loss 0.185864    Objective Loss 0.185864                                        LR 0.000250    Time 0.302877    
2023-09-11 14:05:06,543 - Epoch: [67][   60/   71]    Overall Loss 0.186490    Objective Loss 0.186490                                        LR 0.000250    Time 0.303358    
2023-09-11 14:05:08,893 - Epoch: [67][   70/   71]    Overall Loss 0.185087    Objective Loss 0.185087    Top1 90.234375    LR 0.000250    Time 0.293596    
2023-09-11 14:05:08,977 - Epoch: [67][   71/   71]    Overall Loss 0.185901    Objective Loss 0.185901    Top1 90.178571    LR 0.000250    Time 0.290630    
2023-09-11 14:05:09,073 - --- validate (epoch=67)-----------
2023-09-11 14:05:09,074 - 2000 samples (256 per mini-batch)
2023-09-11 14:05:12,161 - Epoch: [67][    8/    8]    Loss 0.208232    Top1 91.600000    
2023-09-11 14:05:12,260 - ==> Top1: 91.600    Loss: 0.208

2023-09-11 14:05:12,261 - ==> Confusion:
[[917  68]
 [100 915]]

2023-09-11 14:05:12,276 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:05:12,276 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:05:12,278 - 

2023-09-11 14:05:12,278 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:05:16,093 - Epoch: [68][   10/   71]    Overall Loss 0.198097    Objective Loss 0.198097                                        LR 0.000250    Time 0.381399    
2023-09-11 14:05:19,245 - Epoch: [68][   20/   71]    Overall Loss 0.194152    Objective Loss 0.194152                                        LR 0.000250    Time 0.348302    
2023-09-11 14:05:21,266 - Epoch: [68][   30/   71]    Overall Loss 0.194069    Objective Loss 0.194069                                        LR 0.000250    Time 0.299542    
2023-09-11 14:05:25,068 - Epoch: [68][   40/   71]    Overall Loss 0.190730    Objective Loss 0.190730                                        LR 0.000250    Time 0.319695    
2023-09-11 14:05:27,275 - Epoch: [68][   50/   71]    Overall Loss 0.190853    Objective Loss 0.190853                                        LR 0.000250    Time 0.299901    
2023-09-11 14:05:30,808 - Epoch: [68][   60/   71]    Overall Loss 0.192491    Objective Loss 0.192491                                        LR 0.000250    Time 0.308786    
2023-09-11 14:05:32,745 - Epoch: [68][   70/   71]    Overall Loss 0.191531    Objective Loss 0.191531    Top1 91.015625    LR 0.000250    Time 0.292353    
2023-09-11 14:05:32,825 - Epoch: [68][   71/   71]    Overall Loss 0.191434    Objective Loss 0.191434    Top1 91.369048    LR 0.000250    Time 0.289357    
2023-09-11 14:05:32,916 - --- validate (epoch=68)-----------
2023-09-11 14:05:32,916 - 2000 samples (256 per mini-batch)
2023-09-11 14:05:35,337 - Epoch: [68][    8/    8]    Loss 0.196833    Top1 91.200000    
2023-09-11 14:05:35,430 - ==> Top1: 91.200    Loss: 0.197

2023-09-11 14:05:35,430 - ==> Confusion:
[[906  79]
 [ 97 918]]

2023-09-11 14:05:35,431 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:05:35,431 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:05:35,436 - 

2023-09-11 14:05:35,436 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:05:39,141 - Epoch: [69][   10/   71]    Overall Loss 0.200403    Objective Loss 0.200403                                        LR 0.000250    Time 0.370420    
2023-09-11 14:05:41,535 - Epoch: [69][   20/   71]    Overall Loss 0.190536    Objective Loss 0.190536                                        LR 0.000250    Time 0.304902    
2023-09-11 14:05:45,097 - Epoch: [69][   30/   71]    Overall Loss 0.197563    Objective Loss 0.197563                                        LR 0.000250    Time 0.322010    
2023-09-11 14:05:47,764 - Epoch: [69][   40/   71]    Overall Loss 0.191363    Objective Loss 0.191363                                        LR 0.000250    Time 0.308157    
2023-09-11 14:05:50,457 - Epoch: [69][   50/   71]    Overall Loss 0.189601    Objective Loss 0.189601                                        LR 0.000250    Time 0.300387    
2023-09-11 14:05:53,016 - Epoch: [69][   60/   71]    Overall Loss 0.188452    Objective Loss 0.188452                                        LR 0.000250    Time 0.292968    
2023-09-11 14:05:55,328 - Epoch: [69][   70/   71]    Overall Loss 0.187632    Objective Loss 0.187632    Top1 91.406250    LR 0.000250    Time 0.284133    
2023-09-11 14:05:55,405 - Epoch: [69][   71/   71]    Overall Loss 0.187626    Objective Loss 0.187626    Top1 91.369048    LR 0.000250    Time 0.281210    
2023-09-11 14:05:55,493 - --- validate (epoch=69)-----------
2023-09-11 14:05:55,493 - 2000 samples (256 per mini-batch)
2023-09-11 14:05:58,714 - Epoch: [69][    8/    8]    Loss 0.219728    Top1 90.050000    
2023-09-11 14:05:58,852 - ==> Top1: 90.050    Loss: 0.220

2023-09-11 14:05:58,853 - ==> Confusion:
[[863 122]
 [ 77 938]]

2023-09-11 14:05:58,868 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:05:58,869 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:05:58,873 - 

2023-09-11 14:05:58,873 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:06:03,252 - Epoch: [70][   10/   71]    Overall Loss 0.191895    Objective Loss 0.191895                                        LR 0.000250    Time 0.437818    
2023-09-11 14:06:05,293 - Epoch: [70][   20/   71]    Overall Loss 0.187425    Objective Loss 0.187425                                        LR 0.000250    Time 0.320924    
2023-09-11 14:06:07,926 - Epoch: [70][   30/   71]    Overall Loss 0.187085    Objective Loss 0.187085                                        LR 0.000250    Time 0.301731    
2023-09-11 14:06:10,699 - Epoch: [70][   40/   71]    Overall Loss 0.184490    Objective Loss 0.184490                                        LR 0.000250    Time 0.295606    
2023-09-11 14:06:13,187 - Epoch: [70][   50/   71]    Overall Loss 0.186297    Objective Loss 0.186297                                        LR 0.000250    Time 0.286235    
2023-09-11 14:06:16,380 - Epoch: [70][   60/   71]    Overall Loss 0.183535    Objective Loss 0.183535                                        LR 0.000250    Time 0.291745    
2023-09-11 14:06:18,289 - Epoch: [70][   70/   71]    Overall Loss 0.182750    Objective Loss 0.182750    Top1 88.281250    LR 0.000250    Time 0.277339    
2023-09-11 14:06:18,365 - Epoch: [70][   71/   71]    Overall Loss 0.182252    Objective Loss 0.182252    Top1 90.178571    LR 0.000250    Time 0.274501    
2023-09-11 14:06:18,444 - --- validate (epoch=70)-----------
2023-09-11 14:06:18,445 - 2000 samples (256 per mini-batch)
2023-09-11 14:06:21,379 - Epoch: [70][    8/    8]    Loss 0.194180    Top1 91.600000    
2023-09-11 14:06:21,473 - ==> Top1: 91.600    Loss: 0.194

2023-09-11 14:06:21,473 - ==> Confusion:
[[888  97]
 [ 71 944]]

2023-09-11 14:06:21,489 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:06:21,489 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:06:21,492 - 

2023-09-11 14:06:21,492 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:06:25,153 - Epoch: [71][   10/   71]    Overall Loss 0.175629    Objective Loss 0.175629                                        LR 0.000250    Time 0.366032    
2023-09-11 14:06:28,372 - Epoch: [71][   20/   71]    Overall Loss 0.186175    Objective Loss 0.186175                                        LR 0.000250    Time 0.343974    
2023-09-11 14:06:32,060 - Epoch: [71][   30/   71]    Overall Loss 0.184061    Objective Loss 0.184061                                        LR 0.000250    Time 0.352223    
2023-09-11 14:06:34,162 - Epoch: [71][   40/   71]    Overall Loss 0.185751    Objective Loss 0.185751                                        LR 0.000250    Time 0.316701    
2023-09-11 14:06:36,853 - Epoch: [71][   50/   71]    Overall Loss 0.186573    Objective Loss 0.186573                                        LR 0.000250    Time 0.307176    
2023-09-11 14:06:39,044 - Epoch: [71][   60/   71]    Overall Loss 0.184373    Objective Loss 0.184373                                        LR 0.000250    Time 0.292499    
2023-09-11 14:06:42,039 - Epoch: [71][   70/   71]    Overall Loss 0.183490    Objective Loss 0.183490    Top1 91.015625    LR 0.000250    Time 0.293490    
2023-09-11 14:06:42,156 - Epoch: [71][   71/   71]    Overall Loss 0.183224    Objective Loss 0.183224    Top1 91.369048    LR 0.000250    Time 0.291004    
2023-09-11 14:06:42,253 - --- validate (epoch=71)-----------
2023-09-11 14:06:42,254 - 2000 samples (256 per mini-batch)
2023-09-11 14:06:44,977 - Epoch: [71][    8/    8]    Loss 0.201142    Top1 91.600000    
2023-09-11 14:06:45,065 - ==> Top1: 91.600    Loss: 0.201

2023-09-11 14:06:45,065 - ==> Confusion:
[[913  72]
 [ 96 919]]

2023-09-11 14:06:45,081 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:06:45,081 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:06:45,085 - 

2023-09-11 14:06:45,085 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:06:48,921 - Epoch: [72][   10/   71]    Overall Loss 0.189989    Objective Loss 0.189989                                        LR 0.000250    Time 0.383541    
2023-09-11 14:06:52,254 - Epoch: [72][   20/   71]    Overall Loss 0.193197    Objective Loss 0.193197                                        LR 0.000250    Time 0.358399    
2023-09-11 14:06:54,239 - Epoch: [72][   30/   71]    Overall Loss 0.192523    Objective Loss 0.192523                                        LR 0.000250    Time 0.305083    
2023-09-11 14:06:57,067 - Epoch: [72][   40/   71]    Overall Loss 0.190208    Objective Loss 0.190208                                        LR 0.000250    Time 0.299499    
2023-09-11 14:06:59,040 - Epoch: [72][   50/   71]    Overall Loss 0.188939    Objective Loss 0.188939                                        LR 0.000250    Time 0.279056    
2023-09-11 14:07:01,868 - Epoch: [72][   60/   71]    Overall Loss 0.189885    Objective Loss 0.189885                                        LR 0.000250    Time 0.279684    
2023-09-11 14:07:03,870 - Epoch: [72][   70/   71]    Overall Loss 0.186996    Objective Loss 0.186996    Top1 94.921875    LR 0.000250    Time 0.268315    
2023-09-11 14:07:03,951 - Epoch: [72][   71/   71]    Overall Loss 0.186064    Objective Loss 0.186064    Top1 94.940476    LR 0.000250    Time 0.265683    
2023-09-11 14:07:04,039 - --- validate (epoch=72)-----------
2023-09-11 14:07:04,040 - 2000 samples (256 per mini-batch)
2023-09-11 14:07:07,141 - Epoch: [72][    8/    8]    Loss 0.205507    Top1 91.000000    
2023-09-11 14:07:07,235 - ==> Top1: 91.000    Loss: 0.206

2023-09-11 14:07:07,235 - ==> Confusion:
[[906  79]
 [101 914]]

2023-09-11 14:07:07,250 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:07:07,250 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:07:07,253 - 

2023-09-11 14:07:07,253 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:07:11,206 - Epoch: [73][   10/   71]    Overall Loss 0.187254    Objective Loss 0.187254                                        LR 0.000250    Time 0.395315    
2023-09-11 14:07:14,099 - Epoch: [73][   20/   71]    Overall Loss 0.182856    Objective Loss 0.182856                                        LR 0.000250    Time 0.342260    
2023-09-11 14:07:16,364 - Epoch: [73][   30/   71]    Overall Loss 0.185579    Objective Loss 0.185579                                        LR 0.000250    Time 0.303667    
2023-09-11 14:07:19,534 - Epoch: [73][   40/   71]    Overall Loss 0.186372    Objective Loss 0.186372                                        LR 0.000250    Time 0.306995    
2023-09-11 14:07:22,024 - Epoch: [73][   50/   71]    Overall Loss 0.185667    Objective Loss 0.185667                                        LR 0.000250    Time 0.295393    
2023-09-11 14:07:25,291 - Epoch: [73][   60/   71]    Overall Loss 0.184137    Objective Loss 0.184137                                        LR 0.000250    Time 0.300593    
2023-09-11 14:07:27,218 - Epoch: [73][   70/   71]    Overall Loss 0.182773    Objective Loss 0.182773    Top1 91.015625    LR 0.000250    Time 0.285187    
2023-09-11 14:07:27,296 - Epoch: [73][   71/   71]    Overall Loss 0.183266    Objective Loss 0.183266    Top1 91.071429    LR 0.000250    Time 0.282264    
2023-09-11 14:07:27,406 - --- validate (epoch=73)-----------
2023-09-11 14:07:27,406 - 2000 samples (256 per mini-batch)
2023-09-11 14:07:30,293 - Epoch: [73][    8/    8]    Loss 0.217516    Top1 90.900000    
2023-09-11 14:07:30,388 - ==> Top1: 90.900    Loss: 0.218

2023-09-11 14:07:30,389 - ==> Confusion:
[[902  83]
 [ 99 916]]

2023-09-11 14:07:30,390 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:07:30,390 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:07:30,395 - 

2023-09-11 14:07:30,395 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:07:33,613 - Epoch: [74][   10/   71]    Overall Loss 0.187120    Objective Loss 0.187120                                        LR 0.000250    Time 0.321722    
2023-09-11 14:07:36,171 - Epoch: [74][   20/   71]    Overall Loss 0.183056    Objective Loss 0.183056                                        LR 0.000250    Time 0.288780    
2023-09-11 14:07:38,696 - Epoch: [74][   30/   71]    Overall Loss 0.181251    Objective Loss 0.181251                                        LR 0.000250    Time 0.276661    
2023-09-11 14:07:41,930 - Epoch: [74][   40/   71]    Overall Loss 0.181312    Objective Loss 0.181312                                        LR 0.000250    Time 0.288352    
2023-09-11 14:07:44,106 - Epoch: [74][   50/   71]    Overall Loss 0.180094    Objective Loss 0.180094                                        LR 0.000250    Time 0.274194    
2023-09-11 14:07:47,033 - Epoch: [74][   60/   71]    Overall Loss 0.181496    Objective Loss 0.181496                                        LR 0.000250    Time 0.277258    
2023-09-11 14:07:49,091 - Epoch: [74][   70/   71]    Overall Loss 0.181645    Objective Loss 0.181645    Top1 92.578125    LR 0.000250    Time 0.267058    
2023-09-11 14:07:49,168 - Epoch: [74][   71/   71]    Overall Loss 0.181784    Objective Loss 0.181784    Top1 92.261905    LR 0.000250    Time 0.264367    
2023-09-11 14:07:49,260 - --- validate (epoch=74)-----------
2023-09-11 14:07:49,260 - 2000 samples (256 per mini-batch)
2023-09-11 14:07:52,292 - Epoch: [74][    8/    8]    Loss 0.219677    Top1 91.000000    
2023-09-11 14:07:52,385 - ==> Top1: 91.000    Loss: 0.220

2023-09-11 14:07:52,385 - ==> Confusion:
[[910  75]
 [105 910]]

2023-09-11 14:07:52,400 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:07:52,400 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:07:52,403 - 

2023-09-11 14:07:52,403 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:07:56,860 - Epoch: [75][   10/   71]    Overall Loss 0.191604    Objective Loss 0.191604                                        LR 0.000250    Time 0.445699    
2023-09-11 14:07:59,807 - Epoch: [75][   20/   71]    Overall Loss 0.190262    Objective Loss 0.190262                                        LR 0.000250    Time 0.370154    
2023-09-11 14:08:03,000 - Epoch: [75][   30/   71]    Overall Loss 0.186464    Objective Loss 0.186464                                        LR 0.000250    Time 0.353203    
2023-09-11 14:08:05,569 - Epoch: [75][   40/   71]    Overall Loss 0.181261    Objective Loss 0.181261                                        LR 0.000250    Time 0.329122    
2023-09-11 14:08:08,209 - Epoch: [75][   50/   71]    Overall Loss 0.181693    Objective Loss 0.181693                                        LR 0.000250    Time 0.316095    
2023-09-11 14:08:11,143 - Epoch: [75][   60/   71]    Overall Loss 0.180328    Objective Loss 0.180328                                        LR 0.000250    Time 0.312306    
2023-09-11 14:08:13,021 - Epoch: [75][   70/   71]    Overall Loss 0.179314    Objective Loss 0.179314    Top1 91.406250    LR 0.000250    Time 0.294507    
2023-09-11 14:08:13,101 - Epoch: [75][   71/   71]    Overall Loss 0.179500    Objective Loss 0.179500    Top1 91.071429    LR 0.000250    Time 0.291482    
2023-09-11 14:08:13,190 - --- validate (epoch=75)-----------
2023-09-11 14:08:13,190 - 2000 samples (256 per mini-batch)
2023-09-11 14:08:16,266 - Epoch: [75][    8/    8]    Loss 0.203488    Top1 91.050000    
2023-09-11 14:08:16,368 - ==> Top1: 91.050    Loss: 0.203

2023-09-11 14:08:16,368 - ==> Confusion:
[[920  65]
 [114 901]]

2023-09-11 14:08:16,383 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:08:16,384 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:08:16,386 - 

2023-09-11 14:08:16,386 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:08:20,911 - Epoch: [76][   10/   71]    Overall Loss 0.179623    Objective Loss 0.179623                                        LR 0.000250    Time 0.452446    
2023-09-11 14:08:23,767 - Epoch: [76][   20/   71]    Overall Loss 0.184192    Objective Loss 0.184192                                        LR 0.000250    Time 0.369026    
2023-09-11 14:08:26,524 - Epoch: [76][   30/   71]    Overall Loss 0.185231    Objective Loss 0.185231                                        LR 0.000250    Time 0.337880    
2023-09-11 14:08:28,661 - Epoch: [76][   40/   71]    Overall Loss 0.182251    Objective Loss 0.182251                                        LR 0.000250    Time 0.306849    
2023-09-11 14:08:31,349 - Epoch: [76][   50/   71]    Overall Loss 0.181050    Objective Loss 0.181050                                        LR 0.000250    Time 0.299218    
2023-09-11 14:08:33,562 - Epoch: [76][   60/   71]    Overall Loss 0.182999    Objective Loss 0.182999                                        LR 0.000250    Time 0.286226    
2023-09-11 14:08:35,866 - Epoch: [76][   70/   71]    Overall Loss 0.182859    Objective Loss 0.182859    Top1 89.453125    LR 0.000250    Time 0.278249    
2023-09-11 14:08:35,945 - Epoch: [76][   71/   71]    Overall Loss 0.183531    Objective Loss 0.183531    Top1 90.178571    LR 0.000250    Time 0.275441    
2023-09-11 14:08:36,057 - --- validate (epoch=76)-----------
2023-09-11 14:08:36,057 - 2000 samples (256 per mini-batch)
2023-09-11 14:08:39,076 - Epoch: [76][    8/    8]    Loss 0.203070    Top1 91.800000    
2023-09-11 14:08:39,158 - ==> Top1: 91.800    Loss: 0.203

2023-09-11 14:08:39,159 - ==> Confusion:
[[898  87]
 [ 77 938]]

2023-09-11 14:08:39,174 - ==> Best [Top1: 91.950   Sparsity:0.00   Params: 57776 on epoch: 53]
2023-09-11 14:08:39,174 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:08:39,176 - 

2023-09-11 14:08:39,176 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:08:42,573 - Epoch: [77][   10/   71]    Overall Loss 0.198216    Objective Loss 0.198216                                        LR 0.000250    Time 0.339643    
2023-09-11 14:08:46,751 - Epoch: [77][   20/   71]    Overall Loss 0.189626    Objective Loss 0.189626                                        LR 0.000250    Time 0.378722    
2023-09-11 14:08:48,686 - Epoch: [77][   30/   71]    Overall Loss 0.182472    Objective Loss 0.182472                                        LR 0.000250    Time 0.316951    
2023-09-11 14:08:51,219 - Epoch: [77][   40/   71]    Overall Loss 0.179858    Objective Loss 0.179858                                        LR 0.000250    Time 0.301033    
2023-09-11 14:08:53,483 - Epoch: [77][   50/   71]    Overall Loss 0.178019    Objective Loss 0.178019                                        LR 0.000250    Time 0.286106    
2023-09-11 14:08:56,586 - Epoch: [77][   60/   71]    Overall Loss 0.180002    Objective Loss 0.180002                                        LR 0.000250    Time 0.290130    
2023-09-11 14:08:58,550 - Epoch: [77][   70/   71]    Overall Loss 0.179840    Objective Loss 0.179840    Top1 91.796875    LR 0.000250    Time 0.276741    
2023-09-11 14:08:58,629 - Epoch: [77][   71/   71]    Overall Loss 0.179579    Objective Loss 0.179579    Top1 92.857143    LR 0.000250    Time 0.273947    
2023-09-11 14:08:58,728 - --- validate (epoch=77)-----------
2023-09-11 14:08:58,728 - 2000 samples (256 per mini-batch)
2023-09-11 14:09:01,903 - Epoch: [77][    8/    8]    Loss 0.197987    Top1 92.250000    
2023-09-11 14:09:02,030 - ==> Top1: 92.250    Loss: 0.198

2023-09-11 14:09:02,030 - ==> Confusion:
[[898  87]
 [ 68 947]]

2023-09-11 14:09:02,043 - ==> Best [Top1: 92.250   Sparsity:0.00   Params: 57776 on epoch: 77]
2023-09-11 14:09:02,044 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:09:02,049 - 

2023-09-11 14:09:02,049 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:09:06,547 - Epoch: [78][   10/   71]    Overall Loss 0.166856    Objective Loss 0.166856                                        LR 0.000250    Time 0.449732    
2023-09-11 14:09:08,657 - Epoch: [78][   20/   71]    Overall Loss 0.181823    Objective Loss 0.181823                                        LR 0.000250    Time 0.330349    
2023-09-11 14:09:11,275 - Epoch: [78][   30/   71]    Overall Loss 0.179580    Objective Loss 0.179580                                        LR 0.000250    Time 0.307465    
2023-09-11 14:09:13,713 - Epoch: [78][   40/   71]    Overall Loss 0.178072    Objective Loss 0.178072                                        LR 0.000250    Time 0.291553    
2023-09-11 14:09:16,580 - Epoch: [78][   50/   71]    Overall Loss 0.176616    Objective Loss 0.176616                                        LR 0.000250    Time 0.290572    
2023-09-11 14:09:19,954 - Epoch: [78][   60/   71]    Overall Loss 0.175459    Objective Loss 0.175459                                        LR 0.000250    Time 0.298370    
2023-09-11 14:09:22,282 - Epoch: [78][   70/   71]    Overall Loss 0.175782    Objective Loss 0.175782    Top1 94.140625    LR 0.000250    Time 0.289000    
2023-09-11 14:09:22,362 - Epoch: [78][   71/   71]    Overall Loss 0.175655    Objective Loss 0.175655    Top1 94.047619    LR 0.000250    Time 0.286059    
2023-09-11 14:09:22,475 - --- validate (epoch=78)-----------
2023-09-11 14:09:22,475 - 2000 samples (256 per mini-batch)
2023-09-11 14:09:25,162 - Epoch: [78][    8/    8]    Loss 0.207562    Top1 91.000000    
2023-09-11 14:09:25,258 - ==> Top1: 91.000    Loss: 0.208

2023-09-11 14:09:25,258 - ==> Confusion:
[[879 106]
 [ 74 941]]

2023-09-11 14:09:25,269 - ==> Best [Top1: 92.250   Sparsity:0.00   Params: 57776 on epoch: 77]
2023-09-11 14:09:25,270 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:09:25,272 - 

2023-09-11 14:09:25,272 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:09:29,846 - Epoch: [79][   10/   71]    Overall Loss 0.160407    Objective Loss 0.160407                                        LR 0.000250    Time 0.457354    
2023-09-11 14:09:31,861 - Epoch: [79][   20/   71]    Overall Loss 0.163583    Objective Loss 0.163583                                        LR 0.000250    Time 0.329392    
2023-09-11 14:09:34,545 - Epoch: [79][   30/   71]    Overall Loss 0.174761    Objective Loss 0.174761                                        LR 0.000250    Time 0.309054    
2023-09-11 14:09:38,374 - Epoch: [79][   40/   71]    Overall Loss 0.173552    Objective Loss 0.173552                                        LR 0.000250    Time 0.327499    
2023-09-11 14:09:40,515 - Epoch: [79][   50/   71]    Overall Loss 0.172318    Objective Loss 0.172318                                        LR 0.000250    Time 0.304822    
2023-09-11 14:09:43,191 - Epoch: [79][   60/   71]    Overall Loss 0.173251    Objective Loss 0.173251                                        LR 0.000250    Time 0.298607    
2023-09-11 14:09:45,075 - Epoch: [79][   70/   71]    Overall Loss 0.173210    Objective Loss 0.173210    Top1 95.312500    LR 0.000250    Time 0.282865    
2023-09-11 14:09:45,155 - Epoch: [79][   71/   71]    Overall Loss 0.173239    Objective Loss 0.173239    Top1 94.642857    LR 0.000250    Time 0.279998    
2023-09-11 14:09:45,251 - --- validate (epoch=79)-----------
2023-09-11 14:09:45,251 - 2000 samples (256 per mini-batch)
2023-09-11 14:09:48,560 - Epoch: [79][    8/    8]    Loss 0.206681    Top1 91.150000    
2023-09-11 14:09:48,657 - ==> Top1: 91.150    Loss: 0.207

2023-09-11 14:09:48,657 - ==> Confusion:
[[894  91]
 [ 86 929]]

2023-09-11 14:09:48,672 - ==> Best [Top1: 92.250   Sparsity:0.00   Params: 57776 on epoch: 77]
2023-09-11 14:09:48,672 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:09:48,674 - 

2023-09-11 14:09:48,674 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:09:52,082 - Epoch: [80][   10/   71]    Overall Loss 0.165410    Objective Loss 0.165410                                        LR 0.000250    Time 0.340711    
2023-09-11 14:09:54,095 - Epoch: [80][   20/   71]    Overall Loss 0.166029    Objective Loss 0.166029                                        LR 0.000250    Time 0.270995    
2023-09-11 14:09:56,698 - Epoch: [80][   30/   71]    Overall Loss 0.169130    Objective Loss 0.169130                                        LR 0.000250    Time 0.267403    
2023-09-11 14:10:00,130 - Epoch: [80][   40/   71]    Overall Loss 0.170398    Objective Loss 0.170398                                        LR 0.000250    Time 0.286345    
2023-09-11 14:10:03,363 - Epoch: [80][   50/   71]    Overall Loss 0.170214    Objective Loss 0.170214                                        LR 0.000250    Time 0.293745    
2023-09-11 14:10:06,478 - Epoch: [80][   60/   71]    Overall Loss 0.173040    Objective Loss 0.173040                                        LR 0.000250    Time 0.296695    
2023-09-11 14:10:08,890 - Epoch: [80][   70/   71]    Overall Loss 0.175115    Objective Loss 0.175115    Top1 90.625000    LR 0.000250    Time 0.288759    
2023-09-11 14:10:08,967 - Epoch: [80][   71/   71]    Overall Loss 0.174298    Objective Loss 0.174298    Top1 91.071429    LR 0.000250    Time 0.285782    
2023-09-11 14:10:09,078 - --- validate (epoch=80)-----------
2023-09-11 14:10:09,078 - 2000 samples (256 per mini-batch)
2023-09-11 14:10:11,438 - Epoch: [80][    8/    8]    Loss 0.205587    Top1 91.800000    
2023-09-11 14:10:11,537 - ==> Top1: 91.800    Loss: 0.206

2023-09-11 14:10:11,538 - ==> Confusion:
[[882 103]
 [ 61 954]]

2023-09-11 14:10:11,552 - ==> Best [Top1: 92.250   Sparsity:0.00   Params: 57776 on epoch: 77]
2023-09-11 14:10:11,552 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:10:11,555 - 

2023-09-11 14:10:11,555 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:10:15,228 - Epoch: [81][   10/   71]    Overall Loss 0.172562    Objective Loss 0.172562                                        LR 0.000250    Time 0.367298    
2023-09-11 14:10:18,733 - Epoch: [81][   20/   71]    Overall Loss 0.171393    Objective Loss 0.171393                                        LR 0.000250    Time 0.358872    
2023-09-11 14:10:20,833 - Epoch: [81][   30/   71]    Overall Loss 0.170608    Objective Loss 0.170608                                        LR 0.000250    Time 0.309240    
2023-09-11 14:10:24,292 - Epoch: [81][   40/   71]    Overall Loss 0.173893    Objective Loss 0.173893                                        LR 0.000250    Time 0.318396    
2023-09-11 14:10:26,851 - Epoch: [81][   50/   71]    Overall Loss 0.175891    Objective Loss 0.175891                                        LR 0.000250    Time 0.305891    
2023-09-11 14:10:29,584 - Epoch: [81][   60/   71]    Overall Loss 0.176622    Objective Loss 0.176622                                        LR 0.000250    Time 0.300449    
2023-09-11 14:10:32,164 - Epoch: [81][   70/   71]    Overall Loss 0.174014    Objective Loss 0.174014    Top1 91.796875    LR 0.000250    Time 0.294381    
2023-09-11 14:10:32,256 - Epoch: [81][   71/   71]    Overall Loss 0.173938    Objective Loss 0.173938    Top1 92.261905    LR 0.000250    Time 0.291531    
2023-09-11 14:10:32,352 - --- validate (epoch=81)-----------
2023-09-11 14:10:32,352 - 2000 samples (256 per mini-batch)
2023-09-11 14:10:34,758 - Epoch: [81][    8/    8]    Loss 0.209532    Top1 91.450000    
2023-09-11 14:10:34,851 - ==> Top1: 91.450    Loss: 0.210

2023-09-11 14:10:34,851 - ==> Confusion:
[[880 105]
 [ 66 949]]

2023-09-11 14:10:34,851 - ==> Best [Top1: 92.250   Sparsity:0.00   Params: 57776 on epoch: 77]
2023-09-11 14:10:34,852 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:10:34,854 - 

2023-09-11 14:10:34,854 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:10:39,384 - Epoch: [82][   10/   71]    Overall Loss 0.176427    Objective Loss 0.176427                                        LR 0.000250    Time 0.452948    
2023-09-11 14:10:41,450 - Epoch: [82][   20/   71]    Overall Loss 0.181110    Objective Loss 0.181110                                        LR 0.000250    Time 0.329736    
2023-09-11 14:10:44,340 - Epoch: [82][   30/   71]    Overall Loss 0.180008    Objective Loss 0.180008                                        LR 0.000250    Time 0.316155    
2023-09-11 14:10:46,969 - Epoch: [82][   40/   71]    Overall Loss 0.180053    Objective Loss 0.180053                                        LR 0.000250    Time 0.302838    
2023-09-11 14:10:49,682 - Epoch: [82][   50/   71]    Overall Loss 0.174916    Objective Loss 0.174916                                        LR 0.000250    Time 0.296511    
2023-09-11 14:10:52,459 - Epoch: [82][   60/   71]    Overall Loss 0.174933    Objective Loss 0.174933                                        LR 0.000250    Time 0.293377    
2023-09-11 14:10:54,685 - Epoch: [82][   70/   71]    Overall Loss 0.173828    Objective Loss 0.173828    Top1 91.796875    LR 0.000250    Time 0.283261    
2023-09-11 14:10:54,755 - Epoch: [82][   71/   71]    Overall Loss 0.173300    Objective Loss 0.173300    Top1 92.261905    LR 0.000250    Time 0.280255    
2023-09-11 14:10:54,846 - --- validate (epoch=82)-----------
2023-09-11 14:10:54,846 - 2000 samples (256 per mini-batch)
2023-09-11 14:10:57,467 - Epoch: [82][    8/    8]    Loss 0.221770    Top1 90.800000    
2023-09-11 14:10:57,559 - ==> Top1: 90.800    Loss: 0.222

2023-09-11 14:10:57,559 - ==> Confusion:
[[926  59]
 [125 890]]

2023-09-11 14:10:57,574 - ==> Best [Top1: 92.250   Sparsity:0.00   Params: 57776 on epoch: 77]
2023-09-11 14:10:57,574 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:10:57,579 - 

2023-09-11 14:10:57,579 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:11:02,037 - Epoch: [83][   10/   71]    Overall Loss 0.165242    Objective Loss 0.165242                                        LR 0.000250    Time 0.445769    
2023-09-11 14:11:04,115 - Epoch: [83][   20/   71]    Overall Loss 0.166902    Objective Loss 0.166902                                        LR 0.000250    Time 0.326763    
2023-09-11 14:11:06,731 - Epoch: [83][   30/   71]    Overall Loss 0.170209    Objective Loss 0.170209                                        LR 0.000250    Time 0.305017    
2023-09-11 14:11:09,938 - Epoch: [83][   40/   71]    Overall Loss 0.171517    Objective Loss 0.171517                                        LR 0.000250    Time 0.308953    
2023-09-11 14:11:12,165 - Epoch: [83][   50/   71]    Overall Loss 0.170525    Objective Loss 0.170525                                        LR 0.000250    Time 0.291685    
2023-09-11 14:11:14,734 - Epoch: [83][   60/   71]    Overall Loss 0.172556    Objective Loss 0.172556                                        LR 0.000250    Time 0.285886    
2023-09-11 14:11:16,739 - Epoch: [83][   70/   71]    Overall Loss 0.173467    Objective Loss 0.173467    Top1 94.531250    LR 0.000250    Time 0.273685    
2023-09-11 14:11:16,821 - Epoch: [83][   71/   71]    Overall Loss 0.173260    Objective Loss 0.173260    Top1 94.345238    LR 0.000250    Time 0.270981    
2023-09-11 14:11:16,919 - --- validate (epoch=83)-----------
2023-09-11 14:11:16,919 - 2000 samples (256 per mini-batch)
2023-09-11 14:11:20,181 - Epoch: [83][    8/    8]    Loss 0.191952    Top1 91.750000    
2023-09-11 14:11:20,264 - ==> Top1: 91.750    Loss: 0.192

2023-09-11 14:11:20,264 - ==> Confusion:
[[898  87]
 [ 78 937]]

2023-09-11 14:11:20,279 - ==> Best [Top1: 92.250   Sparsity:0.00   Params: 57776 on epoch: 77]
2023-09-11 14:11:20,280 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:11:20,282 - 

2023-09-11 14:11:20,282 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:11:24,748 - Epoch: [84][   10/   71]    Overall Loss 0.189782    Objective Loss 0.189782                                        LR 0.000250    Time 0.446571    
2023-09-11 14:11:26,771 - Epoch: [84][   20/   71]    Overall Loss 0.176138    Objective Loss 0.176138                                        LR 0.000250    Time 0.324421    
2023-09-11 14:11:29,966 - Epoch: [84][   30/   71]    Overall Loss 0.174005    Objective Loss 0.174005                                        LR 0.000250    Time 0.322768    
2023-09-11 14:11:33,233 - Epoch: [84][   40/   71]    Overall Loss 0.173212    Objective Loss 0.173212                                        LR 0.000250    Time 0.323738    
2023-09-11 14:11:36,353 - Epoch: [84][   50/   71]    Overall Loss 0.171643    Objective Loss 0.171643                                        LR 0.000250    Time 0.321384    
2023-09-11 14:11:38,376 - Epoch: [84][   60/   71]    Overall Loss 0.172850    Objective Loss 0.172850                                        LR 0.000250    Time 0.301534    
2023-09-11 14:11:40,836 - Epoch: [84][   70/   71]    Overall Loss 0.172915    Objective Loss 0.172915    Top1 91.406250    LR 0.000250    Time 0.293597    
2023-09-11 14:11:40,919 - Epoch: [84][   71/   71]    Overall Loss 0.172290    Objective Loss 0.172290    Top1 92.559524    LR 0.000250    Time 0.290628    
2023-09-11 14:11:41,013 - --- validate (epoch=84)-----------
2023-09-11 14:11:41,014 - 2000 samples (256 per mini-batch)
2023-09-11 14:11:43,569 - Epoch: [84][    8/    8]    Loss 0.180015    Top1 92.300000    
2023-09-11 14:11:43,674 - ==> Top1: 92.300    Loss: 0.180

2023-09-11 14:11:43,674 - ==> Confusion:
[[930  55]
 [ 99 916]]

2023-09-11 14:11:43,689 - ==> Best [Top1: 92.300   Sparsity:0.00   Params: 57776 on epoch: 84]
2023-09-11 14:11:43,689 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:11:43,694 - 

2023-09-11 14:11:43,694 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:11:48,258 - Epoch: [85][   10/   71]    Overall Loss 0.167408    Objective Loss 0.167408                                        LR 0.000250    Time 0.456287    
2023-09-11 14:11:50,843 - Epoch: [85][   20/   71]    Overall Loss 0.182627    Objective Loss 0.182627                                        LR 0.000250    Time 0.357389    
2023-09-11 14:11:53,318 - Epoch: [85][   30/   71]    Overall Loss 0.180798    Objective Loss 0.180798                                        LR 0.000250    Time 0.320742    
2023-09-11 14:11:55,688 - Epoch: [85][   40/   71]    Overall Loss 0.178201    Objective Loss 0.178201                                        LR 0.000250    Time 0.299820    
2023-09-11 14:11:58,683 - Epoch: [85][   50/   71]    Overall Loss 0.178461    Objective Loss 0.178461                                        LR 0.000250    Time 0.299749    
2023-09-11 14:12:01,251 - Epoch: [85][   60/   71]    Overall Loss 0.178835    Objective Loss 0.178835                                        LR 0.000250    Time 0.292572    
2023-09-11 14:12:03,617 - Epoch: [85][   70/   71]    Overall Loss 0.177598    Objective Loss 0.177598    Top1 91.796875    LR 0.000250    Time 0.284580    
2023-09-11 14:12:03,689 - Epoch: [85][   71/   71]    Overall Loss 0.177759    Objective Loss 0.177759    Top1 91.071429    LR 0.000250    Time 0.281575    
2023-09-11 14:12:03,781 - --- validate (epoch=85)-----------
2023-09-11 14:12:03,782 - 2000 samples (256 per mini-batch)
2023-09-11 14:12:06,797 - Epoch: [85][    8/    8]    Loss 0.194900    Top1 91.600000    
2023-09-11 14:12:06,894 - ==> Top1: 91.600    Loss: 0.195

2023-09-11 14:12:06,895 - ==> Confusion:
[[901  84]
 [ 84 931]]

2023-09-11 14:12:06,897 - ==> Best [Top1: 92.300   Sparsity:0.00   Params: 57776 on epoch: 84]
2023-09-11 14:12:06,897 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:12:06,900 - 

2023-09-11 14:12:06,900 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:12:10,151 - Epoch: [86][   10/   71]    Overall Loss 0.174243    Objective Loss 0.174243                                        LR 0.000250    Time 0.325089    
2023-09-11 14:12:13,443 - Epoch: [86][   20/   71]    Overall Loss 0.184667    Objective Loss 0.184667                                        LR 0.000250    Time 0.327135    
2023-09-11 14:12:16,167 - Epoch: [86][   30/   71]    Overall Loss 0.184026    Objective Loss 0.184026                                        LR 0.000250    Time 0.308869    
2023-09-11 14:12:18,668 - Epoch: [86][   40/   71]    Overall Loss 0.179885    Objective Loss 0.179885                                        LR 0.000250    Time 0.294157    
2023-09-11 14:12:22,122 - Epoch: [86][   50/   71]    Overall Loss 0.174214    Objective Loss 0.174214                                        LR 0.000250    Time 0.304409    
2023-09-11 14:12:24,312 - Epoch: [86][   60/   71]    Overall Loss 0.170317    Objective Loss 0.170317                                        LR 0.000250    Time 0.290165    
2023-09-11 14:12:26,951 - Epoch: [86][   70/   71]    Overall Loss 0.171514    Objective Loss 0.171514    Top1 92.187500    LR 0.000250    Time 0.286409    
2023-09-11 14:12:27,073 - Epoch: [86][   71/   71]    Overall Loss 0.172297    Objective Loss 0.172297    Top1 91.369048    LR 0.000250    Time 0.284097    
2023-09-11 14:12:27,175 - --- validate (epoch=86)-----------
2023-09-11 14:12:27,176 - 2000 samples (256 per mini-batch)
2023-09-11 14:12:30,276 - Epoch: [86][    8/    8]    Loss 0.194908    Top1 91.650000    
2023-09-11 14:12:30,373 - ==> Top1: 91.650    Loss: 0.195

2023-09-11 14:12:30,373 - ==> Confusion:
[[895  90]
 [ 77 938]]

2023-09-11 14:12:30,375 - ==> Best [Top1: 92.300   Sparsity:0.00   Params: 57776 on epoch: 84]
2023-09-11 14:12:30,375 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:12:30,380 - 

2023-09-11 14:12:30,380 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:12:33,664 - Epoch: [87][   10/   71]    Overall Loss 0.169480    Objective Loss 0.169480                                        LR 0.000250    Time 0.328315    
2023-09-11 14:12:36,317 - Epoch: [87][   20/   71]    Overall Loss 0.170471    Objective Loss 0.170471                                        LR 0.000250    Time 0.296786    
2023-09-11 14:12:38,702 - Epoch: [87][   30/   71]    Overall Loss 0.174471    Objective Loss 0.174471                                        LR 0.000250    Time 0.277374    
2023-09-11 14:12:42,415 - Epoch: [87][   40/   71]    Overall Loss 0.171517    Objective Loss 0.171517                                        LR 0.000250    Time 0.300840    
2023-09-11 14:12:44,731 - Epoch: [87][   50/   71]    Overall Loss 0.169939    Objective Loss 0.169939                                        LR 0.000250    Time 0.286979    
2023-09-11 14:12:47,843 - Epoch: [87][   60/   71]    Overall Loss 0.170427    Objective Loss 0.170427                                        LR 0.000250    Time 0.291006    
2023-09-11 14:12:50,401 - Epoch: [87][   70/   71]    Overall Loss 0.170594    Objective Loss 0.170594    Top1 91.406250    LR 0.000250    Time 0.285974    
2023-09-11 14:12:50,491 - Epoch: [87][   71/   71]    Overall Loss 0.170287    Objective Loss 0.170287    Top1 91.666667    LR 0.000250    Time 0.283211    
2023-09-11 14:12:50,591 - --- validate (epoch=87)-----------
2023-09-11 14:12:50,591 - 2000 samples (256 per mini-batch)
2023-09-11 14:12:53,355 - Epoch: [87][    8/    8]    Loss 0.210209    Top1 91.050000    
2023-09-11 14:12:53,451 - ==> Top1: 91.050    Loss: 0.210

2023-09-11 14:12:53,451 - ==> Confusion:
[[867 118]
 [ 61 954]]

2023-09-11 14:12:53,466 - ==> Best [Top1: 92.300   Sparsity:0.00   Params: 57776 on epoch: 84]
2023-09-11 14:12:53,466 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:12:53,469 - 

2023-09-11 14:12:53,469 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:12:58,470 - Epoch: [88][   10/   71]    Overall Loss 0.179149    Objective Loss 0.179149                                        LR 0.000250    Time 0.500069    
2023-09-11 14:13:00,554 - Epoch: [88][   20/   71]    Overall Loss 0.178031    Objective Loss 0.178031                                        LR 0.000250    Time 0.354199    
2023-09-11 14:13:03,345 - Epoch: [88][   30/   71]    Overall Loss 0.174576    Objective Loss 0.174576                                        LR 0.000250    Time 0.329175    
2023-09-11 14:13:05,387 - Epoch: [88][   40/   71]    Overall Loss 0.177559    Objective Loss 0.177559                                        LR 0.000250    Time 0.297904    
2023-09-11 14:13:08,269 - Epoch: [88][   50/   71]    Overall Loss 0.175757    Objective Loss 0.175757                                        LR 0.000250    Time 0.295965    
2023-09-11 14:13:10,673 - Epoch: [88][   60/   71]    Overall Loss 0.174484    Objective Loss 0.174484                                        LR 0.000250    Time 0.286704    
2023-09-11 14:13:13,042 - Epoch: [88][   70/   71]    Overall Loss 0.172377    Objective Loss 0.172377    Top1 94.921875    LR 0.000250    Time 0.279572    
2023-09-11 14:13:13,089 - Epoch: [88][   71/   71]    Overall Loss 0.172759    Objective Loss 0.172759    Top1 94.940476    LR 0.000250    Time 0.276299    
2023-09-11 14:13:13,185 - --- validate (epoch=88)-----------
2023-09-11 14:13:13,185 - 2000 samples (256 per mini-batch)
2023-09-11 14:13:16,310 - Epoch: [88][    8/    8]    Loss 0.188833    Top1 91.900000    
2023-09-11 14:13:16,402 - ==> Top1: 91.900    Loss: 0.189

2023-09-11 14:13:16,403 - ==> Confusion:
[[898  87]
 [ 75 940]]

2023-09-11 14:13:16,419 - ==> Best [Top1: 92.300   Sparsity:0.00   Params: 57776 on epoch: 84]
2023-09-11 14:13:16,419 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:13:16,421 - 

2023-09-11 14:13:16,421 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:13:20,186 - Epoch: [89][   10/   71]    Overall Loss 0.166576    Objective Loss 0.166576                                        LR 0.000250    Time 0.376377    
2023-09-11 14:13:22,244 - Epoch: [89][   20/   71]    Overall Loss 0.167335    Objective Loss 0.167335                                        LR 0.000250    Time 0.291079    
2023-09-11 14:13:24,987 - Epoch: [89][   30/   71]    Overall Loss 0.169125    Objective Loss 0.169125                                        LR 0.000250    Time 0.285489    
2023-09-11 14:13:27,054 - Epoch: [89][   40/   71]    Overall Loss 0.172338    Objective Loss 0.172338                                        LR 0.000250    Time 0.265765    
2023-09-11 14:13:30,762 - Epoch: [89][   50/   71]    Overall Loss 0.172367    Objective Loss 0.172367                                        LR 0.000250    Time 0.286769    
2023-09-11 14:13:32,785 - Epoch: [89][   60/   71]    Overall Loss 0.171878    Objective Loss 0.171878                                        LR 0.000250    Time 0.272690    
2023-09-11 14:13:35,211 - Epoch: [89][   70/   71]    Overall Loss 0.173665    Objective Loss 0.173665    Top1 93.359375    LR 0.000250    Time 0.268388    
2023-09-11 14:13:35,291 - Epoch: [89][   71/   71]    Overall Loss 0.173720    Objective Loss 0.173720    Top1 93.154762    LR 0.000250    Time 0.265733    
2023-09-11 14:13:35,388 - --- validate (epoch=89)-----------
2023-09-11 14:13:35,388 - 2000 samples (256 per mini-batch)
2023-09-11 14:13:37,960 - Epoch: [89][    8/    8]    Loss 0.189567    Top1 91.750000    
2023-09-11 14:13:38,051 - ==> Top1: 91.750    Loss: 0.190

2023-09-11 14:13:38,052 - ==> Confusion:
[[916  69]
 [ 96 919]]

2023-09-11 14:13:38,067 - ==> Best [Top1: 92.300   Sparsity:0.00   Params: 57776 on epoch: 84]
2023-09-11 14:13:38,067 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:13:38,070 - 

2023-09-11 14:13:38,070 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:13:41,268 - Epoch: [90][   10/   71]    Overall Loss 0.161454    Objective Loss 0.161454                                        LR 0.000250    Time 0.319800    
2023-09-11 14:13:44,234 - Epoch: [90][   20/   71]    Overall Loss 0.167940    Objective Loss 0.167940                                        LR 0.000250    Time 0.308177    
2023-09-11 14:13:46,346 - Epoch: [90][   30/   71]    Overall Loss 0.167092    Objective Loss 0.167092                                        LR 0.000250    Time 0.275857    
2023-09-11 14:13:49,784 - Epoch: [90][   40/   71]    Overall Loss 0.167773    Objective Loss 0.167773                                        LR 0.000250    Time 0.292824    
2023-09-11 14:13:53,007 - Epoch: [90][   50/   71]    Overall Loss 0.168747    Objective Loss 0.168747                                        LR 0.000250    Time 0.298709    
2023-09-11 14:13:55,048 - Epoch: [90][   60/   71]    Overall Loss 0.168229    Objective Loss 0.168229                                        LR 0.000250    Time 0.282930    
2023-09-11 14:13:57,562 - Epoch: [90][   70/   71]    Overall Loss 0.171689    Objective Loss 0.171689    Top1 91.796875    LR 0.000250    Time 0.278435    
2023-09-11 14:13:57,642 - Epoch: [90][   71/   71]    Overall Loss 0.172089    Objective Loss 0.172089    Top1 91.666667    LR 0.000250    Time 0.275631    
2023-09-11 14:13:57,733 - --- validate (epoch=90)-----------
2023-09-11 14:13:57,734 - 2000 samples (256 per mini-batch)
2023-09-11 14:14:00,313 - Epoch: [90][    8/    8]    Loss 0.246620    Top1 89.650000    
2023-09-11 14:14:00,418 - ==> Top1: 89.650    Loss: 0.247

2023-09-11 14:14:00,418 - ==> Confusion:
[[948  37]
 [170 845]]

2023-09-11 14:14:00,434 - ==> Best [Top1: 92.300   Sparsity:0.00   Params: 57776 on epoch: 84]
2023-09-11 14:14:00,434 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:14:00,437 - 

2023-09-11 14:14:00,437 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:14:05,421 - Epoch: [91][   10/   71]    Overall Loss 0.191089    Objective Loss 0.191089                                        LR 0.000250    Time 0.498407    
2023-09-11 14:14:07,700 - Epoch: [91][   20/   71]    Overall Loss 0.190392    Objective Loss 0.190392                                        LR 0.000250    Time 0.363139    
2023-09-11 14:14:10,928 - Epoch: [91][   30/   71]    Overall Loss 0.186018    Objective Loss 0.186018                                        LR 0.000250    Time 0.349661    
2023-09-11 14:14:13,817 - Epoch: [91][   40/   71]    Overall Loss 0.179970    Objective Loss 0.179970                                        LR 0.000250    Time 0.334467    
2023-09-11 14:14:16,536 - Epoch: [91][   50/   71]    Overall Loss 0.179278    Objective Loss 0.179278                                        LR 0.000250    Time 0.321943    
2023-09-11 14:14:19,328 - Epoch: [91][   60/   71]    Overall Loss 0.176434    Objective Loss 0.176434                                        LR 0.000250    Time 0.314814    
2023-09-11 14:14:21,251 - Epoch: [91][   70/   71]    Overall Loss 0.176318    Objective Loss 0.176318    Top1 93.359375    LR 0.000250    Time 0.297315    
2023-09-11 14:14:21,336 - Epoch: [91][   71/   71]    Overall Loss 0.175632    Objective Loss 0.175632    Top1 93.452381    LR 0.000250    Time 0.294311    
2023-09-11 14:14:21,431 - --- validate (epoch=91)-----------
2023-09-11 14:14:21,431 - 2000 samples (256 per mini-batch)
2023-09-11 14:14:24,344 - Epoch: [91][    8/    8]    Loss 0.198640    Top1 91.250000    
2023-09-11 14:14:24,444 - ==> Top1: 91.250    Loss: 0.199

2023-09-11 14:14:24,444 - ==> Confusion:
[[917  68]
 [107 908]]

2023-09-11 14:14:24,459 - ==> Best [Top1: 92.300   Sparsity:0.00   Params: 57776 on epoch: 84]
2023-09-11 14:14:24,459 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:14:24,462 - 

2023-09-11 14:14:24,462 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:14:27,666 - Epoch: [92][   10/   71]    Overall Loss 0.172365    Objective Loss 0.172365                                        LR 0.000250    Time 0.320323    
2023-09-11 14:14:31,475 - Epoch: [92][   20/   71]    Overall Loss 0.171160    Objective Loss 0.171160                                        LR 0.000250    Time 0.350642    
2023-09-11 14:14:34,053 - Epoch: [92][   30/   71]    Overall Loss 0.167699    Objective Loss 0.167699                                        LR 0.000250    Time 0.319678    
2023-09-11 14:14:37,057 - Epoch: [92][   40/   71]    Overall Loss 0.166092    Objective Loss 0.166092                                        LR 0.000250    Time 0.314838    
2023-09-11 14:14:39,215 - Epoch: [92][   50/   71]    Overall Loss 0.165662    Objective Loss 0.165662                                        LR 0.000250    Time 0.295023    
2023-09-11 14:14:41,873 - Epoch: [92][   60/   71]    Overall Loss 0.165927    Objective Loss 0.165927                                        LR 0.000250    Time 0.290150    
2023-09-11 14:14:43,942 - Epoch: [92][   70/   71]    Overall Loss 0.166586    Objective Loss 0.166586    Top1 93.750000    LR 0.000250    Time 0.278247    
2023-09-11 14:14:44,019 - Epoch: [92][   71/   71]    Overall Loss 0.166952    Objective Loss 0.166952    Top1 92.559524    LR 0.000250    Time 0.275421    
2023-09-11 14:14:44,111 - --- validate (epoch=92)-----------
2023-09-11 14:14:44,112 - 2000 samples (256 per mini-batch)
2023-09-11 14:14:47,472 - Epoch: [92][    8/    8]    Loss 0.196228    Top1 91.700000    
2023-09-11 14:14:47,575 - ==> Top1: 91.700    Loss: 0.196

2023-09-11 14:14:47,575 - ==> Confusion:
[[878 107]
 [ 59 956]]

2023-09-11 14:14:47,591 - ==> Best [Top1: 92.300   Sparsity:0.00   Params: 57776 on epoch: 84]
2023-09-11 14:14:47,591 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:14:47,594 - 

2023-09-11 14:14:47,594 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:14:51,853 - Epoch: [93][   10/   71]    Overall Loss 0.180006    Objective Loss 0.180006                                        LR 0.000250    Time 0.425942    
2023-09-11 14:14:54,538 - Epoch: [93][   20/   71]    Overall Loss 0.172435    Objective Loss 0.172435                                        LR 0.000250    Time 0.347173    
2023-09-11 14:14:56,761 - Epoch: [93][   30/   71]    Overall Loss 0.171962    Objective Loss 0.171962                                        LR 0.000250    Time 0.305546    
2023-09-11 14:14:59,415 - Epoch: [93][   40/   71]    Overall Loss 0.168024    Objective Loss 0.168024                                        LR 0.000250    Time 0.295492    
2023-09-11 14:15:01,623 - Epoch: [93][   50/   71]    Overall Loss 0.167567    Objective Loss 0.167567                                        LR 0.000250    Time 0.280543    
2023-09-11 14:15:04,289 - Epoch: [93][   60/   71]    Overall Loss 0.164762    Objective Loss 0.164762                                        LR 0.000250    Time 0.278227    
2023-09-11 14:15:06,233 - Epoch: [93][   70/   71]    Overall Loss 0.164611    Objective Loss 0.164611    Top1 95.703125    LR 0.000250    Time 0.266246    
2023-09-11 14:15:06,309 - Epoch: [93][   71/   71]    Overall Loss 0.164235    Objective Loss 0.164235    Top1 95.238095    LR 0.000250    Time 0.263561    
2023-09-11 14:15:06,411 - --- validate (epoch=93)-----------
2023-09-11 14:15:06,411 - 2000 samples (256 per mini-batch)
2023-09-11 14:15:09,553 - Epoch: [93][    8/    8]    Loss 0.210347    Top1 90.950000    
2023-09-11 14:15:09,658 - ==> Top1: 90.950    Loss: 0.210

2023-09-11 14:15:09,659 - ==> Confusion:
[[852 133]
 [ 48 967]]

2023-09-11 14:15:09,673 - ==> Best [Top1: 92.300   Sparsity:0.00   Params: 57776 on epoch: 84]
2023-09-11 14:15:09,674 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:15:09,678 - 

2023-09-11 14:15:09,678 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:15:14,266 - Epoch: [94][   10/   71]    Overall Loss 0.177586    Objective Loss 0.177586                                        LR 0.000250    Time 0.458744    
2023-09-11 14:15:16,228 - Epoch: [94][   20/   71]    Overall Loss 0.167646    Objective Loss 0.167646                                        LR 0.000250    Time 0.327475    
2023-09-11 14:15:19,611 - Epoch: [94][   30/   71]    Overall Loss 0.175110    Objective Loss 0.175110                                        LR 0.000250    Time 0.331057    
2023-09-11 14:15:22,241 - Epoch: [94][   40/   71]    Overall Loss 0.176991    Objective Loss 0.176991                                        LR 0.000250    Time 0.314024    
2023-09-11 14:15:24,586 - Epoch: [94][   50/   71]    Overall Loss 0.177384    Objective Loss 0.177384                                        LR 0.000250    Time 0.298116    
2023-09-11 14:15:27,639 - Epoch: [94][   60/   71]    Overall Loss 0.175930    Objective Loss 0.175930                                        LR 0.000250    Time 0.299307    
2023-09-11 14:15:29,653 - Epoch: [94][   70/   71]    Overall Loss 0.176024    Objective Loss 0.176024    Top1 92.968750    LR 0.000250    Time 0.285317    
2023-09-11 14:15:29,737 - Epoch: [94][   71/   71]    Overall Loss 0.175764    Objective Loss 0.175764    Top1 93.154762    LR 0.000250    Time 0.282482    
2023-09-11 14:15:29,836 - --- validate (epoch=94)-----------
2023-09-11 14:15:29,836 - 2000 samples (256 per mini-batch)
2023-09-11 14:15:32,673 - Epoch: [94][    8/    8]    Loss 0.193169    Top1 91.600000    
2023-09-11 14:15:32,772 - ==> Top1: 91.600    Loss: 0.193

2023-09-11 14:15:32,772 - ==> Confusion:
[[913  72]
 [ 96 919]]

2023-09-11 14:15:32,788 - ==> Best [Top1: 92.300   Sparsity:0.00   Params: 57776 on epoch: 84]
2023-09-11 14:15:32,788 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:15:32,790 - 

2023-09-11 14:15:32,791 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:15:36,370 - Epoch: [95][   10/   71]    Overall Loss 0.159713    Objective Loss 0.159713                                        LR 0.000250    Time 0.357888    
2023-09-11 14:15:38,712 - Epoch: [95][   20/   71]    Overall Loss 0.158096    Objective Loss 0.158096                                        LR 0.000250    Time 0.296047    
2023-09-11 14:15:41,455 - Epoch: [95][   30/   71]    Overall Loss 0.159871    Objective Loss 0.159871                                        LR 0.000250    Time 0.288785    
2023-09-11 14:15:43,522 - Epoch: [95][   40/   71]    Overall Loss 0.166333    Objective Loss 0.166333                                        LR 0.000250    Time 0.268247    
2023-09-11 14:15:46,255 - Epoch: [95][   50/   71]    Overall Loss 0.166696    Objective Loss 0.166696                                        LR 0.000250    Time 0.269244    
2023-09-11 14:15:49,384 - Epoch: [95][   60/   71]    Overall Loss 0.167923    Objective Loss 0.167923                                        LR 0.000250    Time 0.276514    
2023-09-11 14:15:51,799 - Epoch: [95][   70/   71]    Overall Loss 0.171762    Objective Loss 0.171762    Top1 89.453125    LR 0.000250    Time 0.271514    
2023-09-11 14:15:51,882 - Epoch: [95][   71/   71]    Overall Loss 0.173798    Objective Loss 0.173798    Top1 88.988095    LR 0.000250    Time 0.268863    
2023-09-11 14:15:51,984 - --- validate (epoch=95)-----------
2023-09-11 14:15:51,985 - 2000 samples (256 per mini-batch)
2023-09-11 14:15:54,605 - Epoch: [95][    8/    8]    Loss 0.210212    Top1 91.200000    
2023-09-11 14:15:54,705 - ==> Top1: 91.200    Loss: 0.210

2023-09-11 14:15:54,705 - ==> Confusion:
[[951  34]
 [142 873]]

2023-09-11 14:15:54,706 - ==> Best [Top1: 92.300   Sparsity:0.00   Params: 57776 on epoch: 84]
2023-09-11 14:15:54,706 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:15:54,711 - 

2023-09-11 14:15:54,711 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:15:58,701 - Epoch: [96][   10/   71]    Overall Loss 0.167610    Objective Loss 0.167610                                        LR 0.000250    Time 0.398873    
2023-09-11 14:16:01,519 - Epoch: [96][   20/   71]    Overall Loss 0.163180    Objective Loss 0.163180                                        LR 0.000250    Time 0.340349    
2023-09-11 14:16:04,035 - Epoch: [96][   30/   71]    Overall Loss 0.168081    Objective Loss 0.168081                                        LR 0.000250    Time 0.310737    
2023-09-11 14:16:07,382 - Epoch: [96][   40/   71]    Overall Loss 0.170437    Objective Loss 0.170437                                        LR 0.000250    Time 0.316735    
2023-09-11 14:16:09,525 - Epoch: [96][   50/   71]    Overall Loss 0.168819    Objective Loss 0.168819                                        LR 0.000250    Time 0.296234    
2023-09-11 14:16:12,332 - Epoch: [96][   60/   71]    Overall Loss 0.167712    Objective Loss 0.167712                                        LR 0.000250    Time 0.293639    
2023-09-11 14:16:14,599 - Epoch: [96][   70/   71]    Overall Loss 0.169499    Objective Loss 0.169499    Top1 94.140625    LR 0.000250    Time 0.284071    
2023-09-11 14:16:14,699 - Epoch: [96][   71/   71]    Overall Loss 0.170389    Objective Loss 0.170389    Top1 93.154762    LR 0.000250    Time 0.281479    
2023-09-11 14:16:14,793 - --- validate (epoch=96)-----------
2023-09-11 14:16:14,793 - 2000 samples (256 per mini-batch)
2023-09-11 14:16:18,069 - Epoch: [96][    8/    8]    Loss 0.200266    Top1 91.300000    
2023-09-11 14:16:18,161 - ==> Top1: 91.300    Loss: 0.200

2023-09-11 14:16:18,161 - ==> Confusion:
[[907  78]
 [ 96 919]]

2023-09-11 14:16:18,175 - ==> Best [Top1: 92.300   Sparsity:0.00   Params: 57776 on epoch: 84]
2023-09-11 14:16:18,175 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:16:18,180 - 

2023-09-11 14:16:18,180 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:16:23,239 - Epoch: [97][   10/   71]    Overall Loss 0.168783    Objective Loss 0.168783                                        LR 0.000250    Time 0.505807    
2023-09-11 14:16:25,324 - Epoch: [97][   20/   71]    Overall Loss 0.176425    Objective Loss 0.176425                                        LR 0.000250    Time 0.357156    
2023-09-11 14:16:28,169 - Epoch: [97][   30/   71]    Overall Loss 0.173409    Objective Loss 0.173409                                        LR 0.000250    Time 0.332932    
2023-09-11 14:16:30,470 - Epoch: [97][   40/   71]    Overall Loss 0.171410    Objective Loss 0.171410                                        LR 0.000250    Time 0.307223    
2023-09-11 14:16:33,026 - Epoch: [97][   50/   71]    Overall Loss 0.172385    Objective Loss 0.172385                                        LR 0.000250    Time 0.296889    
2023-09-11 14:16:35,798 - Epoch: [97][   60/   71]    Overall Loss 0.169745    Objective Loss 0.169745                                        LR 0.000250    Time 0.293595    
2023-09-11 14:16:37,863 - Epoch: [97][   70/   71]    Overall Loss 0.166689    Objective Loss 0.166689    Top1 92.187500    LR 0.000250    Time 0.281150    
2023-09-11 14:16:37,945 - Epoch: [97][   71/   71]    Overall Loss 0.167816    Objective Loss 0.167816    Top1 90.773810    LR 0.000250    Time 0.278347    
2023-09-11 14:16:38,053 - --- validate (epoch=97)-----------
2023-09-11 14:16:38,053 - 2000 samples (256 per mini-batch)
2023-09-11 14:16:41,124 - Epoch: [97][    8/    8]    Loss 0.189017    Top1 92.250000    
2023-09-11 14:16:41,219 - ==> Top1: 92.250    Loss: 0.189

2023-09-11 14:16:41,219 - ==> Confusion:
[[895  90]
 [ 65 950]]

2023-09-11 14:16:41,234 - ==> Best [Top1: 92.300   Sparsity:0.00   Params: 57776 on epoch: 84]
2023-09-11 14:16:41,234 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:16:41,236 - 

2023-09-11 14:16:41,237 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:16:45,754 - Epoch: [98][   10/   71]    Overall Loss 0.162816    Objective Loss 0.162816                                        LR 0.000250    Time 0.451734    
2023-09-11 14:16:47,774 - Epoch: [98][   20/   71]    Overall Loss 0.168544    Objective Loss 0.168544                                        LR 0.000250    Time 0.326831    
2023-09-11 14:16:50,929 - Epoch: [98][   30/   71]    Overall Loss 0.167241    Objective Loss 0.167241                                        LR 0.000250    Time 0.323054    
2023-09-11 14:16:52,955 - Epoch: [98][   40/   71]    Overall Loss 0.163767    Objective Loss 0.163767                                        LR 0.000250    Time 0.292937    
2023-09-11 14:16:55,840 - Epoch: [98][   50/   71]    Overall Loss 0.164618    Objective Loss 0.164618                                        LR 0.000250    Time 0.292042    
2023-09-11 14:16:57,943 - Epoch: [98][   60/   71]    Overall Loss 0.163995    Objective Loss 0.163995                                        LR 0.000250    Time 0.278409    
2023-09-11 14:17:00,308 - Epoch: [98][   70/   71]    Overall Loss 0.166320    Objective Loss 0.166320    Top1 90.234375    LR 0.000250    Time 0.272411    
2023-09-11 14:17:00,418 - Epoch: [98][   71/   71]    Overall Loss 0.166477    Objective Loss 0.166477    Top1 89.583333    LR 0.000250    Time 0.270122    
2023-09-11 14:17:00,526 - --- validate (epoch=98)-----------
2023-09-11 14:17:00,526 - 2000 samples (256 per mini-batch)
2023-09-11 14:17:03,760 - Epoch: [98][    8/    8]    Loss 0.204048    Top1 91.600000    
2023-09-11 14:17:03,853 - ==> Top1: 91.600    Loss: 0.204

2023-09-11 14:17:03,853 - ==> Confusion:
[[939  46]
 [122 893]]

2023-09-11 14:17:03,868 - ==> Best [Top1: 92.300   Sparsity:0.00   Params: 57776 on epoch: 84]
2023-09-11 14:17:03,869 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:17:03,873 - 

2023-09-11 14:17:03,873 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:17:07,766 - Epoch: [99][   10/   71]    Overall Loss 0.184852    Objective Loss 0.184852                                        LR 0.000250    Time 0.389240    
2023-09-11 14:17:10,809 - Epoch: [99][   20/   71]    Overall Loss 0.174011    Objective Loss 0.174011                                        LR 0.000250    Time 0.346735    
2023-09-11 14:17:12,805 - Epoch: [99][   30/   71]    Overall Loss 0.170335    Objective Loss 0.170335                                        LR 0.000250    Time 0.297681    
2023-09-11 14:17:15,829 - Epoch: [99][   40/   71]    Overall Loss 0.172405    Objective Loss 0.172405                                        LR 0.000250    Time 0.298847    
2023-09-11 14:17:17,904 - Epoch: [99][   50/   71]    Overall Loss 0.170454    Objective Loss 0.170454                                        LR 0.000250    Time 0.280575    
2023-09-11 14:17:20,678 - Epoch: [99][   60/   71]    Overall Loss 0.169586    Objective Loss 0.169586                                        LR 0.000250    Time 0.280044    
2023-09-11 14:17:22,523 - Epoch: [99][   70/   71]    Overall Loss 0.167114    Objective Loss 0.167114    Top1 95.312500    LR 0.000250    Time 0.266379    
2023-09-11 14:17:22,574 - Epoch: [99][   71/   71]    Overall Loss 0.166522    Objective Loss 0.166522    Top1 94.940476    LR 0.000250    Time 0.263345    
2023-09-11 14:17:22,677 - --- validate (epoch=99)-----------
2023-09-11 14:17:22,678 - 2000 samples (256 per mini-batch)
2023-09-11 14:17:26,116 - Epoch: [99][    8/    8]    Loss 0.204708    Top1 91.050000    
2023-09-11 14:17:26,214 - ==> Top1: 91.050    Loss: 0.205

2023-09-11 14:17:26,214 - ==> Confusion:
[[860 125]
 [ 54 961]]

2023-09-11 14:17:26,229 - ==> Best [Top1: 92.300   Sparsity:0.00   Params: 57776 on epoch: 84]
2023-09-11 14:17:26,229 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:17:26,234 - 

2023-09-11 14:17:26,234 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:17:30,223 - Epoch: [100][   10/   71]    Overall Loss 0.156231    Objective Loss 0.156231                                        LR 0.000250    Time 0.398900    
2023-09-11 14:17:33,094 - Epoch: [100][   20/   71]    Overall Loss 0.163714    Objective Loss 0.163714                                        LR 0.000250    Time 0.342976    
2023-09-11 14:17:36,343 - Epoch: [100][   30/   71]    Overall Loss 0.163452    Objective Loss 0.163452                                        LR 0.000250    Time 0.336919    
2023-09-11 14:17:38,985 - Epoch: [100][   40/   71]    Overall Loss 0.161386    Objective Loss 0.161386                                        LR 0.000250    Time 0.318743    
2023-09-11 14:17:41,673 - Epoch: [100][   50/   71]    Overall Loss 0.162253    Objective Loss 0.162253                                        LR 0.000250    Time 0.308749    
2023-09-11 14:17:45,049 - Epoch: [100][   60/   71]    Overall Loss 0.161087    Objective Loss 0.161087                                        LR 0.000250    Time 0.313548    
2023-09-11 14:17:47,489 - Epoch: [100][   70/   71]    Overall Loss 0.161462    Objective Loss 0.161462    Top1 93.750000    LR 0.000250    Time 0.303610    
2023-09-11 14:17:47,569 - Epoch: [100][   71/   71]    Overall Loss 0.161992    Objective Loss 0.161992    Top1 93.452381    LR 0.000250    Time 0.300463    
2023-09-11 14:17:47,668 - --- validate (epoch=100)-----------
2023-09-11 14:17:47,668 - 2000 samples (256 per mini-batch)
2023-09-11 14:17:50,807 - Epoch: [100][    8/    8]    Loss 0.185727    Top1 92.800000    
2023-09-11 14:17:50,902 - ==> Top1: 92.800    Loss: 0.186

2023-09-11 14:17:50,902 - ==> Confusion:
[[921  64]
 [ 80 935]]

2023-09-11 14:17:50,918 - ==> Best [Top1: 92.800   Sparsity:0.00   Params: 57776 on epoch: 100]
2023-09-11 14:17:50,918 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:17:50,921 - 

2023-09-11 14:17:50,921 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:17:55,178 - Epoch: [101][   10/   71]    Overall Loss 0.156850    Objective Loss 0.156850                                        LR 0.000250    Time 0.425591    
2023-09-11 14:17:57,469 - Epoch: [101][   20/   71]    Overall Loss 0.154226    Objective Loss 0.154226                                        LR 0.000250    Time 0.327335    
2023-09-11 14:18:00,465 - Epoch: [101][   30/   71]    Overall Loss 0.156152    Objective Loss 0.156152                                        LR 0.000250    Time 0.318095    
2023-09-11 14:18:02,444 - Epoch: [101][   40/   71]    Overall Loss 0.158698    Objective Loss 0.158698                                        LR 0.000250    Time 0.288022    
2023-09-11 14:18:05,184 - Epoch: [101][   50/   71]    Overall Loss 0.157699    Objective Loss 0.157699                                        LR 0.000250    Time 0.285218    
2023-09-11 14:18:07,277 - Epoch: [101][   60/   71]    Overall Loss 0.156583    Objective Loss 0.156583                                        LR 0.000250    Time 0.272565    
2023-09-11 14:18:09,915 - Epoch: [101][   70/   71]    Overall Loss 0.158038    Objective Loss 0.158038    Top1 91.406250    LR 0.000250    Time 0.271304    
2023-09-11 14:18:10,040 - Epoch: [101][   71/   71]    Overall Loss 0.157563    Objective Loss 0.157563    Top1 91.964286    LR 0.000250    Time 0.269245    
2023-09-11 14:18:10,143 - --- validate (epoch=101)-----------
2023-09-11 14:18:10,143 - 2000 samples (256 per mini-batch)
2023-09-11 14:18:13,017 - Epoch: [101][    8/    8]    Loss 0.206197    Top1 91.600000    
2023-09-11 14:18:13,117 - ==> Top1: 91.600    Loss: 0.206

2023-09-11 14:18:13,118 - ==> Confusion:
[[873 112]
 [ 56 959]]

2023-09-11 14:18:13,133 - ==> Best [Top1: 92.800   Sparsity:0.00   Params: 57776 on epoch: 100]
2023-09-11 14:18:13,133 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:18:13,136 - 

2023-09-11 14:18:13,136 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:18:17,916 - Epoch: [102][   10/   71]    Overall Loss 0.154106    Objective Loss 0.154106                                        LR 0.000250    Time 0.477956    
2023-09-11 14:18:19,923 - Epoch: [102][   20/   71]    Overall Loss 0.159820    Objective Loss 0.159820                                        LR 0.000250    Time 0.339308    
2023-09-11 14:18:23,444 - Epoch: [102][   30/   71]    Overall Loss 0.154715    Objective Loss 0.154715                                        LR 0.000250    Time 0.343550    
2023-09-11 14:18:25,515 - Epoch: [102][   40/   71]    Overall Loss 0.155759    Objective Loss 0.155759                                        LR 0.000250    Time 0.309447    
2023-09-11 14:18:28,842 - Epoch: [102][   50/   71]    Overall Loss 0.157523    Objective Loss 0.157523                                        LR 0.000250    Time 0.314094    
2023-09-11 14:18:30,907 - Epoch: [102][   60/   71]    Overall Loss 0.158726    Objective Loss 0.158726                                        LR 0.000250    Time 0.296155    
2023-09-11 14:18:34,011 - Epoch: [102][   70/   71]    Overall Loss 0.159022    Objective Loss 0.159022    Top1 94.921875    LR 0.000250    Time 0.298183    
2023-09-11 14:18:34,079 - Epoch: [102][   71/   71]    Overall Loss 0.159669    Objective Loss 0.159669    Top1 93.750000    LR 0.000250    Time 0.294932    
2023-09-11 14:18:34,187 - --- validate (epoch=102)-----------
2023-09-11 14:18:34,187 - 2000 samples (256 per mini-batch)
2023-09-11 14:18:37,417 - Epoch: [102][    8/    8]    Loss 0.182677    Top1 92.200000    
2023-09-11 14:18:37,517 - ==> Top1: 92.200    Loss: 0.183

2023-09-11 14:18:37,517 - ==> Confusion:
[[927  58]
 [ 98 917]]

2023-09-11 14:18:37,521 - ==> Best [Top1: 92.800   Sparsity:0.00   Params: 57776 on epoch: 100]
2023-09-11 14:18:37,521 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:18:37,524 - 

2023-09-11 14:18:37,524 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:18:40,928 - Epoch: [103][   10/   71]    Overall Loss 0.162412    Objective Loss 0.162412                                        LR 0.000250    Time 0.340391    
2023-09-11 14:18:43,275 - Epoch: [103][   20/   71]    Overall Loss 0.158648    Objective Loss 0.158648                                        LR 0.000250    Time 0.287517    
2023-09-11 14:18:46,157 - Epoch: [103][   30/   71]    Overall Loss 0.161908    Objective Loss 0.161908                                        LR 0.000250    Time 0.287743    
2023-09-11 14:18:49,232 - Epoch: [103][   40/   71]    Overall Loss 0.162365    Objective Loss 0.162365                                        LR 0.000250    Time 0.292664    
2023-09-11 14:18:51,461 - Epoch: [103][   50/   71]    Overall Loss 0.160468    Objective Loss 0.160468                                        LR 0.000250    Time 0.278699    
2023-09-11 14:18:54,662 - Epoch: [103][   60/   71]    Overall Loss 0.161803    Objective Loss 0.161803                                        LR 0.000250    Time 0.285605    
2023-09-11 14:18:56,668 - Epoch: [103][   70/   71]    Overall Loss 0.160846    Objective Loss 0.160846    Top1 91.796875    LR 0.000250    Time 0.273456    
2023-09-11 14:18:56,776 - Epoch: [103][   71/   71]    Overall Loss 0.160586    Objective Loss 0.160586    Top1 92.857143    LR 0.000250    Time 0.271116    
2023-09-11 14:18:56,871 - --- validate (epoch=103)-----------
2023-09-11 14:18:56,872 - 2000 samples (256 per mini-batch)
2023-09-11 14:19:00,086 - Epoch: [103][    8/    8]    Loss 0.218527    Top1 91.400000    
2023-09-11 14:19:00,184 - ==> Top1: 91.400    Loss: 0.219

2023-09-11 14:19:00,184 - ==> Confusion:
[[926  59]
 [113 902]]

2023-09-11 14:19:00,199 - ==> Best [Top1: 92.800   Sparsity:0.00   Params: 57776 on epoch: 100]
2023-09-11 14:19:00,199 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:19:00,201 - 

2023-09-11 14:19:00,202 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:19:03,798 - Epoch: [104][   10/   71]    Overall Loss 0.172566    Objective Loss 0.172566                                        LR 0.000250    Time 0.359571    
2023-09-11 14:19:05,779 - Epoch: [104][   20/   71]    Overall Loss 0.175436    Objective Loss 0.175436                                        LR 0.000250    Time 0.278811    
2023-09-11 14:19:08,552 - Epoch: [104][   30/   71]    Overall Loss 0.175752    Objective Loss 0.175752                                        LR 0.000250    Time 0.278327    
2023-09-11 14:19:10,626 - Epoch: [104][   40/   71]    Overall Loss 0.178091    Objective Loss 0.178091                                        LR 0.000250    Time 0.260577    
2023-09-11 14:19:13,439 - Epoch: [104][   50/   71]    Overall Loss 0.180067    Objective Loss 0.180067                                        LR 0.000250    Time 0.264718    
2023-09-11 14:19:16,719 - Epoch: [104][   60/   71]    Overall Loss 0.176319    Objective Loss 0.176319                                        LR 0.000250    Time 0.275252    
2023-09-11 14:19:19,070 - Epoch: [104][   70/   71]    Overall Loss 0.172017    Objective Loss 0.172017    Top1 93.750000    LR 0.000250    Time 0.269518    
2023-09-11 14:19:19,118 - Epoch: [104][   71/   71]    Overall Loss 0.171663    Objective Loss 0.171663    Top1 93.750000    LR 0.000250    Time 0.266394    
2023-09-11 14:19:19,204 - --- validate (epoch=104)-----------
2023-09-11 14:19:19,204 - 2000 samples (256 per mini-batch)
2023-09-11 14:19:22,168 - Epoch: [104][    8/    8]    Loss 0.191922    Top1 92.150000    
2023-09-11 14:19:22,266 - ==> Top1: 92.150    Loss: 0.192

2023-09-11 14:19:22,266 - ==> Confusion:
[[905  80]
 [ 77 938]]

2023-09-11 14:19:22,268 - ==> Best [Top1: 92.800   Sparsity:0.00   Params: 57776 on epoch: 100]
2023-09-11 14:19:22,268 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:19:22,273 - 

2023-09-11 14:19:22,273 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:19:26,132 - Epoch: [105][   10/   71]    Overall Loss 0.159154    Objective Loss 0.159154                                        LR 0.000250    Time 0.385879    
2023-09-11 14:19:28,342 - Epoch: [105][   20/   71]    Overall Loss 0.170387    Objective Loss 0.170387                                        LR 0.000250    Time 0.303428    
2023-09-11 14:19:31,124 - Epoch: [105][   30/   71]    Overall Loss 0.166630    Objective Loss 0.166630                                        LR 0.000250    Time 0.294988    
2023-09-11 14:19:33,683 - Epoch: [105][   40/   71]    Overall Loss 0.161344    Objective Loss 0.161344                                        LR 0.000250    Time 0.285207    
2023-09-11 14:19:37,314 - Epoch: [105][   50/   71]    Overall Loss 0.163882    Objective Loss 0.163882                                        LR 0.000250    Time 0.300781    
2023-09-11 14:19:39,823 - Epoch: [105][   60/   71]    Overall Loss 0.166138    Objective Loss 0.166138                                        LR 0.000250    Time 0.292465    
2023-09-11 14:19:41,790 - Epoch: [105][   70/   71]    Overall Loss 0.166402    Objective Loss 0.166402    Top1 94.531250    LR 0.000250    Time 0.278774    
2023-09-11 14:19:41,884 - Epoch: [105][   71/   71]    Overall Loss 0.165853    Objective Loss 0.165853    Top1 94.642857    LR 0.000250    Time 0.276174    
2023-09-11 14:19:41,979 - --- validate (epoch=105)-----------
2023-09-11 14:19:41,979 - 2000 samples (256 per mini-batch)
2023-09-11 14:19:45,122 - Epoch: [105][    8/    8]    Loss 0.171953    Top1 93.400000    
2023-09-11 14:19:45,225 - ==> Top1: 93.400    Loss: 0.172

2023-09-11 14:19:45,225 - ==> Confusion:
[[914  71]
 [ 61 954]]

2023-09-11 14:19:45,241 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:19:45,241 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:19:45,246 - 

2023-09-11 14:19:45,247 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:19:49,572 - Epoch: [106][   10/   71]    Overall Loss 0.145345    Objective Loss 0.145345                                        LR 0.000250    Time 0.432542    
2023-09-11 14:19:51,623 - Epoch: [106][   20/   71]    Overall Loss 0.157191    Objective Loss 0.157191                                        LR 0.000250    Time 0.318779    
2023-09-11 14:19:54,571 - Epoch: [106][   30/   71]    Overall Loss 0.157885    Objective Loss 0.157885                                        LR 0.000250    Time 0.310763    
2023-09-11 14:19:56,673 - Epoch: [106][   40/   71]    Overall Loss 0.156442    Objective Loss 0.156442                                        LR 0.000250    Time 0.285632    
2023-09-11 14:19:59,727 - Epoch: [106][   50/   71]    Overall Loss 0.157988    Objective Loss 0.157988                                        LR 0.000250    Time 0.289568    
2023-09-11 14:20:02,372 - Epoch: [106][   60/   71]    Overall Loss 0.158815    Objective Loss 0.158815                                        LR 0.000250    Time 0.285390    
2023-09-11 14:20:05,191 - Epoch: [106][   70/   71]    Overall Loss 0.156338    Objective Loss 0.156338    Top1 94.140625    LR 0.000250    Time 0.284879    
2023-09-11 14:20:05,244 - Epoch: [106][   71/   71]    Overall Loss 0.156162    Objective Loss 0.156162    Top1 94.047619    LR 0.000250    Time 0.281609    
2023-09-11 14:20:05,341 - --- validate (epoch=106)-----------
2023-09-11 14:20:05,341 - 2000 samples (256 per mini-batch)
2023-09-11 14:20:08,541 - Epoch: [106][    8/    8]    Loss 0.177933    Top1 92.350000    
2023-09-11 14:20:08,640 - ==> Top1: 92.350    Loss: 0.178

2023-09-11 14:20:08,640 - ==> Confusion:
[[893  92]
 [ 61 954]]

2023-09-11 14:20:08,644 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:20:08,644 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:20:08,647 - 

2023-09-11 14:20:08,647 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:20:13,112 - Epoch: [107][   10/   71]    Overall Loss 0.145079    Objective Loss 0.145079                                        LR 0.000250    Time 0.446379    
2023-09-11 14:20:15,186 - Epoch: [107][   20/   71]    Overall Loss 0.164126    Objective Loss 0.164126                                        LR 0.000250    Time 0.326894    
2023-09-11 14:20:18,451 - Epoch: [107][   30/   71]    Overall Loss 0.161951    Objective Loss 0.161951                                        LR 0.000250    Time 0.326754    
2023-09-11 14:20:21,022 - Epoch: [107][   40/   71]    Overall Loss 0.160587    Objective Loss 0.160587                                        LR 0.000250    Time 0.309332    
2023-09-11 14:20:23,666 - Epoch: [107][   50/   71]    Overall Loss 0.160446    Objective Loss 0.160446                                        LR 0.000250    Time 0.300329    
2023-09-11 14:20:26,492 - Epoch: [107][   60/   71]    Overall Loss 0.159784    Objective Loss 0.159784                                        LR 0.000250    Time 0.297371    
2023-09-11 14:20:28,836 - Epoch: [107][   70/   71]    Overall Loss 0.159109    Objective Loss 0.159109    Top1 93.750000    LR 0.000250    Time 0.288377    
2023-09-11 14:20:28,878 - Epoch: [107][   71/   71]    Overall Loss 0.159157    Objective Loss 0.159157    Top1 93.750000    LR 0.000250    Time 0.284903    
2023-09-11 14:20:28,976 - --- validate (epoch=107)-----------
2023-09-11 14:20:28,976 - 2000 samples (256 per mini-batch)
2023-09-11 14:20:31,494 - Epoch: [107][    8/    8]    Loss 0.180029    Top1 92.600000    
2023-09-11 14:20:31,593 - ==> Top1: 92.600    Loss: 0.180

2023-09-11 14:20:31,594 - ==> Confusion:
[[888  97]
 [ 51 964]]

2023-09-11 14:20:31,595 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:20:31,595 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:20:31,600 - 

2023-09-11 14:20:31,600 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:20:36,742 - Epoch: [108][   10/   71]    Overall Loss 0.156600    Objective Loss 0.156600                                        LR 0.000250    Time 0.514130    
2023-09-11 14:20:38,937 - Epoch: [108][   20/   71]    Overall Loss 0.164639    Objective Loss 0.164639                                        LR 0.000250    Time 0.366836    
2023-09-11 14:20:41,768 - Epoch: [108][   30/   71]    Overall Loss 0.161595    Objective Loss 0.161595                                        LR 0.000250    Time 0.338904    
2023-09-11 14:20:44,087 - Epoch: [108][   40/   71]    Overall Loss 0.160896    Objective Loss 0.160896                                        LR 0.000250    Time 0.312134    
2023-09-11 14:20:46,945 - Epoch: [108][   50/   71]    Overall Loss 0.159346    Objective Loss 0.159346                                        LR 0.000250    Time 0.306861    
2023-09-11 14:20:49,072 - Epoch: [108][   60/   71]    Overall Loss 0.159753    Objective Loss 0.159753                                        LR 0.000250    Time 0.291168    
2023-09-11 14:20:51,415 - Epoch: [108][   70/   71]    Overall Loss 0.159803    Objective Loss 0.159803    Top1 91.796875    LR 0.000250    Time 0.283037    
2023-09-11 14:20:51,520 - Epoch: [108][   71/   71]    Overall Loss 0.159666    Objective Loss 0.159666    Top1 91.964286    LR 0.000250    Time 0.280526    
2023-09-11 14:20:51,625 - --- validate (epoch=108)-----------
2023-09-11 14:20:51,625 - 2000 samples (256 per mini-batch)
2023-09-11 14:20:54,684 - Epoch: [108][    8/    8]    Loss 0.188573    Top1 92.000000    
2023-09-11 14:20:54,791 - ==> Top1: 92.000    Loss: 0.189

2023-09-11 14:20:54,791 - ==> Confusion:
[[895  90]
 [ 70 945]]

2023-09-11 14:20:54,806 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:20:54,806 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:20:54,808 - 

2023-09-11 14:20:54,809 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:20:58,725 - Epoch: [109][   10/   71]    Overall Loss 0.138860    Objective Loss 0.138860                                        LR 0.000250    Time 0.391557    
2023-09-11 14:21:02,147 - Epoch: [109][   20/   71]    Overall Loss 0.150663    Objective Loss 0.150663                                        LR 0.000250    Time 0.366900    
2023-09-11 14:21:04,240 - Epoch: [109][   30/   71]    Overall Loss 0.151050    Objective Loss 0.151050                                        LR 0.000250    Time 0.314343    
2023-09-11 14:21:07,602 - Epoch: [109][   40/   71]    Overall Loss 0.155698    Objective Loss 0.155698                                        LR 0.000250    Time 0.319789    
2023-09-11 14:21:09,712 - Epoch: [109][   50/   71]    Overall Loss 0.155425    Objective Loss 0.155425                                        LR 0.000250    Time 0.298039    
2023-09-11 14:21:12,773 - Epoch: [109][   60/   71]    Overall Loss 0.157343    Objective Loss 0.157343                                        LR 0.000250    Time 0.299374    
2023-09-11 14:21:15,142 - Epoch: [109][   70/   71]    Overall Loss 0.155515    Objective Loss 0.155515    Top1 94.531250    LR 0.000250    Time 0.290452    
2023-09-11 14:21:15,232 - Epoch: [109][   71/   71]    Overall Loss 0.155382    Objective Loss 0.155382    Top1 94.940476    LR 0.000250    Time 0.287615    
2023-09-11 14:21:15,331 - --- validate (epoch=109)-----------
2023-09-11 14:21:15,332 - 2000 samples (256 per mini-batch)
2023-09-11 14:21:18,437 - Epoch: [109][    8/    8]    Loss 0.182867    Top1 91.900000    
2023-09-11 14:21:18,556 - ==> Top1: 91.900    Loss: 0.183

2023-09-11 14:21:18,556 - ==> Confusion:
[[905  80]
 [ 82 933]]

2023-09-11 14:21:18,572 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:21:18,572 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:21:18,577 - 

2023-09-11 14:21:18,577 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:21:23,414 - Epoch: [110][   10/   71]    Overall Loss 0.146526    Objective Loss 0.146526                                        LR 0.000250    Time 0.483640    
2023-09-11 14:21:25,508 - Epoch: [110][   20/   71]    Overall Loss 0.156178    Objective Loss 0.156178                                        LR 0.000250    Time 0.346506    
2023-09-11 14:21:28,581 - Epoch: [110][   30/   71]    Overall Loss 0.153178    Objective Loss 0.153178                                        LR 0.000250    Time 0.333429    
2023-09-11 14:21:30,653 - Epoch: [110][   40/   71]    Overall Loss 0.153481    Objective Loss 0.153481                                        LR 0.000250    Time 0.301863    
2023-09-11 14:21:33,555 - Epoch: [110][   50/   71]    Overall Loss 0.153613    Objective Loss 0.153613                                        LR 0.000250    Time 0.299521    
2023-09-11 14:21:35,728 - Epoch: [110][   60/   71]    Overall Loss 0.153514    Objective Loss 0.153514                                        LR 0.000250    Time 0.285816    
2023-09-11 14:21:38,587 - Epoch: [110][   70/   71]    Overall Loss 0.155230    Objective Loss 0.155230    Top1 95.312500    LR 0.000250    Time 0.285823    
2023-09-11 14:21:38,664 - Epoch: [110][   71/   71]    Overall Loss 0.154238    Objective Loss 0.154238    Top1 95.535714    LR 0.000250    Time 0.282880    
2023-09-11 14:21:38,763 - --- validate (epoch=110)-----------
2023-09-11 14:21:38,763 - 2000 samples (256 per mini-batch)
2023-09-11 14:21:41,919 - Epoch: [110][    8/    8]    Loss 0.192630    Top1 92.250000    
2023-09-11 14:21:42,015 - ==> Top1: 92.250    Loss: 0.193

2023-09-11 14:21:42,015 - ==> Confusion:
[[910  75]
 [ 80 935]]

2023-09-11 14:21:42,031 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:21:42,031 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:21:42,034 - 

2023-09-11 14:21:42,034 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:21:45,271 - Epoch: [111][   10/   71]    Overall Loss 0.154457    Objective Loss 0.154457                                        LR 0.000250    Time 0.323700    
2023-09-11 14:21:48,159 - Epoch: [111][   20/   71]    Overall Loss 0.158500    Objective Loss 0.158500                                        LR 0.000250    Time 0.306227    
2023-09-11 14:21:50,968 - Epoch: [111][   30/   71]    Overall Loss 0.158053    Objective Loss 0.158053                                        LR 0.000250    Time 0.297743    
2023-09-11 14:21:53,024 - Epoch: [111][   40/   71]    Overall Loss 0.159179    Objective Loss 0.159179                                        LR 0.000250    Time 0.274719    
2023-09-11 14:21:55,809 - Epoch: [111][   50/   71]    Overall Loss 0.160190    Objective Loss 0.160190                                        LR 0.000250    Time 0.275454    
2023-09-11 14:21:58,737 - Epoch: [111][   60/   71]    Overall Loss 0.158338    Objective Loss 0.158338                                        LR 0.000250    Time 0.278340    
2023-09-11 14:22:01,271 - Epoch: [111][   70/   71]    Overall Loss 0.159379    Objective Loss 0.159379    Top1 90.625000    LR 0.000250    Time 0.274786    
2023-09-11 14:22:01,395 - Epoch: [111][   71/   71]    Overall Loss 0.160600    Objective Loss 0.160600    Top1 90.178571    LR 0.000250    Time 0.272660    
2023-09-11 14:22:01,491 - --- validate (epoch=111)-----------
2023-09-11 14:22:01,492 - 2000 samples (256 per mini-batch)
2023-09-11 14:22:04,273 - Epoch: [111][    8/    8]    Loss 0.187871    Top1 92.450000    
2023-09-11 14:22:04,373 - ==> Top1: 92.450    Loss: 0.188

2023-09-11 14:22:04,373 - ==> Confusion:
[[924  61]
 [ 90 925]]

2023-09-11 14:22:04,386 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:22:04,387 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:22:04,391 - 

2023-09-11 14:22:04,391 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:22:08,195 - Epoch: [112][   10/   71]    Overall Loss 0.162705    Objective Loss 0.162705                                        LR 0.000250    Time 0.380333    
2023-09-11 14:22:11,752 - Epoch: [112][   20/   71]    Overall Loss 0.159159    Objective Loss 0.159159                                        LR 0.000250    Time 0.368011    
2023-09-11 14:22:13,854 - Epoch: [112][   30/   71]    Overall Loss 0.161378    Objective Loss 0.161378                                        LR 0.000250    Time 0.315389    
2023-09-11 14:22:16,358 - Epoch: [112][   40/   71]    Overall Loss 0.161000    Objective Loss 0.161000                                        LR 0.000250    Time 0.299131    
2023-09-11 14:22:19,110 - Epoch: [112][   50/   71]    Overall Loss 0.161352    Objective Loss 0.161352                                        LR 0.000250    Time 0.294335    
2023-09-11 14:22:22,233 - Epoch: [112][   60/   71]    Overall Loss 0.162280    Objective Loss 0.162280                                        LR 0.000250    Time 0.297330    
2023-09-11 14:22:24,888 - Epoch: [112][   70/   71]    Overall Loss 0.163199    Objective Loss 0.163199    Top1 96.093750    LR 0.000250    Time 0.292774    
2023-09-11 14:22:25,011 - Epoch: [112][   71/   71]    Overall Loss 0.163719    Objective Loss 0.163719    Top1 94.940476    LR 0.000250    Time 0.290382    
2023-09-11 14:22:25,113 - --- validate (epoch=112)-----------
2023-09-11 14:22:25,114 - 2000 samples (256 per mini-batch)
2023-09-11 14:22:28,184 - Epoch: [112][    8/    8]    Loss 0.183618    Top1 92.550000    
2023-09-11 14:22:28,285 - ==> Top1: 92.550    Loss: 0.184

2023-09-11 14:22:28,286 - ==> Confusion:
[[909  76]
 [ 73 942]]

2023-09-11 14:22:28,302 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:22:28,302 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:22:28,304 - 

2023-09-11 14:22:28,304 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:22:32,187 - Epoch: [113][   10/   71]    Overall Loss 0.157719    Objective Loss 0.157719                                        LR 0.000250    Time 0.388172    
2023-09-11 14:22:34,580 - Epoch: [113][   20/   71]    Overall Loss 0.152581    Objective Loss 0.152581                                        LR 0.000250    Time 0.313753    
2023-09-11 14:22:37,333 - Epoch: [113][   30/   71]    Overall Loss 0.147929    Objective Loss 0.147929                                        LR 0.000250    Time 0.300911    
2023-09-11 14:22:40,643 - Epoch: [113][   40/   71]    Overall Loss 0.151903    Objective Loss 0.151903                                        LR 0.000250    Time 0.308424    
2023-09-11 14:22:42,774 - Epoch: [113][   50/   71]    Overall Loss 0.150648    Objective Loss 0.150648                                        LR 0.000250    Time 0.289357    
2023-09-11 14:22:45,554 - Epoch: [113][   60/   71]    Overall Loss 0.155184    Objective Loss 0.155184                                        LR 0.000250    Time 0.287464    
2023-09-11 14:22:47,769 - Epoch: [113][   70/   71]    Overall Loss 0.155835    Objective Loss 0.155835    Top1 92.968750    LR 0.000250    Time 0.278038    
2023-09-11 14:22:47,877 - Epoch: [113][   71/   71]    Overall Loss 0.155112    Objective Loss 0.155112    Top1 93.750000    LR 0.000250    Time 0.275637    
2023-09-11 14:22:47,976 - --- validate (epoch=113)-----------
2023-09-11 14:22:47,977 - 2000 samples (256 per mini-batch)
2023-09-11 14:22:50,825 - Epoch: [113][    8/    8]    Loss 0.198310    Top1 91.500000    
2023-09-11 14:22:50,925 - ==> Top1: 91.500    Loss: 0.198

2023-09-11 14:22:50,925 - ==> Confusion:
[[925  60]
 [110 905]]

2023-09-11 14:22:50,941 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:22:50,941 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:22:50,945 - 

2023-09-11 14:22:50,945 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:22:54,450 - Epoch: [114][   10/   71]    Overall Loss 0.142089    Objective Loss 0.142089                                        LR 0.000250    Time 0.350382    
2023-09-11 14:22:56,900 - Epoch: [114][   20/   71]    Overall Loss 0.148716    Objective Loss 0.148716                                        LR 0.000250    Time 0.297707    
2023-09-11 14:22:59,151 - Epoch: [114][   30/   71]    Overall Loss 0.154499    Objective Loss 0.154499                                        LR 0.000250    Time 0.273480    
2023-09-11 14:23:02,225 - Epoch: [114][   40/   71]    Overall Loss 0.153955    Objective Loss 0.153955                                        LR 0.000250    Time 0.281947    
2023-09-11 14:23:05,573 - Epoch: [114][   50/   71]    Overall Loss 0.154632    Objective Loss 0.154632                                        LR 0.000250    Time 0.292515    
2023-09-11 14:23:07,906 - Epoch: [114][   60/   71]    Overall Loss 0.151727    Objective Loss 0.151727                                        LR 0.000250    Time 0.282650    
2023-09-11 14:23:10,556 - Epoch: [114][   70/   71]    Overall Loss 0.153685    Objective Loss 0.153685    Top1 88.281250    LR 0.000250    Time 0.280124    
2023-09-11 14:23:10,676 - Epoch: [114][   71/   71]    Overall Loss 0.153362    Objective Loss 0.153362    Top1 89.583333    LR 0.000250    Time 0.277864    
2023-09-11 14:23:10,768 - --- validate (epoch=114)-----------
2023-09-11 14:23:10,768 - 2000 samples (256 per mini-batch)
2023-09-11 14:23:13,240 - Epoch: [114][    8/    8]    Loss 0.217304    Top1 90.900000    
2023-09-11 14:23:13,339 - ==> Top1: 90.900    Loss: 0.217

2023-09-11 14:23:13,340 - ==> Confusion:
[[943  42]
 [140 875]]

2023-09-11 14:23:13,345 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:23:13,345 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:23:13,348 - 

2023-09-11 14:23:13,348 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:23:17,207 - Epoch: [115][   10/   71]    Overall Loss 0.153090    Objective Loss 0.153090                                        LR 0.000250    Time 0.385915    
2023-09-11 14:23:20,282 - Epoch: [115][   20/   71]    Overall Loss 0.165374    Objective Loss 0.165374                                        LR 0.000250    Time 0.346697    
2023-09-11 14:23:22,272 - Epoch: [115][   30/   71]    Overall Loss 0.164887    Objective Loss 0.164887                                        LR 0.000250    Time 0.297433    
2023-09-11 14:23:24,986 - Epoch: [115][   40/   71]    Overall Loss 0.160846    Objective Loss 0.160846                                        LR 0.000250    Time 0.290928    
2023-09-11 14:23:27,839 - Epoch: [115][   50/   71]    Overall Loss 0.160231    Objective Loss 0.160231                                        LR 0.000250    Time 0.289780    
2023-09-11 14:23:30,667 - Epoch: [115][   60/   71]    Overall Loss 0.161147    Objective Loss 0.161147                                        LR 0.000250    Time 0.288623    
2023-09-11 14:23:32,519 - Epoch: [115][   70/   71]    Overall Loss 0.161751    Objective Loss 0.161751    Top1 94.921875    LR 0.000250    Time 0.273843    
2023-09-11 14:23:32,597 - Epoch: [115][   71/   71]    Overall Loss 0.161790    Objective Loss 0.161790    Top1 94.345238    LR 0.000250    Time 0.271074    
2023-09-11 14:23:32,697 - --- validate (epoch=115)-----------
2023-09-11 14:23:32,697 - 2000 samples (256 per mini-batch)
2023-09-11 14:23:35,769 - Epoch: [115][    8/    8]    Loss 0.182288    Top1 91.800000    
2023-09-11 14:23:35,868 - ==> Top1: 91.800    Loss: 0.182

2023-09-11 14:23:35,869 - ==> Confusion:
[[914  71]
 [ 93 922]]

2023-09-11 14:23:35,883 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:23:35,883 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:23:35,886 - 

2023-09-11 14:23:35,886 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:23:40,531 - Epoch: [116][   10/   71]    Overall Loss 0.138990    Objective Loss 0.138990                                        LR 0.000250    Time 0.464511    
2023-09-11 14:23:42,669 - Epoch: [116][   20/   71]    Overall Loss 0.158642    Objective Loss 0.158642                                        LR 0.000250    Time 0.339139    
2023-09-11 14:23:45,689 - Epoch: [116][   30/   71]    Overall Loss 0.154690    Objective Loss 0.154690                                        LR 0.000250    Time 0.326750    
2023-09-11 14:23:48,768 - Epoch: [116][   40/   71]    Overall Loss 0.154020    Objective Loss 0.154020                                        LR 0.000250    Time 0.322020    
2023-09-11 14:23:50,948 - Epoch: [116][   50/   71]    Overall Loss 0.155154    Objective Loss 0.155154                                        LR 0.000250    Time 0.301215    
2023-09-11 14:23:53,545 - Epoch: [116][   60/   71]    Overall Loss 0.156255    Objective Loss 0.156255                                        LR 0.000250    Time 0.294286    
2023-09-11 14:23:55,403 - Epoch: [116][   70/   71]    Overall Loss 0.156868    Objective Loss 0.156868    Top1 93.750000    LR 0.000250    Time 0.278786    
2023-09-11 14:23:55,461 - Epoch: [116][   71/   71]    Overall Loss 0.158173    Objective Loss 0.158173    Top1 92.857143    LR 0.000250    Time 0.275666    
2023-09-11 14:23:55,551 - --- validate (epoch=116)-----------
2023-09-11 14:23:55,551 - 2000 samples (256 per mini-batch)
2023-09-11 14:23:58,648 - Epoch: [116][    8/    8]    Loss 0.205931    Top1 91.200000    
2023-09-11 14:23:58,747 - ==> Top1: 91.200    Loss: 0.206

2023-09-11 14:23:58,747 - ==> Confusion:
[[872 113]
 [ 63 952]]

2023-09-11 14:23:58,763 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:23:58,764 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:23:58,766 - 

2023-09-11 14:23:58,766 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:24:03,279 - Epoch: [117][   10/   71]    Overall Loss 0.138909    Objective Loss 0.138909                                        LR 0.000250    Time 0.451268    
2023-09-11 14:24:05,330 - Epoch: [117][   20/   71]    Overall Loss 0.148037    Objective Loss 0.148037                                        LR 0.000250    Time 0.328171    
2023-09-11 14:24:07,976 - Epoch: [117][   30/   71]    Overall Loss 0.146152    Objective Loss 0.146152                                        LR 0.000250    Time 0.306953    
2023-09-11 14:24:11,319 - Epoch: [117][   40/   71]    Overall Loss 0.148480    Objective Loss 0.148480                                        LR 0.000250    Time 0.313789    
2023-09-11 14:24:14,015 - Epoch: [117][   50/   71]    Overall Loss 0.149088    Objective Loss 0.149088                                        LR 0.000250    Time 0.304941    
2023-09-11 14:24:16,346 - Epoch: [117][   60/   71]    Overall Loss 0.149498    Objective Loss 0.149498                                        LR 0.000250    Time 0.292967    
2023-09-11 14:24:18,668 - Epoch: [117][   70/   71]    Overall Loss 0.149300    Objective Loss 0.149300    Top1 96.093750    LR 0.000250    Time 0.284278    
2023-09-11 14:24:18,760 - Epoch: [117][   71/   71]    Overall Loss 0.149729    Objective Loss 0.149729    Top1 95.535714    LR 0.000250    Time 0.281562    
2023-09-11 14:24:18,847 - --- validate (epoch=117)-----------
2023-09-11 14:24:18,847 - 2000 samples (256 per mini-batch)
2023-09-11 14:24:21,987 - Epoch: [117][    8/    8]    Loss 0.182746    Top1 92.500000    
2023-09-11 14:24:22,092 - ==> Top1: 92.500    Loss: 0.183

2023-09-11 14:24:22,092 - ==> Confusion:
[[928  57]
 [ 93 922]]

2023-09-11 14:24:22,108 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:24:22,108 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:24:22,112 - 

2023-09-11 14:24:22,113 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:24:26,460 - Epoch: [118][   10/   71]    Overall Loss 0.136541    Objective Loss 0.136541                                        LR 0.000250    Time 0.434664    
2023-09-11 14:24:28,529 - Epoch: [118][   20/   71]    Overall Loss 0.143755    Objective Loss 0.143755                                        LR 0.000250    Time 0.320767    
2023-09-11 14:24:31,693 - Epoch: [118][   30/   71]    Overall Loss 0.143971    Objective Loss 0.143971                                        LR 0.000250    Time 0.319312    
2023-09-11 14:24:34,227 - Epoch: [118][   40/   71]    Overall Loss 0.148381    Objective Loss 0.148381                                        LR 0.000250    Time 0.302809    
2023-09-11 14:24:36,566 - Epoch: [118][   50/   71]    Overall Loss 0.151889    Objective Loss 0.151889                                        LR 0.000250    Time 0.289030    
2023-09-11 14:24:39,012 - Epoch: [118][   60/   71]    Overall Loss 0.152913    Objective Loss 0.152913                                        LR 0.000250    Time 0.281612    
2023-09-11 14:24:41,083 - Epoch: [118][   70/   71]    Overall Loss 0.154050    Objective Loss 0.154050    Top1 94.531250    LR 0.000250    Time 0.270977    
2023-09-11 14:24:41,160 - Epoch: [118][   71/   71]    Overall Loss 0.155474    Objective Loss 0.155474    Top1 93.154762    LR 0.000250    Time 0.268240    
2023-09-11 14:24:41,258 - --- validate (epoch=118)-----------
2023-09-11 14:24:41,258 - 2000 samples (256 per mini-batch)
2023-09-11 14:24:43,811 - Epoch: [118][    8/    8]    Loss 0.193220    Top1 91.700000    
2023-09-11 14:24:43,906 - ==> Top1: 91.700    Loss: 0.193

2023-09-11 14:24:43,907 - ==> Confusion:
[[907  78]
 [ 88 927]]

2023-09-11 14:24:43,922 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:24:43,922 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:24:43,924 - 

2023-09-11 14:24:43,924 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:24:48,445 - Epoch: [119][   10/   71]    Overall Loss 0.143458    Objective Loss 0.143458                                        LR 0.000250    Time 0.452039    
2023-09-11 14:24:50,502 - Epoch: [119][   20/   71]    Overall Loss 0.148373    Objective Loss 0.148373                                        LR 0.000250    Time 0.328819    
2023-09-11 14:24:53,490 - Epoch: [119][   30/   71]    Overall Loss 0.152654    Objective Loss 0.152654                                        LR 0.000250    Time 0.318825    
2023-09-11 14:24:56,081 - Epoch: [119][   40/   71]    Overall Loss 0.155670    Objective Loss 0.155670                                        LR 0.000250    Time 0.303888    
2023-09-11 14:24:59,009 - Epoch: [119][   50/   71]    Overall Loss 0.151970    Objective Loss 0.151970                                        LR 0.000250    Time 0.301663    
2023-09-11 14:25:01,519 - Epoch: [119][   60/   71]    Overall Loss 0.154865    Objective Loss 0.154865                                        LR 0.000250    Time 0.293203    
2023-09-11 14:25:04,066 - Epoch: [119][   70/   71]    Overall Loss 0.154402    Objective Loss 0.154402    Top1 94.921875    LR 0.000250    Time 0.287699    
2023-09-11 14:25:04,142 - Epoch: [119][   71/   71]    Overall Loss 0.153025    Objective Loss 0.153025    Top1 95.535714    LR 0.000250    Time 0.284722    
2023-09-11 14:25:04,253 - --- validate (epoch=119)-----------
2023-09-11 14:25:04,253 - 2000 samples (256 per mini-batch)
2023-09-11 14:25:06,684 - Epoch: [119][    8/    8]    Loss 0.190002    Top1 91.950000    
2023-09-11 14:25:06,791 - ==> Top1: 91.950    Loss: 0.190

2023-09-11 14:25:06,791 - ==> Confusion:
[[902  83]
 [ 78 937]]

2023-09-11 14:25:06,807 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:25:06,807 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:25:06,810 - 

2023-09-11 14:25:06,810 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:25:10,145 - Epoch: [120][   10/   71]    Overall Loss 0.158165    Objective Loss 0.158165                                        LR 0.000250    Time 0.333444    
2023-09-11 14:25:12,969 - Epoch: [120][   20/   71]    Overall Loss 0.155748    Objective Loss 0.155748                                        LR 0.000250    Time 0.307904    
2023-09-11 14:25:15,102 - Epoch: [120][   30/   71]    Overall Loss 0.152589    Objective Loss 0.152589                                        LR 0.000250    Time 0.276370    
2023-09-11 14:25:17,982 - Epoch: [120][   40/   71]    Overall Loss 0.151822    Objective Loss 0.151822                                        LR 0.000250    Time 0.279252    
2023-09-11 14:25:20,772 - Epoch: [120][   50/   71]    Overall Loss 0.151450    Objective Loss 0.151450                                        LR 0.000250    Time 0.279197    
2023-09-11 14:25:22,949 - Epoch: [120][   60/   71]    Overall Loss 0.151350    Objective Loss 0.151350                                        LR 0.000250    Time 0.268955    
2023-09-11 14:25:25,387 - Epoch: [120][   70/   71]    Overall Loss 0.151768    Objective Loss 0.151768    Top1 94.140625    LR 0.000250    Time 0.265348    
2023-09-11 14:25:25,432 - Epoch: [120][   71/   71]    Overall Loss 0.152307    Objective Loss 0.152307    Top1 93.750000    LR 0.000250    Time 0.262243    
2023-09-11 14:25:25,542 - --- validate (epoch=120)-----------
2023-09-11 14:25:25,542 - 2000 samples (256 per mini-batch)
2023-09-11 14:25:28,668 - Epoch: [120][    8/    8]    Loss 0.176358    Top1 93.350000    
2023-09-11 14:25:28,767 - ==> Top1: 93.350    Loss: 0.176

2023-09-11 14:25:28,767 - ==> Confusion:
[[931  54]
 [ 79 936]]

2023-09-11 14:25:28,782 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:25:28,782 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:25:28,784 - 

2023-09-11 14:25:28,784 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:25:32,725 - Epoch: [121][   10/   71]    Overall Loss 0.147763    Objective Loss 0.147763                                        LR 0.000250    Time 0.394072    
2023-09-11 14:25:35,356 - Epoch: [121][   20/   71]    Overall Loss 0.148104    Objective Loss 0.148104                                        LR 0.000250    Time 0.328562    
2023-09-11 14:25:37,380 - Epoch: [121][   30/   71]    Overall Loss 0.148219    Objective Loss 0.148219                                        LR 0.000250    Time 0.286481    
2023-09-11 14:25:40,656 - Epoch: [121][   40/   71]    Overall Loss 0.147496    Objective Loss 0.147496                                        LR 0.000250    Time 0.296770    
2023-09-11 14:25:43,915 - Epoch: [121][   50/   71]    Overall Loss 0.148962    Objective Loss 0.148962                                        LR 0.000250    Time 0.302577    
2023-09-11 14:25:47,096 - Epoch: [121][   60/   71]    Overall Loss 0.152575    Objective Loss 0.152575                                        LR 0.000250    Time 0.305168    
2023-09-11 14:25:49,233 - Epoch: [121][   70/   71]    Overall Loss 0.154188    Objective Loss 0.154188    Top1 94.921875    LR 0.000250    Time 0.292088    
2023-09-11 14:25:49,306 - Epoch: [121][   71/   71]    Overall Loss 0.153444    Objective Loss 0.153444    Top1 95.238095    LR 0.000250    Time 0.289001    
2023-09-11 14:25:49,399 - --- validate (epoch=121)-----------
2023-09-11 14:25:49,399 - 2000 samples (256 per mini-batch)
2023-09-11 14:25:52,387 - Epoch: [121][    8/    8]    Loss 0.181445    Top1 92.900000    
2023-09-11 14:25:52,492 - ==> Top1: 92.900    Loss: 0.181

2023-09-11 14:25:52,493 - ==> Confusion:
[[903  82]
 [ 60 955]]

2023-09-11 14:25:52,509 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:25:52,509 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:25:52,513 - 

2023-09-11 14:25:52,513 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:25:56,139 - Epoch: [122][   10/   71]    Overall Loss 0.159403    Objective Loss 0.159403                                        LR 0.000250    Time 0.362527    
2023-09-11 14:25:58,402 - Epoch: [122][   20/   71]    Overall Loss 0.155574    Objective Loss 0.155574                                        LR 0.000250    Time 0.294397    
2023-09-11 14:26:01,099 - Epoch: [122][   30/   71]    Overall Loss 0.155011    Objective Loss 0.155011                                        LR 0.000250    Time 0.286175    
2023-09-11 14:26:04,594 - Epoch: [122][   40/   71]    Overall Loss 0.154934    Objective Loss 0.154934                                        LR 0.000250    Time 0.301995    
2023-09-11 14:26:06,698 - Epoch: [122][   50/   71]    Overall Loss 0.154435    Objective Loss 0.154435                                        LR 0.000250    Time 0.283671    
2023-09-11 14:26:09,396 - Epoch: [122][   60/   71]    Overall Loss 0.154462    Objective Loss 0.154462                                        LR 0.000250    Time 0.281352    
2023-09-11 14:26:11,330 - Epoch: [122][   70/   71]    Overall Loss 0.153412    Objective Loss 0.153412    Top1 94.921875    LR 0.000250    Time 0.268780    
2023-09-11 14:26:11,390 - Epoch: [122][   71/   71]    Overall Loss 0.153494    Objective Loss 0.153494    Top1 94.345238    LR 0.000250    Time 0.265833    
2023-09-11 14:26:11,488 - --- validate (epoch=122)-----------
2023-09-11 14:26:11,488 - 2000 samples (256 per mini-batch)
2023-09-11 14:26:14,508 - Epoch: [122][    8/    8]    Loss 0.172276    Top1 92.650000    
2023-09-11 14:26:14,606 - ==> Top1: 92.650    Loss: 0.172

2023-09-11 14:26:14,606 - ==> Confusion:
[[907  78]
 [ 69 946]]

2023-09-11 14:26:14,621 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:26:14,621 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:26:14,623 - 

2023-09-11 14:26:14,623 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:26:19,779 - Epoch: [123][   10/   71]    Overall Loss 0.145175    Objective Loss 0.145175                                        LR 0.000250    Time 0.515550    
2023-09-11 14:26:21,805 - Epoch: [123][   20/   71]    Overall Loss 0.146461    Objective Loss 0.146461                                        LR 0.000250    Time 0.359057    
2023-09-11 14:26:25,099 - Epoch: [123][   30/   71]    Overall Loss 0.148759    Objective Loss 0.148759                                        LR 0.000250    Time 0.348988    
2023-09-11 14:26:27,280 - Epoch: [123][   40/   71]    Overall Loss 0.149970    Objective Loss 0.149970                                        LR 0.000250    Time 0.316264    
2023-09-11 14:26:29,981 - Epoch: [123][   50/   71]    Overall Loss 0.150674    Objective Loss 0.150674                                        LR 0.000250    Time 0.307028    
2023-09-11 14:26:32,799 - Epoch: [123][   60/   71]    Overall Loss 0.149635    Objective Loss 0.149635                                        LR 0.000250    Time 0.302819    
2023-09-11 14:26:34,998 - Epoch: [123][   70/   71]    Overall Loss 0.149368    Objective Loss 0.149368    Top1 94.531250    LR 0.000250    Time 0.290970    
2023-09-11 14:26:35,130 - Epoch: [123][   71/   71]    Overall Loss 0.149840    Objective Loss 0.149840    Top1 94.345238    LR 0.000250    Time 0.288720    
2023-09-11 14:26:35,227 - --- validate (epoch=123)-----------
2023-09-11 14:26:35,227 - 2000 samples (256 per mini-batch)
2023-09-11 14:26:38,214 - Epoch: [123][    8/    8]    Loss 0.187360    Top1 91.500000    
2023-09-11 14:26:38,313 - ==> Top1: 91.500    Loss: 0.187

2023-09-11 14:26:38,313 - ==> Confusion:
[[905  80]
 [ 90 925]]

2023-09-11 14:26:38,328 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:26:38,328 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:26:38,330 - 

2023-09-11 14:26:38,330 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:26:41,812 - Epoch: [124][   10/   71]    Overall Loss 0.153550    Objective Loss 0.153550                                        LR 0.000250    Time 0.348103    
2023-09-11 14:26:45,586 - Epoch: [124][   20/   71]    Overall Loss 0.161497    Objective Loss 0.161497                                        LR 0.000250    Time 0.362750    
2023-09-11 14:26:47,782 - Epoch: [124][   30/   71]    Overall Loss 0.161224    Objective Loss 0.161224                                        LR 0.000250    Time 0.315013    
2023-09-11 14:26:50,475 - Epoch: [124][   40/   71]    Overall Loss 0.159362    Objective Loss 0.159362                                        LR 0.000250    Time 0.303582    
2023-09-11 14:26:52,907 - Epoch: [124][   50/   71]    Overall Loss 0.156961    Objective Loss 0.156961                                        LR 0.000250    Time 0.291492    
2023-09-11 14:26:55,522 - Epoch: [124][   60/   71]    Overall Loss 0.155239    Objective Loss 0.155239                                        LR 0.000250    Time 0.286494    
2023-09-11 14:26:57,473 - Epoch: [124][   70/   71]    Overall Loss 0.152487    Objective Loss 0.152487    Top1 92.578125    LR 0.000250    Time 0.273430    
2023-09-11 14:26:57,539 - Epoch: [124][   71/   71]    Overall Loss 0.151267    Objective Loss 0.151267    Top1 93.750000    LR 0.000250    Time 0.270511    
2023-09-11 14:26:57,632 - --- validate (epoch=124)-----------
2023-09-11 14:26:57,632 - 2000 samples (256 per mini-batch)
2023-09-11 14:27:00,719 - Epoch: [124][    8/    8]    Loss 0.178087    Top1 92.950000    
2023-09-11 14:27:00,818 - ==> Top1: 92.950    Loss: 0.178

2023-09-11 14:27:00,818 - ==> Confusion:
[[917  68]
 [ 73 942]]

2023-09-11 14:27:00,832 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:27:00,832 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:27:00,835 - 

2023-09-11 14:27:00,835 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:27:05,308 - Epoch: [125][   10/   71]    Overall Loss 0.146957    Objective Loss 0.146957                                        LR 0.000250    Time 0.447231    
2023-09-11 14:27:07,395 - Epoch: [125][   20/   71]    Overall Loss 0.148726    Objective Loss 0.148726                                        LR 0.000250    Time 0.327946    
2023-09-11 14:27:10,184 - Epoch: [125][   30/   71]    Overall Loss 0.150275    Objective Loss 0.150275                                        LR 0.000250    Time 0.311585    
2023-09-11 14:27:13,416 - Epoch: [125][   40/   71]    Overall Loss 0.153439    Objective Loss 0.153439                                        LR 0.000250    Time 0.314498    
2023-09-11 14:27:16,237 - Epoch: [125][   50/   71]    Overall Loss 0.154053    Objective Loss 0.154053                                        LR 0.000250    Time 0.308005    
2023-09-11 14:27:19,235 - Epoch: [125][   60/   71]    Overall Loss 0.151248    Objective Loss 0.151248                                        LR 0.000250    Time 0.306637    
2023-09-11 14:27:21,512 - Epoch: [125][   70/   71]    Overall Loss 0.153314    Objective Loss 0.153314    Top1 95.312500    LR 0.000250    Time 0.295348    
2023-09-11 14:27:21,593 - Epoch: [125][   71/   71]    Overall Loss 0.153839    Objective Loss 0.153839    Top1 94.345238    LR 0.000250    Time 0.292330    
2023-09-11 14:27:21,684 - --- validate (epoch=125)-----------
2023-09-11 14:27:21,684 - 2000 samples (256 per mini-batch)
2023-09-11 14:27:24,768 - Epoch: [125][    8/    8]    Loss 0.212439    Top1 91.200000    
2023-09-11 14:27:24,866 - ==> Top1: 91.200    Loss: 0.212

2023-09-11 14:27:24,866 - ==> Confusion:
[[852 133]
 [ 43 972]]

2023-09-11 14:27:24,883 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:27:24,883 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:27:24,888 - 

2023-09-11 14:27:24,888 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:27:28,180 - Epoch: [126][   10/   71]    Overall Loss 0.159838    Objective Loss 0.159838                                        LR 0.000250    Time 0.329214    
2023-09-11 14:27:31,786 - Epoch: [126][   20/   71]    Overall Loss 0.154857    Objective Loss 0.154857                                        LR 0.000250    Time 0.344889    
2023-09-11 14:27:33,863 - Epoch: [126][   30/   71]    Overall Loss 0.151944    Objective Loss 0.151944                                        LR 0.000250    Time 0.299120    
2023-09-11 14:27:36,715 - Epoch: [126][   40/   71]    Overall Loss 0.151772    Objective Loss 0.151772                                        LR 0.000250    Time 0.295652    
2023-09-11 14:27:38,923 - Epoch: [126][   50/   71]    Overall Loss 0.151669    Objective Loss 0.151669                                        LR 0.000250    Time 0.280665    
2023-09-11 14:27:42,280 - Epoch: [126][   60/   71]    Overall Loss 0.153885    Objective Loss 0.153885                                        LR 0.000250    Time 0.289832    
2023-09-11 14:27:44,987 - Epoch: [126][   70/   71]    Overall Loss 0.152757    Objective Loss 0.152757    Top1 94.921875    LR 0.000250    Time 0.287090    
2023-09-11 14:27:45,052 - Epoch: [126][   71/   71]    Overall Loss 0.153143    Objective Loss 0.153143    Top1 94.642857    LR 0.000250    Time 0.283969    
2023-09-11 14:27:45,144 - --- validate (epoch=126)-----------
2023-09-11 14:27:45,145 - 2000 samples (256 per mini-batch)
2023-09-11 14:27:48,224 - Epoch: [126][    8/    8]    Loss 0.173259    Top1 92.150000    
2023-09-11 14:27:48,324 - ==> Top1: 92.150    Loss: 0.173

2023-09-11 14:27:48,325 - ==> Confusion:
[[898  87]
 [ 70 945]]

2023-09-11 14:27:48,332 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:27:48,332 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:27:48,335 - 

2023-09-11 14:27:48,335 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:27:51,917 - Epoch: [127][   10/   71]    Overall Loss 0.156472    Objective Loss 0.156472                                        LR 0.000250    Time 0.358186    
2023-09-11 14:27:54,680 - Epoch: [127][   20/   71]    Overall Loss 0.158909    Objective Loss 0.158909                                        LR 0.000250    Time 0.317199    
2023-09-11 14:27:57,898 - Epoch: [127][   30/   71]    Overall Loss 0.157736    Objective Loss 0.157736                                        LR 0.000250    Time 0.318736    
2023-09-11 14:27:59,904 - Epoch: [127][   40/   71]    Overall Loss 0.156398    Objective Loss 0.156398                                        LR 0.000250    Time 0.289190    
2023-09-11 14:28:02,615 - Epoch: [127][   50/   71]    Overall Loss 0.154198    Objective Loss 0.154198                                        LR 0.000250    Time 0.285568    
2023-09-11 14:28:05,250 - Epoch: [127][   60/   71]    Overall Loss 0.151700    Objective Loss 0.151700                                        LR 0.000250    Time 0.281887    
2023-09-11 14:28:07,321 - Epoch: [127][   70/   71]    Overall Loss 0.152589    Objective Loss 0.152589    Top1 94.921875    LR 0.000250    Time 0.271199    
2023-09-11 14:28:07,398 - Epoch: [127][   71/   71]    Overall Loss 0.151469    Objective Loss 0.151469    Top1 95.535714    LR 0.000250    Time 0.268454    
2023-09-11 14:28:07,514 - --- validate (epoch=127)-----------
2023-09-11 14:28:07,514 - 2000 samples (256 per mini-batch)
2023-09-11 14:28:10,211 - Epoch: [127][    8/    8]    Loss 0.203122    Top1 91.650000    
2023-09-11 14:28:10,309 - ==> Top1: 91.650    Loss: 0.203

2023-09-11 14:28:10,309 - ==> Confusion:
[[925  60]
 [107 908]]

2023-09-11 14:28:10,325 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:28:10,325 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:28:10,327 - 

2023-09-11 14:28:10,328 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:28:14,639 - Epoch: [128][   10/   71]    Overall Loss 0.152323    Objective Loss 0.152323                                        LR 0.000250    Time 0.431095    
2023-09-11 14:28:16,637 - Epoch: [128][   20/   71]    Overall Loss 0.146072    Objective Loss 0.146072                                        LR 0.000250    Time 0.315449    
2023-09-11 14:28:19,132 - Epoch: [128][   30/   71]    Overall Loss 0.145345    Objective Loss 0.145345                                        LR 0.000250    Time 0.293457    
2023-09-11 14:28:21,393 - Epoch: [128][   40/   71]    Overall Loss 0.145795    Objective Loss 0.145795                                        LR 0.000250    Time 0.276598    
2023-09-11 14:28:24,776 - Epoch: [128][   50/   71]    Overall Loss 0.143390    Objective Loss 0.143390                                        LR 0.000250    Time 0.288941    
2023-09-11 14:28:26,924 - Epoch: [128][   60/   71]    Overall Loss 0.147056    Objective Loss 0.147056                                        LR 0.000250    Time 0.276578    
2023-09-11 14:28:29,257 - Epoch: [128][   70/   71]    Overall Loss 0.148932    Objective Loss 0.148932    Top1 94.921875    LR 0.000250    Time 0.270382    
2023-09-11 14:28:29,341 - Epoch: [128][   71/   71]    Overall Loss 0.147760    Objective Loss 0.147760    Top1 96.130952    LR 0.000250    Time 0.267753    
2023-09-11 14:28:29,437 - --- validate (epoch=128)-----------
2023-09-11 14:28:29,437 - 2000 samples (256 per mini-batch)
2023-09-11 14:28:32,611 - Epoch: [128][    8/    8]    Loss 0.172684    Top1 92.400000    
2023-09-11 14:28:32,711 - ==> Top1: 92.400    Loss: 0.173

2023-09-11 14:28:32,711 - ==> Confusion:
[[920  65]
 [ 87 928]]

2023-09-11 14:28:32,726 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:28:32,726 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:28:32,729 - 

2023-09-11 14:28:32,729 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:28:36,730 - Epoch: [129][   10/   71]    Overall Loss 0.143648    Objective Loss 0.143648                                        LR 0.000250    Time 0.400093    
2023-09-11 14:28:38,847 - Epoch: [129][   20/   71]    Overall Loss 0.143172    Objective Loss 0.143172                                        LR 0.000250    Time 0.305862    
2023-09-11 14:28:42,151 - Epoch: [129][   30/   71]    Overall Loss 0.146932    Objective Loss 0.146932                                        LR 0.000250    Time 0.314029    
2023-09-11 14:28:44,391 - Epoch: [129][   40/   71]    Overall Loss 0.145313    Objective Loss 0.145313                                        LR 0.000250    Time 0.291516    
2023-09-11 14:28:47,172 - Epoch: [129][   50/   71]    Overall Loss 0.144002    Objective Loss 0.144002                                        LR 0.000250    Time 0.288827    
2023-09-11 14:28:49,332 - Epoch: [129][   60/   71]    Overall Loss 0.144306    Objective Loss 0.144306                                        LR 0.000250    Time 0.276680    
2023-09-11 14:28:51,777 - Epoch: [129][   70/   71]    Overall Loss 0.143729    Objective Loss 0.143729    Top1 94.531250    LR 0.000250    Time 0.272077    
2023-09-11 14:28:51,841 - Epoch: [129][   71/   71]    Overall Loss 0.144571    Objective Loss 0.144571    Top1 93.750000    LR 0.000250    Time 0.269144    
2023-09-11 14:28:51,936 - --- validate (epoch=129)-----------
2023-09-11 14:28:51,937 - 2000 samples (256 per mini-batch)
2023-09-11 14:28:55,163 - Epoch: [129][    8/    8]    Loss 0.176960    Top1 92.950000    
2023-09-11 14:28:55,254 - ==> Top1: 92.950    Loss: 0.177

2023-09-11 14:28:55,254 - ==> Confusion:
[[910  75]
 [ 66 949]]

2023-09-11 14:28:55,270 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:28:55,271 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:28:55,273 - 

2023-09-11 14:28:55,273 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:28:59,767 - Epoch: [130][   10/   71]    Overall Loss 0.133989    Objective Loss 0.133989                                        LR 0.000250    Time 0.449328    
2023-09-11 14:29:01,864 - Epoch: [130][   20/   71]    Overall Loss 0.139110    Objective Loss 0.139110                                        LR 0.000250    Time 0.329508    
2023-09-11 14:29:04,431 - Epoch: [130][   30/   71]    Overall Loss 0.145812    Objective Loss 0.145812                                        LR 0.000250    Time 0.305207    
2023-09-11 14:29:06,811 - Epoch: [130][   40/   71]    Overall Loss 0.148222    Objective Loss 0.148222                                        LR 0.000250    Time 0.288416    
2023-09-11 14:29:09,411 - Epoch: [130][   50/   71]    Overall Loss 0.148935    Objective Loss 0.148935                                        LR 0.000250    Time 0.282722    
2023-09-11 14:29:12,589 - Epoch: [130][   60/   71]    Overall Loss 0.148580    Objective Loss 0.148580                                        LR 0.000250    Time 0.288555    
2023-09-11 14:29:14,827 - Epoch: [130][   70/   71]    Overall Loss 0.147858    Objective Loss 0.147858    Top1 92.968750    LR 0.000250    Time 0.279303    
2023-09-11 14:29:14,878 - Epoch: [130][   71/   71]    Overall Loss 0.147621    Objective Loss 0.147621    Top1 93.154762    LR 0.000250    Time 0.276083    
2023-09-11 14:29:14,970 - --- validate (epoch=130)-----------
2023-09-11 14:29:14,970 - 2000 samples (256 per mini-batch)
2023-09-11 14:29:17,980 - Epoch: [130][    8/    8]    Loss 0.194279    Top1 92.500000    
2023-09-11 14:29:18,090 - ==> Top1: 92.500    Loss: 0.194

2023-09-11 14:29:18,090 - ==> Confusion:
[[896  89]
 [ 61 954]]

2023-09-11 14:29:18,107 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:29:18,107 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:29:18,109 - 

2023-09-11 14:29:18,109 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:29:22,195 - Epoch: [131][   10/   71]    Overall Loss 0.169378    Objective Loss 0.169378                                        LR 0.000250    Time 0.408476    
2023-09-11 14:29:24,407 - Epoch: [131][   20/   71]    Overall Loss 0.172940    Objective Loss 0.172940                                        LR 0.000250    Time 0.314824    
2023-09-11 14:29:27,895 - Epoch: [131][   30/   71]    Overall Loss 0.162268    Objective Loss 0.162268                                        LR 0.000250    Time 0.326135    
2023-09-11 14:29:29,981 - Epoch: [131][   40/   71]    Overall Loss 0.159843    Objective Loss 0.159843                                        LR 0.000250    Time 0.296755    
2023-09-11 14:29:32,767 - Epoch: [131][   50/   71]    Overall Loss 0.160467    Objective Loss 0.160467                                        LR 0.000250    Time 0.293111    
2023-09-11 14:29:35,049 - Epoch: [131][   60/   71]    Overall Loss 0.158357    Objective Loss 0.158357                                        LR 0.000250    Time 0.282297    
2023-09-11 14:29:37,745 - Epoch: [131][   70/   71]    Overall Loss 0.154357    Objective Loss 0.154357    Top1 93.359375    LR 0.000250    Time 0.280465    
2023-09-11 14:29:37,833 - Epoch: [131][   71/   71]    Overall Loss 0.153234    Objective Loss 0.153234    Top1 94.047619    LR 0.000250    Time 0.277757    
2023-09-11 14:29:37,925 - --- validate (epoch=131)-----------
2023-09-11 14:29:37,925 - 2000 samples (256 per mini-batch)
2023-09-11 14:29:40,897 - Epoch: [131][    8/    8]    Loss 0.180635    Top1 92.300000    
2023-09-11 14:29:40,996 - ==> Top1: 92.300    Loss: 0.181

2023-09-11 14:29:40,997 - ==> Confusion:
[[918  67]
 [ 87 928]]

2023-09-11 14:29:41,003 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:29:41,003 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:29:41,008 - 

2023-09-11 14:29:41,008 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:29:44,732 - Epoch: [132][   10/   71]    Overall Loss 0.155653    Objective Loss 0.155653                                        LR 0.000250    Time 0.372401    
2023-09-11 14:29:47,010 - Epoch: [132][   20/   71]    Overall Loss 0.152960    Objective Loss 0.152960                                        LR 0.000250    Time 0.300032    
2023-09-11 14:29:50,533 - Epoch: [132][   30/   71]    Overall Loss 0.150961    Objective Loss 0.150961                                        LR 0.000250    Time 0.317469    
2023-09-11 14:29:52,556 - Epoch: [132][   40/   71]    Overall Loss 0.153491    Objective Loss 0.153491                                        LR 0.000250    Time 0.288667    
2023-09-11 14:29:56,016 - Epoch: [132][   50/   71]    Overall Loss 0.150734    Objective Loss 0.150734                                        LR 0.000250    Time 0.300114    
2023-09-11 14:29:58,085 - Epoch: [132][   60/   71]    Overall Loss 0.151509    Objective Loss 0.151509                                        LR 0.000250    Time 0.284582    
2023-09-11 14:30:00,426 - Epoch: [132][   70/   71]    Overall Loss 0.151010    Objective Loss 0.151010    Top1 93.750000    LR 0.000250    Time 0.277358    
2023-09-11 14:30:00,502 - Epoch: [132][   71/   71]    Overall Loss 0.151110    Objective Loss 0.151110    Top1 93.750000    LR 0.000250    Time 0.274529    
2023-09-11 14:30:00,599 - --- validate (epoch=132)-----------
2023-09-11 14:30:00,599 - 2000 samples (256 per mini-batch)
2023-09-11 14:30:03,308 - Epoch: [132][    8/    8]    Loss 0.195264    Top1 92.350000    
2023-09-11 14:30:03,398 - ==> Top1: 92.350    Loss: 0.195

2023-09-11 14:30:03,399 - ==> Confusion:
[[924  61]
 [ 92 923]]

2023-09-11 14:30:03,409 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:30:03,409 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:30:03,414 - 

2023-09-11 14:30:03,414 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:30:06,881 - Epoch: [133][   10/   71]    Overall Loss 0.154421    Objective Loss 0.154421                                        LR 0.000250    Time 0.346661    
2023-09-11 14:30:09,227 - Epoch: [133][   20/   71]    Overall Loss 0.153585    Objective Loss 0.153585                                        LR 0.000250    Time 0.290646    
2023-09-11 14:30:12,033 - Epoch: [133][   30/   71]    Overall Loss 0.145166    Objective Loss 0.145166                                        LR 0.000250    Time 0.287281    
2023-09-11 14:30:14,808 - Epoch: [133][   40/   71]    Overall Loss 0.144886    Objective Loss 0.144886                                        LR 0.000250    Time 0.284809    
2023-09-11 14:30:17,544 - Epoch: [133][   50/   71]    Overall Loss 0.146801    Objective Loss 0.146801                                        LR 0.000250    Time 0.282572    
2023-09-11 14:30:20,064 - Epoch: [133][   60/   71]    Overall Loss 0.147362    Objective Loss 0.147362                                        LR 0.000250    Time 0.277475    
2023-09-11 14:30:22,796 - Epoch: [133][   70/   71]    Overall Loss 0.146584    Objective Loss 0.146584    Top1 94.921875    LR 0.000250    Time 0.276847    
2023-09-11 14:30:22,872 - Epoch: [133][   71/   71]    Overall Loss 0.145839    Objective Loss 0.145839    Top1 95.238095    LR 0.000250    Time 0.274022    
2023-09-11 14:30:22,969 - --- validate (epoch=133)-----------
2023-09-11 14:30:22,969 - 2000 samples (256 per mini-batch)
2023-09-11 14:30:25,818 - Epoch: [133][    8/    8]    Loss 0.179392    Top1 93.000000    
2023-09-11 14:30:25,925 - ==> Top1: 93.000    Loss: 0.179

2023-09-11 14:30:25,925 - ==> Confusion:
[[917  68]
 [ 72 943]]

2023-09-11 14:30:25,936 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:30:25,937 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:30:25,941 - 

2023-09-11 14:30:25,941 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:30:29,421 - Epoch: [134][   10/   71]    Overall Loss 0.142843    Objective Loss 0.142843                                        LR 0.000250    Time 0.347966    
2023-09-11 14:30:32,341 - Epoch: [134][   20/   71]    Overall Loss 0.147513    Objective Loss 0.147513                                        LR 0.000250    Time 0.319964    
2023-09-11 14:30:34,454 - Epoch: [134][   30/   71]    Overall Loss 0.149436    Objective Loss 0.149436                                        LR 0.000250    Time 0.283716    
2023-09-11 14:30:37,281 - Epoch: [134][   40/   71]    Overall Loss 0.149348    Objective Loss 0.149348                                        LR 0.000250    Time 0.283463    
2023-09-11 14:30:40,393 - Epoch: [134][   50/   71]    Overall Loss 0.150265    Objective Loss 0.150265                                        LR 0.000250    Time 0.288989    
2023-09-11 14:30:43,671 - Epoch: [134][   60/   71]    Overall Loss 0.152650    Objective Loss 0.152650                                        LR 0.000250    Time 0.295454    
2023-09-11 14:30:46,107 - Epoch: [134][   70/   71]    Overall Loss 0.151760    Objective Loss 0.151760    Top1 95.703125    LR 0.000250    Time 0.288046    
2023-09-11 14:30:46,223 - Epoch: [134][   71/   71]    Overall Loss 0.151662    Objective Loss 0.151662    Top1 95.535714    LR 0.000250    Time 0.285624    
2023-09-11 14:30:46,314 - --- validate (epoch=134)-----------
2023-09-11 14:30:46,314 - 2000 samples (256 per mini-batch)
2023-09-11 14:30:49,064 - Epoch: [134][    8/    8]    Loss 0.190496    Top1 91.750000    
2023-09-11 14:30:49,168 - ==> Top1: 91.750    Loss: 0.190

2023-09-11 14:30:49,168 - ==> Confusion:
[[861 124]
 [ 41 974]]

2023-09-11 14:30:49,183 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:30:49,183 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:30:49,185 - 

2023-09-11 14:30:49,185 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:30:52,477 - Epoch: [135][   10/   71]    Overall Loss 0.163237    Objective Loss 0.163237                                        LR 0.000250    Time 0.329069    
2023-09-11 14:30:54,920 - Epoch: [135][   20/   71]    Overall Loss 0.153894    Objective Loss 0.153894                                        LR 0.000250    Time 0.286677    
2023-09-11 14:30:57,480 - Epoch: [135][   30/   71]    Overall Loss 0.150857    Objective Loss 0.150857                                        LR 0.000250    Time 0.276437    
2023-09-11 14:31:00,120 - Epoch: [135][   40/   71]    Overall Loss 0.150887    Objective Loss 0.150887                                        LR 0.000250    Time 0.273328    
2023-09-11 14:31:02,458 - Epoch: [135][   50/   71]    Overall Loss 0.150766    Objective Loss 0.150766                                        LR 0.000250    Time 0.265405    
2023-09-11 14:31:05,079 - Epoch: [135][   60/   71]    Overall Loss 0.154346    Objective Loss 0.154346                                        LR 0.000250    Time 0.264852    
2023-09-11 14:31:07,554 - Epoch: [135][   70/   71]    Overall Loss 0.154839    Objective Loss 0.154839    Top1 92.968750    LR 0.000250    Time 0.262373    
2023-09-11 14:31:07,678 - Epoch: [135][   71/   71]    Overall Loss 0.154324    Objective Loss 0.154324    Top1 93.750000    LR 0.000250    Time 0.260419    
2023-09-11 14:31:07,768 - --- validate (epoch=135)-----------
2023-09-11 14:31:07,769 - 2000 samples (256 per mini-batch)
2023-09-11 14:31:10,486 - Epoch: [135][    8/    8]    Loss 0.201336    Top1 91.950000    
2023-09-11 14:31:10,581 - ==> Top1: 91.950    Loss: 0.201

2023-09-11 14:31:10,582 - ==> Confusion:
[[938  47]
 [114 901]]

2023-09-11 14:31:10,583 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:31:10,583 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:31:10,586 - 

2023-09-11 14:31:10,586 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:31:14,353 - Epoch: [136][   10/   71]    Overall Loss 0.155226    Objective Loss 0.155226                                        LR 0.000250    Time 0.376665    
2023-09-11 14:31:16,377 - Epoch: [136][   20/   71]    Overall Loss 0.160189    Objective Loss 0.160189                                        LR 0.000250    Time 0.289501    
2023-09-11 14:31:19,151 - Epoch: [136][   30/   71]    Overall Loss 0.152189    Objective Loss 0.152189                                        LR 0.000250    Time 0.285432    
2023-09-11 14:31:21,876 - Epoch: [136][   40/   71]    Overall Loss 0.148189    Objective Loss 0.148189                                        LR 0.000250    Time 0.282204    
2023-09-11 14:31:24,545 - Epoch: [136][   50/   71]    Overall Loss 0.149102    Objective Loss 0.149102                                        LR 0.000250    Time 0.279139    
2023-09-11 14:31:27,991 - Epoch: [136][   60/   71]    Overall Loss 0.145139    Objective Loss 0.145139                                        LR 0.000250    Time 0.290042    
2023-09-11 14:31:30,436 - Epoch: [136][   70/   71]    Overall Loss 0.143204    Objective Loss 0.143204    Top1 96.484375    LR 0.000250    Time 0.283526    
2023-09-11 14:31:30,510 - Epoch: [136][   71/   71]    Overall Loss 0.143074    Objective Loss 0.143074    Top1 95.535714    LR 0.000250    Time 0.280579    
2023-09-11 14:31:30,605 - --- validate (epoch=136)-----------
2023-09-11 14:31:30,605 - 2000 samples (256 per mini-batch)
2023-09-11 14:31:33,725 - Epoch: [136][    8/    8]    Loss 0.183390    Top1 92.650000    
2023-09-11 14:31:33,827 - ==> Top1: 92.650    Loss: 0.183

2023-09-11 14:31:33,827 - ==> Confusion:
[[913  72]
 [ 75 940]]

2023-09-11 14:31:33,843 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:31:33,843 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:31:33,845 - 

2023-09-11 14:31:33,845 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:31:37,943 - Epoch: [137][   10/   71]    Overall Loss 0.152552    Objective Loss 0.152552                                        LR 0.000250    Time 0.409689    
2023-09-11 14:31:39,999 - Epoch: [137][   20/   71]    Overall Loss 0.140332    Objective Loss 0.140332                                        LR 0.000250    Time 0.307619    
2023-09-11 14:31:43,179 - Epoch: [137][   30/   71]    Overall Loss 0.146501    Objective Loss 0.146501                                        LR 0.000250    Time 0.311092    
2023-09-11 14:31:45,487 - Epoch: [137][   40/   71]    Overall Loss 0.143306    Objective Loss 0.143306                                        LR 0.000250    Time 0.291005    
2023-09-11 14:31:48,250 - Epoch: [137][   50/   71]    Overall Loss 0.143334    Objective Loss 0.143334                                        LR 0.000250    Time 0.288060    
2023-09-11 14:31:50,315 - Epoch: [137][   60/   71]    Overall Loss 0.145178    Objective Loss 0.145178                                        LR 0.000250    Time 0.274452    
2023-09-11 14:31:52,790 - Epoch: [137][   70/   71]    Overall Loss 0.144973    Objective Loss 0.144973    Top1 93.359375    LR 0.000250    Time 0.270606    
2023-09-11 14:31:52,868 - Epoch: [137][   71/   71]    Overall Loss 0.144332    Objective Loss 0.144332    Top1 94.345238    LR 0.000250    Time 0.267882    
2023-09-11 14:31:52,949 - --- validate (epoch=137)-----------
2023-09-11 14:31:52,949 - 2000 samples (256 per mini-batch)
2023-09-11 14:31:55,338 - Epoch: [137][    8/    8]    Loss 0.182730    Top1 93.100000    
2023-09-11 14:31:55,433 - ==> Top1: 93.100    Loss: 0.183

2023-09-11 14:31:55,433 - ==> Confusion:
[[922  63]
 [ 75 940]]

2023-09-11 14:31:55,449 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:31:55,449 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:31:55,452 - 

2023-09-11 14:31:55,452 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:32:00,008 - Epoch: [138][   10/   71]    Overall Loss 0.162620    Objective Loss 0.162620                                        LR 0.000250    Time 0.455615    
2023-09-11 14:32:02,014 - Epoch: [138][   20/   71]    Overall Loss 0.160275    Objective Loss 0.160275                                        LR 0.000250    Time 0.328078    
2023-09-11 14:32:04,714 - Epoch: [138][   30/   71]    Overall Loss 0.156930    Objective Loss 0.156930                                        LR 0.000250    Time 0.308695    
2023-09-11 14:32:07,266 - Epoch: [138][   40/   71]    Overall Loss 0.155821    Objective Loss 0.155821                                        LR 0.000250    Time 0.295312    
2023-09-11 14:32:10,463 - Epoch: [138][   50/   71]    Overall Loss 0.154124    Objective Loss 0.154124                                        LR 0.000250    Time 0.300185    
2023-09-11 14:32:12,760 - Epoch: [138][   60/   71]    Overall Loss 0.151248    Objective Loss 0.151248                                        LR 0.000250    Time 0.288442    
2023-09-11 14:32:15,119 - Epoch: [138][   70/   71]    Overall Loss 0.151575    Objective Loss 0.151575    Top1 96.484375    LR 0.000250    Time 0.280923    
2023-09-11 14:32:15,196 - Epoch: [138][   71/   71]    Overall Loss 0.151564    Objective Loss 0.151564    Top1 95.535714    LR 0.000250    Time 0.278055    
2023-09-11 14:32:15,296 - --- validate (epoch=138)-----------
2023-09-11 14:32:15,297 - 2000 samples (256 per mini-batch)
2023-09-11 14:32:18,342 - Epoch: [138][    8/    8]    Loss 0.198425    Top1 91.400000    
2023-09-11 14:32:18,438 - ==> Top1: 91.400    Loss: 0.198

2023-09-11 14:32:18,438 - ==> Confusion:
[[928  57]
 [115 900]]

2023-09-11 14:32:18,454 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:32:18,454 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:32:18,456 - 

2023-09-11 14:32:18,456 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:32:22,814 - Epoch: [139][   10/   71]    Overall Loss 0.143969    Objective Loss 0.143969                                        LR 0.000250    Time 0.435681    
2023-09-11 14:32:24,858 - Epoch: [139][   20/   71]    Overall Loss 0.138549    Objective Loss 0.138549                                        LR 0.000250    Time 0.320047    
2023-09-11 14:32:27,663 - Epoch: [139][   30/   71]    Overall Loss 0.143566    Objective Loss 0.143566                                        LR 0.000250    Time 0.306859    
2023-09-11 14:32:30,683 - Epoch: [139][   40/   71]    Overall Loss 0.148024    Objective Loss 0.148024                                        LR 0.000250    Time 0.305632    
2023-09-11 14:32:32,787 - Epoch: [139][   50/   71]    Overall Loss 0.148305    Objective Loss 0.148305                                        LR 0.000250    Time 0.286574    
2023-09-11 14:32:35,424 - Epoch: [139][   60/   71]    Overall Loss 0.147937    Objective Loss 0.147937                                        LR 0.000250    Time 0.282750    
2023-09-11 14:32:37,371 - Epoch: [139][   70/   71]    Overall Loss 0.146612    Objective Loss 0.146612    Top1 95.312500    LR 0.000250    Time 0.270175    
2023-09-11 14:32:37,443 - Epoch: [139][   71/   71]    Overall Loss 0.145893    Objective Loss 0.145893    Top1 95.535714    LR 0.000250    Time 0.267386    
2023-09-11 14:32:37,524 - --- validate (epoch=139)-----------
2023-09-11 14:32:37,524 - 2000 samples (256 per mini-batch)
2023-09-11 14:32:39,944 - Epoch: [139][    8/    8]    Loss 0.172109    Top1 92.900000    
2023-09-11 14:32:40,056 - ==> Top1: 92.900    Loss: 0.172

2023-09-11 14:32:40,057 - ==> Confusion:
[[910  75]
 [ 67 948]]

2023-09-11 14:32:40,072 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:32:40,072 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:32:40,074 - 

2023-09-11 14:32:40,074 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:32:44,531 - Epoch: [140][   10/   71]    Overall Loss 0.137844    Objective Loss 0.137844                                        LR 0.000250    Time 0.445661    
2023-09-11 14:32:46,908 - Epoch: [140][   20/   71]    Overall Loss 0.138486    Objective Loss 0.138486                                        LR 0.000250    Time 0.341625    
2023-09-11 14:32:49,805 - Epoch: [140][   30/   71]    Overall Loss 0.140440    Objective Loss 0.140440                                        LR 0.000250    Time 0.324324    
2023-09-11 14:32:51,799 - Epoch: [140][   40/   71]    Overall Loss 0.143244    Objective Loss 0.143244                                        LR 0.000250    Time 0.293076    
2023-09-11 14:32:54,679 - Epoch: [140][   50/   71]    Overall Loss 0.145232    Objective Loss 0.145232                                        LR 0.000250    Time 0.292055    
2023-09-11 14:32:57,264 - Epoch: [140][   60/   71]    Overall Loss 0.148538    Objective Loss 0.148538                                        LR 0.000250    Time 0.286452    
2023-09-11 14:32:59,797 - Epoch: [140][   70/   71]    Overall Loss 0.147153    Objective Loss 0.147153    Top1 94.140625    LR 0.000250    Time 0.281722    
2023-09-11 14:32:59,915 - Epoch: [140][   71/   71]    Overall Loss 0.146148    Objective Loss 0.146148    Top1 94.940476    LR 0.000250    Time 0.279412    
2023-09-11 14:33:00,008 - --- validate (epoch=140)-----------
2023-09-11 14:33:00,008 - 2000 samples (256 per mini-batch)
2023-09-11 14:33:02,845 - Epoch: [140][    8/    8]    Loss 0.212978    Top1 91.200000    
2023-09-11 14:33:02,931 - ==> Top1: 91.200    Loss: 0.213

2023-09-11 14:33:02,932 - ==> Confusion:
[[946  39]
 [137 878]]

2023-09-11 14:33:02,932 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:33:02,932 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:33:02,935 - 

2023-09-11 14:33:02,935 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:33:07,354 - Epoch: [141][   10/   71]    Overall Loss 0.146552    Objective Loss 0.146552                                        LR 0.000250    Time 0.441844    
2023-09-11 14:33:09,210 - Epoch: [141][   20/   71]    Overall Loss 0.153975    Objective Loss 0.153975                                        LR 0.000250    Time 0.313699    
2023-09-11 14:33:11,908 - Epoch: [141][   30/   71]    Overall Loss 0.151210    Objective Loss 0.151210                                        LR 0.000250    Time 0.299062    
2023-09-11 14:33:14,042 - Epoch: [141][   40/   71]    Overall Loss 0.148038    Objective Loss 0.148038                                        LR 0.000250    Time 0.277629    
2023-09-11 14:33:16,690 - Epoch: [141][   50/   71]    Overall Loss 0.144898    Objective Loss 0.144898                                        LR 0.000250    Time 0.275058    
2023-09-11 14:33:20,918 - Epoch: [141][   60/   71]    Overall Loss 0.145915    Objective Loss 0.145915                                        LR 0.000250    Time 0.299683    
2023-09-11 14:33:22,934 - Epoch: [141][   70/   71]    Overall Loss 0.145070    Objective Loss 0.145070    Top1 92.187500    LR 0.000250    Time 0.285660    
2023-09-11 14:33:23,070 - Epoch: [141][   71/   71]    Overall Loss 0.144928    Objective Loss 0.144928    Top1 92.857143    LR 0.000250    Time 0.283549    
2023-09-11 14:33:23,164 - --- validate (epoch=141)-----------
2023-09-11 14:33:23,165 - 2000 samples (256 per mini-batch)
2023-09-11 14:33:25,959 - Epoch: [141][    8/    8]    Loss 0.178595    Top1 92.150000    
2023-09-11 14:33:26,059 - ==> Top1: 92.150    Loss: 0.179

2023-09-11 14:33:26,060 - ==> Confusion:
[[938  47]
 [110 905]]

2023-09-11 14:33:26,076 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:33:26,076 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:33:26,081 - 

2023-09-11 14:33:26,081 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:33:30,694 - Epoch: [142][   10/   71]    Overall Loss 0.142283    Objective Loss 0.142283                                        LR 0.000250    Time 0.461272    
2023-09-11 14:33:33,190 - Epoch: [142][   20/   71]    Overall Loss 0.144333    Objective Loss 0.144333                                        LR 0.000250    Time 0.355395    
2023-09-11 14:33:35,658 - Epoch: [142][   30/   71]    Overall Loss 0.141440    Objective Loss 0.141440                                        LR 0.000250    Time 0.319183    
2023-09-11 14:33:38,139 - Epoch: [142][   40/   71]    Overall Loss 0.142345    Objective Loss 0.142345                                        LR 0.000250    Time 0.301405    
2023-09-11 14:33:41,181 - Epoch: [142][   50/   71]    Overall Loss 0.145485    Objective Loss 0.145485                                        LR 0.000250    Time 0.301959    
2023-09-11 14:33:44,043 - Epoch: [142][   60/   71]    Overall Loss 0.146029    Objective Loss 0.146029                                        LR 0.000250    Time 0.299342    
2023-09-11 14:33:46,149 - Epoch: [142][   70/   71]    Overall Loss 0.145071    Objective Loss 0.145071    Top1 94.921875    LR 0.000250    Time 0.286657    
2023-09-11 14:33:46,229 - Epoch: [142][   71/   71]    Overall Loss 0.145138    Objective Loss 0.145138    Top1 94.642857    LR 0.000250    Time 0.283734    
2023-09-11 14:33:46,334 - --- validate (epoch=142)-----------
2023-09-11 14:33:46,334 - 2000 samples (256 per mini-batch)
2023-09-11 14:33:49,469 - Epoch: [142][    8/    8]    Loss 0.189400    Top1 92.150000    
2023-09-11 14:33:49,559 - ==> Top1: 92.150    Loss: 0.189

2023-09-11 14:33:49,560 - ==> Confusion:
[[934  51]
 [106 909]]

2023-09-11 14:33:49,575 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:33:49,575 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:33:49,578 - 

2023-09-11 14:33:49,578 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:33:54,477 - Epoch: [143][   10/   71]    Overall Loss 0.147861    Objective Loss 0.147861                                        LR 0.000250    Time 0.489829    
2023-09-11 14:33:56,524 - Epoch: [143][   20/   71]    Overall Loss 0.146568    Objective Loss 0.146568                                        LR 0.000250    Time 0.347284    
2023-09-11 14:33:59,150 - Epoch: [143][   30/   71]    Overall Loss 0.146154    Objective Loss 0.146154                                        LR 0.000250    Time 0.319049    
2023-09-11 14:34:01,201 - Epoch: [143][   40/   71]    Overall Loss 0.142727    Objective Loss 0.142727                                        LR 0.000250    Time 0.290547    
2023-09-11 14:34:03,930 - Epoch: [143][   50/   71]    Overall Loss 0.142083    Objective Loss 0.142083                                        LR 0.000250    Time 0.287009    
2023-09-11 14:34:07,278 - Epoch: [143][   60/   71]    Overall Loss 0.142703    Objective Loss 0.142703                                        LR 0.000250    Time 0.294978    
2023-09-11 14:34:09,256 - Epoch: [143][   70/   71]    Overall Loss 0.143171    Objective Loss 0.143171    Top1 96.093750    LR 0.000250    Time 0.281080    
2023-09-11 14:34:09,333 - Epoch: [143][   71/   71]    Overall Loss 0.144350    Objective Loss 0.144350    Top1 94.940476    LR 0.000250    Time 0.278205    
2023-09-11 14:34:09,432 - --- validate (epoch=143)-----------
2023-09-11 14:34:09,432 - 2000 samples (256 per mini-batch)
2023-09-11 14:34:12,660 - Epoch: [143][    8/    8]    Loss 0.179815    Top1 92.700000    
2023-09-11 14:34:12,762 - ==> Top1: 92.700    Loss: 0.180

2023-09-11 14:34:12,762 - ==> Confusion:
[[930  55]
 [ 91 924]]

2023-09-11 14:34:12,778 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:34:12,778 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:34:12,780 - 

2023-09-11 14:34:12,781 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:34:17,143 - Epoch: [144][   10/   71]    Overall Loss 0.136959    Objective Loss 0.136959                                        LR 0.000250    Time 0.436228    
2023-09-11 14:34:19,136 - Epoch: [144][   20/   71]    Overall Loss 0.141286    Objective Loss 0.141286                                        LR 0.000250    Time 0.317729    
2023-09-11 14:34:21,716 - Epoch: [144][   30/   71]    Overall Loss 0.135099    Objective Loss 0.135099                                        LR 0.000250    Time 0.297587    
2023-09-11 14:34:23,711 - Epoch: [144][   40/   71]    Overall Loss 0.142435    Objective Loss 0.142435                                        LR 0.000250    Time 0.273054    
2023-09-11 14:34:26,302 - Epoch: [144][   50/   71]    Overall Loss 0.143734    Objective Loss 0.143734                                        LR 0.000250    Time 0.270259    
2023-09-11 14:34:28,562 - Epoch: [144][   60/   71]    Overall Loss 0.141154    Objective Loss 0.141154                                        LR 0.000250    Time 0.262872    
2023-09-11 14:34:31,279 - Epoch: [144][   70/   71]    Overall Loss 0.141921    Objective Loss 0.141921    Top1 93.359375    LR 0.000250    Time 0.264138    
2023-09-11 14:34:31,399 - Epoch: [144][   71/   71]    Overall Loss 0.141126    Objective Loss 0.141126    Top1 94.345238    LR 0.000250    Time 0.262103    
2023-09-11 14:34:31,497 - --- validate (epoch=144)-----------
2023-09-11 14:34:31,497 - 2000 samples (256 per mini-batch)
2023-09-11 14:34:34,799 - Epoch: [144][    8/    8]    Loss 0.186136    Top1 92.500000    
2023-09-11 14:34:34,897 - ==> Top1: 92.500    Loss: 0.186

2023-09-11 14:34:34,897 - ==> Confusion:
[[948  37]
 [113 902]]

2023-09-11 14:34:34,913 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:34:34,913 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:34:34,918 - 

2023-09-11 14:34:34,918 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:34:38,668 - Epoch: [145][   10/   71]    Overall Loss 0.139226    Objective Loss 0.139226                                        LR 0.000250    Time 0.375008    
2023-09-11 14:34:41,102 - Epoch: [145][   20/   71]    Overall Loss 0.143591    Objective Loss 0.143591                                        LR 0.000250    Time 0.309175    
2023-09-11 14:34:43,970 - Epoch: [145][   30/   71]    Overall Loss 0.140378    Objective Loss 0.140378                                        LR 0.000250    Time 0.301715    
2023-09-11 14:34:47,510 - Epoch: [145][   40/   71]    Overall Loss 0.140806    Objective Loss 0.140806                                        LR 0.000250    Time 0.314775    
2023-09-11 14:34:49,569 - Epoch: [145][   50/   71]    Overall Loss 0.141182    Objective Loss 0.141182                                        LR 0.000250    Time 0.292982    
2023-09-11 14:34:53,098 - Epoch: [145][   60/   71]    Overall Loss 0.141860    Objective Loss 0.141860                                        LR 0.000250    Time 0.302971    
2023-09-11 14:34:54,916 - Epoch: [145][   70/   71]    Overall Loss 0.142169    Objective Loss 0.142169    Top1 93.750000    LR 0.000250    Time 0.285657    
2023-09-11 14:34:55,011 - Epoch: [145][   71/   71]    Overall Loss 0.144079    Objective Loss 0.144079    Top1 91.964286    LR 0.000250    Time 0.282971    
2023-09-11 14:34:55,098 - --- validate (epoch=145)-----------
2023-09-11 14:34:55,099 - 2000 samples (256 per mini-batch)
2023-09-11 14:34:58,148 - Epoch: [145][    8/    8]    Loss 0.172893    Top1 92.750000    
2023-09-11 14:34:58,243 - ==> Top1: 92.750    Loss: 0.173

2023-09-11 14:34:58,243 - ==> Confusion:
[[896  89]
 [ 56 959]]

2023-09-11 14:34:58,259 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:34:58,259 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:34:58,262 - 

2023-09-11 14:34:58,262 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:35:01,651 - Epoch: [146][   10/   71]    Overall Loss 0.138115    Objective Loss 0.138115                                        LR 0.000250    Time 0.338853    
2023-09-11 14:35:04,728 - Epoch: [146][   20/   71]    Overall Loss 0.144828    Objective Loss 0.144828                                        LR 0.000250    Time 0.323263    
2023-09-11 14:35:06,782 - Epoch: [146][   30/   71]    Overall Loss 0.143555    Objective Loss 0.143555                                        LR 0.000250    Time 0.283981    
2023-09-11 14:35:09,726 - Epoch: [146][   40/   71]    Overall Loss 0.145013    Objective Loss 0.145013                                        LR 0.000250    Time 0.286577    
2023-09-11 14:35:12,435 - Epoch: [146][   50/   71]    Overall Loss 0.144679    Objective Loss 0.144679                                        LR 0.000250    Time 0.283426    
2023-09-11 14:35:14,546 - Epoch: [146][   60/   71]    Overall Loss 0.144077    Objective Loss 0.144077                                        LR 0.000250    Time 0.271362    
2023-09-11 14:35:17,093 - Epoch: [146][   70/   71]    Overall Loss 0.144371    Objective Loss 0.144371    Top1 92.968750    LR 0.000250    Time 0.268980    
2023-09-11 14:35:17,141 - Epoch: [146][   71/   71]    Overall Loss 0.144123    Objective Loss 0.144123    Top1 93.750000    LR 0.000250    Time 0.265867    
2023-09-11 14:35:17,237 - --- validate (epoch=146)-----------
2023-09-11 14:35:17,237 - 2000 samples (256 per mini-batch)
2023-09-11 14:35:20,061 - Epoch: [146][    8/    8]    Loss 0.182260    Top1 92.000000    
2023-09-11 14:35:20,175 - ==> Top1: 92.000    Loss: 0.182

2023-09-11 14:35:20,175 - ==> Confusion:
[[908  77]
 [ 83 932]]

2023-09-11 14:35:20,191 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:35:20,191 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:35:20,193 - 

2023-09-11 14:35:20,194 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:35:24,726 - Epoch: [147][   10/   71]    Overall Loss 0.152309    Objective Loss 0.152309                                        LR 0.000250    Time 0.453163    
2023-09-11 14:35:26,871 - Epoch: [147][   20/   71]    Overall Loss 0.150362    Objective Loss 0.150362                                        LR 0.000250    Time 0.333837    
2023-09-11 14:35:29,891 - Epoch: [147][   30/   71]    Overall Loss 0.143293    Objective Loss 0.143293                                        LR 0.000250    Time 0.323206    
2023-09-11 14:35:32,022 - Epoch: [147][   40/   71]    Overall Loss 0.139178    Objective Loss 0.139178                                        LR 0.000250    Time 0.295670    
2023-09-11 14:35:34,909 - Epoch: [147][   50/   71]    Overall Loss 0.137350    Objective Loss 0.137350                                        LR 0.000250    Time 0.294273    
2023-09-11 14:35:37,061 - Epoch: [147][   60/   71]    Overall Loss 0.138778    Objective Loss 0.138778                                        LR 0.000250    Time 0.281092    
2023-09-11 14:35:39,628 - Epoch: [147][   70/   71]    Overall Loss 0.140749    Objective Loss 0.140749    Top1 94.531250    LR 0.000250    Time 0.277607    
2023-09-11 14:35:39,675 - Epoch: [147][   71/   71]    Overall Loss 0.141748    Objective Loss 0.141748    Top1 94.047619    LR 0.000250    Time 0.274354    
2023-09-11 14:35:39,774 - --- validate (epoch=147)-----------
2023-09-11 14:35:39,774 - 2000 samples (256 per mini-batch)
2023-09-11 14:35:42,982 - Epoch: [147][    8/    8]    Loss 0.180102    Top1 92.450000    
2023-09-11 14:35:43,076 - ==> Top1: 92.450    Loss: 0.180

2023-09-11 14:35:43,077 - ==> Confusion:
[[907  78]
 [ 73 942]]

2023-09-11 14:35:43,093 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:35:43,093 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:35:43,095 - 

2023-09-11 14:35:43,096 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:35:47,241 - Epoch: [148][   10/   71]    Overall Loss 0.135503    Objective Loss 0.135503                                        LR 0.000250    Time 0.414532    
2023-09-11 14:35:49,275 - Epoch: [148][   20/   71]    Overall Loss 0.142288    Objective Loss 0.142288                                        LR 0.000250    Time 0.308926    
2023-09-11 14:35:51,815 - Epoch: [148][   30/   71]    Overall Loss 0.145302    Objective Loss 0.145302                                        LR 0.000250    Time 0.290606    
2023-09-11 14:35:54,254 - Epoch: [148][   40/   71]    Overall Loss 0.148457    Objective Loss 0.148457                                        LR 0.000250    Time 0.278931    
2023-09-11 14:35:57,754 - Epoch: [148][   50/   71]    Overall Loss 0.147964    Objective Loss 0.147964                                        LR 0.000250    Time 0.293128    
2023-09-11 14:35:59,859 - Epoch: [148][   60/   71]    Overall Loss 0.146237    Objective Loss 0.146237                                        LR 0.000250    Time 0.279351    
2023-09-11 14:36:02,135 - Epoch: [148][   70/   71]    Overall Loss 0.144249    Objective Loss 0.144249    Top1 94.531250    LR 0.000250    Time 0.271963    
2023-09-11 14:36:02,214 - Epoch: [148][   71/   71]    Overall Loss 0.144060    Objective Loss 0.144060    Top1 94.642857    LR 0.000250    Time 0.269236    
2023-09-11 14:36:02,317 - --- validate (epoch=148)-----------
2023-09-11 14:36:02,317 - 2000 samples (256 per mini-batch)
2023-09-11 14:36:05,429 - Epoch: [148][    8/    8]    Loss 0.174375    Top1 92.700000    
2023-09-11 14:36:05,511 - ==> Top1: 92.700    Loss: 0.174

2023-09-11 14:36:05,512 - ==> Confusion:
[[933  52]
 [ 94 921]]

2023-09-11 14:36:05,528 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:36:05,528 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:36:05,530 - 

2023-09-11 14:36:05,531 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:36:09,485 - Epoch: [149][   10/   71]    Overall Loss 0.130062    Objective Loss 0.130062                                        LR 0.000250    Time 0.395369    
2023-09-11 14:36:12,746 - Epoch: [149][   20/   71]    Overall Loss 0.135317    Objective Loss 0.135317                                        LR 0.000250    Time 0.360711    
2023-09-11 14:36:14,888 - Epoch: [149][   30/   71]    Overall Loss 0.134581    Objective Loss 0.134581                                        LR 0.000250    Time 0.311866    
2023-09-11 14:36:18,332 - Epoch: [149][   40/   71]    Overall Loss 0.135434    Objective Loss 0.135434                                        LR 0.000250    Time 0.319995    
2023-09-11 14:36:21,587 - Epoch: [149][   50/   71]    Overall Loss 0.133118    Objective Loss 0.133118                                        LR 0.000250    Time 0.321104    
2023-09-11 14:36:23,621 - Epoch: [149][   60/   71]    Overall Loss 0.133327    Objective Loss 0.133327                                        LR 0.000250    Time 0.301473    
2023-09-11 14:36:26,165 - Epoch: [149][   70/   71]    Overall Loss 0.134538    Objective Loss 0.134538    Top1 96.093750    LR 0.000250    Time 0.294750    
2023-09-11 14:36:26,230 - Epoch: [149][   71/   71]    Overall Loss 0.134893    Objective Loss 0.134893    Top1 95.238095    LR 0.000250    Time 0.291507    
2023-09-11 14:36:26,319 - --- validate (epoch=149)-----------
2023-09-11 14:36:26,319 - 2000 samples (256 per mini-batch)
2023-09-11 14:36:29,465 - Epoch: [149][    8/    8]    Loss 0.159635    Top1 93.100000    
2023-09-11 14:36:29,566 - ==> Top1: 93.100    Loss: 0.160

2023-09-11 14:36:29,566 - ==> Confusion:
[[925  60]
 [ 78 937]]

2023-09-11 14:36:29,582 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:36:29,582 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:36:29,584 - 

2023-09-11 14:36:29,584 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:36:33,979 - Epoch: [150][   10/   71]    Overall Loss 0.158186    Objective Loss 0.158186                                        LR 0.000250    Time 0.439386    
2023-09-11 14:36:36,035 - Epoch: [150][   20/   71]    Overall Loss 0.149389    Objective Loss 0.149389                                        LR 0.000250    Time 0.322478    
2023-09-11 14:36:39,870 - Epoch: [150][   30/   71]    Overall Loss 0.148036    Objective Loss 0.148036                                        LR 0.000250    Time 0.342817    
2023-09-11 14:36:42,212 - Epoch: [150][   40/   71]    Overall Loss 0.143113    Objective Loss 0.143113                                        LR 0.000250    Time 0.315662    
2023-09-11 14:36:44,956 - Epoch: [150][   50/   71]    Overall Loss 0.144990    Objective Loss 0.144990                                        LR 0.000250    Time 0.307394    
2023-09-11 14:36:47,080 - Epoch: [150][   60/   71]    Overall Loss 0.146210    Objective Loss 0.146210                                        LR 0.000250    Time 0.291561    
2023-09-11 14:36:49,414 - Epoch: [150][   70/   71]    Overall Loss 0.144060    Objective Loss 0.144060    Top1 94.531250    LR 0.000250    Time 0.283244    
2023-09-11 14:36:49,492 - Epoch: [150][   71/   71]    Overall Loss 0.144973    Objective Loss 0.144973    Top1 94.345238    LR 0.000250    Time 0.280348    
2023-09-11 14:36:49,587 - --- validate (epoch=150)-----------
2023-09-11 14:36:49,588 - 2000 samples (256 per mini-batch)
2023-09-11 14:36:52,549 - Epoch: [150][    8/    8]    Loss 0.187073    Top1 92.650000    
2023-09-11 14:36:52,652 - ==> Top1: 92.650    Loss: 0.187

2023-09-11 14:36:52,652 - ==> Confusion:
[[930  55]
 [ 92 923]]

2023-09-11 14:36:52,668 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:36:52,668 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:36:52,671 - 

2023-09-11 14:36:52,671 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:36:56,786 - Epoch: [151][   10/   71]    Overall Loss 0.135560    Objective Loss 0.135560                                        LR 0.000250    Time 0.411463    
2023-09-11 14:36:58,847 - Epoch: [151][   20/   71]    Overall Loss 0.131192    Objective Loss 0.131192                                        LR 0.000250    Time 0.308799    
2023-09-11 14:37:01,599 - Epoch: [151][   30/   71]    Overall Loss 0.130698    Objective Loss 0.130698                                        LR 0.000250    Time 0.297567    
2023-09-11 14:37:03,660 - Epoch: [151][   40/   71]    Overall Loss 0.131475    Objective Loss 0.131475                                        LR 0.000250    Time 0.274703    
2023-09-11 14:37:07,067 - Epoch: [151][   50/   71]    Overall Loss 0.134438    Objective Loss 0.134438                                        LR 0.000250    Time 0.287890    
2023-09-11 14:37:09,829 - Epoch: [151][   60/   71]    Overall Loss 0.137150    Objective Loss 0.137150                                        LR 0.000250    Time 0.285940    
2023-09-11 14:37:12,296 - Epoch: [151][   70/   71]    Overall Loss 0.140684    Objective Loss 0.140684    Top1 94.140625    LR 0.000250    Time 0.280335    
2023-09-11 14:37:12,369 - Epoch: [151][   71/   71]    Overall Loss 0.140162    Objective Loss 0.140162    Top1 94.345238    LR 0.000250    Time 0.277402    
2023-09-11 14:37:12,451 - --- validate (epoch=151)-----------
2023-09-11 14:37:12,452 - 2000 samples (256 per mini-batch)
2023-09-11 14:37:14,812 - Epoch: [151][    8/    8]    Loss 0.180883    Top1 92.400000    
2023-09-11 14:37:14,917 - ==> Top1: 92.400    Loss: 0.181

2023-09-11 14:37:14,917 - ==> Confusion:
[[893  92]
 [ 60 955]]

2023-09-11 14:37:14,933 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:37:14,933 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:37:14,936 - 

2023-09-11 14:37:14,936 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:37:18,203 - Epoch: [152][   10/   71]    Overall Loss 0.144504    Objective Loss 0.144504                                        LR 0.000250    Time 0.326706    
2023-09-11 14:37:20,642 - Epoch: [152][   20/   71]    Overall Loss 0.144285    Objective Loss 0.144285                                        LR 0.000250    Time 0.285251    
2023-09-11 14:37:23,730 - Epoch: [152][   30/   71]    Overall Loss 0.141729    Objective Loss 0.141729                                        LR 0.000250    Time 0.293095    
2023-09-11 14:37:26,534 - Epoch: [152][   40/   71]    Overall Loss 0.143139    Objective Loss 0.143139                                        LR 0.000250    Time 0.289913    
2023-09-11 14:37:29,483 - Epoch: [152][   50/   71]    Overall Loss 0.141997    Objective Loss 0.141997                                        LR 0.000250    Time 0.290899    
2023-09-11 14:37:31,602 - Epoch: [152][   60/   71]    Overall Loss 0.141396    Objective Loss 0.141396                                        LR 0.000250    Time 0.277737    
2023-09-11 14:37:34,127 - Epoch: [152][   70/   71]    Overall Loss 0.141193    Objective Loss 0.141193    Top1 94.140625    LR 0.000250    Time 0.274118    
2023-09-11 14:37:34,205 - Epoch: [152][   71/   71]    Overall Loss 0.141199    Objective Loss 0.141199    Top1 94.642857    LR 0.000250    Time 0.271361    
2023-09-11 14:37:34,286 - --- validate (epoch=152)-----------
2023-09-11 14:37:34,286 - 2000 samples (256 per mini-batch)
2023-09-11 14:37:37,464 - Epoch: [152][    8/    8]    Loss 0.197767    Top1 92.050000    
2023-09-11 14:37:37,565 - ==> Top1: 92.050    Loss: 0.198

2023-09-11 14:37:37,565 - ==> Confusion:
[[939  46]
 [113 902]]

2023-09-11 14:37:37,566 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:37:37,566 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:37:37,568 - 

2023-09-11 14:37:37,569 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:37:41,232 - Epoch: [153][   10/   71]    Overall Loss 0.141990    Objective Loss 0.141990                                        LR 0.000250    Time 0.366271    
2023-09-11 14:37:43,748 - Epoch: [153][   20/   71]    Overall Loss 0.140454    Objective Loss 0.140454                                        LR 0.000250    Time 0.308905    
2023-09-11 14:37:45,940 - Epoch: [153][   30/   71]    Overall Loss 0.136106    Objective Loss 0.136106                                        LR 0.000250    Time 0.279006    
2023-09-11 14:37:48,454 - Epoch: [153][   40/   71]    Overall Loss 0.136150    Objective Loss 0.136150                                        LR 0.000250    Time 0.272089    
2023-09-11 14:37:50,776 - Epoch: [153][   50/   71]    Overall Loss 0.139164    Objective Loss 0.139164                                        LR 0.000250    Time 0.264108    
2023-09-11 14:37:53,525 - Epoch: [153][   60/   71]    Overall Loss 0.136480    Objective Loss 0.136480                                        LR 0.000250    Time 0.265900    
2023-09-11 14:37:55,787 - Epoch: [153][   70/   71]    Overall Loss 0.138509    Objective Loss 0.138509    Top1 94.531250    LR 0.000250    Time 0.260229    
2023-09-11 14:37:55,859 - Epoch: [153][   71/   71]    Overall Loss 0.137816    Objective Loss 0.137816    Top1 95.238095    LR 0.000250    Time 0.257581    
2023-09-11 14:37:55,959 - --- validate (epoch=153)-----------
2023-09-11 14:37:55,959 - 2000 samples (256 per mini-batch)
2023-09-11 14:37:58,353 - Epoch: [153][    8/    8]    Loss 0.215560    Top1 91.250000    
2023-09-11 14:37:58,448 - ==> Top1: 91.250    Loss: 0.216

2023-09-11 14:37:58,449 - ==> Confusion:
[[836 149]
 [ 26 989]]

2023-09-11 14:37:58,465 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:37:58,465 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:37:58,468 - 

2023-09-11 14:37:58,468 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:38:02,687 - Epoch: [154][   10/   71]    Overall Loss 0.140922    Objective Loss 0.140922                                        LR 0.000250    Time 0.421873    
2023-09-11 14:38:04,720 - Epoch: [154][   20/   71]    Overall Loss 0.135275    Objective Loss 0.135275                                        LR 0.000250    Time 0.312544    
2023-09-11 14:38:07,525 - Epoch: [154][   30/   71]    Overall Loss 0.142118    Objective Loss 0.142118                                        LR 0.000250    Time 0.301861    
2023-09-11 14:38:09,852 - Epoch: [154][   40/   71]    Overall Loss 0.138506    Objective Loss 0.138506                                        LR 0.000250    Time 0.284549    
2023-09-11 14:38:12,837 - Epoch: [154][   50/   71]    Overall Loss 0.139098    Objective Loss 0.139098                                        LR 0.000250    Time 0.287338    
2023-09-11 14:38:14,928 - Epoch: [154][   60/   71]    Overall Loss 0.141296    Objective Loss 0.141296                                        LR 0.000250    Time 0.274302    
2023-09-11 14:38:17,225 - Epoch: [154][   70/   71]    Overall Loss 0.142905    Objective Loss 0.142905    Top1 91.406250    LR 0.000250    Time 0.267918    
2023-09-11 14:38:17,328 - Epoch: [154][   71/   71]    Overall Loss 0.142904    Objective Loss 0.142904    Top1 92.261905    LR 0.000250    Time 0.265590    
2023-09-11 14:38:17,430 - --- validate (epoch=154)-----------
2023-09-11 14:38:17,430 - 2000 samples (256 per mini-batch)
2023-09-11 14:38:20,199 - Epoch: [154][    8/    8]    Loss 0.193705    Top1 91.450000    
2023-09-11 14:38:20,290 - ==> Top1: 91.450    Loss: 0.194

2023-09-11 14:38:20,290 - ==> Confusion:
[[937  48]
 [123 892]]

2023-09-11 14:38:20,306 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:38:20,306 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:38:20,308 - 

2023-09-11 14:38:20,308 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:38:24,149 - Epoch: [155][   10/   71]    Overall Loss 0.155127    Objective Loss 0.155127                                        LR 0.000250    Time 0.384032    
2023-09-11 14:38:26,754 - Epoch: [155][   20/   71]    Overall Loss 0.152524    Objective Loss 0.152524                                        LR 0.000250    Time 0.322225    
2023-09-11 14:38:28,740 - Epoch: [155][   30/   71]    Overall Loss 0.150550    Objective Loss 0.150550                                        LR 0.000250    Time 0.281035    
2023-09-11 14:38:31,374 - Epoch: [155][   40/   71]    Overall Loss 0.145926    Objective Loss 0.145926                                        LR 0.000250    Time 0.276599    
2023-09-11 14:38:34,581 - Epoch: [155][   50/   71]    Overall Loss 0.145236    Objective Loss 0.145236                                        LR 0.000250    Time 0.285423    
2023-09-11 14:38:38,402 - Epoch: [155][   60/   71]    Overall Loss 0.145329    Objective Loss 0.145329                                        LR 0.000250    Time 0.301523    
2023-09-11 14:38:40,809 - Epoch: [155][   70/   71]    Overall Loss 0.145206    Objective Loss 0.145206    Top1 96.093750    LR 0.000250    Time 0.292837    
2023-09-11 14:38:40,908 - Epoch: [155][   71/   71]    Overall Loss 0.146487    Objective Loss 0.146487    Top1 94.940476    LR 0.000250    Time 0.290097    
2023-09-11 14:38:41,004 - --- validate (epoch=155)-----------
2023-09-11 14:38:41,004 - 2000 samples (256 per mini-batch)
2023-09-11 14:38:43,556 - Epoch: [155][    8/    8]    Loss 0.180358    Top1 92.350000    
2023-09-11 14:38:43,659 - ==> Top1: 92.350    Loss: 0.180

2023-09-11 14:38:43,660 - ==> Confusion:
[[906  79]
 [ 74 941]]

2023-09-11 14:38:43,675 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:38:43,675 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:38:43,677 - 

2023-09-11 14:38:43,677 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:38:47,396 - Epoch: [156][   10/   71]    Overall Loss 0.125541    Objective Loss 0.125541                                        LR 0.000250    Time 0.371853    
2023-09-11 14:38:50,323 - Epoch: [156][   20/   71]    Overall Loss 0.130025    Objective Loss 0.130025                                        LR 0.000250    Time 0.332216    
2023-09-11 14:38:53,578 - Epoch: [156][   30/   71]    Overall Loss 0.126393    Objective Loss 0.126393                                        LR 0.000250    Time 0.329967    
2023-09-11 14:38:55,752 - Epoch: [156][   40/   71]    Overall Loss 0.131066    Objective Loss 0.131066                                        LR 0.000250    Time 0.301830    
2023-09-11 14:38:59,598 - Epoch: [156][   50/   71]    Overall Loss 0.130813    Objective Loss 0.130813                                        LR 0.000250    Time 0.318367    
2023-09-11 14:39:01,743 - Epoch: [156][   60/   71]    Overall Loss 0.133392    Objective Loss 0.133392                                        LR 0.000250    Time 0.301063    
2023-09-11 14:39:04,103 - Epoch: [156][   70/   71]    Overall Loss 0.134936    Objective Loss 0.134936    Top1 92.578125    LR 0.000250    Time 0.291763    
2023-09-11 14:39:04,196 - Epoch: [156][   71/   71]    Overall Loss 0.133993    Objective Loss 0.133993    Top1 93.750000    LR 0.000250    Time 0.288962    
2023-09-11 14:39:04,294 - --- validate (epoch=156)-----------
2023-09-11 14:39:04,294 - 2000 samples (256 per mini-batch)
2023-09-11 14:39:07,145 - Epoch: [156][    8/    8]    Loss 0.195592    Top1 92.350000    
2023-09-11 14:39:07,242 - ==> Top1: 92.350    Loss: 0.196

2023-09-11 14:39:07,242 - ==> Confusion:
[[937  48]
 [105 910]]

2023-09-11 14:39:07,258 - ==> Best [Top1: 93.400   Sparsity:0.00   Params: 57776 on epoch: 105]
2023-09-11 14:39:07,258 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:39:07,260 - 

2023-09-11 14:39:07,260 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:39:10,520 - Epoch: [157][   10/   71]    Overall Loss 0.158947    Objective Loss 0.158947                                        LR 0.000250    Time 0.325914    
2023-09-11 14:39:12,782 - Epoch: [157][   20/   71]    Overall Loss 0.145077    Objective Loss 0.145077                                        LR 0.000250    Time 0.276053    
2023-09-11 14:39:15,555 - Epoch: [157][   30/   71]    Overall Loss 0.137404    Objective Loss 0.137404                                        LR 0.000250    Time 0.276456    
2023-09-11 14:39:19,291 - Epoch: [157][   40/   71]    Overall Loss 0.137514    Objective Loss 0.137514                                        LR 0.000250    Time 0.300726    
2023-09-11 14:39:21,593 - Epoch: [157][   50/   71]    Overall Loss 0.138335    Objective Loss 0.138335                                        LR 0.000250    Time 0.286609    
2023-09-11 14:39:24,762 - Epoch: [157][   60/   71]    Overall Loss 0.136350    Objective Loss 0.136350                                        LR 0.000250    Time 0.291654    
2023-09-11 14:39:26,820 - Epoch: [157][   70/   71]    Overall Loss 0.135475    Objective Loss 0.135475    Top1 94.531250    LR 0.000250    Time 0.279390    
2023-09-11 14:39:26,893 - Epoch: [157][   71/   71]    Overall Loss 0.135719    Objective Loss 0.135719    Top1 93.750000    LR 0.000250    Time 0.276481    
2023-09-11 14:39:26,978 - --- validate (epoch=157)-----------
2023-09-11 14:39:26,978 - 2000 samples (256 per mini-batch)
2023-09-11 14:39:29,965 - Epoch: [157][    8/    8]    Loss 0.164704    Top1 93.700000    
2023-09-11 14:39:30,065 - ==> Top1: 93.700    Loss: 0.165

2023-09-11 14:39:30,065 - ==> Confusion:
[[911  74]
 [ 52 963]]

2023-09-11 14:39:30,078 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:39:30,078 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:39:30,081 - 

2023-09-11 14:39:30,081 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:39:34,044 - Epoch: [158][   10/   71]    Overall Loss 0.148368    Objective Loss 0.148368                                        LR 0.000250    Time 0.396215    
2023-09-11 14:39:36,725 - Epoch: [158][   20/   71]    Overall Loss 0.139819    Objective Loss 0.139819                                        LR 0.000250    Time 0.332168    
2023-09-11 14:39:38,775 - Epoch: [158][   30/   71]    Overall Loss 0.139671    Objective Loss 0.139671                                        LR 0.000250    Time 0.289761    
2023-09-11 14:39:42,068 - Epoch: [158][   40/   71]    Overall Loss 0.138871    Objective Loss 0.138871                                        LR 0.000250    Time 0.299630    
2023-09-11 14:39:44,863 - Epoch: [158][   50/   71]    Overall Loss 0.137978    Objective Loss 0.137978                                        LR 0.000250    Time 0.295608    
2023-09-11 14:39:47,464 - Epoch: [158][   60/   71]    Overall Loss 0.137981    Objective Loss 0.137981                                        LR 0.000250    Time 0.289672    
2023-09-11 14:39:49,396 - Epoch: [158][   70/   71]    Overall Loss 0.135996    Objective Loss 0.135996    Top1 94.921875    LR 0.000250    Time 0.275889    
2023-09-11 14:39:49,480 - Epoch: [158][   71/   71]    Overall Loss 0.135544    Objective Loss 0.135544    Top1 95.238095    LR 0.000250    Time 0.273192    
2023-09-11 14:39:49,585 - --- validate (epoch=158)-----------
2023-09-11 14:39:49,585 - 2000 samples (256 per mini-batch)
2023-09-11 14:39:52,646 - Epoch: [158][    8/    8]    Loss 0.194998    Top1 91.850000    
2023-09-11 14:39:52,753 - ==> Top1: 91.850    Loss: 0.195

2023-09-11 14:39:52,754 - ==> Confusion:
[[921  64]
 [ 99 916]]

2023-09-11 14:39:52,770 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:39:52,770 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:39:52,772 - 

2023-09-11 14:39:52,772 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:39:57,277 - Epoch: [159][   10/   71]    Overall Loss 0.127449    Objective Loss 0.127449                                        LR 0.000250    Time 0.450389    
2023-09-11 14:39:59,476 - Epoch: [159][   20/   71]    Overall Loss 0.134861    Objective Loss 0.134861                                        LR 0.000250    Time 0.335161    
2023-09-11 14:40:02,391 - Epoch: [159][   30/   71]    Overall Loss 0.138033    Objective Loss 0.138033                                        LR 0.000250    Time 0.320580    
2023-09-11 14:40:04,443 - Epoch: [159][   40/   71]    Overall Loss 0.136145    Objective Loss 0.136145                                        LR 0.000250    Time 0.291724    
2023-09-11 14:40:08,015 - Epoch: [159][   50/   71]    Overall Loss 0.136789    Objective Loss 0.136789                                        LR 0.000250    Time 0.304820    
2023-09-11 14:40:11,050 - Epoch: [159][   60/   71]    Overall Loss 0.137358    Objective Loss 0.137358                                        LR 0.000250    Time 0.304588    
2023-09-11 14:40:13,214 - Epoch: [159][   70/   71]    Overall Loss 0.137390    Objective Loss 0.137390    Top1 95.703125    LR 0.000250    Time 0.291986    
2023-09-11 14:40:13,258 - Epoch: [159][   71/   71]    Overall Loss 0.136949    Objective Loss 0.136949    Top1 95.833333    LR 0.000250    Time 0.288494    
2023-09-11 14:40:13,358 - --- validate (epoch=159)-----------
2023-09-11 14:40:13,358 - 2000 samples (256 per mini-batch)
2023-09-11 14:40:16,482 - Epoch: [159][    8/    8]    Loss 0.172339    Top1 93.250000    
2023-09-11 14:40:16,581 - ==> Top1: 93.250    Loss: 0.172

2023-09-11 14:40:16,582 - ==> Confusion:
[[939  46]
 [ 89 926]]

2023-09-11 14:40:16,598 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:40:16,598 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:40:16,602 - 

2023-09-11 14:40:16,602 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:40:21,185 - Epoch: [160][   10/   71]    Overall Loss 0.127272    Objective Loss 0.127272                                        LR 0.000250    Time 0.458257    
2023-09-11 14:40:23,712 - Epoch: [160][   20/   71]    Overall Loss 0.130602    Objective Loss 0.130602                                        LR 0.000250    Time 0.355428    
2023-09-11 14:40:26,511 - Epoch: [160][   30/   71]    Overall Loss 0.136898    Objective Loss 0.136898                                        LR 0.000250    Time 0.330237    
2023-09-11 14:40:29,016 - Epoch: [160][   40/   71]    Overall Loss 0.140281    Objective Loss 0.140281                                        LR 0.000250    Time 0.310310    
2023-09-11 14:40:32,526 - Epoch: [160][   50/   71]    Overall Loss 0.140610    Objective Loss 0.140610                                        LR 0.000250    Time 0.318439    
2023-09-11 14:40:35,245 - Epoch: [160][   60/   71]    Overall Loss 0.138607    Objective Loss 0.138607                                        LR 0.000250    Time 0.310684    
2023-09-11 14:40:37,308 - Epoch: [160][   70/   71]    Overall Loss 0.137954    Objective Loss 0.137954    Top1 92.968750    LR 0.000250    Time 0.295758    
2023-09-11 14:40:37,427 - Epoch: [160][   71/   71]    Overall Loss 0.137916    Objective Loss 0.137916    Top1 92.857143    LR 0.000250    Time 0.293266    
2023-09-11 14:40:37,512 - --- validate (epoch=160)-----------
2023-09-11 14:40:37,512 - 2000 samples (256 per mini-batch)
2023-09-11 14:40:40,498 - Epoch: [160][    8/    8]    Loss 0.183178    Top1 92.700000    
2023-09-11 14:40:40,598 - ==> Top1: 92.700    Loss: 0.183

2023-09-11 14:40:40,599 - ==> Confusion:
[[904  81]
 [ 65 950]]

2023-09-11 14:40:40,613 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:40:40,613 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:40:40,616 - 

2023-09-11 14:40:40,616 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:40:44,323 - Epoch: [161][   10/   71]    Overall Loss 0.123808    Objective Loss 0.123808                                        LR 0.000250    Time 0.370695    
2023-09-11 14:40:47,189 - Epoch: [161][   20/   71]    Overall Loss 0.131230    Objective Loss 0.131230                                        LR 0.000250    Time 0.328594    
2023-09-11 14:40:50,092 - Epoch: [161][   30/   71]    Overall Loss 0.131855    Objective Loss 0.131855                                        LR 0.000250    Time 0.315826    
2023-09-11 14:40:52,700 - Epoch: [161][   40/   71]    Overall Loss 0.133592    Objective Loss 0.133592                                        LR 0.000250    Time 0.302070    
2023-09-11 14:40:55,680 - Epoch: [161][   50/   71]    Overall Loss 0.133337    Objective Loss 0.133337                                        LR 0.000250    Time 0.301253    
2023-09-11 14:40:58,916 - Epoch: [161][   60/   71]    Overall Loss 0.133530    Objective Loss 0.133530                                        LR 0.000250    Time 0.304971    
2023-09-11 14:41:01,013 - Epoch: [161][   70/   71]    Overall Loss 0.133453    Objective Loss 0.133453    Top1 92.578125    LR 0.000250    Time 0.291347    
2023-09-11 14:41:01,097 - Epoch: [161][   71/   71]    Overall Loss 0.133963    Objective Loss 0.133963    Top1 92.559524    LR 0.000250    Time 0.288426    
2023-09-11 14:41:01,194 - --- validate (epoch=161)-----------
2023-09-11 14:41:01,194 - 2000 samples (256 per mini-batch)
2023-09-11 14:41:04,124 - Epoch: [161][    8/    8]    Loss 0.192709    Top1 92.050000    
2023-09-11 14:41:04,210 - ==> Top1: 92.050    Loss: 0.193

2023-09-11 14:41:04,211 - ==> Confusion:
[[946  39]
 [120 895]]

2023-09-11 14:41:04,227 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:41:04,227 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:41:04,231 - 

2023-09-11 14:41:04,232 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:41:08,070 - Epoch: [162][   10/   71]    Overall Loss 0.141698    Objective Loss 0.141698                                        LR 0.000250    Time 0.383790    
2023-09-11 14:41:10,116 - Epoch: [162][   20/   71]    Overall Loss 0.152747    Objective Loss 0.152747                                        LR 0.000250    Time 0.294192    
2023-09-11 14:41:12,711 - Epoch: [162][   30/   71]    Overall Loss 0.144171    Objective Loss 0.144171                                        LR 0.000250    Time 0.282613    
2023-09-11 14:41:15,349 - Epoch: [162][   40/   71]    Overall Loss 0.140177    Objective Loss 0.140177                                        LR 0.000250    Time 0.277901    
2023-09-11 14:41:18,961 - Epoch: [162][   50/   71]    Overall Loss 0.139428    Objective Loss 0.139428                                        LR 0.000250    Time 0.294547    
2023-09-11 14:41:21,076 - Epoch: [162][   60/   71]    Overall Loss 0.140653    Objective Loss 0.140653                                        LR 0.000250    Time 0.280707    
2023-09-11 14:41:23,431 - Epoch: [162][   70/   71]    Overall Loss 0.138707    Objective Loss 0.138707    Top1 96.484375    LR 0.000250    Time 0.274240    
2023-09-11 14:41:23,523 - Epoch: [162][   71/   71]    Overall Loss 0.139048    Objective Loss 0.139048    Top1 95.535714    LR 0.000250    Time 0.271673    
2023-09-11 14:41:23,626 - --- validate (epoch=162)-----------
2023-09-11 14:41:23,627 - 2000 samples (256 per mini-batch)
2023-09-11 14:41:26,288 - Epoch: [162][    8/    8]    Loss 0.172017    Top1 92.800000    
2023-09-11 14:41:26,386 - ==> Top1: 92.800    Loss: 0.172

2023-09-11 14:41:26,386 - ==> Confusion:
[[932  53]
 [ 91 924]]

2023-09-11 14:41:26,387 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:41:26,387 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:41:26,392 - 

2023-09-11 14:41:26,392 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:41:29,860 - Epoch: [163][   10/   71]    Overall Loss 0.134971    Objective Loss 0.134971                                        LR 0.000250    Time 0.346731    
2023-09-11 14:41:32,202 - Epoch: [163][   20/   71]    Overall Loss 0.139776    Objective Loss 0.139776                                        LR 0.000250    Time 0.290417    
2023-09-11 14:41:35,453 - Epoch: [163][   30/   71]    Overall Loss 0.140548    Objective Loss 0.140548                                        LR 0.000250    Time 0.301996    
2023-09-11 14:41:38,055 - Epoch: [163][   40/   71]    Overall Loss 0.136687    Objective Loss 0.136687                                        LR 0.000250    Time 0.291534    
2023-09-11 14:41:41,291 - Epoch: [163][   50/   71]    Overall Loss 0.131695    Objective Loss 0.131695                                        LR 0.000250    Time 0.297937    
2023-09-11 14:41:43,865 - Epoch: [163][   60/   71]    Overall Loss 0.133672    Objective Loss 0.133672                                        LR 0.000250    Time 0.291178    
2023-09-11 14:41:45,890 - Epoch: [163][   70/   71]    Overall Loss 0.131670    Objective Loss 0.131670    Top1 94.531250    LR 0.000250    Time 0.278510    
2023-09-11 14:41:45,972 - Epoch: [163][   71/   71]    Overall Loss 0.134635    Objective Loss 0.134635    Top1 91.964286    LR 0.000250    Time 0.275728    
2023-09-11 14:41:46,076 - --- validate (epoch=163)-----------
2023-09-11 14:41:46,076 - 2000 samples (256 per mini-batch)
2023-09-11 14:41:48,472 - Epoch: [163][    8/    8]    Loss 0.176663    Top1 92.500000    
2023-09-11 14:41:48,569 - ==> Top1: 92.500    Loss: 0.177

2023-09-11 14:41:48,570 - ==> Confusion:
[[935  50]
 [100 915]]

2023-09-11 14:41:48,585 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:41:48,585 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:41:48,587 - 

2023-09-11 14:41:48,587 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:41:53,130 - Epoch: [164][   10/   71]    Overall Loss 0.129067    Objective Loss 0.129067                                        LR 0.000250    Time 0.454192    
2023-09-11 14:41:55,290 - Epoch: [164][   20/   71]    Overall Loss 0.133608    Objective Loss 0.133608                                        LR 0.000250    Time 0.335100    
2023-09-11 14:41:57,981 - Epoch: [164][   30/   71]    Overall Loss 0.129368    Objective Loss 0.129368                                        LR 0.000250    Time 0.313100    
2023-09-11 14:42:00,282 - Epoch: [164][   40/   71]    Overall Loss 0.131453    Objective Loss 0.131453                                        LR 0.000250    Time 0.292328    
2023-09-11 14:42:03,817 - Epoch: [164][   50/   71]    Overall Loss 0.130713    Objective Loss 0.130713                                        LR 0.000250    Time 0.304550    
2023-09-11 14:42:06,200 - Epoch: [164][   60/   71]    Overall Loss 0.130805    Objective Loss 0.130805                                        LR 0.000250    Time 0.293514    
2023-09-11 14:42:08,454 - Epoch: [164][   70/   71]    Overall Loss 0.131235    Objective Loss 0.131235    Top1 93.359375    LR 0.000250    Time 0.283769    
2023-09-11 14:42:08,572 - Epoch: [164][   71/   71]    Overall Loss 0.130858    Objective Loss 0.130858    Top1 94.047619    LR 0.000250    Time 0.281442    
2023-09-11 14:42:08,678 - --- validate (epoch=164)-----------
2023-09-11 14:42:08,678 - 2000 samples (256 per mini-batch)
2023-09-11 14:42:11,954 - Epoch: [164][    8/    8]    Loss 0.176193    Top1 92.350000    
2023-09-11 14:42:12,097 - ==> Top1: 92.350    Loss: 0.176

2023-09-11 14:42:12,098 - ==> Confusion:
[[902  83]
 [ 70 945]]

2023-09-11 14:42:12,113 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:42:12,113 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:42:12,118 - 

2023-09-11 14:42:12,118 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:42:17,170 - Epoch: [165][   10/   71]    Overall Loss 0.141816    Objective Loss 0.141816                                        LR 0.000250    Time 0.505176    
2023-09-11 14:42:19,398 - Epoch: [165][   20/   71]    Overall Loss 0.140926    Objective Loss 0.140926                                        LR 0.000250    Time 0.363960    
2023-09-11 14:42:22,592 - Epoch: [165][   30/   71]    Overall Loss 0.139346    Objective Loss 0.139346                                        LR 0.000250    Time 0.349096    
2023-09-11 14:42:24,693 - Epoch: [165][   40/   71]    Overall Loss 0.142904    Objective Loss 0.142904                                        LR 0.000250    Time 0.314334    
2023-09-11 14:42:27,504 - Epoch: [165][   50/   71]    Overall Loss 0.141788    Objective Loss 0.141788                                        LR 0.000250    Time 0.307692    
2023-09-11 14:42:31,208 - Epoch: [165][   60/   71]    Overall Loss 0.144623    Objective Loss 0.144623                                        LR 0.000250    Time 0.318123    
2023-09-11 14:42:33,136 - Epoch: [165][   70/   71]    Overall Loss 0.144675    Objective Loss 0.144675    Top1 95.703125    LR 0.000250    Time 0.300222    
2023-09-11 14:42:33,214 - Epoch: [165][   71/   71]    Overall Loss 0.146900    Objective Loss 0.146900    Top1 95.238095    LR 0.000250    Time 0.297095    
2023-09-11 14:42:33,305 - --- validate (epoch=165)-----------
2023-09-11 14:42:33,306 - 2000 samples (256 per mini-batch)
2023-09-11 14:42:36,111 - Epoch: [165][    8/    8]    Loss 0.175504    Top1 92.800000    
2023-09-11 14:42:36,203 - ==> Top1: 92.800    Loss: 0.176

2023-09-11 14:42:36,203 - ==> Confusion:
[[912  73]
 [ 71 944]]

2023-09-11 14:42:36,218 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:42:36,218 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:42:36,220 - 

2023-09-11 14:42:36,220 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:42:39,537 - Epoch: [166][   10/   71]    Overall Loss 0.125566    Objective Loss 0.125566                                        LR 0.000250    Time 0.331599    
2023-09-11 14:42:41,803 - Epoch: [166][   20/   71]    Overall Loss 0.130810    Objective Loss 0.130810                                        LR 0.000250    Time 0.279105    
2023-09-11 14:42:44,601 - Epoch: [166][   30/   71]    Overall Loss 0.137340    Objective Loss 0.137340                                        LR 0.000250    Time 0.279331    
2023-09-11 14:42:46,560 - Epoch: [166][   40/   71]    Overall Loss 0.136391    Objective Loss 0.136391                                        LR 0.000250    Time 0.258458    
2023-09-11 14:42:49,205 - Epoch: [166][   50/   71]    Overall Loss 0.136528    Objective Loss 0.136528                                        LR 0.000250    Time 0.259661    
2023-09-11 14:42:52,525 - Epoch: [166][   60/   71]    Overall Loss 0.136732    Objective Loss 0.136732                                        LR 0.000250    Time 0.271719    
2023-09-11 14:42:54,348 - Epoch: [166][   70/   71]    Overall Loss 0.135025    Objective Loss 0.135025    Top1 93.359375    LR 0.000250    Time 0.258931    
2023-09-11 14:42:54,404 - Epoch: [166][   71/   71]    Overall Loss 0.134384    Objective Loss 0.134384    Top1 94.047619    LR 0.000250    Time 0.256075    
2023-09-11 14:42:54,489 - --- validate (epoch=166)-----------
2023-09-11 14:42:54,489 - 2000 samples (256 per mini-batch)
2023-09-11 14:42:57,520 - Epoch: [166][    8/    8]    Loss 0.175191    Top1 92.950000    
2023-09-11 14:42:57,621 - ==> Top1: 92.950    Loss: 0.175

2023-09-11 14:42:57,622 - ==> Confusion:
[[889  96]
 [ 45 970]]

2023-09-11 14:42:57,636 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:42:57,636 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:42:57,639 - 

2023-09-11 14:42:57,639 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:43:02,045 - Epoch: [167][   10/   71]    Overall Loss 0.134221    Objective Loss 0.134221                                        LR 0.000250    Time 0.440543    
2023-09-11 14:43:04,048 - Epoch: [167][   20/   71]    Overall Loss 0.135043    Objective Loss 0.135043                                        LR 0.000250    Time 0.320392    
2023-09-11 14:43:07,176 - Epoch: [167][   30/   71]    Overall Loss 0.146022    Objective Loss 0.146022                                        LR 0.000250    Time 0.317874    
2023-09-11 14:43:09,729 - Epoch: [167][   40/   71]    Overall Loss 0.144319    Objective Loss 0.144319                                        LR 0.000250    Time 0.302224    
2023-09-11 14:43:12,314 - Epoch: [167][   50/   71]    Overall Loss 0.141617    Objective Loss 0.141617                                        LR 0.000250    Time 0.293471    
2023-09-11 14:43:15,151 - Epoch: [167][   60/   71]    Overall Loss 0.142042    Objective Loss 0.142042                                        LR 0.000250    Time 0.291841    
2023-09-11 14:43:16,942 - Epoch: [167][   70/   71]    Overall Loss 0.143411    Objective Loss 0.143411    Top1 93.359375    LR 0.000250    Time 0.275726    
2023-09-11 14:43:17,072 - Epoch: [167][   71/   71]    Overall Loss 0.143113    Objective Loss 0.143113    Top1 93.750000    LR 0.000250    Time 0.273665    
2023-09-11 14:43:17,170 - --- validate (epoch=167)-----------
2023-09-11 14:43:17,171 - 2000 samples (256 per mini-batch)
2023-09-11 14:43:20,016 - Epoch: [167][    8/    8]    Loss 0.165769    Top1 93.050000    
2023-09-11 14:43:20,112 - ==> Top1: 93.050    Loss: 0.166

2023-09-11 14:43:20,113 - ==> Confusion:
[[921  64]
 [ 75 940]]

2023-09-11 14:43:20,128 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:43:20,128 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:43:20,131 - 

2023-09-11 14:43:20,131 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:43:23,618 - Epoch: [168][   10/   71]    Overall Loss 0.122227    Objective Loss 0.122227                                        LR 0.000250    Time 0.348689    
2023-09-11 14:43:26,085 - Epoch: [168][   20/   71]    Overall Loss 0.128404    Objective Loss 0.128404                                        LR 0.000250    Time 0.297652    
2023-09-11 14:43:28,900 - Epoch: [168][   30/   71]    Overall Loss 0.132952    Objective Loss 0.132952                                        LR 0.000250    Time 0.292259    
2023-09-11 14:43:30,854 - Epoch: [168][   40/   71]    Overall Loss 0.133645    Objective Loss 0.133645                                        LR 0.000250    Time 0.268048    
2023-09-11 14:43:33,493 - Epoch: [168][   50/   71]    Overall Loss 0.133681    Objective Loss 0.133681                                        LR 0.000250    Time 0.267214    
2023-09-11 14:43:35,541 - Epoch: [168][   60/   71]    Overall Loss 0.136324    Objective Loss 0.136324                                        LR 0.000250    Time 0.256803    
2023-09-11 14:43:37,892 - Epoch: [168][   70/   71]    Overall Loss 0.135753    Objective Loss 0.135753    Top1 94.531250    LR 0.000250    Time 0.253694    
2023-09-11 14:43:37,968 - Epoch: [168][   71/   71]    Overall Loss 0.135240    Objective Loss 0.135240    Top1 95.238095    LR 0.000250    Time 0.251194    
2023-09-11 14:43:38,069 - --- validate (epoch=168)-----------
2023-09-11 14:43:38,069 - 2000 samples (256 per mini-batch)
2023-09-11 14:43:40,500 - Epoch: [168][    8/    8]    Loss 0.175973    Top1 92.800000    
2023-09-11 14:43:40,597 - ==> Top1: 92.800    Loss: 0.176

2023-09-11 14:43:40,597 - ==> Confusion:
[[947  38]
 [106 909]]

2023-09-11 14:43:40,612 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:43:40,612 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:43:40,614 - 

2023-09-11 14:43:40,614 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:43:43,811 - Epoch: [169][   10/   71]    Overall Loss 0.152357    Objective Loss 0.152357                                        LR 0.000250    Time 0.319678    
2023-09-11 14:43:45,874 - Epoch: [169][   20/   71]    Overall Loss 0.143123    Objective Loss 0.143123                                        LR 0.000250    Time 0.262980    
2023-09-11 14:43:48,354 - Epoch: [169][   30/   71]    Overall Loss 0.137579    Objective Loss 0.137579                                        LR 0.000250    Time 0.257950    
2023-09-11 14:43:50,901 - Epoch: [169][   40/   71]    Overall Loss 0.135455    Objective Loss 0.135455                                        LR 0.000250    Time 0.257132    
2023-09-11 14:43:54,020 - Epoch: [169][   50/   71]    Overall Loss 0.136118    Objective Loss 0.136118                                        LR 0.000250    Time 0.268083    
2023-09-11 14:43:56,032 - Epoch: [169][   60/   71]    Overall Loss 0.135532    Objective Loss 0.135532                                        LR 0.000250    Time 0.256929    
2023-09-11 14:43:58,329 - Epoch: [169][   70/   71]    Overall Loss 0.137433    Objective Loss 0.137433    Top1 93.750000    LR 0.000250    Time 0.253037    
2023-09-11 14:43:58,418 - Epoch: [169][   71/   71]    Overall Loss 0.137431    Objective Loss 0.137431    Top1 93.750000    LR 0.000250    Time 0.250722    
2023-09-11 14:43:58,512 - --- validate (epoch=169)-----------
2023-09-11 14:43:58,512 - 2000 samples (256 per mini-batch)
2023-09-11 14:44:01,559 - Epoch: [169][    8/    8]    Loss 0.184606    Top1 92.900000    
2023-09-11 14:44:01,655 - ==> Top1: 92.900    Loss: 0.185

2023-09-11 14:44:01,656 - ==> Confusion:
[[939  46]
 [ 96 919]]

2023-09-11 14:44:01,663 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:44:01,664 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:44:01,666 - 

2023-09-11 14:44:01,666 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:44:05,054 - Epoch: [170][   10/   71]    Overall Loss 0.134194    Objective Loss 0.134194                                        LR 0.000250    Time 0.338796    
2023-09-11 14:44:07,131 - Epoch: [170][   20/   71]    Overall Loss 0.137751    Objective Loss 0.137751                                        LR 0.000250    Time 0.273196    
2023-09-11 14:44:10,411 - Epoch: [170][   30/   71]    Overall Loss 0.137994    Objective Loss 0.137994                                        LR 0.000250    Time 0.291443    
2023-09-11 14:44:12,396 - Epoch: [170][   40/   71]    Overall Loss 0.134324    Objective Loss 0.134324                                        LR 0.000250    Time 0.268207    
2023-09-11 14:44:14,973 - Epoch: [170][   50/   71]    Overall Loss 0.133608    Objective Loss 0.133608                                        LR 0.000250    Time 0.266111    
2023-09-11 14:44:17,017 - Epoch: [170][   60/   71]    Overall Loss 0.131404    Objective Loss 0.131404                                        LR 0.000250    Time 0.255815    
2023-09-11 14:44:19,184 - Epoch: [170][   70/   71]    Overall Loss 0.131643    Objective Loss 0.131643    Top1 95.312500    LR 0.000250    Time 0.250223    
2023-09-11 14:44:19,263 - Epoch: [170][   71/   71]    Overall Loss 0.132586    Objective Loss 0.132586    Top1 94.940476    LR 0.000250    Time 0.247805    
2023-09-11 14:44:19,354 - --- validate (epoch=170)-----------
2023-09-11 14:44:19,355 - 2000 samples (256 per mini-batch)
2023-09-11 14:44:22,346 - Epoch: [170][    8/    8]    Loss 0.167478    Top1 93.050000    
2023-09-11 14:44:22,444 - ==> Top1: 93.050    Loss: 0.167

2023-09-11 14:44:22,445 - ==> Confusion:
[[920  65]
 [ 74 941]]

2023-09-11 14:44:22,459 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:44:22,459 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:44:22,463 - 

2023-09-11 14:44:22,463 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:44:25,544 - Epoch: [171][   10/   71]    Overall Loss 0.133054    Objective Loss 0.133054                                        LR 0.000250    Time 0.308069    
2023-09-11 14:44:27,958 - Epoch: [171][   20/   71]    Overall Loss 0.135992    Objective Loss 0.135992                                        LR 0.000250    Time 0.274680    
2023-09-11 14:44:30,561 - Epoch: [171][   30/   71]    Overall Loss 0.136042    Objective Loss 0.136042                                        LR 0.000250    Time 0.269895    
2023-09-11 14:44:32,773 - Epoch: [171][   40/   71]    Overall Loss 0.135843    Objective Loss 0.135843                                        LR 0.000250    Time 0.257710    
2023-09-11 14:44:35,810 - Epoch: [171][   50/   71]    Overall Loss 0.131087    Objective Loss 0.131087                                        LR 0.000250    Time 0.266910    
2023-09-11 14:44:37,829 - Epoch: [171][   60/   71]    Overall Loss 0.132278    Objective Loss 0.132278                                        LR 0.000250    Time 0.256061    
2023-09-11 14:44:40,216 - Epoch: [171][   70/   71]    Overall Loss 0.132423    Objective Loss 0.132423    Top1 95.312500    LR 0.000250    Time 0.253585    
2023-09-11 14:44:40,305 - Epoch: [171][   71/   71]    Overall Loss 0.132213    Objective Loss 0.132213    Top1 95.238095    LR 0.000250    Time 0.251255    
2023-09-11 14:44:40,396 - --- validate (epoch=171)-----------
2023-09-11 14:44:40,396 - 2000 samples (256 per mini-batch)
2023-09-11 14:44:42,933 - Epoch: [171][    8/    8]    Loss 0.171834    Top1 93.150000    
2023-09-11 14:44:43,051 - ==> Top1: 93.150    Loss: 0.172

2023-09-11 14:44:43,051 - ==> Confusion:
[[913  72]
 [ 65 950]]

2023-09-11 14:44:43,064 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:44:43,064 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:44:43,066 - 

2023-09-11 14:44:43,066 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:44:46,188 - Epoch: [172][   10/   71]    Overall Loss 0.132363    Objective Loss 0.132363                                        LR 0.000250    Time 0.312066    
2023-09-11 14:44:48,399 - Epoch: [172][   20/   71]    Overall Loss 0.135917    Objective Loss 0.135917                                        LR 0.000250    Time 0.266600    
2023-09-11 14:44:51,019 - Epoch: [172][   30/   71]    Overall Loss 0.135509    Objective Loss 0.135509                                        LR 0.000250    Time 0.265050    
2023-09-11 14:44:53,348 - Epoch: [172][   40/   71]    Overall Loss 0.135018    Objective Loss 0.135018                                        LR 0.000250    Time 0.257009    
2023-09-11 14:44:55,695 - Epoch: [172][   50/   71]    Overall Loss 0.132839    Objective Loss 0.132839                                        LR 0.000250    Time 0.252533    
2023-09-11 14:44:58,583 - Epoch: [172][   60/   71]    Overall Loss 0.132216    Objective Loss 0.132216                                        LR 0.000250    Time 0.258569    
2023-09-11 14:45:00,472 - Epoch: [172][   70/   71]    Overall Loss 0.133331    Objective Loss 0.133331    Top1 92.187500    LR 0.000250    Time 0.248621    
2023-09-11 14:45:00,536 - Epoch: [172][   71/   71]    Overall Loss 0.132964    Objective Loss 0.132964    Top1 93.154762    LR 0.000250    Time 0.246009    
2023-09-11 14:45:00,648 - --- validate (epoch=172)-----------
2023-09-11 14:45:00,648 - 2000 samples (256 per mini-batch)
2023-09-11 14:45:03,515 - Epoch: [172][    8/    8]    Loss 0.172758    Top1 92.700000    
2023-09-11 14:45:03,612 - ==> Top1: 92.700    Loss: 0.173

2023-09-11 14:45:03,612 - ==> Confusion:
[[937  48]
 [ 98 917]]

2023-09-11 14:45:03,628 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:45:03,628 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:45:03,633 - 

2023-09-11 14:45:03,633 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:45:06,772 - Epoch: [173][   10/   71]    Overall Loss 0.127918    Objective Loss 0.127918                                        LR 0.000250    Time 0.313812    
2023-09-11 14:45:09,034 - Epoch: [173][   20/   71]    Overall Loss 0.130674    Objective Loss 0.130674                                        LR 0.000250    Time 0.269995    
2023-09-11 14:45:11,497 - Epoch: [173][   30/   71]    Overall Loss 0.133067    Objective Loss 0.133067                                        LR 0.000250    Time 0.262079    
2023-09-11 14:45:13,657 - Epoch: [173][   40/   71]    Overall Loss 0.136387    Objective Loss 0.136387                                        LR 0.000250    Time 0.250566    
2023-09-11 14:45:16,126 - Epoch: [173][   50/   71]    Overall Loss 0.134064    Objective Loss 0.134064                                        LR 0.000250    Time 0.249821    
2023-09-11 14:45:18,247 - Epoch: [173][   60/   71]    Overall Loss 0.133562    Objective Loss 0.133562                                        LR 0.000250    Time 0.243522    
2023-09-11 14:45:20,435 - Epoch: [173][   70/   71]    Overall Loss 0.133620    Objective Loss 0.133620    Top1 94.140625    LR 0.000250    Time 0.239992    
2023-09-11 14:45:20,510 - Epoch: [173][   71/   71]    Overall Loss 0.133404    Objective Loss 0.133404    Top1 94.345238    LR 0.000250    Time 0.237662    
2023-09-11 14:45:20,607 - --- validate (epoch=173)-----------
2023-09-11 14:45:20,607 - 2000 samples (256 per mini-batch)
2023-09-11 14:45:23,143 - Epoch: [173][    8/    8]    Loss 0.170964    Top1 92.950000    
2023-09-11 14:45:23,234 - ==> Top1: 92.950    Loss: 0.171

2023-09-11 14:45:23,234 - ==> Confusion:
[[892  93]
 [ 48 967]]

2023-09-11 14:45:23,249 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:45:23,249 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:45:23,251 - 

2023-09-11 14:45:23,252 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:45:27,498 - Epoch: [174][   10/   71]    Overall Loss 0.128723    Objective Loss 0.128723                                        LR 0.000250    Time 0.424620    
2023-09-11 14:45:29,522 - Epoch: [174][   20/   71]    Overall Loss 0.126560    Objective Loss 0.126560                                        LR 0.000250    Time 0.313457    
2023-09-11 14:45:32,188 - Epoch: [174][   30/   71]    Overall Loss 0.129779    Objective Loss 0.129779                                        LR 0.000250    Time 0.297843    
2023-09-11 14:45:34,612 - Epoch: [174][   40/   71]    Overall Loss 0.133742    Objective Loss 0.133742                                        LR 0.000250    Time 0.283980    
2023-09-11 14:45:37,138 - Epoch: [174][   50/   71]    Overall Loss 0.132168    Objective Loss 0.132168                                        LR 0.000250    Time 0.277691    
2023-09-11 14:45:39,145 - Epoch: [174][   60/   71]    Overall Loss 0.131761    Objective Loss 0.131761                                        LR 0.000250    Time 0.264858    
2023-09-11 14:45:41,444 - Epoch: [174][   70/   71]    Overall Loss 0.131278    Objective Loss 0.131278    Top1 93.750000    LR 0.000250    Time 0.259852    
2023-09-11 14:45:41,494 - Epoch: [174][   71/   71]    Overall Loss 0.130672    Objective Loss 0.130672    Top1 94.642857    LR 0.000250    Time 0.256903    
2023-09-11 14:45:41,590 - --- validate (epoch=174)-----------
2023-09-11 14:45:41,591 - 2000 samples (256 per mini-batch)
2023-09-11 14:45:44,038 - Epoch: [174][    8/    8]    Loss 0.170665    Top1 93.300000    
2023-09-11 14:45:44,127 - ==> Top1: 93.300    Loss: 0.171

2023-09-11 14:45:44,128 - ==> Confusion:
[[900  85]
 [ 49 966]]

2023-09-11 14:45:44,139 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:45:44,140 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:45:44,142 - 

2023-09-11 14:45:44,142 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:45:48,033 - Epoch: [175][   10/   71]    Overall Loss 0.143341    Objective Loss 0.143341                                        LR 0.000250    Time 0.389067    
2023-09-11 14:45:49,984 - Epoch: [175][   20/   71]    Overall Loss 0.148124    Objective Loss 0.148124                                        LR 0.000250    Time 0.292067    
2023-09-11 14:45:52,734 - Epoch: [175][   30/   71]    Overall Loss 0.141271    Objective Loss 0.141271                                        LR 0.000250    Time 0.286359    
2023-09-11 14:45:54,923 - Epoch: [175][   40/   71]    Overall Loss 0.140253    Objective Loss 0.140253                                        LR 0.000250    Time 0.269495    
2023-09-11 14:45:57,512 - Epoch: [175][   50/   71]    Overall Loss 0.139174    Objective Loss 0.139174                                        LR 0.000250    Time 0.267358    
2023-09-11 14:46:00,775 - Epoch: [175][   60/   71]    Overall Loss 0.137687    Objective Loss 0.137687                                        LR 0.000250    Time 0.277170    
2023-09-11 14:46:02,587 - Epoch: [175][   70/   71]    Overall Loss 0.135066    Objective Loss 0.135066    Top1 95.312500    LR 0.000250    Time 0.263468    
2023-09-11 14:46:02,665 - Epoch: [175][   71/   71]    Overall Loss 0.134892    Objective Loss 0.134892    Top1 95.238095    LR 0.000250    Time 0.260851    
2023-09-11 14:46:02,762 - --- validate (epoch=175)-----------
2023-09-11 14:46:02,762 - 2000 samples (256 per mini-batch)
2023-09-11 14:46:05,169 - Epoch: [175][    8/    8]    Loss 0.184913    Top1 92.150000    
2023-09-11 14:46:05,264 - ==> Top1: 92.150    Loss: 0.185

2023-09-11 14:46:05,265 - ==> Confusion:
[[929  56]
 [101 914]]

2023-09-11 14:46:05,267 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:46:05,267 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:46:05,269 - 

2023-09-11 14:46:05,269 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:46:08,522 - Epoch: [176][   10/   71]    Overall Loss 0.142931    Objective Loss 0.142931                                        LR 0.000250    Time 0.325181    
2023-09-11 14:46:11,589 - Epoch: [176][   20/   71]    Overall Loss 0.138541    Objective Loss 0.138541                                        LR 0.000250    Time 0.315956    
2023-09-11 14:46:14,179 - Epoch: [176][   30/   71]    Overall Loss 0.132532    Objective Loss 0.132532                                        LR 0.000250    Time 0.296945    
2023-09-11 14:46:16,220 - Epoch: [176][   40/   71]    Overall Loss 0.133170    Objective Loss 0.133170                                        LR 0.000250    Time 0.273735    
2023-09-11 14:46:18,871 - Epoch: [176][   50/   71]    Overall Loss 0.140070    Objective Loss 0.140070                                        LR 0.000250    Time 0.271986    
2023-09-11 14:46:20,942 - Epoch: [176][   60/   71]    Overall Loss 0.140011    Objective Loss 0.140011                                        LR 0.000250    Time 0.261166    
2023-09-11 14:46:23,207 - Epoch: [176][   70/   71]    Overall Loss 0.138617    Objective Loss 0.138617    Top1 92.968750    LR 0.000250    Time 0.256216    
2023-09-11 14:46:23,284 - Epoch: [176][   71/   71]    Overall Loss 0.138779    Objective Loss 0.138779    Top1 93.154762    LR 0.000250    Time 0.253687    
2023-09-11 14:46:23,372 - --- validate (epoch=176)-----------
2023-09-11 14:46:23,372 - 2000 samples (256 per mini-batch)
2023-09-11 14:46:25,665 - Epoch: [176][    8/    8]    Loss 0.179898    Top1 93.250000    
2023-09-11 14:46:25,761 - ==> Top1: 93.250    Loss: 0.180

2023-09-11 14:46:25,761 - ==> Confusion:
[[937  48]
 [ 87 928]]

2023-09-11 14:46:25,778 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:46:25,778 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:46:25,780 - 

2023-09-11 14:46:25,780 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:46:28,788 - Epoch: [177][   10/   71]    Overall Loss 0.127826    Objective Loss 0.127826                                        LR 0.000250    Time 0.300687    
2023-09-11 14:46:30,933 - Epoch: [177][   20/   71]    Overall Loss 0.134463    Objective Loss 0.134463                                        LR 0.000250    Time 0.257609    
2023-09-11 14:46:33,454 - Epoch: [177][   30/   71]    Overall Loss 0.134255    Objective Loss 0.134255                                        LR 0.000250    Time 0.255741    
2023-09-11 14:46:35,328 - Epoch: [177][   40/   71]    Overall Loss 0.133685    Objective Loss 0.133685                                        LR 0.000250    Time 0.238661    
2023-09-11 14:46:38,021 - Epoch: [177][   50/   71]    Overall Loss 0.132056    Objective Loss 0.132056                                        LR 0.000250    Time 0.244776    
2023-09-11 14:46:40,100 - Epoch: [177][   60/   71]    Overall Loss 0.135550    Objective Loss 0.135550                                        LR 0.000250    Time 0.238617    
2023-09-11 14:46:42,492 - Epoch: [177][   70/   71]    Overall Loss 0.131738    Objective Loss 0.131738    Top1 96.484375    LR 0.000250    Time 0.238700    
2023-09-11 14:46:42,608 - Epoch: [177][   71/   71]    Overall Loss 0.132341    Objective Loss 0.132341    Top1 95.238095    LR 0.000250    Time 0.236979    
2023-09-11 14:46:42,702 - --- validate (epoch=177)-----------
2023-09-11 14:46:42,703 - 2000 samples (256 per mini-batch)
2023-09-11 14:46:45,844 - Epoch: [177][    8/    8]    Loss 0.169291    Top1 92.950000    
2023-09-11 14:46:45,942 - ==> Top1: 92.950    Loss: 0.169

2023-09-11 14:46:45,943 - ==> Confusion:
[[904  81]
 [ 60 955]]

2023-09-11 14:46:45,959 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:46:45,959 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:46:45,964 - 

2023-09-11 14:46:45,964 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:46:50,214 - Epoch: [178][   10/   71]    Overall Loss 0.112617    Objective Loss 0.112617                                        LR 0.000250    Time 0.424936    
2023-09-11 14:46:52,422 - Epoch: [178][   20/   71]    Overall Loss 0.123554    Objective Loss 0.123554                                        LR 0.000250    Time 0.322865    
2023-09-11 14:46:54,938 - Epoch: [178][   30/   71]    Overall Loss 0.122626    Objective Loss 0.122626                                        LR 0.000250    Time 0.299089    
2023-09-11 14:46:57,180 - Epoch: [178][   40/   71]    Overall Loss 0.126571    Objective Loss 0.126571                                        LR 0.000250    Time 0.280359    
2023-09-11 14:46:59,884 - Epoch: [178][   50/   71]    Overall Loss 0.124641    Objective Loss 0.124641                                        LR 0.000250    Time 0.278363    
2023-09-11 14:47:02,016 - Epoch: [178][   60/   71]    Overall Loss 0.125534    Objective Loss 0.125534                                        LR 0.000250    Time 0.267498    
2023-09-11 14:47:04,302 - Epoch: [178][   70/   71]    Overall Loss 0.127335    Objective Loss 0.127335    Top1 94.140625    LR 0.000250    Time 0.261939    
2023-09-11 14:47:04,367 - Epoch: [178][   71/   71]    Overall Loss 0.127490    Objective Loss 0.127490    Top1 93.750000    LR 0.000250    Time 0.259163    
2023-09-11 14:47:04,465 - --- validate (epoch=178)-----------
2023-09-11 14:47:04,465 - 2000 samples (256 per mini-batch)
2023-09-11 14:47:07,577 - Epoch: [178][    8/    8]    Loss 0.182017    Top1 92.150000    
2023-09-11 14:47:07,676 - ==> Top1: 92.150    Loss: 0.182

2023-09-11 14:47:07,676 - ==> Confusion:
[[884 101]
 [ 56 959]]

2023-09-11 14:47:07,691 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:47:07,691 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:47:07,693 - 

2023-09-11 14:47:07,693 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:47:12,004 - Epoch: [179][   10/   71]    Overall Loss 0.141197    Objective Loss 0.141197                                        LR 0.000250    Time 0.431028    
2023-09-11 14:47:14,195 - Epoch: [179][   20/   71]    Overall Loss 0.136102    Objective Loss 0.136102                                        LR 0.000250    Time 0.325012    
2023-09-11 14:47:16,675 - Epoch: [179][   30/   71]    Overall Loss 0.139273    Objective Loss 0.139273                                        LR 0.000250    Time 0.299339    
2023-09-11 14:47:18,748 - Epoch: [179][   40/   71]    Overall Loss 0.132180    Objective Loss 0.132180                                        LR 0.000250    Time 0.276333    
2023-09-11 14:47:21,378 - Epoch: [179][   50/   71]    Overall Loss 0.130518    Objective Loss 0.130518                                        LR 0.000250    Time 0.273661    
2023-09-11 14:47:23,404 - Epoch: [179][   60/   71]    Overall Loss 0.131219    Objective Loss 0.131219                                        LR 0.000250    Time 0.261808    
2023-09-11 14:47:25,676 - Epoch: [179][   70/   71]    Overall Loss 0.129990    Objective Loss 0.129990    Top1 95.312500    LR 0.000250    Time 0.256862    
2023-09-11 14:47:25,755 - Epoch: [179][   71/   71]    Overall Loss 0.130563    Objective Loss 0.130563    Top1 94.642857    LR 0.000250    Time 0.254347    
2023-09-11 14:47:25,861 - --- validate (epoch=179)-----------
2023-09-11 14:47:25,861 - 2000 samples (256 per mini-batch)
2023-09-11 14:47:29,148 - Epoch: [179][    8/    8]    Loss 0.174360    Top1 93.500000    
2023-09-11 14:47:29,251 - ==> Top1: 93.500    Loss: 0.174

2023-09-11 14:47:29,251 - ==> Confusion:
[[929  56]
 [ 74 941]]

2023-09-11 14:47:29,256 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:47:29,256 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:47:29,260 - 

2023-09-11 14:47:29,261 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:47:32,336 - Epoch: [180][   10/   71]    Overall Loss 0.137520    Objective Loss 0.137520                                        LR 0.000250    Time 0.307458    
2023-09-11 14:47:35,656 - Epoch: [180][   20/   71]    Overall Loss 0.137803    Objective Loss 0.137803                                        LR 0.000250    Time 0.319717    
2023-09-11 14:47:38,262 - Epoch: [180][   30/   71]    Overall Loss 0.139084    Objective Loss 0.139084                                        LR 0.000250    Time 0.300019    
2023-09-11 14:47:40,875 - Epoch: [180][   40/   71]    Overall Loss 0.137650    Objective Loss 0.137650                                        LR 0.000250    Time 0.290317    
2023-09-11 14:47:43,006 - Epoch: [180][   50/   71]    Overall Loss 0.136043    Objective Loss 0.136043                                        LR 0.000250    Time 0.274865    
2023-09-11 14:47:46,529 - Epoch: [180][   60/   71]    Overall Loss 0.133523    Objective Loss 0.133523                                        LR 0.000250    Time 0.287772    
2023-09-11 14:47:48,365 - Epoch: [180][   70/   71]    Overall Loss 0.131087    Objective Loss 0.131087    Top1 95.703125    LR 0.000250    Time 0.272890    
2023-09-11 14:47:48,442 - Epoch: [180][   71/   71]    Overall Loss 0.131108    Objective Loss 0.131108    Top1 95.238095    LR 0.000250    Time 0.270119    
2023-09-11 14:47:48,537 - --- validate (epoch=180)-----------
2023-09-11 14:47:48,537 - 2000 samples (256 per mini-batch)
2023-09-11 14:47:50,852 - Epoch: [180][    8/    8]    Loss 0.185665    Top1 92.350000    
2023-09-11 14:47:50,945 - ==> Top1: 92.350    Loss: 0.186

2023-09-11 14:47:50,945 - ==> Confusion:
[[962  23]
 [130 885]]

2023-09-11 14:47:50,961 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:47:50,961 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:47:50,965 - 

2023-09-11 14:47:50,965 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:47:55,381 - Epoch: [181][   10/   71]    Overall Loss 0.127112    Objective Loss 0.127112                                        LR 0.000250    Time 0.441536    
2023-09-11 14:47:57,339 - Epoch: [181][   20/   71]    Overall Loss 0.127441    Objective Loss 0.127441                                        LR 0.000250    Time 0.318639    
2023-09-11 14:48:00,548 - Epoch: [181][   30/   71]    Overall Loss 0.128379    Objective Loss 0.128379                                        LR 0.000250    Time 0.319383    
2023-09-11 14:48:02,749 - Epoch: [181][   40/   71]    Overall Loss 0.131104    Objective Loss 0.131104                                        LR 0.000250    Time 0.294551    
2023-09-11 14:48:05,404 - Epoch: [181][   50/   71]    Overall Loss 0.128096    Objective Loss 0.128096                                        LR 0.000250    Time 0.288736    
2023-09-11 14:48:07,484 - Epoch: [181][   60/   71]    Overall Loss 0.125050    Objective Loss 0.125050                                        LR 0.000250    Time 0.275283    
2023-09-11 14:48:09,658 - Epoch: [181][   70/   71]    Overall Loss 0.126111    Objective Loss 0.126111    Top1 94.531250    LR 0.000250    Time 0.267005    
2023-09-11 14:48:09,785 - Epoch: [181][   71/   71]    Overall Loss 0.126659    Objective Loss 0.126659    Top1 94.047619    LR 0.000250    Time 0.265034    
2023-09-11 14:48:09,880 - --- validate (epoch=181)-----------
2023-09-11 14:48:09,880 - 2000 samples (256 per mini-batch)
2023-09-11 14:48:13,059 - Epoch: [181][    8/    8]    Loss 0.180538    Top1 92.800000    
2023-09-11 14:48:13,139 - ==> Top1: 92.800    Loss: 0.181

2023-09-11 14:48:13,140 - ==> Confusion:
[[878 107]
 [ 37 978]]

2023-09-11 14:48:13,150 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:48:13,150 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:48:13,153 - 

2023-09-11 14:48:13,154 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:48:17,478 - Epoch: [182][   10/   71]    Overall Loss 0.131095    Objective Loss 0.131095                                        LR 0.000250    Time 0.432397    
2023-09-11 14:48:19,696 - Epoch: [182][   20/   71]    Overall Loss 0.133607    Objective Loss 0.133607                                        LR 0.000250    Time 0.327067    
2023-09-11 14:48:22,480 - Epoch: [182][   30/   71]    Overall Loss 0.127322    Objective Loss 0.127322                                        LR 0.000250    Time 0.310830    
2023-09-11 14:48:24,617 - Epoch: [182][   40/   71]    Overall Loss 0.126136    Objective Loss 0.126136                                        LR 0.000250    Time 0.286538    
2023-09-11 14:48:28,254 - Epoch: [182][   50/   71]    Overall Loss 0.126390    Objective Loss 0.126390                                        LR 0.000250    Time 0.301978    
2023-09-11 14:48:30,405 - Epoch: [182][   60/   71]    Overall Loss 0.126358    Objective Loss 0.126358                                        LR 0.000250    Time 0.287492    
2023-09-11 14:48:33,568 - Epoch: [182][   70/   71]    Overall Loss 0.125898    Objective Loss 0.125898    Top1 94.140625    LR 0.000250    Time 0.291593    
2023-09-11 14:48:33,674 - Epoch: [182][   71/   71]    Overall Loss 0.125612    Objective Loss 0.125612    Top1 94.345238    LR 0.000250    Time 0.288980    
2023-09-11 14:48:33,757 - --- validate (epoch=182)-----------
2023-09-11 14:48:33,757 - 2000 samples (256 per mini-batch)
2023-09-11 14:48:36,151 - Epoch: [182][    8/    8]    Loss 0.189474    Top1 92.300000    
2023-09-11 14:48:36,247 - ==> Top1: 92.300    Loss: 0.189

2023-09-11 14:48:36,247 - ==> Confusion:
[[919  66]
 [ 88 927]]

2023-09-11 14:48:36,263 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:48:36,263 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:48:36,268 - 

2023-09-11 14:48:36,268 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:48:41,284 - Epoch: [183][   10/   71]    Overall Loss 0.124312    Objective Loss 0.124312                                        LR 0.000250    Time 0.501569    
2023-09-11 14:48:44,425 - Epoch: [183][   20/   71]    Overall Loss 0.123127    Objective Loss 0.123127                                        LR 0.000250    Time 0.407821    
2023-09-11 14:48:46,442 - Epoch: [183][   30/   71]    Overall Loss 0.126233    Objective Loss 0.126233                                        LR 0.000250    Time 0.339107    
2023-09-11 14:48:49,075 - Epoch: [183][   40/   71]    Overall Loss 0.128457    Objective Loss 0.128457                                        LR 0.000250    Time 0.320142    
2023-09-11 14:48:51,205 - Epoch: [183][   50/   71]    Overall Loss 0.130799    Objective Loss 0.130799                                        LR 0.000250    Time 0.298710    
2023-09-11 14:48:53,901 - Epoch: [183][   60/   71]    Overall Loss 0.130739    Objective Loss 0.130739                                        LR 0.000250    Time 0.293853    
2023-09-11 14:48:56,176 - Epoch: [183][   70/   71]    Overall Loss 0.132916    Objective Loss 0.132916    Top1 93.359375    LR 0.000250    Time 0.284366    
2023-09-11 14:48:56,270 - Epoch: [183][   71/   71]    Overall Loss 0.134186    Objective Loss 0.134186    Top1 92.559524    LR 0.000250    Time 0.281688    
2023-09-11 14:48:56,358 - --- validate (epoch=183)-----------
2023-09-11 14:48:56,359 - 2000 samples (256 per mini-batch)
2023-09-11 14:48:59,690 - Epoch: [183][    8/    8]    Loss 0.170402    Top1 92.750000    
2023-09-11 14:48:59,782 - ==> Top1: 92.750    Loss: 0.170

2023-09-11 14:48:59,782 - ==> Confusion:
[[881 104]
 [ 41 974]]

2023-09-11 14:48:59,791 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:48:59,791 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:48:59,794 - 

2023-09-11 14:48:59,794 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:49:03,810 - Epoch: [184][   10/   71]    Overall Loss 0.143028    Objective Loss 0.143028                                        LR 0.000250    Time 0.401600    
2023-09-11 14:49:06,404 - Epoch: [184][   20/   71]    Overall Loss 0.131458    Objective Loss 0.131458                                        LR 0.000250    Time 0.330479    
2023-09-11 14:49:08,600 - Epoch: [184][   30/   71]    Overall Loss 0.129792    Objective Loss 0.129792                                        LR 0.000250    Time 0.293509    
2023-09-11 14:49:12,310 - Epoch: [184][   40/   71]    Overall Loss 0.129037    Objective Loss 0.129037                                        LR 0.000250    Time 0.312867    
2023-09-11 14:49:14,612 - Epoch: [184][   50/   71]    Overall Loss 0.129801    Objective Loss 0.129801                                        LR 0.000250    Time 0.296331    
2023-09-11 14:49:17,197 - Epoch: [184][   60/   71]    Overall Loss 0.130348    Objective Loss 0.130348                                        LR 0.000250    Time 0.290012    
2023-09-11 14:49:19,132 - Epoch: [184][   70/   71]    Overall Loss 0.127710    Objective Loss 0.127710    Top1 96.484375    LR 0.000250    Time 0.276221    
2023-09-11 14:49:19,215 - Epoch: [184][   71/   71]    Overall Loss 0.127220    Objective Loss 0.127220    Top1 96.428571    LR 0.000250    Time 0.273504    
2023-09-11 14:49:19,310 - --- validate (epoch=184)-----------
2023-09-11 14:49:19,310 - 2000 samples (256 per mini-batch)
2023-09-11 14:49:21,724 - Epoch: [184][    8/    8]    Loss 0.181878    Top1 92.850000    
2023-09-11 14:49:21,832 - ==> Top1: 92.850    Loss: 0.182

2023-09-11 14:49:21,833 - ==> Confusion:
[[923  62]
 [ 81 934]]

2023-09-11 14:49:21,849 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:49:21,849 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:49:21,854 - 

2023-09-11 14:49:21,854 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:49:27,056 - Epoch: [185][   10/   71]    Overall Loss 0.125143    Objective Loss 0.125143                                        LR 0.000250    Time 0.520174    
2023-09-11 14:49:30,496 - Epoch: [185][   20/   71]    Overall Loss 0.125934    Objective Loss 0.125934                                        LR 0.000250    Time 0.432060    
2023-09-11 14:49:33,068 - Epoch: [185][   30/   71]    Overall Loss 0.126115    Objective Loss 0.126115                                        LR 0.000250    Time 0.373753    
2023-09-11 14:49:35,122 - Epoch: [185][   40/   71]    Overall Loss 0.129154    Objective Loss 0.129154                                        LR 0.000250    Time 0.331661    
2023-09-11 14:49:37,882 - Epoch: [185][   50/   71]    Overall Loss 0.128861    Objective Loss 0.128861                                        LR 0.000250    Time 0.320507    
2023-09-11 14:49:40,428 - Epoch: [185][   60/   71]    Overall Loss 0.130212    Objective Loss 0.130212                                        LR 0.000250    Time 0.309522    
2023-09-11 14:49:42,322 - Epoch: [185][   70/   71]    Overall Loss 0.129389    Objective Loss 0.129389    Top1 95.312500    LR 0.000250    Time 0.292357    
2023-09-11 14:49:42,428 - Epoch: [185][   71/   71]    Overall Loss 0.128962    Objective Loss 0.128962    Top1 95.833333    LR 0.000250    Time 0.289727    
2023-09-11 14:49:42,505 - --- validate (epoch=185)-----------
2023-09-11 14:49:42,505 - 2000 samples (256 per mini-batch)
2023-09-11 14:49:45,178 - Epoch: [185][    8/    8]    Loss 0.171780    Top1 92.600000    
2023-09-11 14:49:45,269 - ==> Top1: 92.600    Loss: 0.172

2023-09-11 14:49:45,269 - ==> Confusion:
[[900  85]
 [ 63 952]]

2023-09-11 14:49:45,284 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:49:45,284 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:49:45,288 - 

2023-09-11 14:49:45,289 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:49:50,512 - Epoch: [186][   10/   71]    Overall Loss 0.136277    Objective Loss 0.136277                                        LR 0.000250    Time 0.522292    
2023-09-11 14:49:53,162 - Epoch: [186][   20/   71]    Overall Loss 0.136336    Objective Loss 0.136336                                        LR 0.000250    Time 0.393613    
2023-09-11 14:49:55,653 - Epoch: [186][   30/   71]    Overall Loss 0.132441    Objective Loss 0.132441                                        LR 0.000250    Time 0.345428    
2023-09-11 14:49:57,665 - Epoch: [186][   40/   71]    Overall Loss 0.133013    Objective Loss 0.133013                                        LR 0.000250    Time 0.309374    
2023-09-11 14:50:00,344 - Epoch: [186][   50/   71]    Overall Loss 0.133572    Objective Loss 0.133572                                        LR 0.000250    Time 0.301065    
2023-09-11 14:50:02,394 - Epoch: [186][   60/   71]    Overall Loss 0.131363    Objective Loss 0.131363                                        LR 0.000250    Time 0.285063    
2023-09-11 14:50:04,774 - Epoch: [186][   70/   71]    Overall Loss 0.131570    Objective Loss 0.131570    Top1 89.843750    LR 0.000250    Time 0.278323    
2023-09-11 14:50:04,859 - Epoch: [186][   71/   71]    Overall Loss 0.131634    Objective Loss 0.131634    Top1 90.773810    LR 0.000250    Time 0.275597    
2023-09-11 14:50:04,935 - --- validate (epoch=186)-----------
2023-09-11 14:50:04,935 - 2000 samples (256 per mini-batch)
2023-09-11 14:50:07,444 - Epoch: [186][    8/    8]    Loss 0.168164    Top1 92.850000    
2023-09-11 14:50:07,553 - ==> Top1: 92.850    Loss: 0.168

2023-09-11 14:50:07,553 - ==> Confusion:
[[897  88]
 [ 55 960]]

2023-09-11 14:50:07,569 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:50:07,569 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:50:07,573 - 

2023-09-11 14:50:07,573 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:50:12,098 - Epoch: [187][   10/   71]    Overall Loss 0.132983    Objective Loss 0.132983                                        LR 0.000250    Time 0.452409    
2023-09-11 14:50:14,476 - Epoch: [187][   20/   71]    Overall Loss 0.131615    Objective Loss 0.131615                                        LR 0.000250    Time 0.345077    
2023-09-11 14:50:16,996 - Epoch: [187][   30/   71]    Overall Loss 0.128881    Objective Loss 0.128881                                        LR 0.000250    Time 0.314040    
2023-09-11 14:50:19,012 - Epoch: [187][   40/   71]    Overall Loss 0.129055    Objective Loss 0.129055                                        LR 0.000250    Time 0.285913    
2023-09-11 14:50:21,796 - Epoch: [187][   50/   71]    Overall Loss 0.131309    Objective Loss 0.131309                                        LR 0.000250    Time 0.284419    
2023-09-11 14:50:23,881 - Epoch: [187][   60/   71]    Overall Loss 0.130936    Objective Loss 0.130936                                        LR 0.000250    Time 0.271760    
2023-09-11 14:50:26,171 - Epoch: [187][   70/   71]    Overall Loss 0.130248    Objective Loss 0.130248    Top1 96.093750    LR 0.000250    Time 0.265647    
2023-09-11 14:50:26,274 - Epoch: [187][   71/   71]    Overall Loss 0.129915    Objective Loss 0.129915    Top1 95.535714    LR 0.000250    Time 0.263348    
2023-09-11 14:50:26,375 - --- validate (epoch=187)-----------
2023-09-11 14:50:26,375 - 2000 samples (256 per mini-batch)
2023-09-11 14:50:29,068 - Epoch: [187][    8/    8]    Loss 0.170634    Top1 93.450000    
2023-09-11 14:50:29,171 - ==> Top1: 93.450    Loss: 0.171

2023-09-11 14:50:29,171 - ==> Confusion:
[[928  57]
 [ 74 941]]

2023-09-11 14:50:29,187 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:50:29,187 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:50:29,189 - 

2023-09-11 14:50:29,190 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:50:33,681 - Epoch: [188][   10/   71]    Overall Loss 0.113077    Objective Loss 0.113077                                        LR 0.000250    Time 0.449051    
2023-09-11 14:50:35,690 - Epoch: [188][   20/   71]    Overall Loss 0.119249    Objective Loss 0.119249                                        LR 0.000250    Time 0.324976    
2023-09-11 14:50:38,123 - Epoch: [188][   30/   71]    Overall Loss 0.129002    Objective Loss 0.129002                                        LR 0.000250    Time 0.297744    
2023-09-11 14:50:40,940 - Epoch: [188][   40/   71]    Overall Loss 0.130348    Objective Loss 0.130348                                        LR 0.000250    Time 0.293724    
2023-09-11 14:50:43,114 - Epoch: [188][   50/   71]    Overall Loss 0.129375    Objective Loss 0.129375                                        LR 0.000250    Time 0.278445    
2023-09-11 14:50:45,705 - Epoch: [188][   60/   71]    Overall Loss 0.128252    Objective Loss 0.128252                                        LR 0.000250    Time 0.275215    
2023-09-11 14:50:47,621 - Epoch: [188][   70/   71]    Overall Loss 0.128180    Objective Loss 0.128180    Top1 94.531250    LR 0.000250    Time 0.263274    
2023-09-11 14:50:47,734 - Epoch: [188][   71/   71]    Overall Loss 0.127477    Objective Loss 0.127477    Top1 94.940476    LR 0.000250    Time 0.261146    
2023-09-11 14:50:47,832 - --- validate (epoch=188)-----------
2023-09-11 14:50:47,832 - 2000 samples (256 per mini-batch)
2023-09-11 14:50:50,868 - Epoch: [188][    8/    8]    Loss 0.169592    Top1 93.400000    
2023-09-11 14:50:50,965 - ==> Top1: 93.400    Loss: 0.170

2023-09-11 14:50:50,965 - ==> Confusion:
[[927  58]
 [ 74 941]]

2023-09-11 14:50:50,967 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:50:50,967 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:50:50,970 - 

2023-09-11 14:50:50,970 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:50:54,295 - Epoch: [189][   10/   71]    Overall Loss 0.121154    Objective Loss 0.121154                                        LR 0.000250    Time 0.332412    
2023-09-11 14:50:57,374 - Epoch: [189][   20/   71]    Overall Loss 0.134965    Objective Loss 0.134965                                        LR 0.000250    Time 0.320142    
2023-09-11 14:50:59,933 - Epoch: [189][   30/   71]    Overall Loss 0.128277    Objective Loss 0.128277                                        LR 0.000250    Time 0.298725    
2023-09-11 14:51:01,975 - Epoch: [189][   40/   71]    Overall Loss 0.130441    Objective Loss 0.130441                                        LR 0.000250    Time 0.275076    
2023-09-11 14:51:04,632 - Epoch: [189][   50/   71]    Overall Loss 0.128900    Objective Loss 0.128900                                        LR 0.000250    Time 0.273207    
2023-09-11 14:51:06,693 - Epoch: [189][   60/   71]    Overall Loss 0.127390    Objective Loss 0.127390                                        LR 0.000250    Time 0.262012    
2023-09-11 14:51:08,971 - Epoch: [189][   70/   71]    Overall Loss 0.125540    Objective Loss 0.125540    Top1 95.703125    LR 0.000250    Time 0.257119    
2023-09-11 14:51:09,052 - Epoch: [189][   71/   71]    Overall Loss 0.126030    Objective Loss 0.126030    Top1 95.238095    LR 0.000250    Time 0.254633    
2023-09-11 14:51:09,141 - --- validate (epoch=189)-----------
2023-09-11 14:51:09,141 - 2000 samples (256 per mini-batch)
2023-09-11 14:51:12,092 - Epoch: [189][    8/    8]    Loss 0.170420    Top1 93.050000    
2023-09-11 14:51:12,194 - ==> Top1: 93.050    Loss: 0.170

2023-09-11 14:51:12,194 - ==> Confusion:
[[936  49]
 [ 90 925]]

2023-09-11 14:51:12,209 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:51:12,209 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:51:12,214 - 

2023-09-11 14:51:12,214 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:51:16,576 - Epoch: [190][   10/   71]    Overall Loss 0.131297    Objective Loss 0.131297                                        LR 0.000250    Time 0.436184    
2023-09-11 14:51:18,570 - Epoch: [190][   20/   71]    Overall Loss 0.132174    Objective Loss 0.132174                                        LR 0.000250    Time 0.317788    
2023-09-11 14:51:21,239 - Epoch: [190][   30/   71]    Overall Loss 0.131618    Objective Loss 0.131618                                        LR 0.000250    Time 0.300805    
2023-09-11 14:51:23,316 - Epoch: [190][   40/   71]    Overall Loss 0.133098    Objective Loss 0.133098                                        LR 0.000250    Time 0.277512    
2023-09-11 14:51:25,940 - Epoch: [190][   50/   71]    Overall Loss 0.132999    Objective Loss 0.132999                                        LR 0.000250    Time 0.274496    
2023-09-11 14:51:28,600 - Epoch: [190][   60/   71]    Overall Loss 0.131805    Objective Loss 0.131805                                        LR 0.000250    Time 0.273066    
2023-09-11 14:51:30,819 - Epoch: [190][   70/   71]    Overall Loss 0.130346    Objective Loss 0.130346    Top1 95.703125    LR 0.000250    Time 0.265753    
2023-09-11 14:51:30,897 - Epoch: [190][   71/   71]    Overall Loss 0.130403    Objective Loss 0.130403    Top1 95.833333    LR 0.000250    Time 0.263111    
2023-09-11 14:51:30,994 - --- validate (epoch=190)-----------
2023-09-11 14:51:30,994 - 2000 samples (256 per mini-batch)
2023-09-11 14:51:34,055 - Epoch: [190][    8/    8]    Loss 0.168069    Top1 92.700000    
2023-09-11 14:51:34,167 - ==> Top1: 92.700    Loss: 0.168

2023-09-11 14:51:34,167 - ==> Confusion:
[[914  71]
 [ 75 940]]

2023-09-11 14:51:34,184 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:51:34,184 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:51:34,186 - 

2023-09-11 14:51:34,186 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:51:38,640 - Epoch: [191][   10/   71]    Overall Loss 0.120242    Objective Loss 0.120242                                        LR 0.000250    Time 0.445329    
2023-09-11 14:51:41,832 - Epoch: [191][   20/   71]    Overall Loss 0.132689    Objective Loss 0.132689                                        LR 0.000250    Time 0.382260    
2023-09-11 14:51:43,724 - Epoch: [191][   30/   71]    Overall Loss 0.131703    Objective Loss 0.131703                                        LR 0.000250    Time 0.317870    
2023-09-11 14:51:46,368 - Epoch: [191][   40/   71]    Overall Loss 0.127508    Objective Loss 0.127508                                        LR 0.000250    Time 0.304506    
2023-09-11 14:51:48,496 - Epoch: [191][   50/   71]    Overall Loss 0.127173    Objective Loss 0.127173                                        LR 0.000250    Time 0.286167    
2023-09-11 14:51:51,831 - Epoch: [191][   60/   71]    Overall Loss 0.128063    Objective Loss 0.128063                                        LR 0.000250    Time 0.294043    
2023-09-11 14:51:54,143 - Epoch: [191][   70/   71]    Overall Loss 0.129054    Objective Loss 0.129054    Top1 92.187500    LR 0.000250    Time 0.285057    
2023-09-11 14:51:54,224 - Epoch: [191][   71/   71]    Overall Loss 0.130081    Objective Loss 0.130081    Top1 92.261905    LR 0.000250    Time 0.282188    
2023-09-11 14:51:54,316 - --- validate (epoch=191)-----------
2023-09-11 14:51:54,316 - 2000 samples (256 per mini-batch)
2023-09-11 14:51:56,727 - Epoch: [191][    8/    8]    Loss 0.166168    Top1 92.800000    
2023-09-11 14:51:56,815 - ==> Top1: 92.800    Loss: 0.166

2023-09-11 14:51:56,816 - ==> Confusion:
[[929  56]
 [ 88 927]]

2023-09-11 14:51:56,817 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:51:56,817 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:51:56,819 - 

2023-09-11 14:51:56,819 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:52:01,261 - Epoch: [192][   10/   71]    Overall Loss 0.140559    Objective Loss 0.140559                                        LR 0.000250    Time 0.444128    
2023-09-11 14:52:03,277 - Epoch: [192][   20/   71]    Overall Loss 0.135898    Objective Loss 0.135898                                        LR 0.000250    Time 0.322822    
2023-09-11 14:52:05,714 - Epoch: [192][   30/   71]    Overall Loss 0.124206    Objective Loss 0.124206                                        LR 0.000250    Time 0.296460    
2023-09-11 14:52:07,929 - Epoch: [192][   40/   71]    Overall Loss 0.124901    Objective Loss 0.124901                                        LR 0.000250    Time 0.277700    
2023-09-11 14:52:10,285 - Epoch: [192][   50/   71]    Overall Loss 0.125522    Objective Loss 0.125522                                        LR 0.000250    Time 0.269277    
2023-09-11 14:52:12,550 - Epoch: [192][   60/   71]    Overall Loss 0.126222    Objective Loss 0.126222                                        LR 0.000250    Time 0.262138    
2023-09-11 14:52:14,796 - Epoch: [192][   70/   71]    Overall Loss 0.124071    Objective Loss 0.124071    Top1 95.703125    LR 0.000250    Time 0.256767    
2023-09-11 14:52:14,904 - Epoch: [192][   71/   71]    Overall Loss 0.124169    Objective Loss 0.124169    Top1 95.238095    LR 0.000250    Time 0.254670    
2023-09-11 14:52:14,989 - --- validate (epoch=192)-----------
2023-09-11 14:52:14,990 - 2000 samples (256 per mini-batch)
2023-09-11 14:52:17,373 - Epoch: [192][    8/    8]    Loss 0.200578    Top1 92.150000    
2023-09-11 14:52:17,467 - ==> Top1: 92.150    Loss: 0.201

2023-09-11 14:52:17,467 - ==> Confusion:
[[938  47]
 [110 905]]

2023-09-11 14:52:17,479 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:52:17,479 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:52:17,483 - 

2023-09-11 14:52:17,484 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:52:20,789 - Epoch: [193][   10/   71]    Overall Loss 0.124455    Objective Loss 0.124455                                        LR 0.000250    Time 0.330448    
2023-09-11 14:52:22,861 - Epoch: [193][   20/   71]    Overall Loss 0.124016    Objective Loss 0.124016                                        LR 0.000250    Time 0.268818    
2023-09-11 14:52:26,099 - Epoch: [193][   30/   71]    Overall Loss 0.125611    Objective Loss 0.125611                                        LR 0.000250    Time 0.287150    
2023-09-11 14:52:28,028 - Epoch: [193][   40/   71]    Overall Loss 0.127387    Objective Loss 0.127387                                        LR 0.000250    Time 0.263575    
2023-09-11 14:52:30,736 - Epoch: [193][   50/   71]    Overall Loss 0.130728    Objective Loss 0.130728                                        LR 0.000250    Time 0.264998    
2023-09-11 14:52:32,894 - Epoch: [193][   60/   71]    Overall Loss 0.130074    Objective Loss 0.130074                                        LR 0.000250    Time 0.256805    
2023-09-11 14:52:35,148 - Epoch: [193][   70/   71]    Overall Loss 0.129025    Objective Loss 0.129025    Top1 94.531250    LR 0.000250    Time 0.252304    
2023-09-11 14:52:35,241 - Epoch: [193][   71/   71]    Overall Loss 0.128438    Objective Loss 0.128438    Top1 95.238095    LR 0.000250    Time 0.250056    
2023-09-11 14:52:35,331 - --- validate (epoch=193)-----------
2023-09-11 14:52:35,331 - 2000 samples (256 per mini-batch)
2023-09-11 14:52:38,282 - Epoch: [193][    8/    8]    Loss 0.201018    Top1 92.250000    
2023-09-11 14:52:38,387 - ==> Top1: 92.250    Loss: 0.201

2023-09-11 14:52:38,387 - ==> Confusion:
[[946  39]
 [116 899]]

2023-09-11 14:52:38,392 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:52:38,392 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:52:38,396 - 

2023-09-11 14:52:38,397 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:52:42,866 - Epoch: [194][   10/   71]    Overall Loss 0.119948    Objective Loss 0.119948                                        LR 0.000250    Time 0.446848    
2023-09-11 14:52:44,836 - Epoch: [194][   20/   71]    Overall Loss 0.120730    Objective Loss 0.120730                                        LR 0.000250    Time 0.321910    
2023-09-11 14:52:48,601 - Epoch: [194][   30/   71]    Overall Loss 0.122618    Objective Loss 0.122618                                        LR 0.000250    Time 0.340102    
2023-09-11 14:52:50,735 - Epoch: [194][   40/   71]    Overall Loss 0.124211    Objective Loss 0.124211                                        LR 0.000250    Time 0.308432    
2023-09-11 14:52:53,295 - Epoch: [194][   50/   71]    Overall Loss 0.123295    Objective Loss 0.123295                                        LR 0.000250    Time 0.297922    
2023-09-11 14:52:56,517 - Epoch: [194][   60/   71]    Overall Loss 0.121289    Objective Loss 0.121289                                        LR 0.000250    Time 0.301970    
2023-09-11 14:52:58,416 - Epoch: [194][   70/   71]    Overall Loss 0.122792    Objective Loss 0.122792    Top1 95.312500    LR 0.000250    Time 0.285961    
2023-09-11 14:52:58,503 - Epoch: [194][   71/   71]    Overall Loss 0.123008    Objective Loss 0.123008    Top1 94.642857    LR 0.000250    Time 0.283150    
2023-09-11 14:52:58,597 - --- validate (epoch=194)-----------
2023-09-11 14:52:58,598 - 2000 samples (256 per mini-batch)
2023-09-11 14:53:01,568 - Epoch: [194][    8/    8]    Loss 0.178160    Top1 92.500000    
2023-09-11 14:53:01,664 - ==> Top1: 92.500    Loss: 0.178

2023-09-11 14:53:01,665 - ==> Confusion:
[[931  54]
 [ 96 919]]

2023-09-11 14:53:01,680 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:53:01,680 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:53:01,682 - 

2023-09-11 14:53:01,683 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:53:05,618 - Epoch: [195][   10/   71]    Overall Loss 0.128515    Objective Loss 0.128515                                        LR 0.000250    Time 0.393507    
2023-09-11 14:53:08,576 - Epoch: [195][   20/   71]    Overall Loss 0.127631    Objective Loss 0.127631                                        LR 0.000250    Time 0.344627    
2023-09-11 14:53:10,609 - Epoch: [195][   30/   71]    Overall Loss 0.127666    Objective Loss 0.127666                                        LR 0.000250    Time 0.297494    
2023-09-11 14:53:13,688 - Epoch: [195][   40/   71]    Overall Loss 0.126972    Objective Loss 0.126972                                        LR 0.000250    Time 0.300093    
2023-09-11 14:53:16,519 - Epoch: [195][   50/   71]    Overall Loss 0.128343    Objective Loss 0.128343                                        LR 0.000250    Time 0.296697    
2023-09-11 14:53:19,085 - Epoch: [195][   60/   71]    Overall Loss 0.131771    Objective Loss 0.131771                                        LR 0.000250    Time 0.290001    
2023-09-11 14:53:21,012 - Epoch: [195][   70/   71]    Overall Loss 0.132276    Objective Loss 0.132276    Top1 93.750000    LR 0.000250    Time 0.276102    
2023-09-11 14:53:21,092 - Epoch: [195][   71/   71]    Overall Loss 0.132354    Objective Loss 0.132354    Top1 94.047619    LR 0.000250    Time 0.273331    
2023-09-11 14:53:21,178 - --- validate (epoch=195)-----------
2023-09-11 14:53:21,178 - 2000 samples (256 per mini-batch)
2023-09-11 14:53:24,095 - Epoch: [195][    8/    8]    Loss 0.177391    Top1 92.600000    
2023-09-11 14:53:24,178 - ==> Top1: 92.600    Loss: 0.177

2023-09-11 14:53:24,179 - ==> Confusion:
[[868 117]
 [ 31 984]]

2023-09-11 14:53:24,195 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:53:24,195 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:53:24,197 - 

2023-09-11 14:53:24,197 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:53:28,679 - Epoch: [196][   10/   71]    Overall Loss 0.149076    Objective Loss 0.149076                                        LR 0.000250    Time 0.448113    
2023-09-11 14:53:30,731 - Epoch: [196][   20/   71]    Overall Loss 0.139415    Objective Loss 0.139415                                        LR 0.000250    Time 0.326638    
2023-09-11 14:53:33,831 - Epoch: [196][   30/   71]    Overall Loss 0.137735    Objective Loss 0.137735                                        LR 0.000250    Time 0.321105    
2023-09-11 14:53:36,480 - Epoch: [196][   40/   71]    Overall Loss 0.137124    Objective Loss 0.137124                                        LR 0.000250    Time 0.307039    
2023-09-11 14:53:38,739 - Epoch: [196][   50/   71]    Overall Loss 0.134784    Objective Loss 0.134784                                        LR 0.000250    Time 0.290808    
2023-09-11 14:53:41,132 - Epoch: [196][   60/   71]    Overall Loss 0.131577    Objective Loss 0.131577                                        LR 0.000250    Time 0.282210    
2023-09-11 14:53:43,205 - Epoch: [196][   70/   71]    Overall Loss 0.130031    Objective Loss 0.130031    Top1 93.359375    LR 0.000250    Time 0.271505    
2023-09-11 14:53:43,279 - Epoch: [196][   71/   71]    Overall Loss 0.129977    Objective Loss 0.129977    Top1 93.452381    LR 0.000250    Time 0.268722    
2023-09-11 14:53:43,373 - --- validate (epoch=196)-----------
2023-09-11 14:53:43,373 - 2000 samples (256 per mini-batch)
2023-09-11 14:53:45,697 - Epoch: [196][    8/    8]    Loss 0.169219    Top1 93.050000    
2023-09-11 14:53:45,808 - ==> Top1: 93.050    Loss: 0.169

2023-09-11 14:53:45,809 - ==> Confusion:
[[928  57]
 [ 82 933]]

2023-09-11 14:53:45,823 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:53:45,824 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:53:45,828 - 

2023-09-11 14:53:45,828 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:53:50,058 - Epoch: [197][   10/   71]    Overall Loss 0.117869    Objective Loss 0.117869                                        LR 0.000250    Time 0.422893    
2023-09-11 14:53:52,130 - Epoch: [197][   20/   71]    Overall Loss 0.127838    Objective Loss 0.127838                                        LR 0.000250    Time 0.315029    
2023-09-11 14:53:54,634 - Epoch: [197][   30/   71]    Overall Loss 0.128143    Objective Loss 0.128143                                        LR 0.000250    Time 0.293472    
2023-09-11 14:53:56,753 - Epoch: [197][   40/   71]    Overall Loss 0.130649    Objective Loss 0.130649                                        LR 0.000250    Time 0.273079    
2023-09-11 14:53:59,374 - Epoch: [197][   50/   71]    Overall Loss 0.131871    Objective Loss 0.131871                                        LR 0.000250    Time 0.270875    
2023-09-11 14:54:02,828 - Epoch: [197][   60/   71]    Overall Loss 0.130391    Objective Loss 0.130391                                        LR 0.000250    Time 0.283287    
2023-09-11 14:54:04,829 - Epoch: [197][   70/   71]    Overall Loss 0.130029    Objective Loss 0.130029    Top1 96.093750    LR 0.000250    Time 0.271397    
2023-09-11 14:54:04,908 - Epoch: [197][   71/   71]    Overall Loss 0.130208    Objective Loss 0.130208    Top1 94.940476    LR 0.000250    Time 0.268695    
2023-09-11 14:54:05,006 - --- validate (epoch=197)-----------
2023-09-11 14:54:05,006 - 2000 samples (256 per mini-batch)
2023-09-11 14:54:08,150 - Epoch: [197][    8/    8]    Loss 0.167798    Top1 93.650000    
2023-09-11 14:54:08,249 - ==> Top1: 93.650    Loss: 0.168

2023-09-11 14:54:08,249 - ==> Confusion:
[[918  67]
 [ 60 955]]

2023-09-11 14:54:08,265 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:54:08,265 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:54:08,268 - 

2023-09-11 14:54:08,268 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:54:12,700 - Epoch: [198][   10/   71]    Overall Loss 0.119994    Objective Loss 0.119994                                        LR 0.000250    Time 0.443182    
2023-09-11 14:54:14,710 - Epoch: [198][   20/   71]    Overall Loss 0.127062    Objective Loss 0.127062                                        LR 0.000250    Time 0.322079    
2023-09-11 14:54:17,194 - Epoch: [198][   30/   71]    Overall Loss 0.129556    Objective Loss 0.129556                                        LR 0.000250    Time 0.297482    
2023-09-11 14:54:19,327 - Epoch: [198][   40/   71]    Overall Loss 0.129119    Objective Loss 0.129119                                        LR 0.000250    Time 0.276445    
2023-09-11 14:54:21,879 - Epoch: [198][   50/   71]    Overall Loss 0.129296    Objective Loss 0.129296                                        LR 0.000250    Time 0.272181    
2023-09-11 14:54:24,357 - Epoch: [198][   60/   71]    Overall Loss 0.127042    Objective Loss 0.127042                                        LR 0.000250    Time 0.268110    
2023-09-11 14:54:26,250 - Epoch: [198][   70/   71]    Overall Loss 0.128644    Objective Loss 0.128644    Top1 94.140625    LR 0.000250    Time 0.256853    
2023-09-11 14:54:26,331 - Epoch: [198][   71/   71]    Overall Loss 0.129553    Objective Loss 0.129553    Top1 93.750000    LR 0.000250    Time 0.254369    
2023-09-11 14:54:26,428 - --- validate (epoch=198)-----------
2023-09-11 14:54:26,428 - 2000 samples (256 per mini-batch)
2023-09-11 14:54:29,604 - Epoch: [198][    8/    8]    Loss 0.176949    Top1 92.600000    
2023-09-11 14:54:29,695 - ==> Top1: 92.600    Loss: 0.177

2023-09-11 14:54:29,695 - ==> Confusion:
[[881 104]
 [ 44 971]]

2023-09-11 14:54:29,711 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:54:29,711 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:54:29,713 - 

2023-09-11 14:54:29,713 - Training epoch: 18000 samples (256 per mini-batch)
2023-09-11 14:54:33,871 - Epoch: [199][   10/   71]    Overall Loss 0.130919    Objective Loss 0.130919                                        LR 0.000250    Time 0.415678    
2023-09-11 14:54:35,773 - Epoch: [199][   20/   71]    Overall Loss 0.125064    Objective Loss 0.125064                                        LR 0.000250    Time 0.302931    
2023-09-11 14:54:38,336 - Epoch: [199][   30/   71]    Overall Loss 0.119365    Objective Loss 0.119365                                        LR 0.000250    Time 0.287394    
2023-09-11 14:54:40,303 - Epoch: [199][   40/   71]    Overall Loss 0.124143    Objective Loss 0.124143                                        LR 0.000250    Time 0.264711    
2023-09-11 14:54:42,939 - Epoch: [199][   50/   71]    Overall Loss 0.125196    Objective Loss 0.125196                                        LR 0.000250    Time 0.264475    
2023-09-11 14:54:45,004 - Epoch: [199][   60/   71]    Overall Loss 0.126671    Objective Loss 0.126671                                        LR 0.000250    Time 0.254804    
2023-09-11 14:54:47,959 - Epoch: [199][   70/   71]    Overall Loss 0.126687    Objective Loss 0.126687    Top1 92.578125    LR 0.000250    Time 0.260622    
2023-09-11 14:54:48,040 - Epoch: [199][   71/   71]    Overall Loss 0.126936    Objective Loss 0.126936    Top1 92.559524    LR 0.000250    Time 0.258083    
2023-09-11 14:54:48,139 - --- validate (epoch=199)-----------
2023-09-11 14:54:48,139 - 2000 samples (256 per mini-batch)
2023-09-11 14:54:50,795 - Epoch: [199][    8/    8]    Loss 0.185939    Top1 91.800000    
2023-09-11 14:54:50,884 - ==> Top1: 91.800    Loss: 0.186

2023-09-11 14:54:50,885 - ==> Confusion:
[[924  61]
 [103 912]]

2023-09-11 14:54:50,893 - ==> Best [Top1: 93.700   Sparsity:0.00   Params: 57776 on epoch: 157]
2023-09-11 14:54:50,893 - Saving checkpoint to: logs/2023.09.11-134109/qat_checkpoint.pth.tar
2023-09-11 14:54:50,897 - --- test ---------------------
2023-09-11 14:54:50,897 - 5000 samples (256 per mini-batch)
2023-09-11 14:54:52,546 - Test: [   10/   20]    Loss 0.138967    Top1 94.218750    
2023-09-11 14:54:53,483 - Test: [   20/   20]    Loss 0.131839    Top1 94.620000    
2023-09-11 14:54:53,577 - ==> Top1: 94.620    Loss: 0.132

2023-09-11 14:54:53,577 - ==> Confusion:
[[2390  110]
 [ 159 2341]]

2023-09-11 14:54:53,579 - 
2023-09-11 14:54:53,579 - Log file for this run: /home/ermanokman/repos/ai8x-training/logs/2023.09.11-134109/2023.09.11-134109.log
